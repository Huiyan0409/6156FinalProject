{"signature": "def p_file_type_2(self, p):", "body": "self.error = True<EOL>msg = ERROR_MESSAGES['<STR_LIT>'].format(p.lineno(<NUM_LIT:1>))<EOL>self.logger.log(msg)<EOL>", "docstring": "file_type : FILE_TYPE error", "id": "f3753:c0:m57"}
{"signature": "def traverse(self, start_node = None, level = None):", "body": "if start_node is None:<EOL><INDENT>start_node  = self.__root<EOL>level = <NUM_LIT:0><EOL><DEDENT>if start_node is None:<EOL><INDENT>return []<EOL><DEDENT>items = [ (level, start_node) ]<EOL>for child in self.children(start_node):<EOL><INDENT>if child is not None:<EOL><INDENT>items += self.traverse(child, level + <NUM_LIT:1>)<EOL><DEDENT><DEDENT>return items<EOL>", "docstring": "!\n        @brief Traverses all nodes of subtree that is defined by node specified in input parameter.\n\n        @param[in] start_node (node): Node from that travering of subtree is performed.\n        @param[in, out] level (uint): Should be ignored by application.\n\n        @return (list) All nodes of the subtree.", "id": "f15715:c2:m14"}
{"signature": "def _make_response(self, nonce, salt, iteration_count):", "body": "self._salted_password = self.Hi(self.Normalize(self.password), salt,<EOL>iteration_count)<EOL>self.password = None <EOL>if self.channel_binding:<EOL><INDENT>channel_binding = b\"<STR_LIT>\" + standard_b64encode(self._gs2_header +<EOL>self._cb_data)<EOL><DEDENT>else:<EOL><INDENT>channel_binding = b\"<STR_LIT>\" + standard_b64encode(self._gs2_header)<EOL><DEDENT>client_final_message_without_proof = (channel_binding + b\"<STR_LIT>\" + nonce)<EOL>client_key = self.HMAC(self._salted_password, b\"<STR_LIT>\")<EOL>stored_key = self.H(client_key)<EOL>auth_message = ( self._client_first_message_bare + b\"<STR_LIT:U+002C>\" +<EOL>self._server_first_message + b\"<STR_LIT:U+002C>\" +<EOL>client_final_message_without_proof )<EOL>self._auth_message = auth_message<EOL>client_signature = self.HMAC(stored_key, auth_message)<EOL>client_proof = self.XOR(client_key, client_signature)<EOL>proof = b\"<STR_LIT>\" + standard_b64encode(client_proof)<EOL>client_final_message = (client_final_message_without_proof + b\"<STR_LIT:U+002C>\" +<EOL>proof)<EOL>return Response(client_final_message)<EOL>", "docstring": "Make a response for the first challenge from the server.\n\n        :return: the response or a failure indicator.\n        :returntype: `sasl.Response` or `sasl.Failure`", "id": "f15238:c1:m4"}
{"signature": "def __improve_parameters(self, centers, available_indexes = None):", "body": "if available_indexes and len(available_indexes) == <NUM_LIT:1>:<EOL><INDENT>index_center = available_indexes[<NUM_LIT:0>]<EOL>return [ available_indexes ], self.__pointer_data[index_center]<EOL><DEDENT>local_data = self.__pointer_data<EOL>if available_indexes:<EOL><INDENT>local_data = [ self.__pointer_data[i] for i in available_indexes ]<EOL><DEDENT>local_centers = centers<EOL>if centers is None:<EOL><INDENT>local_centers = kmeans_plusplus_initializer(local_data, <NUM_LIT:2>, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()<EOL><DEDENT>kmeans_instance = kmeans(local_data, local_centers, tolerance=self.__tolerance, ccore=False)<EOL>kmeans_instance.process()<EOL>local_centers = kmeans_instance.get_centers()<EOL>clusters = kmeans_instance.get_clusters()<EOL>if available_indexes:<EOL><INDENT>clusters = self.__local_to_global_clusters(clusters, available_indexes)<EOL><DEDENT>return clusters, local_centers<EOL>", "docstring": "!\n        @brief Performs k-means clustering in the specified region.\n\n        @param[in] centers (list): Centers of clusters.\n        @param[in] available_indexes (list): Indexes that defines which points can be used for k-means clustering, if None - then all points are used.\n\n        @return (list) List of allocated clusters, each cluster contains indexes of objects in list of data.", "id": "f15593:c1:m5"}
{"signature": "def end(self):", "body": "results = self.communicationChannel.receive()<EOL>if self.nruns != len(results):<EOL><INDENT>import logging<EOL>logger = logging.getLogger(__name__)<EOL>logger.warning(<EOL>'<STR_LIT>'.format(<EOL>len(results),<EOL>self.nruns<EOL>))<EOL><DEDENT>return results<EOL>", "docstring": "wait until all event loops end and returns the results.", "id": "f9110:c0:m8"}
{"signature": "def get_temperature(self, format='<STR_LIT>', sensor=<NUM_LIT:0>):", "body": "results = self.get_temperatures(sensors=[sensor,])<EOL>if format == '<STR_LIT>':<EOL><INDENT>return results[sensor]['<STR_LIT>']<EOL><DEDENT>elif format == '<STR_LIT>':<EOL><INDENT>return results[sensor]['<STR_LIT>']<EOL><DEDENT>elif format == '<STR_LIT>':<EOL><INDENT>return results[sensor]['<STR_LIT>']<EOL><DEDENT>else:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>", "docstring": "Get device temperature reading.", "id": "f11313:c0:m10"}
{"signature": "def printInput(self, x):", "body": "print(\"<STR_LIT>\")<EOL>for c in range(self.numberOfCols):<EOL><INDENT>print(int(x[c]), end='<STR_LIT:U+0020>')<EOL><DEDENT>print()<EOL>", "docstring": "TODO: document\n\n:param x: \n:return:", "id": "f17565:c0:m27"}
{"signature": "def is_not_equal_to(self, other):", "body": "if self.val == other:<EOL><INDENT>self._err('<STR_LIT>' % (self.val, other))<EOL><DEDENT>return self<EOL>", "docstring": "Asserts that val is not equal to other.", "id": "f9307:c0:m3"}
{"signature": "def get_frame(self):", "body": "return self.legendPatch<EOL>", "docstring": "return the Rectangle instance used to frame the legend", "id": "f17172:c0:m12"}
{"signature": "def create_effect_instances(self):", "body": "raise NotImplementedError()<EOL>", "docstring": "Create instances of effects.\nMust be implemented or ``NotImplementedError`` is raised.", "id": "f14467:c0:m4"}
{"signature": "def plot_potential(<EOL>galaxy, grid, mask=None, extract_array_from_mask=False, zoom_around_mask=False, positions=None, as_subplot=False,<EOL>units='<STR_LIT>', kpc_per_arcsec=None, figsize=(<NUM_LIT:7>, <NUM_LIT:7>), aspect='<STR_LIT>',<EOL>cmap='<STR_LIT>', norm='<STR_LIT>', norm_min=None, norm_max=None, linthresh=<NUM_LIT>, linscale=<NUM_LIT>,<EOL>cb_ticksize=<NUM_LIT:10>, cb_fraction=<NUM_LIT>, cb_pad=<NUM_LIT>, cb_tick_values=None, cb_tick_labels=None,<EOL>title='<STR_LIT>', titlesize=<NUM_LIT:16>, xlabelsize=<NUM_LIT:16>, ylabelsize=<NUM_LIT:16>, xyticksize=<NUM_LIT:16>,<EOL>mask_pointsize=<NUM_LIT:10>, position_pointsize=<NUM_LIT>, grid_pointsize=<NUM_LIT:1>,<EOL>output_path=None, output_format='<STR_LIT>', output_filename='<STR_LIT>'):", "body": "potential = galaxy.potential_from_grid(grid=grid)<EOL>potential = grid.scaled_array_2d_from_array_1d(potential)<EOL>array_plotters.plot_array(<EOL>array=potential, mask=mask, extract_array_from_mask=extract_array_from_mask,<EOL>zoom_around_mask=zoom_around_mask, positions=positions, as_subplot=as_subplot,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>title=title, titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>mask_pointsize=mask_pointsize, position_pointsize=position_pointsize, grid_pointsize=grid_pointsize,<EOL>output_path=output_path, output_format=output_format, output_filename=output_filename)<EOL>", "docstring": "Plot the potential of a galaxy, on a regular grid of (y,x) coordinates.\n\n     Set *autolens.datas.array.plotters.array_plotters* for a description of all innput parameters not described below.\n\n     Parameters\n     -----------\n    galaxy : model.galaxy.galaxy.Galaxy\n         The galaxy whose potential is plotted.\n    grid : ndarray or datas.array.grid_stacks.RegularGrid\n         The (y,x) coordinates of the grid, in an array of shape (total_coordinates, 2)", "id": "f5960:m2"}
{"signature": "def create(self, validated_data):", "body": "pass<EOL>", "docstring": "Do not perform any operations for state changing requests.", "id": "f16072:c0:m0"}
{"signature": "def __init__(self, pos, shape=None, param_prefix='<STR_LIT>', category='<STR_LIT>',<EOL>support_pad=<NUM_LIT:4>, float_precision=np.float64):", "body": "if pos.ndim != <NUM_LIT:2>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>self.category = category<EOL>self.support_pad = support_pad<EOL>self.pos = pos.astype('<STR_LIT:float>')<EOL>self.param_prefix = param_prefix<EOL>if float_precision not in (np.float64, np.float32, np.float16):<EOL><INDENT>raise ValueError('<STR_LIT>' +<EOL>'<STR_LIT>')<EOL><DEDENT>self.float_precision = float_precision<EOL>self.shape = shape<EOL>self.setup_variables()<EOL>if self.shape:<EOL><INDENT>self.inner = self.shape.copy()<EOL>self.tile = self.inner.copy()<EOL>self.initialize()<EOL><DEDENT>", "docstring": "Parent class for a large collection of particles, such as spheres or\npoints or ellipsoids or rods.\n\nThis class is good for a collection of objects which each have a\nposition as well as (possibly) some other parameters, like particle\nradius, aspect ratio, or brightness. Its .get() method returns a\nfield of the drawn particles, selected on the current tile. Any\ndaughter classes need the following methods:\n\n* _draw_particle\n* _update_type\n* setup_variables\n* get_values\n* set_values\n* add_particle\n* remove_particle\n\nIn addition, the following methods should be modified for particles\nwith more parameters than just positions:\n\n* _drawargs\n* _tile\n* param_particle\n* exports\n* _p2i\n\nIf you have a few objects to group, like 2 or 3 slabs, group them\nwith a `peri.comp.ComponentCollection` instead.\n\nParameters\n----------\npos : ndarray [N,d]\n    Initial positions of the particles. Re-cast as float internally\n\nshape : ``peri.util.Tile``, optional\n    Shape of the field over which to draw the platonic spheres.\n    Default is None.\n\nparam_prefix : string, optional\n    Prefix for the particle parameter names. Default is `'sph'`\n\ncategory : string, optional\n    Category, as in comp.Component. Default is `'obj'`.\n\nsupport_pad : Int, optional\n    How much to pad the boundary of particles when calculating the\n    support so that particles do not leak out the edges. Default is 4\n\nfloat_precision : numpy float datatype, optional\n    One of numpy.float16, numpy.float32, numpy.float64; precision\n    for precomputed arrays. Default is np.float64; make it 16 or 32\n    to save memory.", "id": "f5759:c0:m0"}
{"signature": "def unixtime_seconds_to_gtfs_datetime(self, unixtime):", "body": "return datetime.datetime.fromtimestamp(unixtime, self._timezone)<EOL>", "docstring": "Convert unixtime to localized datetime\n\nParameters\n----------\nunixtime : int\n\nReturns\n-------\ngtfs_datetime: datetime.datetime\n    time localized to gtfs_datetime's timezone", "id": "f12922:c0:m17"}
{"signature": "def getwindowsize(self, window_name):", "body": "return self.getobjectsize(window_name)<EOL>", "docstring": "Get window size.\n\n@param window_name: Window name to get size of.\n@type window_name: string\n\n@return: list of dimensions [x, y, w, h]\n@rtype: list", "id": "f10327:c0:m26"}
{"signature": "def attr(*args, **kwargs):", "body": "ctx = dom_tag._with_contexts[_get_thread_context()]<EOL>if ctx and ctx[-<NUM_LIT:1>]:<EOL><INDENT>dicts = args + (kwargs,)<EOL>for d in dicts:<EOL><INDENT>for attr, value in d.items():<EOL><INDENT>ctx[-<NUM_LIT:1>].tag.set_attribute(*dom_tag.clean_pair(attr, value))<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>", "docstring": "Set attributes on the current active tag context", "id": "f4773:m1"}
{"signature": "def authenticate(self, request):", "body": "return None<EOL>", "docstring": "Gets the a user if they are authenticated; else None.\n\n        @retval False    Unable to authenticate.\n        @retval None     Able to authenticate but failed.\n        @retval <user>   User object representing the current user.", "id": "f10134:c0:m1"}
{"signature": "def register_validator(name, parse_function):", "body": "name = name.lower()<EOL>if name in VALIDATORS:<EOL><INDENT>raise Exception(\"<STR_LIT>\".format(name))<EOL><DEDENT>VALIDATORS[name] = parse_function<EOL>", "docstring": "Registers a validator for use by this library\n        Name is the string name for validator\n\n        Parse function does parse(config_node) and returns a Validator object\n        Validator functions have signature:\n            validate(response_body, context=None) - context is a bindings.Context object\n\n        Validators return true or false and optionally can return a Failure instead of false\n        This allows for passing more details", "id": "f232:m6"}
{"signature": "def miles_to_feet(miles):", "body": "return miles * float(<NUM_LIT>)<EOL>", "docstring": "Converts a number of miles to feet.\n\nArgs:\n    miles: Number of miles we want to convert.\n\nReturns:\n    Floating point number as the number of\n    feet in the given miles.", "id": "f1541:m0"}
{"signature": "def grant_access_token(self):", "body": "self._check_appid_appsecret()<EOL>if callable(self.__access_token_refreshfunc):<EOL><INDENT>self.__access_token, self.__access_token_expires_at = self.__access_token_refreshfunc()<EOL>return<EOL><DEDENT>response_json = self.__request.get(<EOL>url=\"<STR_LIT>\",<EOL>params={<EOL>\"<STR_LIT>\": \"<STR_LIT>\",<EOL>\"<STR_LIT>\": self.__appid,<EOL>\"<STR_LIT>\": self.__appsecret,<EOL>},<EOL>access_token=self.__access_token<EOL>)<EOL>self.__access_token = response_json['<STR_LIT>']<EOL>self.__access_token_expires_at = int(time.time()) + response_json['<STR_LIT>']<EOL>if callable(self.__access_token_setfunc):<EOL><INDENT>self.__access_token_setfunc(self.__access_token, self.__access_token_expires_at)<EOL><DEDENT>return response_json<EOL>", "docstring": "\u83b7\u53d6 access token \u5e76\u66f4\u65b0\u5f53\u524d\u914d\u7f6e\n:return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305 (\u4f20\u5165 access_token_refreshfunc \u53c2\u6570\u540e\u8fd4\u56de None)", "id": "f587:c0:m16"}
{"signature": "@contextlib.contextmanager<EOL>def prepend_syspath(path):", "body": "sys.path.insert(<NUM_LIT:0>, path)<EOL>yield<EOL>sys.path.pop(<NUM_LIT:0>)<EOL>", "docstring": "Contect manager (with statement) that prepends path to sys.path", "id": "f4510:m3"}
{"signature": "def pluralize_collection(base, local_cls, referred_cls, constraint):", "body": "\"<STR_LIT>\"<EOL>referred_name = referred_cls.__name__<EOL>uncamelized = re.sub(r'<STR_LIT>',<EOL>lambda m: \"<STR_LIT>\" % m.group(<NUM_LIT:0>).lower(),<EOL>referred_name)[<NUM_LIT:1>:]<EOL>pluralized = _pluralizer.plural(uncamelized)<EOL>return pluralized<EOL>", "docstring": "Produce an 'uncamelized', 'pluralized' class name, e.g.", "id": "f1564:m1"}
{"signature": "def __getattr__(self, a):", "body": "if a in self:<EOL><INDENT>return self[a]<EOL><DEDENT>raise AttributeError(\"<STR_LIT>\"+a+\"<STR_LIT:'>\")<EOL>", "docstring": "Keys in the dictionaries are accessible as attributes.", "id": "f11607:c0:m4"}
{"signature": "def _set_name(self, name):", "body": "if self.own.get('<STR_LIT:name>'):<EOL><INDENT>self.func_name = name<EOL>self.own['<STR_LIT:name>']['<STR_LIT:value>'] = Js(name)<EOL><DEDENT>", "docstring": "name is py type", "id": "f11719:c7:m1"}
{"signature": "def write_backreferences(seen_backrefs, gallery_conf,<EOL>target_dir, fname, snippet):", "body": "example_file = os.path.join(target_dir, fname)<EOL>backrefs = scan_used_functions(example_file, gallery_conf)<EOL>for backref in backrefs:<EOL><INDENT>include_path = os.path.join(gallery_conf['<STR_LIT>'],<EOL>'<STR_LIT>' % backref)<EOL>seen = backref in seen_backrefs<EOL>with open(include_path, '<STR_LIT:a>' if seen else '<STR_LIT:w>') as ex_file:<EOL><INDENT>if not seen:<EOL><INDENT>heading = '<STR_LIT>' % backref<EOL>ex_file.write(heading + '<STR_LIT:\\n>')<EOL>ex_file.write('<STR_LIT>' * len(heading) + '<STR_LIT:\\n>')<EOL><DEDENT>ex_file.write(_thumbnail_div(target_dir, fname, snippet,<EOL>is_backref=True))<EOL>seen_backrefs.add(backref)<EOL><DEDENT><DEDENT>", "docstring": "Writes down back reference files, which include a thumbnail list\n    of examples using a certain module", "id": "f10905:m4"}
{"signature": "def get_refkey(self, obj, referent):", "body": "if isinstance(obj, dict):<EOL><INDENT>for k, v in obj.items():<EOL><INDENT>if v is referent:<EOL><INDENT>return \"<STR_LIT>\" % k<EOL><DEDENT><DEDENT><DEDENT>for k in dir(obj) + ['<STR_LIT>']:<EOL><INDENT>if getattr(obj, k, None) is referent:<EOL><INDENT>return \"<STR_LIT>\" % k<EOL><DEDENT><DEDENT>return \"<STR_LIT>\"<EOL>", "docstring": "Return the dict key or attribute name of obj which refers to\n        referent.", "id": "f11991:c0:m2"}
{"signature": "def add_tags(self, tags):", "body": "return self.get_data(<EOL>\"<STR_LIT>\" % self.id,<EOL>type=POST,<EOL>params={\"<STR_LIT>\": tags}<EOL>)<EOL>", "docstring": "Add tags to this Firewall.", "id": "f1468:c5:m7"}
{"signature": "def parse_ls_date(self, s, *, now=None):", "body": "with setlocale(\"<STR_LIT:C>\"):<EOL><INDENT>try:<EOL><INDENT>if now is None:<EOL><INDENT>now = datetime.datetime.now()<EOL><DEDENT>d = datetime.datetime.strptime(s, \"<STR_LIT>\")<EOL>d = d.replace(year=now.year)<EOL>diff = (now - d).total_seconds()<EOL>if diff > HALF_OF_YEAR_IN_SECONDS:<EOL><INDENT>d = d.replace(year=now.year + <NUM_LIT:1>)<EOL><DEDENT>elif diff < -HALF_OF_YEAR_IN_SECONDS:<EOL><INDENT>d = d.replace(year=now.year - <NUM_LIT:1>)<EOL><DEDENT><DEDENT>except ValueError:<EOL><INDENT>d = datetime.datetime.strptime(s, \"<STR_LIT>\")<EOL><DEDENT><DEDENT>return self.format_date_time(d)<EOL>", "docstring": "Parsing dates from the ls unix utility. For example,\n\"Nov 18  1958\" and \"Nov 18 12:29\".\n\n:param s: ls date\n:type s: :py:class:`str`\n\n:rtype: :py:class:`str`", "id": "f9225:c2:m12"}
{"signature": "def make_request(self, url, method='<STR_LIT>', headers=None, data=None,<EOL>callback=None, errors=STRICT, verify=False, timeout=None, **params):", "body": "error_modes = (STRICT, GRACEFUL, IGNORE)<EOL>error_mode = errors or GRACEFUL<EOL>if error_mode.lower() not in error_modes:<EOL><INDENT>raise ValueError(<EOL>'<STR_LIT>'<EOL>% '<STR_LIT:U+002C>'.join(error_modes))<EOL><DEDENT>if callback is None:<EOL><INDENT>callback = self._default_resp_callback<EOL><DEDENT>request = getattr(requests, method.lower())<EOL>log.debug('<STR_LIT>' % url)<EOL>log.debug('<STR_LIT>' % method)<EOL>log.debug('<STR_LIT>' % params)<EOL>log.debug('<STR_LIT>' % headers)<EOL>log.debug('<STR_LIT>' % timeout)<EOL>r = request(<EOL>url, headers=headers, data=data, verify=verify, timeout=timeout, params=params)<EOL>log.debug('<STR_LIT>' % r.url)<EOL>try:<EOL><INDENT>r.raise_for_status()<EOL>return callback(r)<EOL><DEDENT>except Exception as e:<EOL><INDENT>return self._with_error_handling(r, e,<EOL>error_mode, self.response_format)<EOL><DEDENT>", "docstring": "Reusable method for performing requests.\n:param url - URL to request\n:param method - request method, default is 'get'\n:param headers - request headers\n:param data - post data\n:param callback - callback to be applied to response,\n                  default callback will parse response as json object.\n:param errors - specifies communication errors handling mode, possible\n                values are:\n                 * strict (default) - throw an error as soon as one\n                    occurred\n                 * graceful - ignore certain errors, e.g. EmptyResponse\n                 * ignore - ignore all errors and return a result in\n                            any case.\n                            NOTE that it DOES NOT mean that no\n                            exceptions can be\n                            raised from this method, it mostly ignores\n                            communication\n                            related errors.\n                 * None or empty string equals to default\n:param verify - whether or not to verify SSL cert, default to False\n:param timeout - the timeout of the request in second, default to None\n:param params - additional query parameters for request", "id": "f13322:c5:m2"}
{"signature": "def deny_method(self, verb, resource):", "body": "self._add_method('<STR_LIT>', verb, resource, [])<EOL>", "docstring": "Adds an API Gateway method (Http verb + Resource path)\nto the list of denied methods for the policy", "id": "f5530:c1:m7"}
{"signature": "def get_volume(self):", "body": "return self.H2O_mass / <NUM_LIT:1.0> + self.get_solid_mass() / self.solid_density<EOL>", "docstring": "Determine the volume of self.", "id": "f15826:c1:m15"}
{"signature": "def process(self, nemo):", "body": "self.__nemo__ = nemo<EOL>for annotation in self.__annotations__:<EOL><INDENT>annotation.target.expanded = frozenset(<EOL>self.__getinnerreffs__(<EOL>objectId=annotation.target.objectId,<EOL>subreference=annotation.target.subreference<EOL>)<EOL>)<EOL><DEDENT>", "docstring": "Register nemo and parses annotations\n\n        .. note:: Process parses the annotation and extends informations about the target URNs by retrieving resource in range\n\n        :param nemo: Nemo", "id": "f9925:c0:m2"}
{"signature": "def load_class(path):", "body": "package, klass = path.rsplit('<STR_LIT:.>', <NUM_LIT:1>)<EOL>module = import_module(package)<EOL>return getattr(module, klass)<EOL>", "docstring": "dynamically load a class given a string of the format\n\npackage.Class", "id": "f2950:m6"}
{"signature": "def get_heron_tracker_conf_dir():", "body": "conf_path = os.path.join(get_heron_tracker_dir(), CONF_DIR)<EOL>return conf_path<EOL>", "docstring": "This will provide heron tracker conf directory from .pex file.\n:return: absolute path of heron conf directory", "id": "f7361:m12"}
{"signature": "def euclidean_distance(point1, point2):", "body": "distance = euclidean_distance_square(point1, point2)<EOL>return distance ** <NUM_LIT:0.5><EOL>", "docstring": "!\n    @brief Calculate Euclidean distance between two vectors.\n    @details The Euclidean between vectors (points) a and b is calculated by following formula:\n\n    \\f[\n    dist(a, b) = \\sqrt{ \\sum_{i=0}^{N}(a_{i} - b_{i})^{2} };\n    \\f]\n\n    Where N is a length of each vector.\n\n    @param[in] point1 (array_like): The first vector.\n    @param[in] point2 (array_like): The second vector.\n\n    @return (double) Euclidean distance between two vectors.\n\n    @see euclidean_distance_square, manhattan_distance, chebyshev_distance", "id": "f15695:m0"}
{"signature": "def resetOptions():", "body": "global options<EOL>defaultOptions = {<EOL>'<STR_LIT>' : '<STR_LIT:utf-8>',  <EOL>'<STR_LIT>' : '<STR_LIT:utf-8>', <EOL>'<STR_LIT>' : '<STR_LIT:?>', <EOL>'<STR_LIT>' : UNRECOGNISED_ECHO,   <EOL>'<STR_LIT>' : False,   <EOL>}<EOL>options = defaultOptions.copy()<EOL>", "docstring": "Reset options to their default values.", "id": "f8767:m0"}
{"signature": "def text_antialias(self, flag=True):", "body": "antialias = pgmagick.DrawableTextAntialias(flag)<EOL>self.drawer.append(antialias)<EOL>", "docstring": "text antialias\n\n        :param flag: True or False. (default is True)\n        :type flag: bool", "id": "f8458:c1:m34"}
{"signature": "def aad_cache():", "body": "return jsonpickle.decode(get_config_value('<STR_LIT>', fallback=None)),jsonpickle.decode(get_config_value('<STR_LIT>', fallback=None))<EOL>", "docstring": "AAD token cache.", "id": "f2344:m11"}
{"signature": "@staticmethod<EOL><INDENT>def create_packet(header, data):<DEDENT>", "body": "packet = IncomingPacket()<EOL>packet.header = header<EOL>packet.data = data<EOL>if len(header) == HeronProtocol.HEADER_SIZE:<EOL><INDENT>packet.is_header_read = True<EOL>if len(data) == packet.get_datasize():<EOL><INDENT>packet.is_complete = True<EOL><DEDENT><DEDENT>return packet<EOL>", "docstring": "Creates an IncomingPacket object from header and data\n\n        This method is for testing purposes", "id": "f7474:c2:m1"}
{"signature": "@property<EOL><INDENT>def A(self):<DEDENT>", "body": "return self._A<EOL>", "docstring": "Matrix A.", "id": "f13588:c0:m2"}
{"signature": "@abc.abstractmethod<EOL><INDENT>def stop_scan(self, timeout_sec):<DEDENT>", "body": "raise NotImplementedError<EOL>", "docstring": "Stop scanning for BLE devices with this adapter.", "id": "f9586:c0:m2"}
{"signature": "@staticmethod<EOL><INDENT>def _handle_base_case(klass, symbol):<DEDENT>", "body": "def the_func(value, variant=<NUM_LIT:0>):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>(obj_level, func_level) = _ToDbusXformer._variant_levels(<EOL><NUM_LIT:0>, variant)<EOL>return (klass(value, variant_level=obj_level), func_level)<EOL><DEDENT>return lambda: (the_func, symbol)<EOL>", "docstring": "Handle a base case.\n\n:param type klass: the class constructor\n:param str symbol: the type code", "id": "f4373:c0:m4"}
{"signature": "@abstractmethod<EOL><INDENT>def remove_handler(self, handler):<DEDENT>", "body": "pass<EOL>", "docstring": "Add a new handler to the main loop.\n\n        Do nothing if the handler is not registered at the main loop.\n\n        :Parameters:\n            - `handler`: the handler object to add\n        :Types:\n            - `handler`: `IOHandler` or `EventHandler` or `TimeoutHandler`", "id": "f15266:c8:m1"}
{"signature": "def isect_polygon__naive(points):", "body": "isect = []<EOL>n = len(points)<EOL>if Real is float:<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>points = [(Real(p[<NUM_LIT:0>]), Real(p[<NUM_LIT:1>])) for p in points]<EOL><DEDENT>for i in range(n):<EOL><INDENT>a0, a1 = points[i], points[(i + <NUM_LIT:1>) % n]<EOL>for j in range(i + <NUM_LIT:1>, n):<EOL><INDENT>b0, b1 = points[j], points[(j + <NUM_LIT:1>) % n]<EOL>if a0 not in (b0, b1) and a1 not in (b0, b1):<EOL><INDENT>ix = isect_seg_seg_v2_point(a0, a1, b0, b1)<EOL>if ix is not None:<EOL><INDENT>if USE_IGNORE_SEGMENT_ENDINGS:<EOL><INDENT>if ((len_squared_v2v2(ix, a0) < NUM_EPS_SQ or<EOL>len_squared_v2v2(ix, a1) < NUM_EPS_SQ) and<EOL>(len_squared_v2v2(ix, b0) < NUM_EPS_SQ or<EOL>len_squared_v2v2(ix, b1) < NUM_EPS_SQ)):<EOL><INDENT>continue<EOL><DEDENT><DEDENT>isect.append(ix)<EOL><DEDENT><DEDENT><DEDENT><DEDENT>return isect<EOL>", "docstring": "Brute force O(n2) version of ``isect_polygon`` for test validation.", "id": "f16266:m13"}
{"signature": "def get_images(self, private=False, type=None):", "body": "params = {}<EOL>if private:<EOL><INDENT>params['<STR_LIT>'] = '<STR_LIT:true>'<EOL><DEDENT>if type:<EOL><INDENT>params['<STR_LIT:type>'] = type<EOL><DEDENT>data = self.get_data(\"<STR_LIT>\", params=params)<EOL>images = list()<EOL>for jsoned in data['<STR_LIT>']:<EOL><INDENT>image = Image(**jsoned)<EOL>image.token = self.token<EOL>images.append(image)<EOL><DEDENT>return images<EOL>", "docstring": "This function returns a list of Image object.", "id": "f1473:c0:m6"}
{"signature": "def _calculate_H_coal(self, T):", "body": "m_C = <NUM_LIT:0>  <EOL>m_H = <NUM_LIT:0>  <EOL>m_O = <NUM_LIT:0>  <EOL>m_N = <NUM_LIT:0>  <EOL>m_S = <NUM_LIT:0>  <EOL>H = <NUM_LIT:0.0>  <EOL>for compound in self.material.compounds:<EOL><INDENT>index = self.material.get_compound_index(compound)<EOL>if stoich.element_mass_fraction(compound, '<STR_LIT:C>') == <NUM_LIT:1.0>:<EOL><INDENT>m_C += self._compound_masses[index]<EOL><DEDENT>elif stoich.element_mass_fraction(compound, '<STR_LIT:H>') == <NUM_LIT:1.0>:<EOL><INDENT>m_H += self._compound_masses[index]<EOL><DEDENT>elif stoich.element_mass_fraction(compound, '<STR_LIT:O>') == <NUM_LIT:1.0>:<EOL><INDENT>m_O += self._compound_masses[index]<EOL><DEDENT>elif stoich.element_mass_fraction(compound, '<STR_LIT:N>') == <NUM_LIT:1.0>:<EOL><INDENT>m_N += self._compound_masses[index]<EOL><DEDENT>elif stoich.element_mass_fraction(compound, '<STR_LIT:S>') == <NUM_LIT:1.0>:<EOL><INDENT>m_S += self._compound_masses[index]<EOL><DEDENT>else:<EOL><INDENT>dH = thermo.H(compound, T, self._compound_masses[index])<EOL>H += dH<EOL><DEDENT><DEDENT>m_total = y_C + y_H + y_O + y_N + y_S  <EOL>y_C = m_C / m_total<EOL>y_H = m_H / m_total<EOL>y_O = m_O / m_total<EOL>y_N = m_N / m_total<EOL>y_S = m_S / m_total<EOL>hmodel = coals.DafHTy()<EOL>H = hmodel.calculate(T=T+<NUM_LIT>, y_C=y_C, y_H=y_H, y_O=y_O, y_N=y_N,<EOL>y_S=y_S) / <NUM_LIT>  <EOL>H298 = hmodel.calculate(T=<NUM_LIT>, y_C=y_C, y_H=y_H, y_O=y_O, y_N=y_N,<EOL>y_S=y_S) / <NUM_LIT>  <EOL>Hdaf = H - H298 + self._DH298  <EOL>Hdaf *= m_total  <EOL>H += Hdaf<EOL>return H<EOL>", "docstring": "Calculate the enthalpy of the package at the specified temperature, in\ncase the material is coal.\n\n:param T: [\u00b0C] temperature\n\n:returns: [kWh] enthalpy", "id": "f15823:c1:m6"}
{"signature": "def _SkipFieldValue(tokenizer):", "body": "<EOL>if tokenizer.TryConsumeByteString():<EOL><INDENT>while tokenizer.TryConsumeByteString():<EOL><INDENT>pass<EOL><DEDENT>return<EOL><DEDENT>if (not tokenizer.TryConsumeIdentifier() and<EOL>not tokenizer.TryConsumeInt64() and<EOL>not tokenizer.TryConsumeUint64() and<EOL>not tokenizer.TryConsumeFloat()):<EOL><INDENT>raise ParseError('<STR_LIT>' + tokenizer.token)<EOL><DEDENT>", "docstring": "Skips over a field value.\n\n    Args:\n      tokenizer: A tokenizer to parse the field name and values.\n\n    Raises:\n      ParseError: In case an invalid field value is found.", "id": "f8657:m12"}
{"signature": "def change_nick(self,new_nick):", "body": "new_room_jid=JID(self.room_jid.node,self.room_jid.domain,new_nick)<EOL>p=Presence(to_jid=new_room_jid)<EOL>self.manager.stream.send(p)<EOL>", "docstring": "Send a nick change request to the room.\n\n:Parameters:\n    - `new_nick`: the new nickname requested.\n:Types:\n    - `new_nick`: `unicode`", "id": "f15256:c2:m7"}
{"signature": "def volumeByVenueDF(symbol, token='<STR_LIT>', version='<STR_LIT>'):", "body": "df = pd.DataFrame(volumeByVenue(symbol, token, version))<EOL>_toDatetime(df)<EOL>_reindex(df, '<STR_LIT>')<EOL>return df<EOL>", "docstring": "This returns 15 minute delayed and 30 day average consolidated volume percentage of a stock, by market.\n    This call will always return 13 values, and will be sorted in ascending order by current day trading volume percentage.\n\n    https://iexcloud.io/docs/api/#volume-by-venue\n    Updated during regular market hours 9:30am-4pm ET\n\n\n    Args:\n        symbol (string); Ticker to request\n        token (string); Access token\n        version (string); API version\n\n    Returns:\n        DataFrame: result", "id": "f2330:m87"}
{"signature": "def dzip(items1, items2, cls=dict):", "body": "try:<EOL><INDENT>len(items1)<EOL><DEDENT>except TypeError:<EOL><INDENT>items1 = list(items1)<EOL><DEDENT>try:<EOL><INDENT>len(items2)<EOL><DEDENT>except TypeError:<EOL><INDENT>items2 = list(items2)<EOL><DEDENT>if len(items1) == <NUM_LIT:0> and len(items2) == <NUM_LIT:1>:<EOL><INDENT>items2 = []<EOL><DEDENT>if len(items2) == <NUM_LIT:1> and len(items1) > <NUM_LIT:1>:<EOL><INDENT>items2 = items2 * len(items1)<EOL><DEDENT>if len(items1) != len(items2):<EOL><INDENT>raise ValueError('<STR_LIT>' % (<EOL>len(items1), len(items2)))<EOL><DEDENT>return cls(zip(items1, items2))<EOL>", "docstring": "Zips elementwise pairs between items1 and items2 into a dictionary. Values\nfrom items2 can be broadcast onto items1.\n\nArgs:\n    items1 (Iterable): full sequence\n    items2 (Iterable): can either be a sequence of one item or a sequence\n        of equal length to `items1`\n    cls (Type[dict]): dictionary type to use. Defaults to dict, but could\n        be ordered dict instead.\n\nReturns:\n    dict: similar to dict(zip(items1, items2))\n\nExample:\n    >>> assert dzip([1, 2, 3], [4]) == {1: 4, 2: 4, 3: 4}\n    >>> assert dzip([1, 2, 3], [4, 4, 4]) == {1: 4, 2: 4, 3: 4}\n    >>> assert dzip([], [4]) == {}", "id": "f5144:m0"}
{"signature": "def receive(self, transport, myname):", "body": "with self.lock:<EOL><INDENT>self.transport = transport<EOL>transport.set_target(self)<EOL>self.me = JID(myname)<EOL>self.initiator = False<EOL>self._setup_stream_element_handlers()<EOL><DEDENT>", "docstring": "Receive an XMPP connection over the `transport`.\n\n        :Parameters:\n            - `transport`: an XMPP transport instance\n            - `myname`: local stream endpoint name.", "id": "f15247:c0:m3"}
{"signature": "@resources.command(use_fields_as_options=False)<EOL><INDENT>def get(self, pk):<DEDENT>", "body": "<EOL>try:<EOL><INDENT>return next(s for s in self.list()['<STR_LIT>'] if s['<STR_LIT:id>'] == pk)<EOL><DEDENT>except StopIteration:<EOL><INDENT>raise exc.NotFound('<STR_LIT>')<EOL><DEDENT>", "docstring": "Return one and exactly one object\n\n        =====API DOCS=====\n        Return one and exactly one Tower setting.\n\n        :param pk: Primary key of the Tower setting to retrieve\n        :type pk: int\n        :returns: loaded JSON of the retrieved Tower setting object.\n        :rtype: dict\n        :raises tower_cli.exceptions.NotFound: When no specified Tower setting exists.\n\n        =====API DOCS=====", "id": "f3376:c0:m1"}
{"signature": "def assess(model, reaction, flux_coefficient_cutoff=<NUM_LIT>, solver=None):", "body": "reaction = model.reactions.get_by_any(reaction)[<NUM_LIT:0>]<EOL>with model as m:<EOL><INDENT>m.objective = reaction<EOL>if _optimize_or_value(m, solver=solver) >= flux_coefficient_cutoff:<EOL><INDENT>return True<EOL><DEDENT>else:<EOL><INDENT>results = dict()<EOL>results['<STR_LIT>'] = assess_component(<EOL>model, reaction, '<STR_LIT>', flux_coefficient_cutoff)<EOL>results['<STR_LIT>'] = assess_component(<EOL>model, reaction, '<STR_LIT>', flux_coefficient_cutoff)<EOL>return results<EOL><DEDENT><DEDENT>", "docstring": "Assesses production capacity.\n\n    Assesses the capacity of the model to produce the precursors for the\n    reaction and absorb the production of the reaction while the reaction is\n    operating at, or above, the specified cutoff.\n\n    Parameters\n    ----------\n    model : cobra.Model\n        The cobra model to assess production capacity for\n\n    reaction : reaction identifier or cobra.Reaction\n        The reaction to assess\n\n    flux_coefficient_cutoff :  float\n        The minimum flux that reaction must carry to be considered active.\n\n    solver : basestring\n        Solver name. If None, the default solver will be used.\n\n    Returns\n    -------\n    bool or dict\n        True if the model can produce the precursors and absorb the products\n        for the reaction operating at, or above, flux_coefficient_cutoff.\n        Otherwise, a dictionary of {'precursor': Status, 'product': Status}.\n        Where Status is the results from assess_precursors and\n        assess_products, respectively.", "id": "f15949:m0"}
{"signature": "def run(self, steps=<NUM_LIT:1000>):", "body": "for step in range(steps):<EOL><INDENT>if self.is_done(): return<EOL>self.step()<EOL><DEDENT>", "docstring": "Run the Environment for given number of time steps.", "id": "f1676:c2:m8"}
{"signature": "def writeRaw8(self, value):", "body": "value = value & <NUM_LIT><EOL>self._idle()<EOL>self._transaction_start()<EOL>self._i2c_start()<EOL>self._i2c_write_bytes([self._address_byte(False), value])<EOL>self._i2c_stop()<EOL>response = self._transaction_end()<EOL>self._verify_acks(response)<EOL>", "docstring": "Write an 8-bit value on the bus (without register).", "id": "f8002:c2:m12"}
{"signature": "@classmethod<EOL><INDENT>def from_xml(cls, element):<DEDENT>", "body": "<EOL>items = []<EOL>jids = set()<EOL>if element.tag != QUERY_TAG:<EOL><INDENT>raise ValueError(\"<STR_LIT>\".format(element))<EOL><DEDENT>version = element.get(\"<STR_LIT>\")<EOL>for child in element:<EOL><INDENT>if child.tag != ITEM_TAG:<EOL><INDENT>logger.debug(\"<STR_LIT>\".format(child))<EOL>continue<EOL><DEDENT>item = RosterItem.from_xml(child)<EOL>if item.jid in jids:<EOL><INDENT>logger.warning(\"<STR_LIT>\".format(<EOL>item.jid))<EOL>continue<EOL><DEDENT>jids.add(item.jid)<EOL>items.append(item)<EOL><DEDENT>return cls(items, version)<EOL>", "docstring": "Create a `RosterPayload` object from an XML element.\n\n:Parameters:\n    - `element`: the XML element\n:Types:\n    - `element`: :etree:`ElementTree.Element`\n\n:return: a freshly created roster payload\n:returntype: `cls`", "id": "f15245:c4:m1"}
{"signature": "def update_node_count(self, node, add_to_count):", "body": "current_score = <NUM_LIT:0><EOL>count_string = self.parser.getAttribute(node, '<STR_LIT>')<EOL>if count_string:<EOL><INDENT>current_score = int(count_string)<EOL><DEDENT>new_score = current_score + add_to_count<EOL>self.parser.setAttribute(node, \"<STR_LIT>\", str(new_score))<EOL>", "docstring": "\\\n        stores how many decent nodes are under a parent node", "id": "f14074:c0:m10"}
{"signature": "@tuple_args<EOL>def rand_brightness(imagez, scale=<NUM_LIT:1.0>, randfun=rand.normal(<NUM_LIT:0.>, <NUM_LIT>), clamp=(<NUM_LIT:0.>, <NUM_LIT:1.>)):", "body": "l, h = clamp<EOL>r = randfun((imagez[<NUM_LIT:0>].shape[<NUM_LIT:0>], <NUM_LIT:1>, <NUM_LIT:1>, <NUM_LIT:1>)) * scale<EOL>def apply(im):<EOL><INDENT>im += r<EOL>im[im < l] = l<EOL>im[im > h] = h<EOL>return im<EOL><DEDENT>return tuple(map(apply, imagez))<EOL>", "docstring": ":param images:\n:param scale: scale for random value\n:param randfun: any randfun binding except shape\n:param clamp: clamping range\n:return:", "id": "f6390:m16"}
{"signature": "def _new_pattern_collection(self):", "body": "return self._spec.new_pattern_collection()<EOL>", "docstring": "Create a new pattern collection.\n\n        :return: a new specified pattern collection for\n          :meth:`knitting_pattern_set`", "id": "f560:c1:m9"}
{"signature": "def unique_flags(items, key=None):", "body": "len_ = len(items)<EOL>if key is None:<EOL><INDENT>item_to_index = dict(zip(reversed(items), reversed(range(len_))))<EOL>indices = item_to_index.values()<EOL><DEDENT>else:<EOL><INDENT>indices = argunique(items, key=key)<EOL><DEDENT>flags = boolmask(indices, len_)<EOL>return flags<EOL>", "docstring": "Returns a list of booleans corresponding to the first instance of each\nunique item.\n\nArgs:\n    items (Sequence): indexable collection of items\n\n    key (Callable, optional): custom normalization function.\n        If specified returns items where `key(item)` is unique.\n\nReturns:\n    List[bool] : flags the items that are unique\n\nExample:\n    >>> import ubelt as ub\n    >>> items = [0, 2, 1, 1, 0, 9, 2]\n    >>> flags = unique_flags(items)\n    >>> assert flags == [True, True, True, False, False, True, False]\n    >>> flags = unique_flags(items, key=lambda x: x % 2 == 0)\n    >>> assert flags == [True, False, True, False, False, False, False]", "id": "f5139:m6"}
{"signature": "def STOP(self):", "body": "raise EndTx('<STR_LIT>')<EOL>", "docstring": "Halts execution", "id": "f17019:c16:m41"}
{"signature": "def _WX28_clipped_agg_as_bitmap(agg, bbox):", "body": "l, b, width, height = bbox.get_bounds()<EOL>r = l + width<EOL>t = b + height<EOL>srcBmp = wx.BitmapFromBufferRGBA(int(agg.width), int(agg.height),<EOL>agg.buffer_rgba(<NUM_LIT:0>, <NUM_LIT:0>))<EOL>srcDC = wx.MemoryDC()<EOL>srcDC.SelectObject(srcBmp)<EOL>destBmp = wx.EmptyBitmap(int(width), int(height))<EOL>destDC = wx.MemoryDC()<EOL>destDC.SelectObject(destBmp)<EOL>destDC.BeginDrawing()<EOL>x = int(l)<EOL>y = int(int(agg.height) - t)<EOL>destDC.Blit(<NUM_LIT:0>, <NUM_LIT:0>, int(width), int(height), srcDC, x, y)<EOL>destDC.EndDrawing()<EOL>srcDC.SelectObject(wx.NullBitmap)<EOL>destDC.SelectObject(wx.NullBitmap)<EOL>return destBmp<EOL>", "docstring": "Convert the region of a the agg buffer bounded by bbox to a wx.Bitmap.\n\nNote: agg must be a backend_agg.RendererAgg instance.", "id": "f17220:m6"}
{"signature": "def ConsumeInt64(self):", "body": "try:<EOL><INDENT>result = ParseInteger(self.token, is_signed=True, is_long=True)<EOL><DEDENT>except ValueError as e:<EOL><INDENT>raise self._ParseError(str(e))<EOL><DEDENT>self.NextToken()<EOL>return result<EOL>", "docstring": "Consumes a signed 64bit integer number.\n\n        Returns:\n          The integer parsed.\n\n        Raises:\n          ParseError: If a signed 64bit integer couldn't be consumed.", "id": "f8657:c5:m12"}
{"signature": "def remove_group_user(self, group_id, user_id):", "body": "response = self._perform_request(<EOL>url='<STR_LIT>' % (group_id, user_id),<EOL>method='<STR_LIT>')<EOL>return response<EOL>", "docstring": "Removes a user from a group.\n\n:param      group_id: The unique ID of the group.\n:type       group_id: ``str``\n\n:param      user_id: The unique ID of the user.\n:type       user_id: ``str``", "id": "f811:c0:m89"}
{"signature": "def get_graph_by_id(self, network_id: int) -> BELGraph:", "body": "return self.networks[network_id]<EOL>", "docstring": "Get a graph by its identifier.", "id": "f9395:c1:m2"}
{"signature": "def print_header(cmd):", "body": "textcolors = {'<STR_LIT>': '<STR_LIT>'}<EOL>libcolors = {'<STR_LIT>': '<STR_LIT>', '<STR_LIT>': '<STR_LIT>'}<EOL>vercolors = {'<STR_LIT>': '<STR_LIT>'}<EOL>execolors = {'<STR_LIT>': '<STR_LIT>', '<STR_LIT>': '<STR_LIT>'}<EOL>argcolors = {'<STR_LIT>': '<STR_LIT>'}<EOL>def fmt_app_info(name, ver):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>return C('<STR_LIT>', **textcolors).join(<EOL>C(name, **libcolors),<EOL>C(ver, **vercolors)<EOL>)<EOL><DEDENT>def fmt_cmd_args(cmdargs):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>return C('<STR_LIT:U+0020>').join(<EOL>C(cmdargs[<NUM_LIT:0>], **execolors),<EOL>C('<STR_LIT:U+0020>').join(C(s, **argcolors) for s in cmdargs[<NUM_LIT:1>:]),<EOL>).join('<STR_LIT:(>', '<STR_LIT:)>', style='<STR_LIT>')<EOL><DEDENT>print('<STR_LIT>'.format(<EOL>C('<STR_LIT:U+0020>').join(<EOL>C('<STR_LIT>', **textcolors),<EOL>fmt_app_info(APPNAME, APPVERSION),<EOL>C('<STR_LIT>', **textcolors),<EOL>fmt_app_info('<STR_LIT>', green_version),<EOL>fmt_cmd_args(cmd),<EOL>)<EOL>))<EOL>print(<EOL>C('<STR_LIT>').join(<EOL>C('<STR_LIT>', '<STR_LIT>'),<EOL>C(os.getcwd(), '<STR_LIT>', style='<STR_LIT>'),<EOL>),<EOL>)<EOL>", "docstring": "Print some info about the Colr and Green versions being used.", "id": "f9612:m11"}
{"signature": "@property<EOL><INDENT>def x0(self):<DEDENT>", "body": "return self._x0<EOL>", "docstring": "Minimum x coordinate of rectangle.", "id": "f12117:c0:m13"}
{"signature": "def get_room_id(self):", "body": "return self._room_id<EOL>", "docstring": "Get room ID.\n\n        Returns:\n            int. Room ID", "id": "f7984:c1:m1"}
{"signature": "def _function_argument(self, value):", "body": "if value in FUNCTION_CONSTANT:<EOL><INDENT>return FUNCTION_CONSTANT[value]<EOL><DEDENT>else:<EOL><INDENT>return float(value)<EOL><DEDENT>", "docstring": "Resolve function, convert to float if not found.", "id": "f3874:c3:m8"}
{"signature": "def wasSuccessful(self):", "body": "return len(self.failures) == len(self.errors) == <NUM_LIT:0><EOL>", "docstring": "Tells whether or not this result was a success", "id": "f16469:c0:m14"}
{"signature": "def ensure_annotations_equal(name, expected, actual):", "body": "maybe_replace_any_if_equal(name, expected, actual)<EOL>", "docstring": "Raise ValueError if `expected` isn't equal to `actual`.\n\n    If --replace-any is used, the Any type in `actual` is considered equal.", "id": "f1486:m57"}
{"signature": "def take_screenshot(self):", "body": "return self._execute(Command.ELEMENT_SCREENSHOT)<EOL>", "docstring": "Gets the screenshot of the current element\n           as a base64 encoded string.\n\n        Support:\n            Android iOS Web(WebView)\n\n        Returns:\n            Base64 encoded string of the screenshot.", "id": "f4435:c0:m30"}
{"signature": "def getParticleBest(self, particleId):", "body": "return self._particleBest.get(particleId, (None, None))<EOL>", "docstring": "Return the best score and position for a given particle. The position\n        is given as a dict, with varName:varPosition items in it.\n\n        Parameters:\n        ---------------------------------------------------------------------\n        particleId:    which particle\n        retval:        (bestResult, bestPosition)", "id": "f17579:c0:m14"}
{"signature": "def bytes(self):", "body": "if self.__text_is_expected:<EOL><INDENT>return self.string().encode(self.__encoding)<EOL><DEDENT>else:<EOL><INDENT>return self._bytes()<EOL><DEDENT>", "docstring": ":return: the dump as bytes.", "id": "f564:c0:m4"}
{"signature": "def build_hugo_md(filename, tag, bump):", "body": "header = [<EOL>'<STR_LIT>',<EOL>'<STR_LIT>'.format(date.today().isoformat()),<EOL>'<STR_LIT>'.format(tag),<EOL>'<STR_LIT>',<EOL>'<STR_LIT>'.format(bump),<EOL>'<STR_LIT>',<EOL>'<STR_LIT:\\n>'<EOL>]<EOL>with open(filename, \"<STR_LIT:r>\") as file_h:<EOL><INDENT>content = insert_break(file_h.readlines())<EOL><DEDENT>header.extend(content)<EOL>with open(filename, \"<STR_LIT:w>\") as file_h:<EOL><INDENT>file_h.writelines(header)<EOL><DEDENT>", "docstring": "Build the markdown release notes for Hugo.\n\nInserts the required TOML header with specific values and adds a break\nfor long release notes.\n\nParameters\n----------\nfilename : str, path\n    The release notes file.\ntag : str\n    The tag, following semantic versioning, of the current release.\nbump : {\"major\", \"minor\", \"patch\", \"alpha\", \"beta\"}\n    The type of release.", "id": "f15985:m1"}
{"signature": "def log_calls(function):", "body": "def wrapper(self,*args,**kwargs):  <EOL><INDENT>self.log.log(group=function.__name__,message='<STR_LIT>') <EOL>function(self,*args,**kwargs)<EOL>self.log.log(group=function.__name__,message='<STR_LIT>') <EOL><DEDENT>return wrapper<EOL>", "docstring": "Decorator that logs function calls in their self.log", "id": "f10492:m0"}
{"signature": "def update_links_and_ffts(self):", "body": "figure_counter = <NUM_LIT:0><EOL>for field in record_get_field_instances(self.record,<EOL>tag='<STR_LIT>',<EOL>ind1='<STR_LIT:4>'):<EOL><INDENT>subs = field_get_subfields(field)<EOL>newsubs = []<EOL>remove = False<EOL>if '<STR_LIT:z>' in subs:<EOL><INDENT>is_figure = [s for s in subs['<STR_LIT:z>'] if \"<STR_LIT>\" in s.lower()]<EOL>if is_figure and '<STR_LIT:u>' in subs:<EOL><INDENT>is_subformat = [<EOL>s for s in subs['<STR_LIT:u>'] if \"<STR_LIT>\" in s.lower()]<EOL>if not is_subformat:<EOL><INDENT>url = subs['<STR_LIT:u>'][<NUM_LIT:0>]<EOL>if url.endswith(\"<STR_LIT>\"):<EOL><INDENT>fd, local_url = mkstemp(suffix=os.path.basename(url))<EOL>os.close(fd)<EOL>self.logger.info(<EOL>\"<STR_LIT>\" % (url, local_url))<EOL>plotfile = \"<STR_LIT>\"<EOL>try:<EOL><INDENT>plotfile = download_file(url=url,<EOL>download_to_file=local_url)<EOL><DEDENT>except Exception as e:<EOL><INDENT>self.logger.exception(e)<EOL>remove = True<EOL><DEDENT>if plotfile:<EOL><INDENT>converted = convert_images([plotfile])<EOL>if converted:<EOL><INDENT>url = converted.pop()<EOL>msg = \"<STR_LIT>\"% (local_url, url)<EOL>self.logger.info(msg)<EOL><DEDENT>else:<EOL><INDENT>msg = \"<STR_LIT>\"% (local_url,)<EOL>self.logger.error(msg)<EOL>url = None<EOL>remove = True<EOL><DEDENT><DEDENT><DEDENT>if url:<EOL><INDENT>newsubs.append(('<STR_LIT:a>', url))<EOL>newsubs.append(('<STR_LIT:t>', '<STR_LIT>'))<EOL>figure_counter += <NUM_LIT:1><EOL>if '<STR_LIT:y>' in subs:<EOL><INDENT>newsubs.append(<EOL>('<STR_LIT:d>', \"<STR_LIT>\" % (figure_counter, subs['<STR_LIT:y>'][<NUM_LIT:0>])))<EOL>newsubs.append(('<STR_LIT:n>', subs['<STR_LIT:y>'][<NUM_LIT:0>]))<EOL><DEDENT>else:<EOL><INDENT>name = os.path.basename(<EOL>os.path.splitext(subs['<STR_LIT:u>'][<NUM_LIT:0>])[<NUM_LIT:0>])<EOL>newsubs.append(<EOL>('<STR_LIT:d>', \"<STR_LIT>\" % (figure_counter, name)))<EOL>newsubs.append(('<STR_LIT:n>', name))<EOL><DEDENT><DEDENT><DEDENT><DEDENT><DEDENT>if not newsubs and '<STR_LIT:u>' in subs:<EOL><INDENT>is_fulltext = [s for s in subs['<STR_LIT:u>'] if \"<STR_LIT>\" in s]<EOL>if is_fulltext:<EOL><INDENT>newsubs = [('<STR_LIT:t>', '<STR_LIT>'), ('<STR_LIT:a>', subs['<STR_LIT:u>'][<NUM_LIT:0>])]<EOL><DEDENT><DEDENT>if not newsubs and '<STR_LIT:u>' in subs:<EOL><INDENT>remove = True<EOL>is_zipfile = [s for s in subs['<STR_LIT:u>'] if \"<STR_LIT>\" in s]<EOL>if is_zipfile:<EOL><INDENT>url = is_zipfile[<NUM_LIT:0>]<EOL>local_url = os.path.join(self.get_local_folder(), os.path.basename(url))<EOL>self.logger.info(\"<STR_LIT>\" %<EOL>(url, local_url))<EOL>zipped_archive = \"<STR_LIT>\"<EOL>try:<EOL><INDENT>zipped_archive = download_file(url=is_zipfile[<NUM_LIT:0>],<EOL>download_to_file=local_url)<EOL><DEDENT>except Exception as e:<EOL><INDENT>self.logger.exception(e)<EOL>remove = True<EOL><DEDENT>if zipped_archive:<EOL><INDENT>unzipped_archive = unzip(zipped_archive)<EOL>list_of_pngs = locate(\"<STR_LIT>\", unzipped_archive)<EOL>for png in list_of_pngs:<EOL><INDENT>if \"<STR_LIT>\" in png or \"<STR_LIT>\" in png:<EOL><INDENT>continue<EOL><DEDENT>figure_counter += <NUM_LIT:1><EOL>plotsubs = []<EOL>plotsubs.append(('<STR_LIT:a>', png))<EOL>caption = '<STR_LIT>' % (<EOL>figure_counter, os.path.basename(png))<EOL>plotsubs.append(('<STR_LIT:d>', caption))<EOL>plotsubs.append(('<STR_LIT:t>', '<STR_LIT>'))<EOL>record_add_field(<EOL>self.record, '<STR_LIT>', subfields=plotsubs)<EOL><DEDENT><DEDENT><DEDENT><DEDENT>if not remove and not newsubs and '<STR_LIT:u>' in subs:<EOL><INDENT>urls = ('<STR_LIT>', '<STR_LIT>',<EOL>'<STR_LIT>', '<STR_LIT>',<EOL>'<STR_LIT>', '<STR_LIT>')<EOL>for val in subs['<STR_LIT:u>']:<EOL><INDENT>if any(url in val for url in urls):<EOL><INDENT>remove = True<EOL>break<EOL><DEDENT>if val.endswith('<STR_LIT>'):<EOL><INDENT>remove = True<EOL><DEDENT><DEDENT><DEDENT>if newsubs:<EOL><INDENT>record_add_field(self.record, '<STR_LIT>', subfields=newsubs)<EOL>remove = True<EOL><DEDENT>if remove:<EOL><INDENT>record_delete_field(self.record, '<STR_LIT>', ind1='<STR_LIT:4>',<EOL>field_position_global=field[<NUM_LIT:4>])<EOL><DEDENT><DEDENT>", "docstring": "FFT (856) Dealing with graphs.", "id": "f7925:c0:m16"}
{"signature": "def findobj(self, match=None):", "body": "if match is None: <EOL><INDENT>def matchfunc(x): return True<EOL><DEDENT>elif cbook.issubclass_safe(match, Artist):<EOL><INDENT>def matchfunc(x):<EOL><INDENT>return isinstance(x, match)<EOL><DEDENT><DEDENT>elif callable(match):<EOL><INDENT>matchfunc = match<EOL><DEDENT>else:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>artists = []<EOL>for c in self.get_children():<EOL><INDENT>if matchfunc(c):<EOL><INDENT>artists.append(c)<EOL><DEDENT>artists.extend([thisc for thisc in c.findobj(matchfunc) if matchfunc(thisc)])<EOL><DEDENT>if matchfunc(self):<EOL><INDENT>artists.append(self)<EOL><DEDENT>return artists<EOL>", "docstring": "pyplot signature:\n  findobj(o=gcf(), match=None)\n\nRecursively find all :class:matplotlib.artist.Artist instances\ncontained in self.\n\n*match* can be\n\n  - None: return all objects contained in artist (including artist)\n\n  - function with signature ``boolean = match(artist)`` used to filter matches\n\n  - class instance: eg Line2D.  Only return artists of class type\n\n.. plot:: mpl_examples/pylab_examples/findobj_demo.py", "id": "f17255:c0:m52"}
{"signature": "def resolve_provider(self, path):", "body": "<EOL>share = None<EOL>lower_path = path.lower()<EOL>for r in self.sorted_share_list:<EOL><INDENT>if r == \"<STR_LIT:/>\":<EOL><INDENT>share = r<EOL>break<EOL><DEDENT>elif lower_path == r or lower_path.startswith(r + \"<STR_LIT:/>\"):<EOL><INDENT>share = r<EOL>break<EOL><DEDENT><DEDENT>if share is None:<EOL><INDENT>return None, None<EOL><DEDENT>return share, self.provider_map.get(share)<EOL>", "docstring": "Get the registered DAVProvider for a given path.\n\n        Returns:\n            tuple: (share, provider)", "id": "f8583:c0:m2"}
{"signature": "@property<EOL><INDENT>def fname(self):<DEDENT>", "body": "return self.stem<EOL>", "docstring": "r\"\"\"\n        File name without extension.\n\n        Example: ``readme`` for ``C:\\User\\admin\\readme.txt``", "id": "f2882:c0:m8"}
{"signature": "@abc.abstractproperty<EOL><INDENT>def uuid(self):<DEDENT>", "body": "raise NotImplementedError<EOL>", "docstring": "Return the UUID of this GATT descriptor.", "id": "f9589:c2:m0"}
{"signature": "def _popToTag(self, name, inclusivePop=True):", "body": "<EOL>if name == self.ROOT_TAG_NAME:<EOL><INDENT>return<EOL><DEDENT>numPops = <NUM_LIT:0><EOL>mostRecentTag = None<EOL>for i in range(len(self.tagStack)-<NUM_LIT:1>, <NUM_LIT:0>, -<NUM_LIT:1>):<EOL><INDENT>if name == self.tagStack[i].name:<EOL><INDENT>numPops = len(self.tagStack)-i<EOL>break<EOL><DEDENT><DEDENT>if not inclusivePop:<EOL><INDENT>numPops = numPops - <NUM_LIT:1><EOL><DEDENT>for i in range(<NUM_LIT:0>, numPops):<EOL><INDENT>mostRecentTag = self.popTag()<EOL><DEDENT>return mostRecentTag<EOL>", "docstring": "Pops the tag stack up to and including the most recent\n        instance of the given tag. If inclusivePop is false, pops the tag\n        stack up to but *not* including the most recent instqance of\n        the given tag.", "id": "f11596:c9:m9"}
{"signature": "@api.check(<NUM_LIT:2>, \"<STR_LIT>\")<EOL>def win_get_state(title, **kwargs):", "body": "text = kwargs.get(\"<STR_LIT:text>\", \"<STR_LIT>\")<EOL>res = AUTO_IT.AU3_WinGetState(LPCWSTR(title), LPCWSTR(text))<EOL>return res<EOL>", "docstring": "Retrieves the state of a given window.\n:param title:\n:param text:\n:return:\n1 = Window exists\n2 = Window is visible\n4 = Windows is enabled\n8 = Window is active\n16 = Window is minimized\n32 = Windows is maximized", "id": "f5587:m19"}
{"signature": "def register_default_types():", "body": "register_type(type, pipe.map)<EOL>register_type(types.FunctionType, pipe.map)<EOL>register_type(types.MethodType, pipe.map)<EOL>register_type(tuple, seq)<EOL>register_type(list, seq)<EOL>register_type(types.GeneratorType, seq)<EOL>register_type(string_type, sh)<EOL>register_type(unicode_type, sh)<EOL>register_type(file_type, fileobj)<EOL>if is_py3:<EOL><INDENT>register_type(range, seq)<EOL>register_type(map, seq)<EOL><DEDENT>", "docstring": "Regiser all default type-to-pipe convertors.", "id": "f2259:m29"}
{"signature": "def parse(self, s, dpi = <NUM_LIT>, prop = None):", "body": "if prop is None:<EOL><INDENT>prop = FontProperties()<EOL><DEDENT>cacheKey = (s, dpi, hash(prop))<EOL>result = self._cache.get(cacheKey)<EOL>if result is not None:<EOL><INDENT>return result<EOL><DEDENT>if self._output == '<STR_LIT>' and rcParams['<STR_LIT>']:<EOL><INDENT>font_output = StandardPsFonts(prop)<EOL><DEDENT>else:<EOL><INDENT>backend = self._backend_mapping[self._output]()<EOL>fontset = rcParams['<STR_LIT>']<EOL>fontset_class = self._font_type_mapping.get(fontset.lower())<EOL>if fontset_class is not None:<EOL><INDENT>font_output = fontset_class(prop, backend)<EOL><DEDENT>else:<EOL><INDENT>raise ValueError(<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL><DEDENT><DEDENT>fontsize = prop.get_size_in_points()<EOL>if self._parser is None:<EOL><INDENT>self.__class__._parser = Parser()<EOL><DEDENT>box = self._parser.parse(s, font_output, fontsize, dpi)<EOL>font_output.set_canvas_size(box.width, box.height, box.depth)<EOL>result = font_output.get_results(box)<EOL>self._cache[cacheKey] = result<EOL>self._parser.clear()<EOL>font_output.destroy()<EOL>font_output.mathtext_backend.fonts_object = None<EOL>font_output.mathtext_backend = None<EOL>return result<EOL>", "docstring": "Parse the given math expression *s* at the given *dpi*.  If\n*prop* is provided, it is a\n:class:`~matplotlib.font_manager.FontProperties` object\nspecifying the \"default\" font to use in the math expression,\nused for all non-math text.\n\nThe results are cached, so multiple calls to :meth:`parse`\nwith the same expression should be fast.", "id": "f17193:c45:m1"}
{"signature": "def is_before(self):", "body": "return self.before<EOL>", "docstring": "Returns True if the direction is set to before.\n\n        :returns: True if the direction is set to before, otherwise False\n        :rtype: bool", "id": "f11267:c2:m5"}
{"signature": "@staticmethod<EOL><INDENT>def getTemporalDelay(inferenceElement, key=None):<DEDENT>", "body": "<EOL>if inferenceElement in (InferenceElement.prediction,<EOL>InferenceElement.encodings):<EOL><INDENT>return <NUM_LIT:1><EOL><DEDENT>if inferenceElement in (InferenceElement.anomalyScore,<EOL>InferenceElement.anomalyLabel,<EOL>InferenceElement.classification,<EOL>InferenceElement.classConfidences):<EOL><INDENT>return <NUM_LIT:0><EOL><DEDENT>if inferenceElement in (InferenceElement.multiStepPredictions,<EOL>InferenceElement.multiStepBestPredictions):<EOL><INDENT>return int(key)<EOL><DEDENT>return <NUM_LIT:0><EOL>", "docstring": "Returns the number of records that elapse between when an inference is\n        made and when the corresponding input record will appear. For example, a\n        multistep prediction for 3 timesteps out will have a delay of 3\n\n\n        Parameters:\n        -----------------------------------------------------------------------\n\n        inferenceElement:   The InferenceElement value being delayed\n        key:                If the inference is a dictionary type, this specifies\n                            key for the sub-inference that is being delayed", "id": "f17595:c0:m2"}
{"signature": "def plot_figure(array, as_subplot, units, kpc_per_arcsec, figsize, aspect, cmap, norm, norm_min, norm_max,<EOL>linthresh, linscale, xticks_manual, yticks_manual):", "body": "fig = plotter_util.setup_figure(figsize=figsize, as_subplot=as_subplot)<EOL>norm_min, norm_max = get_normalization_min_max(array=array, norm_min=norm_min, norm_max=norm_max)<EOL>norm_scale = get_normalization_scale(norm=norm, norm_min=norm_min, norm_max=norm_max,<EOL>linthresh=linthresh, linscale=linscale)<EOL>extent = get_extent(array=array, units=units, kpc_per_arcsec=kpc_per_arcsec,<EOL>xticks_manual=xticks_manual, yticks_manual=yticks_manual)<EOL>plt.imshow(array, aspect=aspect, cmap=cmap, norm=norm_scale, extent=extent)<EOL>return fig<EOL>", "docstring": "Open a matplotlib figure and plot the array of data on it.\n\n    Parameters\n    -----------\n    array : data.array.scaled_array.ScaledArray\n        The 2D array of data which is plotted.\n    as_subplot : bool\n        Whether the array is plotted as part of a subplot, in which case the grid figure is not opened / closed.\n    units : str\n        The units of the y / x axis of the plots, in arc-seconds ('arcsec') or kiloparsecs ('kpc').\n    kpc_per_arcsec : float or None\n        The conversion factor between arc-seconds and kiloparsecs, required to plot the units in kpc.\n    figsize : (int, int)\n        The size of the figure in (rows, columns).\n    aspect : str\n        The aspect ratio of the array, specifically whether it is forced to be square ('equal') or adapts its size to \\\n        the figure size ('auto').\n    cmap : str\n        The colormap the array is plotted using, which may be chosen from the standard matplotlib colormaps.\n    norm : str\n        The normalization of the colormap used to plot the image, specifically whether it is linear ('linear'), log \\\n        ('log') or a symmetric log normalization ('symmetric_log').\n    norm_min : float or None\n        The minimum array value the colormap map spans (all values below this value are plotted the same color).\n    norm_max : float or None\n        The maximum array value the colormap map spans (all values above this value are plotted the same color).\n    linthresh : float\n        For the 'symmetric_log' colormap normalization ,this specifies the range of values within which the colormap \\\n        is linear.\n    linscale : float\n        For the 'symmetric_log' colormap normalization, this allowws the linear range set by linthresh to be stretched \\\n        relative to the logarithmic range.\n    xticks_manual :  [] or None\n        If input, the xticks do not use the array's default xticks but instead overwrite them as these values.\n    yticks_manual :  [] or None\n        If input, the yticks do not use the array's default yticks but instead overwrite them as these values.", "id": "f5985:m1"}
{"signature": "def getInhibitionRadius(self):", "body": "return self._inhibitionRadius<EOL>", "docstring": ":returns: (int) the inhibition radius", "id": "f17561:c4:m17"}
{"signature": "def get_initial_acl(self):", "body": "return None<EOL>", "docstring": "ACL system: If you define this function, you must return\n        all the 'event' names that you want your User (the established\n        virtual Socket) to have access to.\n\n        If you do not define this function, the user will have free\n        access to all of the ``on_*()`` and ``recv_*()`` functions,\n        etc.. methods.\n\n        Return something like: ``set(['recv_connect', 'on_public_method'])``\n\n        You can later modify this list dynamically (inside\n        ``on_connect()`` for example) using:\n\n        .. code-block:: python\n\n           self.add_acl_method('on_secure_method')\n\n        ``self.request`` is available in here, if you're already ready to\n        do some auth. check.\n\n        The ACLs are checked by the :meth:`process_packet` and/or\n        :meth:`process_event` default implementations, before calling\n        the class's methods.\n\n        **Beware**, returning ``None`` leaves the namespace completely\n        accessible.\n\n        The methods that are open are stored in the ``allowed_methods``\n        attribute of the ``Namespace`` instance.", "id": "f11816:c0:m5"}
{"signature": "@staticmethod<EOL><INDENT>def _cli_add_write_bel_namespace(main: click.Group) -> click.Group:<DEDENT>", "body": "return add_cli_write_bel_namespace(main)<EOL>", "docstring": "Add the write BEL namespace command.", "id": "f1579:c1:m28"}
{"signature": "def prevent_locking(self):", "body": "if self.rev is not None:<EOL><INDENT>return True<EOL><DEDENT>return False<EOL>", "docstring": "Return True, to prevent locking.\n\n        See prevent_locking()", "id": "f8623:c0:m17"}
{"signature": "def set_offsets(self, xy):", "body": "self.x = xy[:,<NUM_LIT:0>]<EOL>self.y = xy[:,<NUM_LIT:1>]<EOL>x,y,u,v = delete_masked_points(self.x.ravel(), self.y.ravel(), self.u,<EOL>self.v)<EOL>xy = np.hstack((x[:,np.newaxis], y[:,np.newaxis]))<EOL>collections.PolyCollection.set_offsets(self, xy)<EOL>", "docstring": "Set the offsets for the barb polygons.  This saves the offets passed in\nand actually sets version masked as appropriate for the existing U/V\ndata. *offsets* should be a sequence.\n\nACCEPTS: sequence of pairs of floats", "id": "f17167:c2:m5"}
{"signature": "def ch(c):", "body": "<EOL>istdout.info(c)<EOL>", "docstring": "print one or more characters without a newline at the end\n\n    example --\n        for x in range(1000):\n            echo.ch(\".\")\n\n    c -- string -- the chars that will be output", "id": "f6248:m6"}
{"signature": "def __repr__(self):", "body": "return \"<STR_LIT>\" % (self.__class__.__name__, self.__nupicJobID)<EOL>", "docstring": "Parameters:\n----------------------------------------------------------------------\nretval:         representation of this _NupicJob instance", "id": "f17598:c4:m1"}
{"signature": "@property<EOL><INDENT>def angular_velocity(self):<DEDENT>", "body": "return np.array(self.ode_body.getAngularVel())<EOL>", "docstring": "Current angular velocity of this body (in world coordinates).", "id": "f14887:c1:m13"}
{"signature": "def _partition_data(datavol, roivol, roivalue, maskvol=None, zeroe=True):", "body": "if maskvol is not None:<EOL><INDENT>indices = (roivol == roivalue) * (maskvol > <NUM_LIT:0>)<EOL><DEDENT>else:<EOL><INDENT>indices = roivol == roivalue<EOL><DEDENT>if datavol.ndim == <NUM_LIT:4>:<EOL><INDENT>ts = datavol[indices, :]<EOL><DEDENT>else:<EOL><INDENT>ts = datavol[indices]<EOL><DEDENT>if zeroe:<EOL><INDENT>if datavol.ndim == <NUM_LIT:4>:<EOL><INDENT>ts = ts[ts.sum(axis=<NUM_LIT:1>) != <NUM_LIT:0>, :]<EOL><DEDENT><DEDENT>return ts<EOL>", "docstring": "Extracts the values in `datavol` that are in the ROI with value `roivalue` in `roivol`.\n    The ROI can be masked by `maskvol`.\n\n    Parameters\n    ----------\n    datavol: numpy.ndarray\n        4D timeseries volume or a 3D volume to be partitioned\n\n    roivol: numpy.ndarray\n        3D ROIs volume\n\n    roivalue: int or float\n        A value from roivol that represents the ROI to be used for extraction.\n\n    maskvol: numpy.ndarray\n        3D mask volume\n\n    zeroe: bool\n        If true will remove the null timeseries voxels.  Only applied to timeseries (4D) data.\n\n    Returns\n    -------\n    values: np.array\n        An array of the values in the indicated ROI.\n        A 2D matrix if `datavol` is 4D or a 1D vector if `datavol` is 3D.", "id": "f4088:m11"}
{"signature": "def get_ip_number(self):", "body": "return self._ip_num<EOL>", "docstring": "Return the number of usable IP addresses.", "id": "f3601:c3:m11"}
{"signature": "def select(self):", "body": "if sys.platform=='<STR_LIT:win32>':<EOL><INDENT>self.dc.SelectObject(self.bitmap)<EOL>self.IsSelected = True<EOL><DEDENT>", "docstring": "Select the current bitmap into this wxDC instance", "id": "f17227:c2:m1"}
{"signature": "def read_and_discard_input(environ):", "body": "if environ.get(\"<STR_LIT>\") or environ.get(\"<STR_LIT>\"):<EOL><INDENT>return<EOL><DEDENT>cl = get_content_length(environ)<EOL>assert cl >= <NUM_LIT:0><EOL>if cl == <NUM_LIT:0>:<EOL><INDENT>return<EOL><DEDENT>READ_ALL = True<EOL>environ[\"<STR_LIT>\"] = <NUM_LIT:1><EOL>if READ_ALL:<EOL><INDENT>environ[\"<STR_LIT>\"] = <NUM_LIT:1><EOL><DEDENT>wsgi_input = environ[\"<STR_LIT>\"]<EOL>if hasattr(wsgi_input, \"<STR_LIT>\") and hasattr(wsgi_input, \"<STR_LIT>\"):<EOL><INDENT>if wsgi_input._consumed == <NUM_LIT:0> and wsgi_input.length > <NUM_LIT:0>:<EOL><INDENT>if READ_ALL:<EOL><INDENT>n = wsgi_input.length<EOL><DEDENT>else:<EOL><INDENT>n = <NUM_LIT:1><EOL><DEDENT>body = wsgi_input.read(n)<EOL>_logger.debug(<EOL>\"<STR_LIT>\".format(<EOL>n, body[:<NUM_LIT:50>]<EOL>)<EOL>)<EOL><DEDENT><DEDENT>elif hasattr(wsgi_input, \"<STR_LIT>\") and hasattr(wsgi_input._sock, \"<STR_LIT>\"):<EOL><INDENT>try:<EOL><INDENT>sock = wsgi_input._sock<EOL>timeout = sock.gettimeout()<EOL>sock.settimeout(<NUM_LIT:0>)<EOL>try:<EOL><INDENT>if READ_ALL:<EOL><INDENT>n = cl<EOL><DEDENT>else:<EOL><INDENT>n = <NUM_LIT:1><EOL><DEDENT>body = wsgi_input.read(n)<EOL>_logger.debug(<EOL>\"<STR_LIT>\".format(<EOL>n, body[:<NUM_LIT:50>]<EOL>)<EOL>)<EOL><DEDENT>except socket.error as se:<EOL><INDENT>_logger.error(\"<STR_LIT>\".format(n, se))<EOL><DEDENT>sock.settimeout(timeout)<EOL><DEDENT>except Exception:<EOL><INDENT>_logger.error(\"<STR_LIT>\".format(sys.exc_info()))<EOL><DEDENT><DEDENT>", "docstring": "Read 1 byte from wsgi.input, if this has not been done yet.\n\n    Returning a response without reading from a request body might confuse the\n    WebDAV client.\n    This may happen, if an exception like '401 Not authorized', or\n    '500 Internal error' was raised BEFORE anything was read from the request\n    stream.\n\n    See GC issue 13, issue 23\n    See http://groups.google.com/group/paste-users/browse_frm/thread/fc0c9476047e9a47?hl=en\n\n    Note that with persistent sessions (HTTP/1.1) we must make sure, that the\n    'Connection: closed' header is set with the response, to prevent reusing\n    the current stream.", "id": "f8588:m22"}
{"signature": "@abstractmethod<EOL><INDENT>def close(self):<DEDENT>", "body": "pass<EOL>", "docstring": "Close the channell immediately, so it won't expect more events.", "id": "f15266:c3:m11"}
{"signature": "def push_job(self, fun, *args, **kwargs):", "body": "assert callable(fun)<EOL>return self.put((fun, args, kwargs), block=True)<EOL>", "docstring": "put job if possible, non-blocking\n:param fun:\n:param args:\n:param kwargs:\n:return:", "id": "f6356:c0:m4"}
{"signature": "@abstractmethod<EOL><INDENT>def emitPeriodicMetrics(self, metrics):<DEDENT>", "body": "", "docstring": "Emits periodic metrics to stdout in JSON.\n\n        :param metrics: A list of metrics as returned by\n              :meth:`nupic.frameworks.opf.opf_task_driver.OPFTaskDriver.getMetrics`.", "id": "f17681:c0:m0"}
{"signature": "@property<EOL><INDENT>def DocumentRange(self) -> TextRange:<DEDENT>", "body": "return TextRange(self.pattern.DocumentRange)<EOL>", "docstring": "Property DocumentRange.\nCall IUIAutomationTextPattern::get_DocumentRange.\nReturn `TextRange`, a text range that encloses the main text of a document.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationtextpattern-get_documentrange", "id": "f1782:c70:m1"}
{"signature": "@instruction<EOL><INDENT>def JECXZ(cpu, target):<DEDENT>", "body": "cpu.PC = Operators.ITEBV(cpu.address_bit_size, cpu.ECX == <NUM_LIT:0>, target.read(), cpu.PC)<EOL>", "docstring": "Jumps short if ECX register is 0.\n\n:param cpu: current CPU.\n:param target: destination operand.", "id": "f16975:c2:m120"}
{"signature": "def create_course_completion(self, user_id, payload):", "body": "url = self.enterprise_configuration.sapsf_base_url + self.global_sap_config.completion_status_api_path<EOL>return self._call_post_with_user_override(user_id, url, payload)<EOL>", "docstring": "Send a completion status payload to the SuccessFactors OCN Completion Status endpoint\n\nArgs:\n    user_id (str): The sap user id that the completion status is being sent for.\n    payload (str): JSON encoded object (serialized from SapSuccessFactorsLearnerDataTransmissionAudit)\n        containing completion status fields per SuccessFactors documentation.\n\nReturns:\n    The body of the response from SAP SuccessFactors, if successful\nRaises:\n    HTTPError: if we received a failure response code from SAP SuccessFactors", "id": "f16189:c0:m3"}
{"signature": "def _use_accelerator(state):", "body": "global _convert_agg_to_wx_image<EOL>global _convert_agg_to_wx_bitmap<EOL>if getattr(wx, '<STR_LIT>', '<STR_LIT>')[<NUM_LIT:0>:<NUM_LIT:3>] < '<STR_LIT>':<EOL><INDENT>if state and _wxagg is not None:<EOL><INDENT>_convert_agg_to_wx_image  = _wxagg.convert_agg_to_wx_image<EOL>_convert_agg_to_wx_bitmap = _wxagg.convert_agg_to_wx_bitmap<EOL><DEDENT>else:<EOL><INDENT>_convert_agg_to_wx_image  = _py_convert_agg_to_wx_image<EOL>_convert_agg_to_wx_bitmap = _py_convert_agg_to_wx_bitmap<EOL><DEDENT><DEDENT>else:<EOL><INDENT>_convert_agg_to_wx_image  = _py_WX28_convert_agg_to_wx_image<EOL>_convert_agg_to_wx_bitmap = _py_WX28_convert_agg_to_wx_bitmap<EOL><DEDENT>", "docstring": "Enable or disable the WXAgg accelerator, if it is present and is also\ncompatible with whatever version of wxPython is in use.", "id": "f17220:m7"}
{"signature": "def trigger(self, *args):", "body": "self.task_spec._on_trigger(self, *args)<EOL>", "docstring": "If recursive is True, the state is applied to the tree recursively.", "id": "f7734:c0:m38"}
{"signature": "def stack_size(size=None):", "body": "if size is not None:<EOL><INDENT>raise error(\"<STR_LIT>\")<EOL><DEDENT>return <NUM_LIT:0><EOL>", "docstring": "Dummy implementation of thread.stack_size().", "id": "f16467:m4"}
{"signature": "def get_pplan(self, topologyName, callback=None):", "body": "if callback:<EOL><INDENT>self.pplan_watchers[topologyName].append(callback)<EOL><DEDENT>else:<EOL><INDENT>pplan_path = self.get_pplan_path(topologyName)<EOL>with open(pplan_path) as f:<EOL><INDENT>data = f.read()<EOL>pplan = PhysicalPlan()<EOL>pplan.ParseFromString(data)<EOL>return pplan<EOL><DEDENT><DEDENT>", "docstring": "Get physical plan of a topology", "id": "f7422:c0:m9"}
{"signature": "def inserttext(self, window_name, object_name, position, data):", "body": "object_handle = self._get_object_handle(window_name, object_name)<EOL>if not object_handle.AXEnabled:<EOL><INDENT>raise LdtpServerException(u\"<STR_LIT>\" % object_name)<EOL><DEDENT>existing_data = object_handle.AXValue<EOL>size = len(existing_data)<EOL>if position < <NUM_LIT:0>:<EOL><INDENT>position = <NUM_LIT:0><EOL><DEDENT>if position > size:<EOL><INDENT>position = size<EOL><DEDENT>object_handle.AXValue = existing_data[:position] + data +existing_data[position:]<EOL>return <NUM_LIT:1><EOL>", "docstring": "Insert string sequence in given position.\n\n@param window_name: Window name to type in, either full name,\nLDTP's name convention, or a Unix glob.\n@type window_name: string\n@param object_name: Object name to type in, either full name,\nLDTP's name convention, or a Unix glob. \n@type object_name: string\n@param position: position where text has to be entered.\n@type data: int\n@param data: data to type.\n@type data: string\n\n@return: 1 on success.\n@rtype: integer", "id": "f10324:c0:m6"}
{"signature": "def fail(self, message, param=None, ctx=None):", "body": "raise BadParameter(message, ctx=ctx, param=param)<EOL>", "docstring": "Helper method to fail with an invalid value message.", "id": "f8334:c0:m5"}
{"signature": "def __init__(self, *args, **kwargs):", "body": "super(Conversations, self).__init__(*args, **kwargs)<EOL>self.endpoint = '<STR_LIT>'<EOL>self.conversation_id = None<EOL>self.messages = ConversationMessages(self)<EOL>", "docstring": "Initialize the endpoint", "id": "f272:c0:m0"}
{"signature": "def get_response(self, resp):", "body": "try:<EOL><INDENT>return resp.json()<EOL><DEDENT>except JSON_ERROR:<EOL><INDENT>return {}<EOL><DEDENT>", "docstring": "Retrieve response as json and deserialize as dict\n\nParameters\n----------\nresp: requests.models.Response", "id": "f4452:c1:m1"}
{"signature": "def __init__(self, need_global_best=False, need_population_best=False, need_mean_ff=False):", "body": "<EOL>self._global_best_result = {'<STR_LIT>': [], '<STR_LIT>': []}<EOL>self._best_population_result = {'<STR_LIT>': [], '<STR_LIT>': []}<EOL>self._mean_ff_result = []<EOL>self._need_global_best = need_global_best<EOL>self._need_population_best = need_population_best<EOL>self._need_mean_ff = need_mean_ff<EOL>", "docstring": "!\n        @brief Constructs genetic algorithm observer to collect specific information.\n\n        @param[in] need_global_best (bool): If 'True' then the best chromosomes and its fitness function value (global optimum) for each iteration are stored.\n        @param[in] need_population_best (bool): If 'True' then current (on each iteration) best chromosomes and its fitness function value (local optimum) are stored.\n        @param[in] need_mean_ff (bool): If 'True' then average value of fitness function on each iteration is stored.", "id": "f15596:c0:m0"}
{"signature": "def _move(self, index, new_priority):", "body": "item, old_priority = self._memory[index]<EOL>old_priority = old_priority or <NUM_LIT:0><EOL>self._memory[index] = _SumRow(item, new_priority)<EOL>self._update_internal_nodes(index, new_priority - old_priority)<EOL>", "docstring": "Change the priority of a leaf node.", "id": "f14354:c0:m3"}
{"signature": "def sign(self, account=None, **kwargs):", "body": "if not account:<EOL><INDENT>if \"<STR_LIT>\" in self.blockchain.config:<EOL><INDENT>account = self.blockchain.config[\"<STR_LIT>\"]<EOL><DEDENT><DEDENT>if not account:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>account = self.account_class(account, blockchain_instance=self.blockchain)<EOL>wif = self.blockchain.wallet.getPrivateKeyForPublicKey(<EOL>account[\"<STR_LIT>\"][\"<STR_LIT>\"]<EOL>)<EOL>payload = [<EOL>\"<STR_LIT>\",<EOL>account[\"<STR_LIT:name>\"],<EOL>\"<STR_LIT:key>\",<EOL>account[\"<STR_LIT>\"][\"<STR_LIT>\"],<EOL>\"<STR_LIT:time>\",<EOL>str(datetime.utcnow()),<EOL>\"<STR_LIT:text>\",<EOL>self.message,<EOL>]<EOL>enc_message = json.dumps(payload, separators=(\"<STR_LIT:U+002C>\", \"<STR_LIT::>\"))<EOL>signature = hexlify(sign_message(enc_message, wif)).decode(\"<STR_LIT:ascii>\")<EOL>return dict(signed=enc_message, payload=payload, signature=signature)<EOL>", "docstring": "Sign a message with an account's memo key\n\n            :param str account: (optional) the account that owns the bet\n                (defaults to ``default_account``)\n            :raises ValueError: If not account for signing is provided\n\n            :returns: the signed message encapsulated in a known format", "id": "f8247:c1:m1"}
{"signature": "def __rshift__(self, other):", "body": "return np.right_shift(self, other)<EOL>", "docstring": "x.__rshift__(y) <==> x>>y", "id": "f4853:c0:m26"}
{"signature": "@yaz.task<EOL><INDENT>def main(self, pos1: str, pos_or_key1: str = \"<STR_LIT>\", *, key1: str = \"<STR_LIT>\", extra1: str = \"<STR_LIT>\", **kwargs):<DEDENT>", "body": "return super().main(pos1, pos_or_key1=pos_or_key1, key1=key1, **kwargs) + [extra1]<EOL>", "docstring": "required from water-custom-plugin", "id": "f5608:c6:m0"}
{"signature": "def fetch(self, cluster, metric, topology, component, instance, timerange, envirn=None):", "body": "pass<EOL>", "docstring": ":param cluster:\n:param metric:\n:param topology:\n:param component:\n:param instance:\n:param timerange:\n:param envirn:\n:return:", "id": "f7411:c0:m0"}
{"signature": "def get_task(benchmark, env_id):", "body": "return next(filter(lambda task: task['<STR_LIT>'] == env_id, benchmark['<STR_LIT>']), None)<EOL>", "docstring": "Get a task by env_id. Return None if the benchmark doesn't have the env", "id": "f1300:m3"}
{"signature": "def create_nginx_config(self):", "body": "cfg = '<STR_LIT>'.format(self._project_name)<EOL>if not self._shared_hosting:<EOL><INDENT>if self._user:<EOL><INDENT>cfg += '<STR_LIT>'.format(self._user)<EOL><DEDENT>cfg += '<STR_LIT>'.format(os.path.join(self._log_dir,self._project_name), os.path.join(self._var_dir, self._project_name))<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>if self._include_mimetypes:<EOL><INDENT>cfg += '<STR_LIT>'<EOL><DEDENT>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'.format(os.path.join(self._log_dir, self._project_name))<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL><DEDENT>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'.format(self._port)<EOL>if self._server_name:<EOL><INDENT>cfg += '<STR_LIT>'.format(self._server_name)<EOL><DEDENT>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'.format(os.path.join(self._var_dir, self._project_name))<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>cfg += '<STR_LIT>'<EOL>if not self._shared_hosting:<EOL><INDENT>cfg += '<STR_LIT>'<EOL><DEDENT>f = open(self._nginx_config, '<STR_LIT:w>')<EOL>f.write(cfg)<EOL>f.close()<EOL>", "docstring": "Creates the Nginx configuration for the project", "id": "f5485:c0:m10"}
{"signature": "@view_config(route_name='<STR_LIT>', request_method='<STR_LIT:GET>',<EOL>renderer='<STR_LIT>',<EOL>permission='<STR_LIT>', http_cache=<NUM_LIT:0>)<EOL>def admin_print_styles_single(request):", "body": "style = request.matchdict['<STR_LIT>']<EOL>with db_connect(cursor_factory=DictCursor) as db_conn:<EOL><INDENT>with db_conn.cursor() as cursor:<EOL><INDENT>if style != '<STR_LIT>':<EOL><INDENT>cursor.execute(\"\"\"<STR_LIT>\"\"\", vars=(style,))<EOL>info = cursor.fetchall()<EOL>if len(info) < <NUM_LIT:1>:<EOL><INDENT>current_recipe = None<EOL>recipe_type = None<EOL>status = None<EOL><DEDENT>else:<EOL><INDENT>current_recipe = info[<NUM_LIT:0>]['<STR_LIT>']<EOL>recipe_type = info[<NUM_LIT:0>]['<STR_LIT>']<EOL>status = '<STR_LIT>'<EOL><DEDENT>cursor.execute(\"\"\"<STR_LIT>\"\"\", vars=(style,))<EOL><DEDENT>else:<EOL><INDENT>current_recipe = '<STR_LIT>'<EOL>recipe_type = '<STR_LIT>'<EOL>cursor.execute(\"\"\"<STR_LIT>\"\"\", vars=(style,))<EOL>status = '<STR_LIT>'<EOL><DEDENT>collections = []<EOL>for row in cursor.fetchall():<EOL><INDENT>recipe = row['<STR_LIT>']<EOL>if (status != '<STR_LIT>' and<EOL>current_recipe is not None and<EOL>recipe != current_recipe):<EOL><INDENT>status = '<STR_LIT>'<EOL><DEDENT>collections.append({<EOL>'<STR_LIT:title>': row['<STR_LIT:name>'].decode('<STR_LIT:utf-8>'),<EOL>'<STR_LIT>': row['<STR_LIT>'],<EOL>'<STR_LIT>': row['<STR_LIT>'],<EOL>'<STR_LIT>': row['<STR_LIT>'],<EOL>'<STR_LIT>': request.route_path('<STR_LIT>',<EOL>hash=row['<STR_LIT>']),<EOL>'<STR_LIT>': row['<STR_LIT>'],<EOL>'<STR_LIT>': row['<STR_LIT>'],<EOL>'<STR_LIT>': request.route_path('<STR_LIT>',<EOL>ident_hash=row['<STR_LIT>']),<EOL>'<STR_LIT:status>': status,<EOL>'<STR_LIT>': request.route_path(<EOL>'<STR_LIT>', uuid=row['<STR_LIT>']),<EOL>})<EOL><DEDENT><DEDENT><DEDENT>return {'<STR_LIT>': len(collections),<EOL>'<STR_LIT>': collections,<EOL>'<STR_LIT>': style,<EOL>'<STR_LIT>': recipe_type}<EOL>", "docstring": "Returns all books with any version of the given print style.\n\n    Returns the print_style, recipe type, num books using the print_style,\n    along with a dictionary of the book, author, revision date, recipe,\n    tag of the print_style, and a link to the content.", "id": "f15214:m1"}
{"signature": "def init_yaml_constructor():", "body": "def utf_encoding_string_constructor(loader, node):<EOL><INDENT>return loader.construct_scalar(node).encode('<STR_LIT:utf-8>')<EOL><DEDENT>yaml.SafeLoader.add_constructor(u'<STR_LIT>', utf_encoding_string_constructor)<EOL>", "docstring": "This dark magic is used to make yaml.safe_load encode all strings as utf-8,\nwhere otherwise python unicode strings would be returned for non-ascii chars", "id": "f3207:m3"}
{"signature": "@task<EOL>def printoptions():", "body": "x = json.dumps(environment.options,<EOL>indent=<NUM_LIT:4>,<EOL>sort_keys=True,<EOL>skipkeys=True,<EOL>cls=MyEncoder)<EOL>print(x)<EOL>", "docstring": "print paver options.\n\n    Prettified by json.\n    `long_description` is removed", "id": "f3727:m1"}
{"signature": "def _seed(self, seed=-<NUM_LIT:1>):", "body": "if seed != -<NUM_LIT:1>:<EOL><INDENT>self.random = NupicRandom(seed)<EOL><DEDENT>else:<EOL><INDENT>self.random = NupicRandom()<EOL><DEDENT>", "docstring": "Initialize the random seed", "id": "f17552:c0:m2"}
{"signature": "def get_zip_class():", "body": "class ContextualZipFile(zipfile.ZipFile):<EOL><INDENT>def __enter__(self):<EOL><INDENT>return self<EOL><DEDENT>def __exit__(self, type, value, traceback):<EOL><INDENT>self.close<EOL><DEDENT><DEDENT>return zipfile.ZipFile if hasattr(zipfile.ZipFile, '<STR_LIT>') elseContextualZipFile<EOL>", "docstring": "Supplement ZipFile class to support context manager for Python 2.6", "id": "f5454:m3"}
{"signature": "def getLocalizedName(self):", "body": "return self._getLocalizedName()<EOL>", "docstring": "Return the localized name of the application.", "id": "f10331:c1:m13"}
{"signature": "def get_neurommsig_scores(graph: BELGraph,<EOL>genes: List[Gene],<EOL>annotation: str = '<STR_LIT>',<EOL>ora_weight: Optional[float] = None,<EOL>hub_weight: Optional[float] = None,<EOL>top_percent: Optional[float] = None,<EOL>topology_weight: Optional[float] = None,<EOL>preprocess: bool = False<EOL>) -> Optional[Mapping[str, float]]:", "body": "if preprocess:<EOL><INDENT>graph = neurommsig_graph_preprocessor.run(graph)<EOL><DEDENT>if not any(gene in graph for gene in genes):<EOL><INDENT>logger.debug('<STR_LIT>')<EOL>return<EOL><DEDENT>subgraphs = get_subgraphs_by_annotation(graph, annotation=annotation)<EOL>return get_neurommsig_scores_prestratified(<EOL>subgraphs=subgraphs,<EOL>genes=genes,<EOL>ora_weight=ora_weight,<EOL>hub_weight=hub_weight,<EOL>top_percent=top_percent,<EOL>topology_weight=topology_weight,<EOL>)<EOL>", "docstring": "Preprocess the graph, stratify by the given annotation, then run the NeuroMMSig algorithm on each.\n\n    :param graph: A BEL graph\n    :param genes: A list of gene nodes\n    :param annotation: The annotation to use to stratify the graph to subgraphs\n    :param ora_weight: The relative weight of the over-enrichment analysis score from\n     :py:func:`neurommsig_gene_ora`. Defaults to 1.0.\n    :param hub_weight: The relative weight of the hub analysis score from :py:func:`neurommsig_hubs`.\n     Defaults to 1.0.\n    :param top_percent: The percentage of top genes to use as hubs. Defaults to 5% (0.05).\n    :param topology_weight: The relative weight of the topolgical analysis core from\n     :py:func:`neurommsig_topology`. Defaults to 1.0.\n    :param preprocess: If true, preprocess the graph.\n    :return: A dictionary from {annotation value: NeuroMMSig composite score}\n\n    Pre-processing steps:\n\n    1. Infer the central dogma with :func:``\n    2. Collapse all proteins, RNAs and miRNAs to genes with :func:``\n    3. Collapse variants to genes with :func:``", "id": "f9408:m0"}
{"signature": "def assert_amnesty(self, input_code, errors, expected):", "body": "input_code = textwrap.dedent(input_code)<EOL>expected = textwrap.dedent(expected)<EOL>errors_by_line = defaultdict(list)<EOL>for error in errors:<EOL><INDENT>errors_by_line[error.linenum].append(error)<EOL><DEDENT>output_lines = itertools.chain.from_iterable(<EOL>fix_pylint(line, errors_by_line[lineno])<EOL>for lineno, line<EOL>in enumerate(StringIO(input_code), start=<NUM_LIT:1>)<EOL>)<EOL>self.assertEqual(expected.split(u'<STR_LIT:\\n>'), \"<STR_LIT>\".join(output_lines).split(u'<STR_LIT:\\n>'))<EOL>", "docstring": "Assert that fix_pylint produces ``expected`` when fed ``input_code`` and the\nlist of errors ``errors``.\n\nArguments:\n    input_code: A string of python code. Will be textwrap.dedented.\n    errors: A list of PylintErrors\n    expected: A string of python code. Will be textwrap.dedented.", "id": "f1688:c0:m0"}
{"signature": "def __init__(self, pattern=None):", "body": "self.pattern = pattern<EOL>", "docstring": "Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nn-uiautomationclient-iuiautomationrangevaluepattern", "id": "f1782:c56:m0"}
{"signature": "def scalar_read(self, address, block_count, data_file, meta_file):", "body": "cmd = [\"<STR_LIT>\", \"<STR_LIT>\", self.envs, \"<STR_LIT>\".format(address),<EOL>\"<STR_LIT>\".format(block_count - <NUM_LIT:1>), \"<STR_LIT>\".format(data_file),<EOL>\"<STR_LIT>\".format(meta_file),<EOL>\"<STR_LIT>\".format(block_count * self.get_env(\"<STR_LIT>\", \"<STR_LIT>\")),<EOL>\"<STR_LIT>\".format(block_count * self.get_env(\"<STR_LIT>\", \"<STR_LIT>\"))]<EOL>status, _, _ = cij.ssh.command(cmd, shell=True)<EOL>return status<EOL>", "docstring": "nvme read", "id": "f13332:c0:m18"}
{"signature": "def apply(f, obj, *args, **kwargs):", "body": "return vectorize(f)(obj, *args, **kwargs)<EOL>", "docstring": "Apply a function in parallel to each element of the input", "id": "f4855:m22"}
{"signature": "def printOneTrainingVector(x):", "body": "print('<STR_LIT>'.join('<STR_LIT:1>' if k != <NUM_LIT:0> else '<STR_LIT:.>' for k in x))<EOL>", "docstring": "Print a single vector succinctly.", "id": "f17347:m0"}
{"signature": "@classmethod<EOL><INDENT>def create_option_parser(cls):<DEDENT>", "body": "return OptionParser(<EOL>usage=(\"<STR_LIT>\"<EOL>\"<STR_LIT>\"),<EOL>version=\"<STR_LIT>\" % (cls.get_version()))<EOL>", "docstring": "Override in subclass if required.", "id": "f7709:c0:m17"}
{"signature": "def align_bulge_disk_phi_tag_from_align_bulge_disk_phi(align_bulge_disk_phi):", "body": "if not align_bulge_disk_phi:<EOL><INDENT>return '<STR_LIT>'<EOL><DEDENT>elif align_bulge_disk_phi:<EOL><INDENT>return '<STR_LIT>'<EOL><DEDENT>", "docstring": "Generate a tag for if the bulge and disk of a bulge-disk system are aligned or not, to customize phase names \\\n    based on the bulge-disk model. This changes the phase name 'phase_name' as follows:\n\n    bd_align_phi = False -> phase_name\n    bd_align_phi = True -> phase_name_bd_align_phi", "id": "f5966:m11"}
{"signature": "def teardown(self):", "body": "<EOL>", "docstring": "Runs after every method to clean up previous testing.", "id": "f13659:c0:m1"}
{"signature": "def XMLfromPython(xmlObj,saveAs=False):", "body": "return<EOL>", "docstring": "given a an XML object, return XML string.\noptionally, save it to disk.", "id": "f11406:m16"}
{"signature": "def raise_disconnection():", "body": "raise TLSDisconnectError('<STR_LIT>')<EOL>", "docstring": "Raises a TLSDisconnectError due to a disconnection\n\n:raises:\n    TLSDisconnectError", "id": "f9524:m16"}
{"signature": "def exception_handler( type, value, tb ):", "body": "msg = '<STR_LIT>'<EOL>if hasattr(value, '<STR_LIT:filename>') and value.filename != None:<EOL><INDENT>msg = value.filename + '<STR_LIT>'<EOL><DEDENT>if hasattr(value, '<STR_LIT>') and value.strerror != None:<EOL><INDENT>msg += value.strerror<EOL><DEDENT>else:<EOL><INDENT>msg += str(value)<EOL><DEDENT>if len( msg ) : error_msg_qt( msg )<EOL>", "docstring": "Handle uncaught exceptions\n    It does not catch SystemExit", "id": "f17217:m6"}
{"signature": "@path_required<EOL><INDENT>def create_package(self, path=None, name=None, mode=None):<DEDENT>", "body": "<EOL>assert mode in (None, '<STR_LIT:w>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'), '<STR_LIT>'%str(mode)<EOL>if mode is None:<EOL><INDENT>mode = '<STR_LIT>'<EOL><DEDENT>if path is None:<EOL><INDENT>root = os.path.split(self.__path)[<NUM_LIT:0>]<EOL><DEDENT>elif path.strip() in ('<STR_LIT>','<STR_LIT:.>'):<EOL><INDENT>root = os.getcwd()<EOL><DEDENT>else:<EOL><INDENT>root = os.path.realpath( os.path.expanduser(path) )<EOL><DEDENT>assert os.path.isdir(root), '<STR_LIT>'%path<EOL>if name is None:<EOL><INDENT>ext = mode.split(\"<STR_LIT::>\")<EOL>if len(ext) == <NUM_LIT:2>:<EOL><INDENT>if len(ext[<NUM_LIT:1>]):<EOL><INDENT>ext = \"<STR_LIT:.>\"+ext[<NUM_LIT:1>]<EOL><DEDENT>else:<EOL><INDENT>ext = '<STR_LIT>'<EOL><DEDENT><DEDENT>else:<EOL><INDENT>ext = '<STR_LIT>'<EOL><DEDENT>name = os.path.split(self.__path)[<NUM_LIT:1>]+ext<EOL><DEDENT>tarfilePath = os.path.join(root, name)<EOL>try:<EOL><INDENT>tarHandler = tarfile.TarFile.open(tarfilePath, mode=mode)<EOL><DEDENT>except Exception as e:<EOL><INDENT>raise Exception(\"<STR_LIT>\"%e)<EOL><DEDENT>for dpath in sorted(list(self.walk_directories_path(recursive=True))):<EOL><INDENT>t = tarfile.TarInfo( dpath )<EOL>t.type = tarfile.DIRTYPE<EOL>tarHandler.addfile(t)<EOL>tarHandler.add(os.path.join(self.__path,dpath,self.__dirInfo), arcname=self.__dirInfo)<EOL><DEDENT>for fpath in self.walk_files_path(recursive=True):<EOL><INDENT>relaPath, fname = os.path.split(fpath)<EOL>tarHandler.add(os.path.join(self.__path,fpath), arcname=fname)<EOL>tarHandler.add(os.path.join(self.__path,relaPath,self.__fileInfo%fname), arcname=self.__fileInfo%fname)<EOL>tarHandler.add(os.path.join(self.__path,relaPath,self.__fileClass%fname), arcname=self.__fileClass%fname)<EOL><DEDENT>tarHandler.add(os.path.join(self.__path,self.__repoFile), arcname=\"<STR_LIT>\")<EOL>tarHandler.close()<EOL>", "docstring": "Create a tar file package of all the repository files and directories.\nOnly files and directories that are tracked in the repository\nare stored in the package tar file.\n\n**N.B. On some systems packaging requires root permissions.**\n\n:Parameters:\n    #. path (None, string): The real absolute path where to create the\n       package. If None, it will be created in the same directory as\n       the repository. If '.' or an empty string is passed, the current\n       working directory will be used.\n    #. name (None, string): The name to give to the package file\n       If None, the package directory name will be used with the\n       appropriate extension added.\n    #. mode (None, string): The writing mode of the tarfile.\n       If None, automatically the best compression mode will be chose.\n       Available modes are ('w', 'w:', 'w:gz', 'w:bz2')", "id": "f13917:c1:m33"}
{"signature": "def get_paginator(self):", "body": "return self.paginator<EOL>", "docstring": "Return pagination for our model", "id": "f967:c8:m1"}
{"signature": "def send(self):", "body": "context = {<EOL>\"<STR_LIT>\": app_settings.PASSWORD_RESET_URL.format(key=self.key)<EOL>}<EOL>email_utils.send_email(<EOL>context=context,<EOL>from_email=settings.DEFAULT_FROM_EMAIL,<EOL>recipient_list=[self.email.email],<EOL>subject=_(\"<STR_LIT>\"),<EOL>template_name=\"<STR_LIT>\",<EOL>)<EOL>logger.info(\"<STR_LIT>\", self.email)<EOL>", "docstring": "Send the password reset token to the user.", "id": "f4819:c2:m1"}
{"signature": "@property<EOL><INDENT>def previous_sending_chain_length(self):<DEDENT>", "body": "return self.__previous_sending_chain_length<EOL>", "docstring": "Get the length of the previous sending chain.\n\n:returns: Either an integer representing the length of the previous sending chain\n    or None, if the current one is the first sending chain.", "id": "f5432:c0:m4"}
{"signature": "def write_block(self, block_, body):", "body": "self.write('<STR_LIT>')<EOL>with self.indent_block():<EOL><INDENT>self.write('<STR_LIT>')<EOL>self.write('<STR_LIT>')<EOL>for checkpoint in block_.checkpoints:<EOL><INDENT>self.write_tmpl('<STR_LIT>', state=checkpoint)<EOL><DEDENT>self.write('<STR_LIT>')<EOL>self.write('<STR_LIT:}>')<EOL>with self.indent_block(-<NUM_LIT:1>):<EOL><INDENT>self.write(body)<EOL><DEDENT><DEDENT>self.write('<STR_LIT:}>')<EOL>", "docstring": "Outputs the boilerplate necessary for code blocks like functions.\n\n        Args:\n          block_: The Block object representing the code block.\n          body: String containing Go code making up the body of the code block.", "id": "f16382:c4:m4"}
{"signature": "def read(self, output_tile, **kwargs):", "body": "try:<EOL><INDENT>return ma.masked_values(<EOL>read_raster_no_crs(<EOL>self.get_path(output_tile), indexes=(<NUM_LIT:4> if self.old_band_num else <NUM_LIT:2>)<EOL>),<EOL><NUM_LIT:0><EOL>)<EOL><DEDENT>except FileNotFoundError:<EOL><INDENT>return self.empty(output_tile)<EOL><DEDENT>", "docstring": "Read existing process output.\n\nParameters\n----------\noutput_tile : ``BufferedTile``\n    must be member of output ``TilePyramid``\n\nReturns\n-------\nprocess output : ``BufferedTile`` with appended data", "id": "f12814:c0:m2"}
{"signature": "@decorator_util.jit()<EOL>def masked_sub_grid_1d_index_to_2d_sub_pixel_index_from_mask(mask, sub_grid_size):", "body": "total_sub_pixels = total_sub_pixels_from_mask_and_sub_grid_size(mask=mask, sub_grid_size=sub_grid_size)<EOL>sub_grid_to_sub_pixel = np.zeros(shape=(total_sub_pixels, <NUM_LIT:2>))<EOL>sub_pixel_count = <NUM_LIT:0><EOL>for y in range(mask.shape[<NUM_LIT:0>]):<EOL><INDENT>for x in range(mask.shape[<NUM_LIT:1>]):<EOL><INDENT>if not mask[y, x]:<EOL><INDENT>for y1 in range(sub_grid_size):<EOL><INDENT>for x1 in range(sub_grid_size):<EOL><INDENT>sub_grid_to_sub_pixel[sub_pixel_count, :] = (y*sub_grid_size)+y1, (x*sub_grid_size)+x1<EOL>sub_pixel_count += <NUM_LIT:1><EOL><DEDENT><DEDENT><DEDENT><DEDENT><DEDENT>return sub_grid_to_sub_pixel<EOL>", "docstring": "Compute a 1D array that maps every unmasked pixel to its corresponding 2d pixel using its (y,x) pixel indexes.\n\n    For howtolens if pixel [2,5] corresponds to the second pixel on the 1D array, grid_to_pixel[1] = [2,5]", "id": "f5994:m12"}
{"signature": "def __init__(self, ax, onselect, direction, minspan=None, useblit=False, rectprops=None, onmove_callback=None):", "body": "if rectprops is None:<EOL><INDENT>rectprops = dict(facecolor='<STR_LIT>', alpha=<NUM_LIT:0.5>)<EOL><DEDENT>assert direction in ['<STR_LIT>', '<STR_LIT>'], '<STR_LIT>'<EOL>self.direction = direction<EOL>self.ax = None<EOL>self.canvas = None<EOL>self.visible = True<EOL>self.cids=[]<EOL>self.rect = None<EOL>self.background = None<EOL>self.pressv = None<EOL>self.rectprops = rectprops<EOL>self.onselect = onselect<EOL>self.onmove_callback = onmove_callback<EOL>self.useblit = useblit<EOL>self.minspan = minspan<EOL>self.buttonDown = False<EOL>self.prev = (<NUM_LIT:0>, <NUM_LIT:0>)<EOL>self.new_axes(ax)<EOL>", "docstring": "Create a span selector in ax.  When a selection is made, clear\nthe span and call onselect with\n\n  onselect(vmin, vmax)\n\nand clear the span.\n\ndirection must be 'horizontal' or 'vertical'\n\nIf minspan is not None, ignore events smaller than minspan\n\nThe span rect is drawn with rectprops; default\n  rectprops = dict(facecolor='red', alpha=0.5)\n\nset the visible attribute to False if you want to turn off\nthe functionality of the span selector", "id": "f17235:c9:m0"}
{"signature": "def __getstate__(self):", "body": "state = self.__dict__.copy()<EOL>if '<STR_LIT>' in state:<EOL><INDENT>state['<STR_LIT>'] = None<EOL><DEDENT>return state<EOL>", "docstring": "To prevent excessive replication during deepcopy.", "id": "f15898:c0:m4"}
{"signature": "def get_ordering(self):", "body": "if self.__ordering is None:<EOL><INDENT>self.__ordering = []<EOL>for cluster in self.__clusters:<EOL><INDENT>for index_object in cluster:<EOL><INDENT>optics_object = self.__optics_objects[index_object]<EOL>if optics_object.reachability_distance is not None:<EOL><INDENT>self.__ordering.append(optics_object.reachability_distance)<EOL><DEDENT><DEDENT><DEDENT><DEDENT>return self.__ordering<EOL>", "docstring": "!\n        @brief Returns clustering ordering information about the input data set.\n        @details Clustering ordering of data-set contains the information about the internal clustering structure in line with connectivity radius.\n\n        @return (ordering_analyser) Analyser of clustering ordering.\n\n        @see process()\n        @see get_clusters()\n        @see get_noise()\n        @see get_radius()\n        @see get_optics_objects()", "id": "f15542:c3:m8"}
{"signature": "def poly2ac(poly, efinal):", "body": "results = rlevinson(poly, efinal)<EOL>return results[<NUM_LIT:0>]<EOL>", "docstring": "Convert prediction filter polynomial to autocorrelation sequence\n\n    :param array poly: the AR parameters\n    :param efinal: an estimate of the final error\n    :return: the autocorrelation  sequence in complex format.\n\n    .. doctest::\n\n        >>> from numpy import array\n        >>> from spectrum import poly2ac\n        >>> poly = [ 1. ,  0.38 , -0.05]\n        >>> efinal = 4.1895\n        >>> poly2ac(poly, efinal)\n        array([ 5.00+0.j, -2.00+0.j,  1.01-0.j])", "id": "f10925:m2"}
{"signature": "@classmethod<EOL><INDENT>def from_cursor_ref(self, cursor):<DEDENT>", "body": "return self.get(**cursor)<EOL>", "docstring": "Returns model instance from unique cursor reference", "id": "f967:c4:m5"}
{"signature": "def get_self(session, user_details=None):", "body": "<EOL>if user_details:<EOL><INDENT>user_details['<STR_LIT>'] = True<EOL><DEDENT>response = make_get_request(session, '<STR_LIT>', params_data=user_details)<EOL>json_data = response.json()<EOL>if response.status_code == <NUM_LIT:200>:<EOL><INDENT>return json_data['<STR_LIT:result>']<EOL><DEDENT>else:<EOL><INDENT>raise SelfNotRetrievedException(<EOL>message=json_data['<STR_LIT:message>'],<EOL>error_code=json_data['<STR_LIT>'],<EOL>request_id=json_data['<STR_LIT>']<EOL>)<EOL><DEDENT>", "docstring": "Get details about the currently authenticated user", "id": "f12288:m0"}
{"signature": "def html_synthese(fct, df):", "body": "html = str()<EOL>res_count = dict()<EOL>buf = StringIO()<EOL>polluant, res = fct(df)<EOL>html += '<STR_LIT>'.format(polluant)<EOL>for k, v in res.items():<EOL><INDENT>buf.write(\"<STR_LIT>\")<EOL>comp = compresse(v)<EOL>if not comp.empty:<EOL><INDENT>comp.index.name = k<EOL>comp.to_html(buf=buf,<EOL>sparsify=True,<EOL>na_rep=\"<STR_LIT>\")<EOL><DEDENT>else:<EOL><INDENT>buf.write(<EOL>'<STR_LIT>'.format(<EOL>k))<EOL><DEDENT>buf.write(\"<STR_LIT>\")<EOL>res_count[k] = v.count()<EOL><DEDENT>res_count = pd.DataFrame(res_count).T<EOL>res_count.index.name = \"<STR_LIT>\"<EOL>html += \"<STR_LIT>\"<EOL>html += res_count.to_html(sparsify=True)<EOL>html += \"<STR_LIT>\"<EOL>html += buf.getvalue()<EOL>return html<EOL>", "docstring": "Retourne au format html une synth\u00e8se des calculs r\u00e9glementaires en\nfournissant les valeurs calcul\u00e9es suivant les r\u00e9glementations d\u00e9finies dans\nchaque fonction de calcul et un tableau de nombre de d\u00e9passement.\n\nParam\u00e8tres:\nfct: fonction renvoyant les \u00e9l\u00e9ments calcul\u00e9es\ndf: DataFrame de valeurs d'entr\u00e9e \u00e0 fournir \u00e0 la fonction\n\nRetourne:\nUne chaine de caract\u00e8re pr\u00eate \u00e0 \u00eatre utilis\u00e9 dans une page html", "id": "f6427:m21"}
{"signature": "def acgtn_only(infile, outfile):", "body": "f = utils.open_file_write(outfile)<EOL>for seq in sequences.file_reader(infile):<EOL><INDENT>seq.replace_non_acgt()<EOL>print(seq, file=f)<EOL><DEDENT>utils.close(f)<EOL>", "docstring": "Replace every non-acgtn (case insensitve) character with an N", "id": "f405:m0"}
{"signature": "def union_fill_gap(self, i):", "body": "return Interval(min(self.start, i.start), max(self.end, i.end))<EOL>", "docstring": "Like union, but ignores whether the two intervals intersect or not", "id": "f408:c1:m11"}
{"signature": "def intersection(l1, l2):", "body": "if len(l1) == <NUM_LIT:0> or len(l2) == <NUM_LIT:0>:<EOL><INDENT>return []<EOL><DEDENT>out = []<EOL>l2_pos = <NUM_LIT:0><EOL>for l in l1:<EOL><INDENT>while l2_pos < len(l2) and l2[l2_pos].end < l.start:<EOL><INDENT>l2_pos += <NUM_LIT:1><EOL><DEDENT>if l2_pos == len(l2):<EOL><INDENT>break<EOL><DEDENT>while l2_pos < len(l2) and l.intersects(l2[l2_pos]):<EOL><INDENT>out.append(l.intersection(l2[l2_pos]))<EOL>l2_pos += <NUM_LIT:1><EOL><DEDENT>l2_pos = max(<NUM_LIT:0>, l2_pos - <NUM_LIT:1>)<EOL><DEDENT>return out<EOL>", "docstring": "Returns intersection of two lists.  Assumes the lists are sorted by start positions", "id": "f408:m0"}
{"signature": "def intervals(self, range_start=datetime.datetime.min, range_end=datetime.datetime.max):", "body": "<EOL>current_period = None<EOL>max_continuous_days = <NUM_LIT><EOL>range_start = self.to_timezone(range_start)<EOL>range_end = self.to_timezone(range_end)<EOL>for period in self._daily_periods(range_start.date(), range_end.date()):<EOL><INDENT>if period.end < range_start or period.start > range_end:<EOL><INDENT>continue<EOL><DEDENT>if current_period is None:<EOL><INDENT>current_period = period<EOL><DEDENT>else:<EOL><INDENT>if ( ((period.start < current_period.end)<EOL>or (period.start - current_period.end) <= datetime.timedelta(minutes=<NUM_LIT:1>))<EOL>and (current_period.end - current_period.start) < datetime.timedelta(days=max_continuous_days)):<EOL><INDENT>current_period = Period(current_period.start, period.end)<EOL><DEDENT>else:<EOL><INDENT>yield current_period<EOL>current_period = period<EOL><DEDENT><DEDENT><DEDENT>if current_period:<EOL><INDENT>yield current_period<EOL><DEDENT>", "docstring": "Returns an iterator of Period tuples for continuous stretches of time during\n        which this event is in effect, between range_start and range_end.", "id": "f4285:c2:m5"}
{"signature": "def transform_grid_to_reference_frame(self, grid):", "body": "if self.__class__.__name__.startswith(\"<STR_LIT>\"):<EOL><INDENT>return super().transform_grid_to_reference_frame(grid)<EOL><DEDENT>shifted_coordinates = np.subtract(grid, self.centre)<EOL>radius = np.sqrt(np.sum(shifted_coordinates ** <NUM_LIT>, <NUM_LIT:1>))<EOL>theta_coordinate_to_profile = np.arctan2(shifted_coordinates[:, <NUM_LIT:0>],<EOL>shifted_coordinates[:, <NUM_LIT:1>]) - self.phi_radians<EOL>transformed = np.vstack(<EOL>(radius * np.sin(theta_coordinate_to_profile), radius * np.cos(theta_coordinate_to_profile))).T<EOL>return transformed.view(TransformedGrid)<EOL>", "docstring": "Transform a grid of (y,x) coordinates to the reference frame of the profile, including a translation to \\\n        its centre and a rotation to it orientation.\n\n        Parameters\n        ----------\n        grid : ndarray\n            The (y, x) coordinates in the original reference frame of the grid.", "id": "f5954:c3:m9"}
{"signature": "def is_success(op):", "body": "return is_done(op) and ('<STR_LIT:error>' not in op)<EOL>", "docstring": "Return whether the operation has completed successfully.", "id": "f6206:m8"}
{"signature": "def _create_parameter(model, pid, value, sbo=None, constant=True, units=None,<EOL>flux_udef=None):", "body": "parameter = model.createParameter()  <EOL>parameter.setId(pid)<EOL>parameter.setValue(value)<EOL>parameter.setConstant(constant)<EOL>if sbo:<EOL><INDENT>parameter.setSBOTerm(sbo)<EOL><DEDENT>if units:<EOL><INDENT>parameter.setUnits(flux_udef.getId())<EOL><DEDENT>", "docstring": "Create parameter in SBML model.", "id": "f15889:m13"}
{"signature": "def mrv(assignment, csp):", "body": "return argmin_random_tie(<EOL>[v for v in csp.vars if v not in assignment],<EOL>lambda var: num_legal_values(csp, var, assignment))<EOL>", "docstring": "Minimum-remaining-values heuristic.", "id": "f1681:m3"}
{"signature": "def setUp(self):", "body": "super(BaseTestEnterpriseCustomerTransmitCoursesView, self).setUp()<EOL>self.user = UserFactory.create(is_staff=True, is_active=True, id=<NUM_LIT:1>)<EOL>self.user.set_password('<STR_LIT>')<EOL>self.user.save()<EOL>self.enterprise_channel_worker = UserFactory.create(is_staff=True, is_active=True)<EOL>self.enterprise_customer = EnterpriseCustomerFactory()<EOL>self.default_context = {<EOL>'<STR_LIT>': True,<EOL>'<STR_LIT>': self.enterprise_customer._meta,<EOL>'<STR_LIT:user>': self.user<EOL>}<EOL>self.transmit_courses_metadata_form = TransmitEnterpriseCoursesForm()<EOL>self.view_url = reverse(<EOL>'<STR_LIT>' + enterprise_admin.utils.UrlNames.TRANSMIT_COURSES_METADATA,<EOL>args=(self.enterprise_customer.uuid,)<EOL>)<EOL>self.client = Client()<EOL>self.context_parameters = EnterpriseCustomerTransmitCoursesView.ContextParameters<EOL>", "docstring": "Test set up", "id": "f16142:c6:m0"}
{"signature": "def to_hour(num) -> str:", "body": "to_str = str(int(num))<EOL>return pd.Timestamp(f'<STR_LIT>').strftime('<STR_LIT>')<EOL>", "docstring": "Convert YAML input to hours\n\nArgs:\n    num: number in YMAL file, e.g., 900, 1700, etc.\n\nReturns:\n    str\n\nExamples:\n    >>> to_hour(900)\n    '09:00'\n    >>> to_hour(1700)\n    '17:00'", "id": "f162:m2"}
{"signature": "def clear(self):", "body": "self.desc.clear()<EOL>", "docstring": "Clear description to default values", "id": "f1930:c0:m3"}
{"signature": "def pvpc_data_dia(str_dia, str_dia_fin=None):", "body": "params = {'<STR_LIT>': DATE_FMT, '<STR_LIT>': False,<EOL>'<STR_LIT>': pvpc_procesa_datos_dia, '<STR_LIT>': pvpc_url_dia,<EOL>'<STR_LIT>': {'<STR_LIT>': True, '<STR_LIT>': HEADERS}}<EOL>if str_dia_fin is not None:<EOL><INDENT>params['<STR_LIT>'] = True<EOL>data, hay_errores, str_import = get_data_en_intervalo(str_dia, str_dia_fin, **params)<EOL><DEDENT>else:<EOL><INDENT>data, hay_errores, str_import = get_data_en_intervalo(str_dia, str_dia, **params)<EOL><DEDENT>if not hay_errores:<EOL><INDENT>return data<EOL><DEDENT>else:<EOL><INDENT>return str_import<EOL><DEDENT>", "docstring": "Obtiene datos de PVPC en un d\u00eda concreto o un intervalo, accediendo directamente a la web.", "id": "f14223:m4"}
{"signature": "def get_remote_id(self, identity_provider, username):", "body": "return self._get_results(identity_provider, '<STR_LIT:username>', username, '<STR_LIT>')<EOL>", "docstring": "Retrieve the remote identifier for the given username.\n\nArgs:\n* ``identity_provider`` (str): identifier slug for the third-party authentication service used during SSO.\n* ``username`` (str): The username ID identifying the user for which to retrieve the remote name.\n\nReturns:\n    string or None: the remote name of the given user.  None if not found.", "id": "f16094:c5:m0"}
{"signature": "def intersection(self, i):", "body": "if self.intersects(i):<EOL><INDENT>return Interval(max(self.start, i.start), min(self.end, i.end))<EOL><DEDENT>else:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "If intervals intersect, returns their intersection, otherwise returns None", "id": "f408:c1:m12"}
{"signature": "def map_batches_async(self, batches, chunksize=None, callback=None, error_callback=None):", "body": "assert isinstance(batches, list), (\"<STR_LIT>\"<EOL>+ \"<STR_LIT>\") % (type(batches),)<EOL>return self.pool.map_async(_Pool_starworker, self._handle_batch_ids(batches),<EOL>chunksize=chunksize, callback=callback, error_callback=error_callback)<EOL>", "docstring": "Augment batches asynchonously.\n\nParameters\n----------\nbatches : list of imgaug.augmentables.batches.Batch\n    The batches to augment.\n\nchunksize : None or int, optional\n    Rough indicator of how many tasks should be sent to each worker. Increasing this number can improve\n    performance.\n\ncallback : None or callable, optional\n    Function to call upon finish. See `multiprocessing.Pool`.\n\nerror_callback : None or callable, optional\n    Function to call upon errors. See `multiprocessing.Pool`.\n\nReturns\n-------\nmultiprocessing.MapResult\n    Asynchonous result. See `multiprocessing.Pool`.", "id": "f16272:c0:m3"}
{"signature": "def set_user_loader(self, loader):", "body": "self._user_loader = loader<EOL>", "docstring": "Set user loader, which is used to load current user.\n        An example::\n\n            from flask_login import current_user\n            rbac.set_user_loader(lambda: current_user)\n\n        :param loader: Current user function.", "id": "f12671:c2:m6"}
{"signature": "def make_server(host, port, app=None,<EOL>server_class=AsyncWsgiServer,<EOL>handler_class=AsyncWsgiHandler,<EOL>ws_handler_class=None,<EOL>ws_path='<STR_LIT>'):", "body": "handler_class.ws_handler_class = ws_handler_class<EOL>handler_class.ws_path = ws_path<EOL>httpd = server_class((host, port), RequestHandlerClass=handler_class)<EOL>httpd.set_app(app)<EOL>return httpd<EOL>", "docstring": "Create server instance with an optional WebSocket handler\n\n    For pure WebSocket server ``app`` may be ``None`` but an attempt to access\n    any path other than ``ws_path`` will cause server error.\n\n    :param host: hostname or IP\n    :type host: str\n    :param port: server port\n    :type port: int\n    :param app: WSGI application\n    :param server_class: WSGI server class, defaults to AsyncWsgiServer\n    :param handler_class: WSGI handler class, defaults to AsyncWsgiHandler\n    :param ws_handler_class: WebSocket hanlder class, defaults to ``None``\n    :param ws_path: WebSocket path on the server, defaults to '/ws'\n    :type ws_path: str, optional\n    :return: initialized server instance", "id": "f9796:m3"}
{"signature": "def occurrence_halved_fingerprint(<EOL>word, n_bits=<NUM_LIT:16>, most_common=MOST_COMMON_LETTERS_CG<EOL>):", "body": "return OccurrenceHalved().fingerprint(word, n_bits, most_common)<EOL>", "docstring": "Return the occurrence halved fingerprint.\n\n    This is a wrapper for :py:meth:`OccurrenceHalved.fingerprint`.\n\n    Parameters\n    ----------\n    word : str\n        The word to fingerprint\n    n_bits : int\n        Number of bits in the fingerprint returned\n    most_common : list\n        The most common tokens in the target language, ordered by frequency\n\n    Returns\n    -------\n    int\n        The occurrence halved fingerprint\n\n    Examples\n    --------\n    >>> bin(occurrence_halved_fingerprint('hat'))\n    '0b1010000000010'\n    >>> bin(occurrence_halved_fingerprint('niall'))\n    '0b10010100000'\n    >>> bin(occurrence_halved_fingerprint('colin'))\n    '0b1001010000'\n    >>> bin(occurrence_halved_fingerprint('atcg'))\n    '0b10100000000000'\n    >>> bin(occurrence_halved_fingerprint('entreatment'))\n    '0b1111010000110000'", "id": "f6671:m0"}
{"signature": "def __add__(self, other):", "body": "return np.add(self, other)<EOL>", "docstring": "x.__add__(y) <==> x+y", "id": "f4853:c1:m26"}
{"signature": "def reset_reviews(self):", "body": "<EOL>self.review_date_set = False<EOL>self.review_comment_set = False<EOL>", "docstring": "Resets the builder's state to allow building new reviews.", "id": "f3754:c4:m1"}
{"signature": "def _getBundleId(self):", "body": "ra = AppKit.NSRunningApplication<EOL>app = ra.runningApplicationWithProcessIdentifier_(<EOL>self._getPid())<EOL>return app.bundleIdentifier()<EOL>", "docstring": "Return the bundle ID of the application.", "id": "f10331:c0:m43"}
{"signature": "def _apply_user_agent(headers, user_agent):", "body": "if user_agent is not None:<EOL><INDENT>if '<STR_LIT>' in headers:<EOL><INDENT>headers['<STR_LIT>'] = (user_agent + '<STR_LIT:U+0020>' + headers['<STR_LIT>'])<EOL><DEDENT>else:<EOL><INDENT>headers['<STR_LIT>'] = user_agent<EOL><DEDENT><DEDENT>return headers<EOL>", "docstring": "Adds a user-agent to the headers.\n\n    Args:\n        headers: dict, request headers to add / modify user\n                 agent within.\n        user_agent: str, the user agent to add.\n\n    Returns:\n        dict, the original headers passed in, but modified if the\n        user agent is not None.", "id": "f2474:m3"}
{"signature": "def set_mfc(self, val):", "body": "self.set_markerfacecolor(val)<EOL>", "docstring": "alias for set_markerfacecolor", "id": "f17180:c0:m88"}
{"signature": "def __init__(self, X):", "body": "X = asarray(X, float)<EOL>m = X.shape[<NUM_LIT:1>]<EOL>self._effsizes = Vector(zeros(m))<EOL>self._effsizes.bounds = [(-<NUM_LIT>, +<NUM_LIT:200>)] * m<EOL>self._X = X<EOL>Function.__init__(self, \"<STR_LIT>\", effsizes=self._effsizes)<EOL>", "docstring": "Constructor.\n\nParameters\n----------\nX : array_like\n    Covariates X, from X\ud835\udf36.", "id": "f13596:c0:m0"}
{"signature": "def __init__(self, signals):", "body": "self.signals = set(signals)<EOL>self.callbacks = dict([(s, dict()) for s in signals])<EOL>self._cid = <NUM_LIT:0><EOL>", "docstring": "*signals* is a sequence of valid signals", "id": "f17209:c6:m0"}
{"signature": "@property<EOL><INDENT>def ChildId(self) -> int:<DEDENT>", "body": "return self.pattern.CurrentChildId<EOL>", "docstring": "Property ChildId.\nCall IUIAutomationLegacyIAccessiblePattern::get_CurrentChildId.\nReturn int, the Microsoft Active Accessibility child identifier for the element.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationlegacyiaccessiblepattern-get_currentchildid", "id": "f1782:c53:m1"}
{"signature": "def extract(self):", "body": "return self.__extract_data(self.__package_pointer)<EOL>", "docstring": "!\n        @brief Performs unpacking procedure of the pyclustering package to the data.\n\n        @return (list) Extracted data from the pyclustering package.", "id": "f15609:c3:m1"}
{"signature": "def function_inclusion_filter_builder(func: Strings) -> NodePredicate:", "body": "if isinstance(func, str):<EOL><INDENT>return _single_function_inclusion_filter_builder(func)<EOL><DEDENT>elif isinstance(func, Iterable):<EOL><INDENT>return _collection_function_inclusion_builder(func)<EOL><DEDENT>raise ValueError('<STR_LIT>'.format(func))<EOL>", "docstring": "Build a filter that only passes on nodes of the given function(s).\n\n    :param func: A BEL Function or list/set/tuple of BEL functions", "id": "f9391:m5"}
{"signature": "@map_types<EOL><INDENT>def __init__(self,<EOL>centre: dim.Position = (<NUM_LIT:0.0>, <NUM_LIT:0.0>),<EOL>intensity: dim.Luminosity = <NUM_LIT:0.1>,<EOL>effective_radius: dim.Length = <NUM_LIT>):<DEDENT>", "body": "super(SphericalDevVaucouleurs, self).__init__(centre=centre, axis_ratio=<NUM_LIT:1.0>, phi=<NUM_LIT:0.0>, intensity=intensity,<EOL>effective_radius=effective_radius)<EOL>", "docstring": "The spherical Dev Vaucouleurs light profile.\n\n        This is a subset of the elliptical Sersic profile, specific to the case that sersic_index = 1.0.\n\n        Parameters\n        ----------\n        centre : (float, float)\n            The (y,x) arc-second coordinates of the profile centre.\n        intensity : float\n            Overall intensity normalisation of the light profiles (electrons per second).\n        effective_radius : float\n            The circular radius containing half the light of this profile.", "id": "f5955:c10:m0"}
{"signature": "@view_config(route_name='<STR_LIT>',<EOL>permission='<STR_LIT>',<EOL>request_method='<STR_LIT:POST>', accept='<STR_LIT:application/json>', http_cache=<NUM_LIT:0>)<EOL>def post_roles_request(request):", "body": "uuid_ = request.matchdict['<STR_LIT>']<EOL>posted_roles = request.json<EOL>with db_connect() as db_conn:<EOL><INDENT>with db_conn.cursor() as cursor:<EOL><INDENT>cursor.execute(\"\"\"<STR_LIT>\"\"\", (uuid_,))<EOL>try:<EOL><INDENT>cursor.fetchone()[<NUM_LIT:0>]<EOL><DEDENT>except TypeError:<EOL><INDENT>if request.has_permission('<STR_LIT>'):<EOL><INDENT>cursor.execute(\"\"\"<STR_LIT>\"\"\", (uuid_,))<EOL><DEDENT>else:<EOL><INDENT>raise httpexceptions.HTTPNotFound()<EOL><DEDENT><DEDENT>try:<EOL><INDENT>upsert_users(cursor, [r['<STR_LIT>'] for r in posted_roles])<EOL><DEDENT>except UserFetchError as exc:<EOL><INDENT>raise httpexceptions.HTTPBadRequest(exc.message)<EOL><DEDENT>upsert_role_requests(cursor, uuid_, posted_roles)<EOL><DEDENT><DEDENT>resp = request.response<EOL>resp.status_int = <NUM_LIT><EOL>return resp<EOL>", "docstring": "Submission to create a role acceptance request.", "id": "f15220:m4"}
{"signature": "@staticmethod<EOL><INDENT>def extract_acked_seqs(bitmap, ssc_seq):<DEDENT>", "body": "acked_seqs = []<EOL>for idx, val in enumerate(bitmap):<EOL><INDENT>if int(val) == <NUM_LIT:1>:<EOL><INDENT>seq = (ssc_seq + idx) % <NUM_LIT><EOL>acked_seqs.append(seq)<EOL><DEDENT><DEDENT>return acked_seqs<EOL>", "docstring": "extracts acknowledged sequences from bitmap and\n        starting sequence number.\n        :bitmap: str\n        :ssc_seq: int\n        :return: int[]\n            acknowledged sequence numbers", "id": "f1089:c12:m5"}
{"signature": "def main(argv=None):", "body": "if argv is None:<EOL><INDENT>argv = sys.argv<EOL><DEDENT>else:<EOL><INDENT>sys.argv.extend(argv)<EOL><DEDENT>program_name = os.path.basename(sys.argv[<NUM_LIT:0>])<EOL>program_version = \"<STR_LIT>\" % __version__<EOL>program_build_date = str(__updated__)<EOL>program_version_message = '<STR_LIT>' % (program_version,<EOL>program_build_date)<EOL>program_shortdesc = __import__('<STR_LIT:__main__>').__doc__.split(\"<STR_LIT:\\n>\")[<NUM_LIT:1>]<EOL>program_license =", "docstring": "Command line options.", "id": "f827:m5"}
{"signature": "def get_arr_int(self, background_threshold=<NUM_LIT>, background_class_id=None):", "body": "if self.input_was[<NUM_LIT:0>] in [\"<STR_LIT:bool>\", \"<STR_LIT:float>\"]:<EOL><INDENT>ia.do_assert(background_class_id is None,<EOL>\"<STR_LIT>\"<EOL>+ \"<STR_LIT>\")<EOL><DEDENT>if background_class_id is None:<EOL><INDENT>background_class_id = <NUM_LIT:0><EOL><DEDENT>channelwise_max_idx = np.argmax(self.arr, axis=<NUM_LIT:2>)<EOL>if self.input_was[<NUM_LIT:0>] in [\"<STR_LIT:bool>\", \"<STR_LIT:float>\"]:<EOL><INDENT>result = <NUM_LIT:1> + channelwise_max_idx<EOL><DEDENT>else:  <EOL><INDENT>result = channelwise_max_idx<EOL><DEDENT>if background_threshold is not None and background_threshold > <NUM_LIT:0>:<EOL><INDENT>probs = np.amax(self.arr, axis=<NUM_LIT:2>)<EOL>result[probs < background_threshold] = background_class_id<EOL><DEDENT>return result.astype(np.int32)<EOL>", "docstring": "Get the segmentation map array as an integer array of shape (H, W).\n\nEach pixel in that array contains an integer value representing the pixel's class.\nIf multiple classes overlap, the one with the highest local float value is picked.\nIf that highest local value is below `background_threshold`, the method instead uses\nthe background class id as the pixel's class value.\nBy default, class id 0 is the background class. This may only be changed if the original\ninput to the segmentation map object was an integer map.\n\nParameters\n----------\nbackground_threshold : float, optional\n    At each pixel, each class-heatmap has a value between 0.0 and 1.0. If none of the\n    class-heatmaps has a value above this threshold, the method uses the background class\n    id instead.\n\nbackground_class_id : None or int, optional\n    Class id to fall back to if no class-heatmap passes the threshold at a spatial\n    location. May only be provided if the original input was an integer mask and in these\n    cases defaults to 0. If the input were float or boolean masks, the background class id\n    may not be set as it is assumed that the background is implicitly defined\n    as 'any spatial location that has zero-like values in all masks'.\n\nReturns\n-------\nresult : (H,W) ndarray\n    Segmentation map array (int32).\n    If the original input consisted of boolean or float masks, then the highest possible\n    class id is ``1+C``, where ``C`` is the number of provided float/boolean masks. The value\n    ``0`` in the integer mask then denotes the background class.", "id": "f16283:c0:m1"}
{"signature": "def delete_environment(self, environment_name):", "body": "self.ebs.terminate_environment(environment_name=environment_name, terminate_resources=True)<EOL>", "docstring": "Deletes an environment", "id": "f15103:c1:m11"}
{"signature": "def complete_task_from_id(self, task_id):", "body": "if task_id is None:<EOL><INDENT>raise WorkflowException(self.spec, '<STR_LIT>')<EOL><DEDENT>for task in self.task_tree:<EOL><INDENT>if task.id == task_id:<EOL><INDENT>return task.complete()<EOL><DEDENT><DEDENT>msg = '<STR_LIT>' % task_id<EOL>raise WorkflowException(self.spec, msg)<EOL>", "docstring": "Runs the task with the given id.\n\n:type  task_id: integer\n:param task_id: The id of the Task object.", "id": "f7705:c0:m11"}
{"signature": "@property<EOL><INDENT>def alpha(self):<DEDENT>", "body": "return phase_select_property(phase=self.phase, l=self.alphal, g=self.alphag)<EOL>", "docstring": "r'''Thermal diffusivity of the chemical at its current temperature,\n        pressure, and phase in units of [m^2/s].\n\n        .. math::\n            \\alpha = \\frac{k}{\\rho Cp}\n\n        Examples\n        --------\n        >>> Chemical('furfural').alpha\n        8.696537158635412e-08", "id": "f15812:c0:m103"}
{"signature": "def download_files(self, dataset_files, destination='<STR_LIT:.>'):", "body": "if not isinstance(dataset_files, list):<EOL><INDENT>dataset_files = [dataset_files]<EOL><DEDENT>for f in dataset_files:<EOL><INDENT>filename = f.path.lstrip('<STR_LIT:/>')<EOL>local_path = os.path.join(destination, filename)<EOL>if not os.path.isdir(os.path.dirname(local_path)):<EOL><INDENT>os.makedirs(os.path.dirname(local_path))<EOL><DEDENT>r = requests.get(f.url, stream=True)<EOL>with open(local_path, '<STR_LIT:wb>') as output_file:<EOL><INDENT>shutil.copyfileobj(r.raw, output_file)<EOL><DEDENT><DEDENT>", "docstring": "Downloads file(s) to a local destination.\n\n:param dataset_files:\n:type dataset_files: list of :class: `DatasetFile`\n:param destination: The path to the desired local download destination\n:type destination: str\n:param chunk: Whether or not to chunk the file. Default True\n:type chunk: bool", "id": "f3511:c0:m7"}
{"signature": "def set_latitude_grid(self, degrees):", "body": "number = (<NUM_LIT> / degrees) + <NUM_LIT:1><EOL>self.yaxis.set_major_locator(<EOL>FixedLocator(<EOL>np.linspace(-np.pi / <NUM_LIT>, np.pi / <NUM_LIT>, number, True)[<NUM_LIT:1>:-<NUM_LIT:1>]))<EOL>self._latitude_degrees = degrees<EOL>self.yaxis.set_major_formatter(self.ThetaFormatter(degrees))<EOL>", "docstring": "Set the number of degrees between each longitude grid.", "id": "f17181:c0:m14"}
{"signature": "def handle_package_has_file_helper(self, pkg_file):", "body": "nodes = list(self.graph.triples((None, self.spdx_namespace.fileName, Literal(pkg_file.name))))<EOL>if len(nodes) == <NUM_LIT:1>:<EOL><INDENT>return nodes[<NUM_LIT:0>][<NUM_LIT:0>]<EOL><DEDENT>else:<EOL><INDENT>raise InvalidDocumentError('<STR_LIT>' +<EOL>'<STR_LIT>'.format(pkg_file.name))<EOL><DEDENT>", "docstring": "Return node representing pkg_file\npkg_file should be instance of spdx.file.", "id": "f3742:c7:m6"}
{"signature": "def get_transit_events(self, start_time_ut=None, end_time_ut=None, route_type=None):", "body": "table_name = self._get_day_trips_table_name()<EOL>event_query = \"<STR_LIT>\"\"<STR_LIT>\"\"<STR_LIT>\" + table_name + \"<STR_LIT:U+0020>\"\"<STR_LIT>\"\"<STR_LIT>\"\"<STR_LIT>\"<EOL>where_clauses = []<EOL>if end_time_ut:<EOL><INDENT>where_clauses.append(table_name + \"<STR_LIT>\".format(end_time_ut=end_time_ut))<EOL>where_clauses.append(\"<STR_LIT>\".format(end_time_ut=end_time_ut))<EOL><DEDENT>if start_time_ut:<EOL><INDENT>where_clauses.append(table_name + \"<STR_LIT>\".format(start_time_ut=start_time_ut))<EOL>where_clauses.append(\"<STR_LIT>\".format(start_time_ut=start_time_ut))<EOL><DEDENT>if route_type is not None:<EOL><INDENT>assert route_type in ALL_ROUTE_TYPES<EOL>where_clauses.append(\"<STR_LIT>\".format(route_type=route_type))<EOL><DEDENT>if len(where_clauses) > <NUM_LIT:0>:<EOL><INDENT>event_query += \"<STR_LIT>\"<EOL>for i, where_clause in enumerate(where_clauses):<EOL><INDENT>if i is not <NUM_LIT:0>:<EOL><INDENT>event_query += \"<STR_LIT>\"<EOL><DEDENT>event_query += where_clause<EOL><DEDENT><DEDENT>event_query += \"<STR_LIT>\"<EOL>events_result = pd.read_sql_query(event_query, self.conn)<EOL>from_indices = numpy.nonzero(<EOL>(events_result['<STR_LIT>'][:-<NUM_LIT:1>].values == events_result['<STR_LIT>'][<NUM_LIT:1>:].values) *<EOL>(events_result['<STR_LIT>'][:-<NUM_LIT:1>].values < events_result['<STR_LIT>'][<NUM_LIT:1>:].values)<EOL>)[<NUM_LIT:0>]<EOL>to_indices = from_indices + <NUM_LIT:1><EOL>assert (events_result['<STR_LIT>'][from_indices].values == events_result['<STR_LIT>'][to_indices].values).all()<EOL>trip_Is = events_result['<STR_LIT>'][from_indices]<EOL>from_stops = events_result['<STR_LIT>'][from_indices]<EOL>to_stops = events_result['<STR_LIT>'][to_indices]<EOL>shape_ids = events_result['<STR_LIT>'][from_indices]<EOL>dep_times = events_result['<STR_LIT>'][from_indices]<EOL>arr_times = events_result['<STR_LIT>'][to_indices]<EOL>route_types = events_result['<STR_LIT>'][from_indices]<EOL>route_ids = events_result['<STR_LIT>'][from_indices]<EOL>route_Is = events_result['<STR_LIT>'][from_indices]<EOL>durations = arr_times.values - dep_times.values<EOL>assert (durations >= <NUM_LIT:0>).all()<EOL>from_seqs = events_result['<STR_LIT>'][from_indices]<EOL>to_seqs = events_result['<STR_LIT>'][to_indices]<EOL>data_tuples = zip(from_stops, to_stops, dep_times, arr_times,<EOL>shape_ids, route_types, route_ids, trip_Is,<EOL>durations, from_seqs, to_seqs, route_Is)<EOL>columns = [\"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\",<EOL>\"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\",<EOL>\"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\"]<EOL>df = pd.DataFrame.from_records(data_tuples, columns=columns)<EOL>return df<EOL>", "docstring": "Obtain a list of events that take place during a time interval.\nEach event needs to be only partially overlap the given time interval.\nDoes not include walking events.\n\nParameters\n----------\nstart_time_ut : int\n    start of the time interval in unix time (seconds)\nend_time_ut: int\n    end of the time interval in unix time (seconds)\nroute_type: int\n    consider only events for this route_type\n\nReturns\n-------\nevents: pandas.DataFrame\n    with the following columns and types\n        dep_time_ut: int\n        arr_time_ut: int\n        from_stop_I: int\n        to_stop_I: int\n        trip_I : int\n        shape_id : int\n        route_type : int\n\nSee also\n--------\nget_transit_events_in_time_span : an older version of the same thing", "id": "f12922:c0:m50"}
{"signature": "def delete(self):", "body": "i = self.index()<EOL>if i != None: del self.canvas.layers[i]<EOL>", "docstring": "Removes this layer from the canvas.", "id": "f11554:c2:m3"}
{"signature": "def _formatMessage(self, msg, standardMsg):", "body": "if not self.longMessage:<EOL><INDENT>return msg or standardMsg<EOL><DEDENT>if msg is None:<EOL><INDENT>return standardMsg<EOL><DEDENT>try:<EOL><INDENT>return '<STR_LIT>' % (standardMsg, msg)<EOL><DEDENT>except UnicodeDecodeError:<EOL><INDENT>return  '<STR_LIT>' % (safe_repr(standardMsg), safe_repr(msg))<EOL><DEDENT>", "docstring": "Honour the longMessage attribute when generating failure messages.\n        If longMessage is False this means:\n        * Use only an explicit message if it is provided\n        * Otherwise use the standard message for the assert\n\n        If longMessage is True:\n        * Use the standard message\n        * If an explicit message is provided, plus ' : ' and the explicit message", "id": "f16396:c5:m25"}
{"signature": "@staticmethod<EOL><INDENT>def Write(log: Any, consoleColor: int = ConsoleColor.Default, writeToFile: bool = True, printToStdout: bool = True, logFile: str = None, printTruncateLen: int = <NUM_LIT:0>) -> None:<DEDENT>", "body": "if not isinstance(log, str):<EOL><INDENT>log = str(log)<EOL><DEDENT>if printToStdout and sys.stdout:<EOL><INDENT>isValidColor = (consoleColor >= ConsoleColor.Black and consoleColor <= ConsoleColor.White)<EOL>if isValidColor:<EOL><INDENT>SetConsoleColor(consoleColor)<EOL><DEDENT>try:<EOL><INDENT>if printTruncateLen > <NUM_LIT:0> and len(log) > printTruncateLen:<EOL><INDENT>sys.stdout.write(log[:printTruncateLen] + '<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>sys.stdout.write(log)<EOL><DEDENT><DEDENT>except Exception as ex:<EOL><INDENT>SetConsoleColor(ConsoleColor.Red)<EOL>isValidColor = True<EOL>sys.stdout.write(ex.__class__.__name__ + '<STR_LIT>')<EOL>if log.endswith('<STR_LIT:\\n>'):<EOL><INDENT>sys.stdout.write('<STR_LIT:\\n>')<EOL><DEDENT><DEDENT>if isValidColor:<EOL><INDENT>ResetConsoleColor()<EOL><DEDENT>sys.stdout.flush()<EOL><DEDENT>if not writeToFile:<EOL><INDENT>return<EOL><DEDENT>fileName = logFile if logFile else Logger.FileName<EOL>try:<EOL><INDENT>fout = open(fileName, '<STR_LIT>', encoding='<STR_LIT:utf-8>')<EOL>fout.write(log)<EOL><DEDENT>except Exception as ex:<EOL><INDENT>if sys.stdout:<EOL><INDENT>sys.stdout.write(ex.__class__.__name__ + '<STR_LIT>')<EOL><DEDENT><DEDENT>finally:<EOL><INDENT>if fout:<EOL><INDENT>fout.close()<EOL><DEDENT><DEDENT>", "docstring": "log: any type.\nconsoleColor: int, a value in class `ConsoleColor`, such as `ConsoleColor.DarkGreen`.\nwriteToFile: bool.\nprintToStdout: bool.\nlogFile: str, log file path.\nprintTruncateLen: int, if <= 0, log is not truncated when print.", "id": "f1782:c41:m1"}
{"signature": "def GetGridPattern(self) -> GridPattern:", "body": "return self.GetPattern(PatternId.GridPattern)<EOL>", "docstring": "Return `GridPattern` if it supports the pattern else None(Conditional support according to MSDN).", "id": "f1782:c109:m1"}
{"signature": "def drawTriangle(self, x0, y0, x1, y1, x2, y2, color=None, aa=False):", "body": "md.draw_triangle(self.set, x0, y0, x1, y1, x2, y2, color, aa)<EOL>", "docstring": "Draw triangle with vertices (x0, y0), (x1, y1) and (x2, y2)\n\n:param aa: if True, use Bresenham's algorithm for line drawing;\n    Otherwise use Xiaolin Wu's algorithm", "id": "f2045:c0:m23"}
{"signature": "def is_valid_url(url):", "body": "regex = re.compile(r'<STR_LIT>'<EOL>r'<STR_LIT>'<EOL>r'<STR_LIT>'<EOL>r'<STR_LIT>'<EOL>r'<STR_LIT>'<EOL>r'<STR_LIT>', re.IGNORECASE)<EOL>if regex.match(url):<EOL><INDENT>logger.info(\"<STR_LIT>\")<EOL>return True<EOL><DEDENT>else:<EOL><INDENT>return False<EOL><DEDENT>", "docstring": "Check if a given string is in the correct URL format or not\n\n:param str url:\n:return: True or False", "id": "f7878:m2"}
{"signature": "def step(self, substeps=<NUM_LIT:2>):", "body": "self.frame_no += <NUM_LIT:1><EOL>dt = self.dt / substeps<EOL>for _ in range(substeps):<EOL><INDENT>self.ode_contactgroup.empty()<EOL>self.ode_space.collide(None, self.on_collision)<EOL>self.ode_world.step(dt)<EOL><DEDENT>", "docstring": "Step the world forward by one frame.\n\n        Parameters\n        ----------\n        substeps : int, optional\n            Split the step into this many sub-steps. This helps to prevent the\n            time delta for an update from being too large.", "id": "f14887:c17:m16"}
{"signature": "def t_import_t_semicolon(self, t):", "body": "t.lexer.pop_state()<EOL>return t<EOL>", "docstring": "r';", "id": "f12428:c0:m25"}
{"signature": "def dump_nparray(self, obj, class_name=numpy_ndarray_class_name):", "body": "return {\"<STR_LIT:$>\" + class_name: self._json_convert(obj.tolist())}<EOL>", "docstring": "``numpy.ndarray`` dumper.", "id": "f15148:c3:m0"}
{"signature": "def closenessScores(self, expValues, actValues, fractional=True):", "body": "<EOL>if self.encoders is None:<EOL><INDENT>err = abs(expValues[<NUM_LIT:0>] - actValues[<NUM_LIT:0>])<EOL>if fractional:<EOL><INDENT>denom = max(expValues[<NUM_LIT:0>], actValues[<NUM_LIT:0>])<EOL>if denom == <NUM_LIT:0>:<EOL><INDENT>denom = <NUM_LIT:1.0><EOL><DEDENT>closeness = <NUM_LIT:1.0> - float(err)/denom<EOL>if closeness < <NUM_LIT:0>:<EOL><INDENT>closeness = <NUM_LIT:0><EOL><DEDENT><DEDENT>else:<EOL><INDENT>closeness = err<EOL><DEDENT>return numpy.array([closeness])<EOL><DEDENT>scalarIdx = <NUM_LIT:0><EOL>retVals = numpy.array([])<EOL>for (name, encoder, offset) in self.encoders:<EOL><INDENT>values = encoder.closenessScores(expValues[scalarIdx:], actValues[scalarIdx:],<EOL>fractional=fractional)<EOL>scalarIdx += len(values)<EOL>retVals = numpy.hstack((retVals, values))<EOL><DEDENT>return retVals<EOL>", "docstring": "Compute closeness scores between the expected scalar value(s) and actual\nscalar value(s). The expected scalar values are typically those obtained\nfrom the :meth:`.getScalars` method. The actual scalar values are typically\nthose returned from :meth:`.topDownCompute`.\n\nThis method returns one closeness score for each value in expValues (or\nactValues which must be the same length). The closeness score ranges from\n0 to 1.0, 1.0 being a perfect match and 0 being the worst possible match.\n\nIf this encoder is a simple, single field encoder, then it will expect\njust 1 item in each of the ``expValues`` and ``actValues`` arrays.\nMulti-encoders will expect 1 item per sub-encoder.\n\nEach encoder type can define it's own metric for closeness. For example,\na category encoder may return either 1 or 0, if the scalar matches exactly\nor not. A scalar encoder might return a percentage match, etc.\n\n:param expValues: Array of expected scalar values, typically obtained from\n                 :meth:`.getScalars`\n:param actValues: Array of actual values, typically obtained from\n                 :meth:`.topDownCompute`\n\n:return: Array of closeness scores, one per item in expValues (or\n         actValues).", "id": "f17542:c0:m24"}
{"signature": "def __call__(self, r):", "body": "if not is_secure_transport(r.url):<EOL><INDENT>raise InsecureTransportError()<EOL><DEDENT>r.url, r.headers, r.body = self._client.add_token(<EOL>r.url, http_method=r.method, body=r.body, headers=r.headers<EOL>)<EOL>return r<EOL>", "docstring": "Append an OAuth 2 token to the request.\n\n        Note that currently HTTPS is required for all requests. There may be\n        a token type that allows for plain HTTP in the future and then this\n        should be updated to allow plain HTTP on a white list basis.", "id": "f6401:c0:m1"}
{"signature": "def priceTarget(symbol, token='<STR_LIT>', version='<STR_LIT>'):", "body": "_raiseIfNotStr(symbol)<EOL>return _getJson('<STR_LIT>' + symbol + '<STR_LIT>', token, version)<EOL>", "docstring": "Provides the latest avg, high, and low analyst price target for a symbol.\n\n    https://iexcloud.io/docs/api/#price-target\n    Updates at 10am, 11am, 12pm UTC every day\n\n    Args:\n        symbol (string); Ticker to request\n        token (string); Access token\n        version (string); API version\n\n    Returns:\n        dict: result", "id": "f2330:m75"}
{"signature": "def output(self, to=None, *args, **kwargs):", "body": "to.write(cgi.escape(str(self._value)))<EOL>", "docstring": "Outputs the set text", "id": "f3318:c1:m0"}
{"signature": "def get_rgba(self, tex, fontsize=None, dpi=None, rgb=(<NUM_LIT:0>,<NUM_LIT:0>,<NUM_LIT:0>)):", "body": "if not fontsize: fontsize = rcParams['<STR_LIT>']<EOL>if not dpi: dpi = rcParams['<STR_LIT>']<EOL>r,g,b = rgb<EOL>key = tex, self.get_font_config(), fontsize, dpi, tuple(rgb)<EOL>Z = self.rgba_arrayd.get(key)<EOL>if Z is None:<EOL><INDENT>alpha = self.get_grey(tex, fontsize, dpi)<EOL>Z = np.zeros((alpha.shape[<NUM_LIT:0>], alpha.shape[<NUM_LIT:1>], <NUM_LIT:4>), np.float)<EOL>Z[:,:,<NUM_LIT:0>] = r<EOL>Z[:,:,<NUM_LIT:1>] = g<EOL>Z[:,:,<NUM_LIT:2>] = b<EOL>Z[:,:,<NUM_LIT:3>] = alpha<EOL>self.rgba_arrayd[key] = Z<EOL><DEDENT>return Z<EOL>", "docstring": "Returns latex's rendering of the tex string as an rgba array", "id": "f17185:c0:m12"}
{"signature": "def finalize_headers(self, environ, response_headers):", "body": "pass<EOL>", "docstring": "Perform custom operations on the response headers.\n\n        This gets called before the response is started.\n        It enables adding additional headers or modifying the default ones.", "id": "f8596:c0:m43"}
{"signature": "def window_riemann(N):", "body": "n = linspace(-N/<NUM_LIT>, (N)/<NUM_LIT>, N)<EOL>w = sin(n/float(N)*<NUM_LIT>*pi) / (n / float(N)*<NUM_LIT>*pi)<EOL>return w<EOL>", "docstring": "r\"\"\"Riemann tapering window\n\n    :param int N: window length\n\n    .. math:: w(n) = 1 - \\left| \\frac{n}{N/2}  \\right|^2\n\n    with :math:`-N/2 \\leq n \\leq N/2`.\n\n    .. plot::\n        :width: 80%\n        :include-source:\n\n        from spectrum import window_visu\n        window_visu(64, 'riesz')\n\n    .. seealso:: :func:`create_window`, :class:`Window`", "id": "f10924:m25"}
{"signature": "def add_value_predicate(self, field_name, value_predicate,<EOL>code=VALUE_PREDICATE_FALSE,<EOL>message=MESSAGES[VALUE_PREDICATE_FALSE],<EOL>modulus=<NUM_LIT:1>):", "body": "assert field_name in self._field_names, '<STR_LIT>' % field_name<EOL>assert callable(value_predicate), '<STR_LIT>'<EOL>t = field_name, value_predicate, code, message, modulus<EOL>self._value_predicates.append(t)<EOL>", "docstring": "Add a value predicate function for the specified field.\n\nN.B., everything you can do with value predicates can also be done with\nvalue check functions, whether you use one or the other is a matter of\nstyle.\n\nArguments\n---------\n\n`field_name` - the name of the field to attach the value predicate\nfunction to\n\n`value_predicate` - a function that accepts a single argument (a value)\nand returns False if the value is not valid\n\n`code` - problem code to report if a value is not valid, defaults to\n`VALUE_PREDICATE_FALSE`\n\n`message` - problem message to report if a value is not valid\n\n`modulus` - apply the check to every nth record, defaults to 1 (check\nevery record)", "id": "f10012:c1:m4"}
{"signature": "def parse(data):", "body": "sections = re.compile(\"<STR_LIT>\", re.MULTILINE).split(data)<EOL>headings = re.findall(\"<STR_LIT>\", data, re.MULTILINE)<EOL>sections.pop(<NUM_LIT:0>)<EOL>parsed = []<EOL>def func(h, s):<EOL><INDENT>p = parse_heading(h)<EOL>p[\"<STR_LIT:content>\"] = s<EOL>parsed.append(p)<EOL><DEDENT>list(map(func, headings, sections))<EOL>return parsed<EOL>", "docstring": "Parse the given ChangeLog data into a list of Hashes.\n\n@param [String] data File data from the ChangeLog.md\n@return [Array<Hash>] Parsed data, e.g. [{ 'version' => ..., 'url' => ..., 'date' => ..., 'content' => ...}, ...]", "id": "f4924:m1"}
{"signature": "def t_COMMENT(self, t):", "body": "pass<EOL>", "docstring": "r'(\\#|(//)).*", "id": "f11424:c0:m27"}
{"signature": "@instruction<EOL><INDENT>def CBZ(cpu, op, dest):<DEDENT>", "body": "cpu.PC = Operators.ITEBV(cpu.address_bit_size,<EOL>op.read(), cpu.PC, dest.read())<EOL>", "docstring": "Compare and Branch on Zero compares the value in a register with zero, and conditionally branches forward\na constant value. It does not affect the condition flags.\n\n:param ARMv7Operand op: Specifies the register that contains the first operand.\n:param ARMv7Operand dest:\n    Specifies the label of the instruction that is to be branched to. The assembler calculates the\n    required value of the offset from the PC value of the CBZ instruction to this label, then\n    selects an encoding that will set imm32 to that offset. Allowed offsets are even numbers in\n    the range 0 to 126.", "id": "f16977:c4:m60"}
{"signature": "def get_evolution_covariances(self):", "body": "return self.__covariances_evolution<EOL>", "docstring": "!\n        @return (list) Covariance matrix (or variance in case of one-dimensional data) of each cluster on each step of clustering.", "id": "f15595:c2:m4"}
{"signature": "@lru_cache(maxsize=<NUM_LIT:8>)<EOL>def _get_scheme_map(input_encoding, output_encoding):", "body": "return SchemeMap(SCHEMES[input_encoding], SCHEMES[output_encoding])<EOL>", "docstring": "Provides a caching layer on top of `SchemeMap` objects to allow faster\n    access to scheme maps we've instantiated once.\n\n    :param input_encoding: Input encoding. Must be defined in `SCHEMES`.\n    :param output_encoding: Input encoding. Must be defined in `SCHEMES`.", "id": "f8765:m0"}
{"signature": "def valid_path(path):", "body": "<EOL>if path.endswith('<STR_LIT:*>'):<EOL><INDENT>Log.debug('<STR_LIT>', path[:-<NUM_LIT:1>])<EOL>if os.path.isdir(path[:-<NUM_LIT:1>]):<EOL><INDENT>return True<EOL><DEDENT>return False<EOL><DEDENT>Log.debug('<STR_LIT>', path)<EOL>if os.path.isdir(path):<EOL><INDENT>return True<EOL><DEDENT>else:<EOL><INDENT>Log.debug('<STR_LIT>', path)<EOL>if os.path.isfile(path):<EOL><INDENT>return True<EOL><DEDENT><DEDENT>return False<EOL>", "docstring": "Check if an entry in the class path exists as either a directory or a file", "id": "f7417:m0"}
{"signature": "def waitForValueToChange(self, timeout=<NUM_LIT:10>):", "body": "<EOL>callback = AXCallbacks.returnElemCallback<EOL>retelem = None<EOL>return self.waitFor(timeout, '<STR_LIT>', callback=callback,<EOL>args=(retelem,))<EOL>", "docstring": "Convenience method to wait for value attribute of given element to\n        change.\n\n        Some types of elements (e.g. menu items) have their titles change,\n        so this will not work for those.  This seems to work best if you set\n        the notification at the application level.\n\n        Returns: Element or None", "id": "f10331:c1:m36"}
{"signature": "def _shift_required(tiles):", "body": "if tiles[<NUM_LIT:0>][<NUM_LIT:0>].tile_pyramid.is_global:<EOL><INDENT>tile_cols = sorted(list(set([t[<NUM_LIT:0>].col for t in tiles])))<EOL>if tile_cols == list(range(min(tile_cols), max(tile_cols) + <NUM_LIT:1>)):<EOL><INDENT>return False<EOL><DEDENT>else:<EOL><INDENT>def gen_groups(items):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>j = items[<NUM_LIT:0>]<EOL>group = [j]<EOL>for i in items[<NUM_LIT:1>:]:<EOL><INDENT>if i == j + <NUM_LIT:1>:<EOL><INDENT>group.append(i)<EOL><DEDENT>else:<EOL><INDENT>yield group<EOL>group = [i]<EOL><DEDENT>j = i<EOL><DEDENT>yield group<EOL><DEDENT>groups = list(gen_groups(tile_cols))<EOL>if len(groups) == <NUM_LIT:1>:<EOL><INDENT>return False<EOL><DEDENT>normal_distance = groups[-<NUM_LIT:1>][-<NUM_LIT:1>] - groups[<NUM_LIT:0>][<NUM_LIT:0>]<EOL>antimeridian_distance = (<EOL>groups[<NUM_LIT:0>][-<NUM_LIT:1>] + tiles[<NUM_LIT:0>][<NUM_LIT:0>].tile_pyramid.matrix_width(tiles[<NUM_LIT:0>][<NUM_LIT:0>].zoom)<EOL>) - groups[-<NUM_LIT:1>][<NUM_LIT:0>]<EOL>return antimeridian_distance < normal_distance<EOL><DEDENT><DEDENT>else:<EOL><INDENT>return False<EOL><DEDENT>", "docstring": "Determine if distance over antimeridian is shorter than normal distance.", "id": "f12820:m15"}
{"signature": "def check_values(self, values, args):", "body": "return (values, args)<EOL>", "docstring": "check_values(values : Values, args : [string])\n-> (values : Values, args : [string])\n\nCheck that the supplied option values and leftover arguments are\nvalid.  Returns the option values and leftover arguments\n(possibly adjusted, possibly completely new -- whatever you\nlike).  Default implementation just returns the passed-in\nvalues; subclasses may override as desired.", "id": "f16458:c13:m19"}
{"signature": "def p_subidentifier(self, p):", "body": "n = len(p)<EOL>if n == <NUM_LIT:2>:<EOL><INDENT>p[<NUM_LIT:0>] = p[<NUM_LIT:1>]<EOL><DEDENT>elif n == <NUM_LIT:5>:<EOL><INDENT>p[<NUM_LIT:0>] = (p[<NUM_LIT:1>], p[<NUM_LIT:3>])<EOL><DEDENT>", "docstring": "subidentifier : fuzzy_lowercase_identifier\n                         | NUMBER\n                         | LOWERCASE_IDENTIFIER '(' NUMBER ')", "id": "f5672:c0:m107"}
{"signature": "def _oct_to_dec(ip, check=True):", "body": "if check and not is_oct(ip):<EOL><INDENT>raise ValueError('<STR_LIT>' % ip)<EOL><DEDENT>if isinstance(ip, int):<EOL><INDENT>ip = oct(ip)<EOL><DEDENT>return int(str(ip), <NUM_LIT:8>)<EOL>", "docstring": "Octal to decimal conversion.", "id": "f3601:m19"}
{"signature": "@property<EOL><INDENT>def all_frames(self):<DEDENT>", "body": "return glob.glob(self.frameglob)<EOL>", "docstring": "Unordered list of all frames currently held on disk.", "id": "f6863:c0:m2"}
{"signature": "def inspectABF(abf=exampleABF,saveToo=False,justPlot=False):", "body": "pylab.close('<STR_LIT:all>')<EOL>print(\"<STR_LIT>\")<EOL>if type(abf) is str:<EOL><INDENT>abf=swhlab.ABF(abf)<EOL><DEDENT>swhlab.plot.new(abf,forceNewFigure=True)<EOL>if abf.sweepInterval*abf.sweeps<<NUM_LIT>*<NUM_LIT:5>: <EOL><INDENT>pylab.subplot(<NUM_LIT>)<EOL>pylab.title(\"<STR_LIT>\"%(abf.ID,abf.protoComment))<EOL>swhlab.plot.sweep(abf,'<STR_LIT:all>')<EOL>pylab.subplot(<NUM_LIT>)<EOL>swhlab.plot.sweep(abf,'<STR_LIT:all>',continuous=True)<EOL>swhlab.plot.comments(abf)<EOL><DEDENT>else:<EOL><INDENT>print(\"<STR_LIT>\")<EOL>swhlab.plot.sweep(abf,'<STR_LIT:all>',continuous=True,minutes=True)<EOL>swhlab.plot.comments(abf,minutes=True)<EOL>pylab.title(\"<STR_LIT>\"%(abf.ID,abf.protoComment))<EOL><DEDENT>swhlab.plot.annotate(abf)<EOL>if justPlot:<EOL><INDENT>return<EOL><DEDENT>if saveToo:<EOL><INDENT>path=os.path.split(abf.fname)[<NUM_LIT:0>]<EOL>basename=os.path.basename(abf.fname)<EOL>pylab.savefig(os.path.join(path,\"<STR_LIT:_>\"+basename.replace(\"<STR_LIT>\",\"<STR_LIT>\")))<EOL><DEDENT>pylab.show()<EOL>return<EOL>", "docstring": "May be given an ABF object or filename.", "id": "f11406:m44"}
{"signature": "@db.non_transactional(allow_existing=True)<EOL><INDENT>def locked_put(self, credentials):<DEDENT>", "body": "entity = self._model.get_or_insert(self._key_name)<EOL>setattr(entity, self._property_name, credentials)<EOL>entity.put()<EOL>if self._cache:<EOL><INDENT>self._cache.set(self._key_name, credentials.to_json())<EOL><DEDENT>", "docstring": "Write a Credentials to the datastore.\n\n        Args:\n            credentials: Credentials, the credentials to store.", "id": "f2463:c4:m5"}
{"signature": "def _learnPhase1(self, activeColumns, readOnly=False):", "body": "<EOL>self.lrnActiveState['<STR_LIT:t>'].fill(<NUM_LIT:0>)<EOL>numUnpredictedColumns = <NUM_LIT:0><EOL>for c in activeColumns:<EOL><INDENT>predictingCells = numpy.where(self.lrnPredictedState['<STR_LIT>'][c] == <NUM_LIT:1>)[<NUM_LIT:0>]<EOL>numPredictedCells = len(predictingCells)<EOL>assert numPredictedCells <= <NUM_LIT:1><EOL>if numPredictedCells == <NUM_LIT:1>:<EOL><INDENT>i = predictingCells[<NUM_LIT:0>]<EOL>self.lrnActiveState['<STR_LIT:t>'][c, i] = <NUM_LIT:1><EOL>continue<EOL><DEDENT>numUnpredictedColumns += <NUM_LIT:1><EOL>if readOnly:<EOL><INDENT>continue<EOL><DEDENT>i, s, numActive = self._getBestMatchingCell(<EOL>c, self.lrnActiveState['<STR_LIT>'], self.minThreshold)<EOL>if s is not None and s.isSequenceSegment():<EOL><INDENT>if self.verbosity >= <NUM_LIT:4>:<EOL><INDENT>print(\"<STR_LIT>\", c)<EOL><DEDENT>self.lrnActiveState['<STR_LIT:t>'][c, i] = <NUM_LIT:1><EOL>segUpdate = self._getSegmentActiveSynapses(<EOL>c, i, s, self.lrnActiveState['<STR_LIT>'], newSynapses = True)<EOL>s.totalActivations += <NUM_LIT:1><EOL>trimSegment = self._adaptSegment(segUpdate)<EOL>if trimSegment:<EOL><INDENT>self._trimSegmentsInCell(c, i, [s], minPermanence = <NUM_LIT>,<EOL>minNumSyns = <NUM_LIT:0>)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>i = self._getCellForNewSegment(c)<EOL>if (self.verbosity >= <NUM_LIT:4>):<EOL><INDENT>print(\"<STR_LIT>\", c, end='<STR_LIT:U+0020>')<EOL>print(\"<STR_LIT>\", i)<EOL><DEDENT>self.lrnActiveState['<STR_LIT:t>'][c, i] = <NUM_LIT:1><EOL>segUpdate = self._getSegmentActiveSynapses(<EOL>c, i, None, self.lrnActiveState['<STR_LIT>'], newSynapses=True)<EOL>segUpdate.sequenceSegment = True <EOL>self._adaptSegment(segUpdate)  <EOL><DEDENT><DEDENT>numBottomUpColumns = len(activeColumns)<EOL>if numUnpredictedColumns < numBottomUpColumns / <NUM_LIT:2>:<EOL><INDENT>return True   <EOL><DEDENT>else:<EOL><INDENT>return False<EOL><DEDENT>", "docstring": "Compute the learning active state given the predicted state and\nthe bottom-up input.\n\n:param activeColumns list of active bottom-ups\n:param readOnly      True if being called from backtracking logic.\n                     This tells us not to increment any segment\n                     duty cycles or queue up any updates.\n:returns: True if the current input was sufficiently predicted, OR\n         if we started over on startCells. False indicates that the current\n         input was NOT predicted, well enough to consider it as \"inSequence\"\n\nThis looks at:\n    - @ref lrnActiveState['t-1']\n    - @ref lrnPredictedState['t-1']\n\nThis modifies:\n    - @ref lrnActiveState['t']\n    - @ref lrnActiveState['t-1']", "id": "f17565:c0:m57"}
{"signature": "def __init__(<EOL>self,<EOL>mode=None,<EOL>source_game=None<EOL>):", "body": "<EOL>if mode is None:<EOL><INDENT>raise ValueError(<EOL>\"<STR_LIT>\"<EOL>)<EOL><DEDENT>if not isinstance(mode, GameMode):<EOL><INDENT>raise TypeError(<EOL>\"<STR_LIT>\"<EOL>)<EOL><DEDENT>self._key = None                    <EOL>self._status = None                 <EOL>self._ttl = None                    <EOL>self._answer = None                 <EOL>self._mode = None                   <EOL>self._guesses_remaining = None      <EOL>self._guesses_made = None           <EOL>if source_game:<EOL><INDENT>self.load(source=source_game)<EOL><DEDENT>else:<EOL><INDENT>self.new(mode=mode)<EOL><DEDENT>", "docstring": "Initialize a game object to hold the state, properties, and control of the game.\n\n:param mode: <required> A GameMode object defining the game play mode.\n:param source_game: <optional> A JSON Serialized representation of the game.", "id": "f12733:c0:m0"}
{"signature": "def _transliterate(self, text, outFormat):", "body": "def getResult(): <EOL><INDENT>if curMatch.isspace():<EOL><INDENT>result.append(curMatch)<EOL>return<EOL><DEDENT>if prevMatch in self:<EOL><INDENT>prev = self[prevMatch]<EOL><DEDENT>else:<EOL><INDENT>prev = None<EOL><DEDENT>if nextMatch in self:<EOL><INDENT>next = self[nextMatch]<EOL><DEDENT>else:<EOL><INDENT>next = None<EOL><DEDENT>try:<EOL><INDENT>equiv = outFormat._equivalent(self[curMatch], <EOL>prev, <EOL>next, <EOL>self._implicitA)<EOL><DEDENT>except KeyError:<EOL><INDENT>equiv = _unrecognised(curMatch)<EOL><DEDENT>for e in equiv:<EOL><INDENT>result.append(e)<EOL><DEDENT><DEDENT>def incr(c):<EOL><INDENT>if self._longestEntry == <NUM_LIT:1>:<EOL><INDENT>return <NUM_LIT:1><EOL><DEDENT>return len(c)<EOL><DEDENT>result = []<EOL>text = self._preprocess(text)<EOL>i = <NUM_LIT:0><EOL>prevMatch = None<EOL>nextMatch = None<EOL>curMatch = self._getNextChar(text, i)<EOL>i = i + len(curMatch)<EOL>while i < len(text):<EOL><INDENT>nextMatch = self._getNextChar(text, i)<EOL>getResult()<EOL>i = i + len(nextMatch)<EOL>prevMatch = curMatch<EOL>curMatch = nextMatch<EOL>nextMatch = None<EOL><DEDENT>getResult() <EOL>return result<EOL>", "docstring": "Transliterate a devanagari text into the target format.\n\n        Transliterating a character to or from Devanagari is not a simple \n        lookup: it depends on the preceding and following characters.", "id": "f8768:c4:m0"}
{"signature": "@constrain.setter<EOL><INDENT>def constrain(self, value):<DEDENT>", "body": "self._constrain = bool(value)<EOL>", "docstring": "Set state of constrain to axis mode.", "id": "f15133:c0:m4"}
{"signature": "@property<EOL><INDENT>def Vml(self):<DEDENT>", "body": "return self.VolumeLiquidMixture(T=self.T, P=self.P, zs=self.zs, ws=self.ws)<EOL>", "docstring": "r'''Liquid-phase molar volume of the mixture at its current\n        temperature, pressure, and composition in units of [m^3/mol]. For\n        calculation of this property at other temperatures or pressures or\n        compositions, or specifying manually the method used to calculate it,\n        and more - see the object oriented interface\n        :obj:`thermo.volume.VolumeLiquidMixture`; each Mixture instance\n        creates one to actually perform the calculations.\n\n        Examples\n        --------\n        >>> Mixture(['cyclobutane'], ws=[1], T=225).Vml\n        7.42395423425395e-05", "id": "f15811:c0:m109"}
{"signature": "def min_item(self):", "body": "if self.is_empty():<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>node = self._root<EOL>while node.left is not None:<EOL><INDENT>node = node.left<EOL><DEDENT>return node.key, node.value<EOL>", "docstring": "Get item with min key of tree, raises ValueError if tree is empty.", "id": "f16266:c3:m5"}
{"signature": "def inquire(self):", "body": "name = copy.deepcopy(self.fname)<EOL>stats = fortran_cdf.inquire(name)<EOL>status = stats[<NUM_LIT:0>]<EOL>if status == <NUM_LIT:0>:<EOL><INDENT>self._num_dims = stats[<NUM_LIT:1>]<EOL>self._dim_sizes = stats[<NUM_LIT:2>]<EOL>self._encoding = stats[<NUM_LIT:3>]<EOL>self._majority = stats[<NUM_LIT:4>]<EOL>self._max_rec = stats[<NUM_LIT:5>]<EOL>self._num_r_vars = stats[<NUM_LIT:6>]<EOL>self._num_z_vars = stats[<NUM_LIT:7>]<EOL>self._num_attrs = stats[<NUM_LIT:8>]<EOL><DEDENT>else:<EOL><INDENT>raise IOError(fortran_cdf.statusreporter(status))<EOL><DEDENT>", "docstring": "Maps to fortran CDF_Inquire.\n\n        Assigns parameters returned by CDF_Inquire\n        to pysatCDF instance. Not intended\n        for regular direct use by user.", "id": "f13658:c0:m4"}
{"signature": "def add_file(self, filename):", "body": "notify.warning('<STR_LIT>'.format(filename))<EOL>", "docstring": "Stage a file for committing, or commit it directly (depending on the SCM).", "id": "f3287:c0:m1"}
{"signature": "@property<EOL><INDENT>def mass(self):<DEDENT>", "body": "return self.ode_body.getMass()<EOL>", "docstring": "The ODE mass object for this body.", "id": "f14887:c1:m2"}
{"signature": "def is_after(self, other):", "body": "if type(self.val) is not datetime.datetime:<EOL><INDENT>raise TypeError('<STR_LIT>' % type(self.val).__name__)<EOL><DEDENT>if type(other) is not datetime.datetime:<EOL><INDENT>raise TypeError('<STR_LIT>' % type(other).__name__)<EOL><DEDENT>if self.val <= other:<EOL><INDENT>self._err('<STR_LIT>' % (self.val.strftime('<STR_LIT>'), other.strftime('<STR_LIT>')))<EOL><DEDENT>return self<EOL>", "docstring": "Asserts that val is a date and is after other date.", "id": "f9307:c0:m63"}
{"signature": "def setFieldStats(self, fieldName, fieldStatistics):", "body": "pass<EOL>", "docstring": "This method is called by the model to set the statistics like min and\nmax for the underlying encoders if this information is available.\n\n:param fieldName: name of the field this encoder is encoding, provided by\n      :class:`~.nupic.encoders.multi.MultiEncoder`.\n\n:param fieldStatistics: dictionary of dictionaries with the first level being\n      the fieldname and the second index the statistic ie:\n      ``fieldStatistics['pounds']['min']``", "id": "f17542:c0:m3"}
{"signature": "def update(self, list_id, segment_id, data):", "body": "return self._mc_client._patch(url=self._build_path(list_id, '<STR_LIT>', segment_id), data=data)<EOL>", "docstring": "updates an existing list segment.", "id": "f256:c0:m3"}
{"signature": "def _find_first_bigger(self, timestamps, target, lower_bound, upper_bound):", "body": "while lower_bound < upper_bound:<EOL><INDENT>pos = lower_bound + (upper_bound - lower_bound) / <NUM_LIT:2><EOL>if timestamps[pos] > target:<EOL><INDENT>upper_bound = pos<EOL><DEDENT>else:<EOL><INDENT>lower_bound = pos + <NUM_LIT:1><EOL><DEDENT><DEDENT>return pos<EOL>", "docstring": "Find the first element in timestamps whose value is bigger than target.\nparam list values: list of timestamps(epoch number).\nparam target: target value.\nparam lower_bound: lower bound for binary search.\nparam upper_bound: upper bound for binary search.", "id": "f7847:c0:m3"}
{"signature": "def is_dot(ip):", "body": "octets = str(ip).split('<STR_LIT:.>')<EOL>if len(octets) != <NUM_LIT:4>:<EOL><INDENT>return False<EOL><DEDENT>for i in octets:<EOL><INDENT>try:<EOL><INDENT>val = int(i)<EOL><DEDENT>except ValueError:<EOL><INDENT>return False<EOL><DEDENT>if val > <NUM_LIT:255> or val < <NUM_LIT:0>:<EOL><INDENT>return False<EOL><DEDENT><DEDENT>return True<EOL>", "docstring": "Return true if the IP address is in dotted decimal notation.", "id": "f3601:m2"}
{"signature": "def delete(self, key):", "body": "(_value, mem) = LRUCache.get(self, key)<EOL>self._mem -= mem<EOL>LRUCache.delete(self, key)<EOL>", "docstring": ">>> c = MemSizeLRUCache()\n>>> c.put(1, 1)\n>>> c.mem()\n24\n>>> c.delete(1)\n>>> c.mem()\n0", "id": "f4367:c1:m4"}
{"signature": "def guid(self, guid):", "body": "return self._json(self._get(self._build_url('<STR_LIT>', guid)), <NUM_LIT:200>)['<STR_LIT:data>']['<STR_LIT:type>']<EOL>", "docstring": "Determines JSONAPI type for provided GUID", "id": "f5389:c0:m3"}
{"signature": "def get_argument_cluster(self):", "body": "try:<EOL><INDENT>return self.get_argument(constants.PARAM_CLUSTER)<EOL><DEDENT>except tornado.web.MissingArgumentError as e:<EOL><INDENT>raise Exception(e.log_message)<EOL><DEDENT>", "docstring": "Helper function to get request argument.\nRaises exception if argument is missing.\nReturns the cluster argument.", "id": "f7329:c0:m8"}
{"signature": "def EXP_gas(self, base, exponent):", "body": "EXP_SUPPLEMENTAL_GAS = <NUM_LIT:10>   <EOL>def nbytes(e):<EOL><INDENT>result = <NUM_LIT:0><EOL>for i in range(<NUM_LIT:32>):<EOL><INDENT>result = Operators.ITEBV(<NUM_LIT>, Operators.EXTRACT(e, i * <NUM_LIT:8>, <NUM_LIT:8>) != <NUM_LIT:0>, i + <NUM_LIT:1>, result)<EOL><DEDENT>return result<EOL><DEDENT>return EXP_SUPPLEMENTAL_GAS * nbytes(exponent)<EOL>", "docstring": "Calculate extra gas fee", "id": "f17019:c16:m51"}
{"signature": "def save_dataset(self, dataset=None, dataset_name=None, **kwargs):", "body": "self._fill_project_info(kwargs)<EOL>if dataset_name is None:<EOL><INDENT>raise Exception(\"<STR_LIT>\")<EOL><DEDENT>kwargs.update({'<STR_LIT>': dataset_name})<EOL>s = time.time()<EOL>try:<EOL><INDENT>dataset_id = self.dataset_fs.put(self._serialization(dataset))<EOL>kwargs.update({'<STR_LIT>': dataset_id, '<STR_LIT:time>': datetime.utcnow()})<EOL>self.db.Dataset.insert_one(kwargs)<EOL>print(\"<STR_LIT>\".format(round(time.time() - s, <NUM_LIT:2>)))<EOL>return True<EOL><DEDENT>except Exception as e:<EOL><INDENT>exc_type, exc_obj, exc_tb = sys.exc_info()<EOL>fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[<NUM_LIT:1>]<EOL>logging.info(\"<STR_LIT>\".format(exc_type, exc_obj, fname, exc_tb.tb_lineno, e))<EOL>print(\"<STR_LIT>\")<EOL>return False<EOL><DEDENT>", "docstring": "Saves one dataset into database, timestamp will be added automatically.\n\n        Parameters\n        ----------\n        dataset : any type\n            The dataset you want to store.\n        dataset_name : str\n            The name of dataset.\n        kwargs : other events\n            Other events, such as description, author and etc (optinal).\n\n        Examples\n        ----------\n        Save dataset\n        >>> db.save_dataset([X_train, y_train, X_test, y_test], 'mnist', description='this is a tutorial')\n\n        Get dataset\n        >>> dataset = db.find_top_dataset('mnist')\n\n        Returns\n        ---------\n        boolean : Return True if save success, otherwise, return False.", "id": "f11203:c0:m8"}
{"signature": "@api.check(<NUM_LIT:2>, \"<STR_LIT>\")<EOL>def control_move(title, control, x, y, width=-<NUM_LIT:1>, height=-<NUM_LIT:1>, **kwargs):", "body": "text = kwargs.get(\"<STR_LIT:text>\", \"<STR_LIT>\")<EOL>ret = AUTO_IT.AU3_ControlMove(<EOL>LPCWSTR(title), LPCWSTR(text), LPCWSTR(control),<EOL>INT(x), INT(y), INT(width), INT(height)<EOL>)<EOL>return ret<EOL>", "docstring": ":param title:\n:param control:\n:param x:\n:param y:\n:param kwargs:\n:return:", "id": "f5585:m22"}
{"signature": "def p_Text(self, p):", "body": "p[<NUM_LIT:0>] = p[<NUM_LIT:1>][<NUM_LIT:1>:-<NUM_LIT:1>]<EOL>", "docstring": "Text : QUOTED_STRING", "id": "f5672:c0:m103"}
{"signature": "def __neg__(self):", "body": "return np.negative(self)<EOL>", "docstring": "x.__neg__() <==> -x", "id": "f4853:c0:m51"}
{"signature": "@modify_output<EOL>def get_role(role, flags=FLAGS.ALL, **conn):", "body": "role = modify(role, output='<STR_LIT>')<EOL>_conn_from_args(role, conn)<EOL>return registry.build_out(flags, start_with=role, pass_datastructure=True, **conn)<EOL>", "docstring": "Orchestrates all the calls required to fully build out an IAM Role in the following format:\n\n{\n    \"Arn\": ...,\n    \"AssumeRolePolicyDocument\": ...,\n    \"CreateDate\": ...,  # str\n    \"InlinePolicies\": ...,\n    \"InstanceProfiles\": ...,\n    \"ManagedPolicies\": ...,\n    \"Path\": ...,\n    \"RoleId\": ...,\n    \"RoleName\": ...,\n    \"Tags\": {},\n    \"_version\": 3\n}\n\n:param role: dict containing (at the very least) role_name and/or arn.\n:param output: Determines whether keys should be returned camelized or underscored.\n:param conn: dict containing enough information to make a connection to the desired account.\nMust at least have 'assume_role' key.\n:return: dict containing a fully built out role.", "id": "f10849:m5"}
{"signature": "def detach(self, droplet_id, region):", "body": "return self.get_data(<EOL>\"<STR_LIT>\" % self.id,<EOL>type=POST,<EOL>params={\"<STR_LIT:type>\": \"<STR_LIT>\",<EOL>\"<STR_LIT>\": droplet_id,<EOL>\"<STR_LIT>\": region}<EOL>)<EOL>", "docstring": "Detach a Volume to a Droplet.\n\nArgs:\n    droplet_id: int - droplet id\n    region: string - slug identifier for the region", "id": "f1478:c0:m7"}
{"signature": "@staticmethod<EOL><INDENT>def _is_string(value):<DEDENT>", "body": "if type(value) in [type(u'<STR_LIT>'), type('<STR_LIT>')]:<EOL><INDENT>return True<EOL><DEDENT>elif type(value) in [int, type(<NUM_LIT:2> ** <NUM_LIT:64>)]:<EOL><INDENT>return False<EOL><DEDENT>else:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Checks if the value provided is a string (True) or not integer\n(False) or something else (None).", "id": "f1476:c0:m2"}
{"signature": "def assign(self, var, val, assignment):", "body": "oldval = assignment.get(var, None)<EOL>if val != oldval:<EOL><INDENT>if oldval is not None: <EOL><INDENT>self.record_conflict(assignment, var, oldval, -<NUM_LIT:1>)<EOL><DEDENT>self.record_conflict(assignment, var, val, +<NUM_LIT:1>)<EOL>CSP.assign(self, var, val, assignment)<EOL><DEDENT>", "docstring": "Assign var, and keep track of conflicts.", "id": "f1681:c2:m2"}
{"signature": "@logExceptions(_LOGGER)<EOL><INDENT>@g_retrySQL<EOL>def jobSetCompleted(self, jobID, completionReason, completionMsg,<EOL>useConnectionID = True):<DEDENT>", "body": "<EOL>with ConnectionFactory.get() as conn:<EOL><INDENT>query = '<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>'% (self.jobsTableName,)<EOL>sqlParams = [self.STATUS_COMPLETED, completionReason, completionMsg,<EOL>jobID]<EOL>if useConnectionID:<EOL><INDENT>query += '<STR_LIT>'<EOL>sqlParams.append(self._connectionID)<EOL><DEDENT>result = conn.cursor.execute(query, sqlParams)<EOL>if result != <NUM_LIT:1>:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (jobID))<EOL><DEDENT><DEDENT>", "docstring": "Change the status on the given job to completed\n\n        Parameters:\n        ----------------------------------------------------------------\n        job:                 jobID of the job to mark as completed\n        completionReason:    completionReason string\n        completionMsg:       completionMsg string\n\n        useConnectionID: True if the connection id of the calling function\n        must be the same as the connection that created the job. Set\n        to False for hypersearch workers", "id": "f17555:c1:m35"}
{"signature": "@app.route('<STR_LIT>', methods=['<STR_LIT:GET>'])<EOL><INDENT>def acme_challenge_ping(self, request):<DEDENT>", "body": "request.setResponseCode(OK)<EOL>write_request_json(request, {'<STR_LIT:message>': '<STR_LIT>'})<EOL>", "docstring": "Respond to requests on ``/.well-known/acme-challenge/ping`` to debug\npath routing issues.", "id": "f13697:c0:m3"}
{"signature": "def check_timers(self):", "body": "if self._current is None:<EOL><INDENT>advance = min([self.clocks] + [x for x in self.timers if x is not None]) + <NUM_LIT:1><EOL>logger.debug(f\"<STR_LIT>\")<EOL>self.clocks = advance<EOL><DEDENT>for procid in range(len(self.timers)):<EOL><INDENT>if self.timers[procid] is not None:<EOL><INDENT>if self.clocks > self.timers[procid]:<EOL><INDENT>self.procs[procid].PC += self.procs[procid].instruction.size<EOL>self.awake(procid)<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Awake process if timer has expired", "id": "f17024:c9:m107"}
{"signature": "def __update_peripheral_neurons(self, t, step, next_membrane, next_active_sodium, next_inactive_sodium, next_active_potassium):", "body": "self._membrane_potential = next_membrane[:];<EOL>self._active_cond_sodium = next_active_sodium[:];<EOL>self._inactive_cond_sodium = next_inactive_sodium[:];<EOL>self._active_cond_potassium = next_active_potassium[:];<EOL>for index in range(<NUM_LIT:0>, self._num_osc):<EOL><INDENT>if (self._pulse_generation[index] is False):<EOL><INDENT>if (self._membrane_potential[index] >= <NUM_LIT:0.0>):<EOL><INDENT>self._pulse_generation[index] = True;<EOL>self._pulse_generation_time[index].append(t);<EOL><DEDENT><DEDENT>elif (self._membrane_potential[index] < <NUM_LIT:0.0>):<EOL><INDENT>self._pulse_generation[index] = False;<EOL><DEDENT>if (self._link_weight3[index] == <NUM_LIT:0.0>):<EOL><INDENT>if (self._membrane_potential[index] > self._params.threshold):<EOL><INDENT>self._link_pulse_counter[index] += step;<EOL>if (self._link_pulse_counter[index] >= <NUM_LIT:1> / self._params.eps):<EOL><INDENT>self._link_weight3[index] = self._params.w3;<EOL>self._link_activation_time[index] = t;<EOL><DEDENT><DEDENT><DEDENT>elif ( not ((self._link_activation_time[index] < t) and (t < self._link_activation_time[index] + self._params.deltah)) ):<EOL><INDENT>self._link_weight3[index] = <NUM_LIT:0.0>;<EOL>self._link_pulse_counter[index] = <NUM_LIT:0.0>;<EOL><DEDENT><DEDENT>", "docstring": "!\n        @brief Update peripheral neurons in line with new values of current in channels.\n\n        @param[in] t (doubles): Current time of simulation.\n        @param[in] step (uint): Step (time duration) during simulation when states of oscillators should be calculated.\n        @param[in] next_membrane (list): New values of membrane potentials for peripheral neurons.\n        @Param[in] next_active_sodium (list): New values of activation conductances of the sodium channels for peripheral neurons.\n        @param[in] next_inactive_sodium (list): New values of inactivaton conductances of the sodium channels for peripheral neurons.\n        @param[in] next_active_potassium (list): New values of activation conductances of the potassium channel for peripheral neurons.", "id": "f15660:c2:m5"}
{"signature": "def reset(self):", "body": "self.timestep_ix = <NUM_LIT:0><EOL>", "docstring": "Resets the clock's timestep index to '0'.", "id": "f15874:c1:m2"}
{"signature": "def _source_call_only(self):", "body": "line = self.source[self.col_offset:]<EOL>regex = re.compile('''<STR_LIT>''')<EOL>match = regex.match(line)<EOL>if not match:<EOL><INDENT>return self.source<EOL><DEDENT>return match.group(<NUM_LIT:1>)<EOL>", "docstring": "Return the source line stripped down to just the pyconfig call.", "id": "f12386:c2:m6"}
{"signature": "@property<EOL><INDENT>def private_ip(self):<DEDENT>", "body": "ip = None<EOL>for eth in self.networks['<STR_LIT>']:<EOL><INDENT>if eth['<STR_LIT:type>'] == '<STR_LIT>':<EOL><INDENT>ip = eth['<STR_LIT>']<EOL>break<EOL><DEDENT><DEDENT>if ip is None:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>return ip<EOL>", "docstring": "Private ip_address", "id": "f4446:c1:m28"}
{"signature": "@property<EOL><INDENT>def width(self):<DEDENT>", "body": "return self.shape[<NUM_LIT:1>]<EOL>", "docstring": "Get the width of the image on which the bounding boxes fall.\n\nReturns\n-------\nint\n    Image width.", "id": "f16277:c1:m2"}
{"signature": "def print_exception(etype, value, tb, limit=None, file=None):", "body": "if file is None:<EOL><INDENT>file = open('<STR_LIT>', '<STR_LIT:w>')<EOL><DEDENT>if tb:<EOL><INDENT>_print(file, '<STR_LIT>')<EOL>print_tb(tb, limit, file)<EOL><DEDENT>lines = format_exception_only(etype, value)<EOL>for line in lines:<EOL><INDENT>_print(file, line, '<STR_LIT>')<EOL><DEDENT>", "docstring": "Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.", "id": "f16448:m6"}
{"signature": "def num_items(self):", "body": "return len(self._items_list)<EOL>", "docstring": "Get the number of items in the cache.\n\n        :return: number of items.\n        :returntype: `int`", "id": "f15241:c2:m6"}
{"signature": "@staticmethod<EOL><INDENT>def get_unicodes(codepoint):<DEDENT>", "body": "result = re.sub('<STR_LIT>', '<STR_LIT>', codepoint.text)<EOL>return Extension.convert_to_list_of_unicodes(result)<EOL>", "docstring": "Return list of unicodes for <scanning-codepoints>", "id": "f16564:c0:m2"}
{"signature": "def _mpsse_enable(self):", "body": "<EOL>self._check(ftdi.set_bitmode, <NUM_LIT:0>, <NUM_LIT:0>)<EOL>self._check(ftdi.set_bitmode, <NUM_LIT:0>, <NUM_LIT:2>)<EOL>", "docstring": "Enable MPSSE mode on the FTDI device.", "id": "f8002:c0:m5"}
{"signature": "def get_correlation_result(self):", "body": "return self.correlation_result<EOL>", "docstring": "Get correlation result.\n:return CorrelationResult: a CorrelationResult object represents the correlation result.", "id": "f7849:c0:m2"}
{"signature": "def set_chksum(self, doc, chk_sum):", "body": "if chk_sum:<EOL><INDENT>doc.ext_document_references[-<NUM_LIT:1>].check_sum = checksum.Algorithm(<EOL>'<STR_LIT>', chk_sum)<EOL><DEDENT>else:<EOL><INDENT>raise SPDXValueError('<STR_LIT>')<EOL><DEDENT>", "docstring": "Sets the external document reference's check sum, if not already set.\nchk_sum - The checksum value in the form of a string.", "id": "f3750:c1:m0"}
{"signature": "def pprint(arr, columns=('<STR_LIT>', '<STR_LIT>'),<EOL>names=('<STR_LIT>', '<STR_LIT>'),<EOL>max_rows=<NUM_LIT:32>, precision=<NUM_LIT:2>):", "body": "if max_rows is True:<EOL><INDENT>pd.set_option('<STR_LIT>', <NUM_LIT:1000>)<EOL><DEDENT>elif type(max_rows) is int:<EOL><INDENT>pd.set_option('<STR_LIT>', max_rows)<EOL><DEDENT>pd.set_option('<STR_LIT>', precision)<EOL>df = pd.DataFrame(arr.flatten(), index=arr['<STR_LIT:id>'].flatten(),<EOL>columns=columns)<EOL>df.columns = names<EOL>return df.style.format({names[<NUM_LIT:0>]: '<STR_LIT>',<EOL>names[<NUM_LIT:1>]: '<STR_LIT>'})<EOL>", "docstring": "Create a pandas DataFrame from a numpy ndarray.\n\nBy default use temp and lum with max rows of 32 and precision of 2.\n\narr - An numpy.ndarray.\ncolumns - The columns to include in the pandas DataFrame. Defaults to\n          temperature and luminosity.\nnames - The column names for the pandas DataFrame. Defaults to\n        Temperature and Luminosity.\nmax_rows - If max_rows is an integer then set the pandas\n           display.max_rows option to that value. If max_rows\n           is True then set display.max_rows option  to 1000.\nprecision - An integer to set the pandas precision option.", "id": "f11253:m1"}
{"signature": "def scopemap(self):", "body": "utility.debug_print(self.result)<EOL>", "docstring": "Output scopemap.", "id": "f12427:c2:m3"}
{"signature": "def get_mutation_aspect(self):", "body": "return self._mutation_aspect<EOL>", "docstring": "Return the aspect ratio of the bbox mutation.", "id": "f17197:c16:m6"}
{"signature": "def PLL_cbb(x,fs,loop_type,Kv,fn,zeta):", "body": "T = <NUM_LIT:1>/float(fs)<EOL>Kv = <NUM_LIT:2>*np.pi*Kv <EOL>if loop_type == <NUM_LIT:1>:<EOL><INDENT>K = <NUM_LIT:2>*np.pi*fn <EOL><DEDENT>elif loop_type == <NUM_LIT:2>:<EOL><INDENT>K = <NUM_LIT:4> *np.pi*zeta*fn <EOL>tau2 = zeta/(np.pi*fn)<EOL><DEDENT>elif loop_type == <NUM_LIT:3>:<EOL><INDENT>K = Kv <EOL>tau1 = K/((<NUM_LIT:2>*np.pi*fn)^<NUM_LIT:2>);<EOL>tau2 = <NUM_LIT:2>*zeta/(<NUM_LIT:2>*np.pi*fn)*(<NUM_LIT:1> - <NUM_LIT:2>*np.pi*fn/K*<NUM_LIT:1>/(<NUM_LIT:2>*zeta))<EOL><DEDENT>else:<EOL><INDENT>print('<STR_LIT>')<EOL><DEDENT>filt_in_last = <NUM_LIT:0>; filt_out_last = <NUM_LIT:0>;<EOL>vco_in_last = <NUM_LIT:0>; vco_out = <NUM_LIT:0>; vco_out_last = <NUM_LIT:0>;<EOL>vco_out_cbb = <NUM_LIT:0><EOL>n = np.arange(len(x))<EOL>theta_hat = np.zeros(len(x))<EOL>ev = np.zeros(len(x))<EOL>phi = np.zeros(len(x))<EOL>for k in  range(len(n)):<EOL><INDENT>phi[k] = np.imag(x[k] * np.conj(vco_out_cbb))<EOL>pd_out = phi[k]<EOL>gain_out = K/Kv*pd_out <EOL>if loop_type == <NUM_LIT:2>:<EOL><INDENT>filt_in = (<NUM_LIT:1>/tau2)*gain_out<EOL>filt_out = filt_out_last + T/<NUM_LIT:2>*(filt_in + filt_in_last)<EOL>filt_in_last = filt_in<EOL>filt_out_last = filt_out<EOL>filt_out = filt_out + gain_out<EOL><DEDENT>elif loop_type == <NUM_LIT:3>:<EOL><INDENT>filt_in = (tau2/tau1)*gain_out - (<NUM_LIT:1>/tau1)*filt_out_last<EOL>u3 = filt_in + (<NUM_LIT:1>/tau2)*filt_out_last<EOL>filt_out = filt_out_last + T/<NUM_LIT:2>*(filt_in + filt_in_last)<EOL>filt_in_last = filt_in<EOL>filt_out_last = filt_out<EOL><DEDENT>else:<EOL><INDENT>filt_out = gain_out;<EOL><DEDENT>vco_in = filt_out<EOL>if loop_type == <NUM_LIT:3>:<EOL><INDENT>vco_in = u3<EOL><DEDENT>vco_out = vco_out_last + T/<NUM_LIT:2>*(vco_in + vco_in_last)<EOL>vco_in_last = vco_in<EOL>vco_out_last = vco_out<EOL>vco_out = Kv*vco_out <EOL>vco_out_cbb = np.exp(<NUM_LIT>*vco_out)<EOL>ev[k] = vco_in<EOL>theta_hat[k] = vco_out<EOL><DEDENT>return theta_hat, ev, phi<EOL>", "docstring": "Baseband Analog PLL Simulation Model\n\n:param x: input phase deviation in radians\n:param fs: sampling rate in sample per second or Hz\n:param loop_type: 1, first-order loop filter F(s)=K_LF; 2, integrator\n            with lead compensation F(s) = (1 + s tau2)/(s tau1),\n            i.e., a type II, or 3, lowpass with lead compensation\n            F(s) = (1 + s tau2)/(1 + s tau1)\n:param Kv: VCO gain in Hz/v; note presently assume Kp = 1v/rad\n            and K_LF = 1; the user can easily change this\n:param fn: Loop natural frequency (loops 2 & 3) or cutoff\n            frequency (loop 1)\n:param zeta: Damping factor for loops 2 & 3\n:return: theta_hat = Output phase estimate of the input theta in radians,\n         ev = VCO control voltage,\n         phi = phase error = theta - theta_hat\n\nMark Wickert, April 2007 for ECE 5625/4625\nModified February 2008 and July 2014 for ECE 5675/4675\nPython version August 2014", "id": "f14907:m5"}
{"signature": "def stem(self, word, early_english=False):", "body": "<EOL>word = normalize('<STR_LIT>', text_type(word.lower()))<EOL>if len(word) < <NUM_LIT:3>:<EOL><INDENT>return word<EOL><DEDENT>if word[<NUM_LIT:0>] == '<STR_LIT:y>':<EOL><INDENT>word = '<STR_LIT:Y>' + word[<NUM_LIT:1>:]<EOL><DEDENT>for i in range(<NUM_LIT:1>, len(word)):<EOL><INDENT>if word[i] == '<STR_LIT:y>' and word[i - <NUM_LIT:1>] in self._vowels:<EOL><INDENT>word = word[:i] + '<STR_LIT:Y>' + word[i + <NUM_LIT:1> :]<EOL><DEDENT><DEDENT>if word[-<NUM_LIT:1>] == '<STR_LIT:s>':<EOL><INDENT>if word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT>elif word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT>elif word[-<NUM_LIT:2>:] == '<STR_LIT>':<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT><DEDENT>step1b_flag = False<EOL>if word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:2>:] == '<STR_LIT>':<EOL><INDENT>if self._has_vowel(word[:-<NUM_LIT:2>]):<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL>step1b_flag = True<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._has_vowel(word[:-<NUM_LIT:3>]):<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL>step1b_flag = True<EOL><DEDENT><DEDENT>elif early_english:<EOL><INDENT>if word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._has_vowel(word[:-<NUM_LIT:3>]):<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL>step1b_flag = True<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._has_vowel(word[:-<NUM_LIT:3>]):<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL>step1b_flag = True<EOL><DEDENT><DEDENT><DEDENT>if step1b_flag:<EOL><INDENT>if word[-<NUM_LIT:2>:] in {'<STR_LIT>', '<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>word += '<STR_LIT:e>'<EOL><DEDENT>elif self._ends_in_doubled_cons(word) and word[-<NUM_LIT:1>] not in {<EOL>'<STR_LIT:l>',<EOL>'<STR_LIT:s>',<EOL>'<STR_LIT:z>',<EOL>}:<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT>elif self._m_degree(word) == <NUM_LIT:1> and self._ends_in_cvc(word):<EOL><INDENT>word += '<STR_LIT:e>'<EOL><DEDENT><DEDENT>if word[-<NUM_LIT:1>] in {'<STR_LIT:Y>', '<STR_LIT:y>'} and self._has_vowel(word[:-<NUM_LIT:1>]):<EOL><INDENT>word = word[:-<NUM_LIT:1>] + '<STR_LIT:i>'<EOL><DEDENT>if len(word) > <NUM_LIT:1>:<EOL><INDENT>if word[-<NUM_LIT:2>] == '<STR_LIT:a>':<EOL><INDENT>if word[-<NUM_LIT:7>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:7>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:5>] + '<STR_LIT:e>'<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:6>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:6>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT><DEDENT>elif word[-<NUM_LIT:2>] == '<STR_LIT:c>':<EOL><INDENT>if word[-<NUM_LIT:4>:] in {'<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:1>] + '<STR_LIT:e>'<EOL><DEDENT><DEDENT><DEDENT>elif word[-<NUM_LIT:2>] == '<STR_LIT:e>':<EOL><INDENT>if word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT><DEDENT><DEDENT>elif word[-<NUM_LIT:2>] == '<STR_LIT:g>':<EOL><INDENT>if word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT><DEDENT><DEDENT>elif word[-<NUM_LIT:2>] == '<STR_LIT:l>':<EOL><INDENT>if word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:1>] + '<STR_LIT:e>'<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT><DEDENT>elif word[-<NUM_LIT:2>] == '<STR_LIT:o>':<EOL><INDENT>if word[-<NUM_LIT:7>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:7>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:5>] + '<STR_LIT:e>'<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:3>] + '<STR_LIT:e>'<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:2>] + '<STR_LIT:e>'<EOL><DEDENT><DEDENT><DEDENT>elif word[-<NUM_LIT:2>] == '<STR_LIT:s>':<EOL><INDENT>if word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:7>:] in {'<STR_LIT>', '<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:7>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:4>]<EOL><DEDENT><DEDENT><DEDENT>elif word[-<NUM_LIT:2>] == '<STR_LIT:t>':<EOL><INDENT>if word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:3>] + '<STR_LIT:e>'<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:6>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:6>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:5>] + '<STR_LIT>'<EOL><DEDENT><DEDENT><DEDENT><DEDENT>if word[-<NUM_LIT:5>:] in '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:5>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:5>:] in {'<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:0>:<EOL><INDENT>word = word[:-<NUM_LIT:4>]<EOL><DEDENT><DEDENT>if word[-<NUM_LIT:2>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:2>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] in {'<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:4>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:2>:] in {'<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:2>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] in {'<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:4>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:5>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:5>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:5>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:4>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:4>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:3>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:4>:] in {'<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:2>:] == '<STR_LIT>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:2>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:2>]<EOL><DEDENT><DEDENT>elif word[-<NUM_LIT:3>:] in {'<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:3>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:3>]<EOL><DEDENT><DEDENT>if word[-<NUM_LIT:1>] == '<STR_LIT:e>':<EOL><INDENT>if self._m_degree(word[:-<NUM_LIT:1>]) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT>elif self._m_degree(word[:-<NUM_LIT:1>]) == <NUM_LIT:1> and not self._ends_in_cvc(<EOL>word[:-<NUM_LIT:1>]<EOL>):<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT><DEDENT>if word[-<NUM_LIT:2>:] == '<STR_LIT>' and self._m_degree(word) > <NUM_LIT:1>:<EOL><INDENT>word = word[:-<NUM_LIT:1>]<EOL><DEDENT>for i in range(len(word)):<EOL><INDENT>if word[i] == '<STR_LIT:Y>':<EOL><INDENT>word = word[:i] + '<STR_LIT:y>' + word[i + <NUM_LIT:1> :]<EOL><DEDENT><DEDENT>return word<EOL>", "docstring": "Return Porter stem.\n\n        Parameters\n        ----------\n        word : str\n            The word to stem\n        early_english : bool\n            Set to True in order to remove -eth & -est (2nd & 3rd person\n            singular verbal agreement suffixes)\n\n        Returns\n        -------\n        str\n            Word stem\n\n        Examples\n        --------\n        >>> stmr = Porter()\n        >>> stmr.stem('reading')\n        'read'\n        >>> stmr.stem('suspension')\n        'suspens'\n        >>> stmr.stem('elusiveness')\n        'elus'\n\n        >>> stmr.stem('eateth', early_english=True)\n        'eat'", "id": "f6566:c0:m4"}
{"signature": "def is_tag(self, tag):", "body": "return tag[<NUM_LIT:0>:len(self.get_prefix())] == self.get_prefix()<EOL>", "docstring": "Is a given tag this type?", "id": "f10743:c1:m9"}
{"signature": "def register_checkers(linter):", "body": "if FILENAME:<EOL><INDENT>linter.register_checker(ModuleTracingChecker(linter))<EOL><DEDENT>", "docstring": "Register checkers.", "id": "f1710:m0"}
{"signature": "def valid_content_type(self, content_type, accept):", "body": "accept_tokens = accept.replace('<STR_LIT:U+0020>', '<STR_LIT>').split('<STR_LIT:;>')<EOL>content_type_tokens = content_type.replace('<STR_LIT:U+0020>', '<STR_LIT>').split('<STR_LIT:;>')<EOL>return (<EOL>all(elem in content_type_tokens for elem in accept_tokens) and<EOL>(content_type_tokens[<NUM_LIT:0>] == '<STR_LIT>' or<EOL>content_type_tokens[<NUM_LIT:0>] == '<STR_LIT>')<EOL>)<EOL>", "docstring": "Check that the server is returning a valid Content-Type\n\n        Args:\n            content_type (str): ``Content-Type:`` header value\n            accept (str): media type to include in the ``Accept:`` header.", "id": "f4931:c10:m1"}
{"signature": "def p_expression_or(p):", "body": "p[<NUM_LIT:0>] = p[<NUM_LIT:1>] | p[<NUM_LIT:3>]<EOL>", "docstring": "expression : expression OR expression", "id": "f16995:m11"}
{"signature": "def _swap_mode(self):", "body": "assert self.mode in (cs.CS_MODE_ARM, cs.CS_MODE_THUMB)<EOL>if self.mode == cs.CS_MODE_ARM:<EOL><INDENT>self.mode = cs.CS_MODE_THUMB<EOL><DEDENT>else:<EOL><INDENT>self.mode = cs.CS_MODE_ARM<EOL><DEDENT>", "docstring": "Toggle between ARM and Thumb mode", "id": "f16977:c4:m6"}
{"signature": "def orthogonal(shape, scale=<NUM_LIT>):", "body": "flat_shape = (shape[<NUM_LIT:0>], np.prod(shape[<NUM_LIT:1>:]))<EOL>a = np.random.normal(<NUM_LIT:0.0>, <NUM_LIT:1.0>, flat_shape)<EOL>u, _, v = np.linalg.svd(a, full_matrices=False)<EOL>q = u if u.shape == flat_shape else v <EOL>q = q.reshape(shape)<EOL>return sharedX(scale * q[:shape[<NUM_LIT:0>], :shape[<NUM_LIT:1>]])<EOL>", "docstring": "benanne lasagne ortho init (faster than qr approach)", "id": "f12769:m2"}
{"signature": "def is_block_device(self):", "body": "try:<EOL><INDENT>return S_ISBLK(self.stat().st_mode)<EOL><DEDENT>except OSError as e:<EOL><INDENT>if e.errno not in (ENOENT, ENOTDIR):<EOL><INDENT>raise<EOL><DEDENT>return False<EOL><DEDENT>", "docstring": "Whether this path is a block device.", "id": "f2883:c14:m38"}
{"signature": "def run(self):", "body": "self._logger.debug(\"<STR_LIT>\" % (self._modelID))<EOL>periodic = self._initPeriodicActivities()<EOL>self._optimizedMetricLabel = self._optimizeKeyPattern<EOL>self._reportMetricLabels = [self._optimizeKeyPattern]<EOL>if self._iterations >= <NUM_LIT:0>:<EOL><INDENT>iterTracker = iter(xrange(self._iterations))<EOL><DEDENT>else:<EOL><INDENT>iterTracker = iter(itertools.count())<EOL><DEDENT>doSysExit = False<EOL>if self._sysExitModelRange is not None:<EOL><INDENT>modelAndCounters = self._jobsDAO.modelsGetUpdateCounters(self._jobID)<EOL>modelIDs = [x[<NUM_LIT:0>] for x in modelAndCounters]<EOL>modelIDs.sort()<EOL>(beg,end) = self._sysExitModelRange<EOL>if self._modelID in modelIDs[int(beg):int(end)]:<EOL><INDENT>doSysExit = True<EOL><DEDENT><DEDENT>if self._delayModelRange is not None:<EOL><INDENT>modelAndCounters = self._jobsDAO.modelsGetUpdateCounters(self._jobID)<EOL>modelIDs = [x[<NUM_LIT:0>] for x in modelAndCounters]<EOL>modelIDs.sort()<EOL>(beg,end) = self._delayModelRange<EOL>if self._modelID in modelIDs[int(beg):int(end)]:<EOL><INDENT>time.sleep(<NUM_LIT:10>)<EOL><DEDENT><DEDENT>if self._errModelRange is not None:<EOL><INDENT>modelAndCounters = self._jobsDAO.modelsGetUpdateCounters(self._jobID)<EOL>modelIDs = [x[<NUM_LIT:0>] for x in modelAndCounters]<EOL>modelIDs.sort()<EOL>(beg,end) = self._errModelRange<EOL>if self._modelID in modelIDs[int(beg):int(end)]:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\")<EOL><DEDENT><DEDENT>if self._delay is not None:<EOL><INDENT>time.sleep(self._delay)<EOL><DEDENT>self._currentRecordIndex = <NUM_LIT:0><EOL>while True:<EOL><INDENT>if self._isKilled:<EOL><INDENT>break<EOL><DEDENT>if self._isCanceled:<EOL><INDENT>break<EOL><DEDENT>if self._isMature:<EOL><INDENT>if not self._isBestModel:<EOL><INDENT>self._cmpReason = self._jobsDAO.CMPL_REASON_STOPPED<EOL>break<EOL><DEDENT>else:<EOL><INDENT>self._cmpReason = self._jobsDAO.CMPL_REASON_EOF<EOL><DEDENT><DEDENT>try:<EOL><INDENT>self._currentRecordIndex = next(iterTracker)<EOL><DEDENT>except StopIteration:<EOL><INDENT>break<EOL><DEDENT>self._writePrediction(ModelResult(None, None, None, None))<EOL>periodic.tick()<EOL>if self.__shouldSysExit(self._currentRecordIndex):<EOL><INDENT>sys.exit(<NUM_LIT:1>)<EOL><DEDENT>if self._busyWaitTime is not None:<EOL><INDENT>time.sleep(self._busyWaitTime)<EOL>self.__computeWaitTime()<EOL><DEDENT>if doSysExit:<EOL><INDENT>sys.exit(<NUM_LIT:1>)<EOL><DEDENT>if self._jobFailErr:<EOL><INDENT>raise utils.JobFailException(\"<STR_LIT>\",<EOL>\"<STR_LIT>\")<EOL><DEDENT><DEDENT>if self._doFinalize:<EOL><INDENT>if not self._makeCheckpoint:<EOL><INDENT>self._model = None<EOL><DEDENT>if self._finalDelay is not None:<EOL><INDENT>time.sleep(self._finalDelay)<EOL><DEDENT>self._finalize()<EOL><DEDENT>self._logger.info(\"<STR_LIT>\"% (self._modelID))<EOL>return (self._cmpReason, None)<EOL>", "docstring": "Runs the given OPF task against the given Model instance", "id": "f17592:c0:m4"}
{"signature": "def _double_inverted_gaussian(x,<EOL>amp1, loc1, std1,<EOL>amp2, loc2, std2):", "body": "gaussian1 = -_gaussian(x,amp1,loc1,std1)<EOL>gaussian2 = -_gaussian(x,amp2,loc2,std2)<EOL>return gaussian1 + gaussian2<EOL>", "docstring": "This is a double inverted gaussian.\n\n    Parameters\n    ----------\n\n    x : np.array\n        The items at which the Gaussian is evaluated.\n\n    amp1,amp2 : float\n        The amplitude of Gaussian 1 and Gaussian 2.\n\n    loc1,loc2 : float\n        The central value of Gaussian 1 and Gaussian 2.\n\n    std1,std2 : float\n        The standard deviation of Gaussian 1 and Gaussian 2.\n\n    Returns\n    -------\n\n    np.array\n        Returns a double inverted Gaussian function evaluated at the items in\n        `x`, using the provided parameters of `amp`, `loc`, and `std` for two\n        component Gaussians 1 and 2.", "id": "f14739:m1"}
{"signature": "def read(self, output_tile):", "body": "if self.config.mode not in [\"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\"]:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if isinstance(output_tile, tuple):<EOL><INDENT>output_tile = self.config.output_pyramid.tile(*output_tile)<EOL><DEDENT>elif isinstance(output_tile, BufferedTile):<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>raise TypeError(\"<STR_LIT>\")<EOL><DEDENT>return self.config.output.read(output_tile)<EOL>", "docstring": "Read from written process output.\n\nParameters\n----------\noutput_tile : BufferedTile or tile index tuple\n    Member of the output tile pyramid (not necessarily the process\n    pyramid, if output has a different metatiling setting)\n\nReturns\n-------\ndata : NumPy array or features\n    process output", "id": "f12819:c0:m6"}
{"signature": "def renyi(alphas, Ks, dim, required, min_val=np.spacing(<NUM_LIT:1>),<EOL>clamp=True, to_self=False):", "body": "alphas = np.reshape(alphas, (-<NUM_LIT:1>, <NUM_LIT:1>))<EOL>est = required<EOL>est = np.maximum(est, min_val)  <EOL>np.log(est, out=est)<EOL>est /= alphas - <NUM_LIT:1><EOL>if clamp:<EOL><INDENT>np.maximum(est, <NUM_LIT:0>, out=est)<EOL><DEDENT>return est<EOL>", "docstring": "r'''\n    Estimate the Renyi-alpha divergence between distributions, based on kNN\n    distances:  1/(\\alpha-1) \\log \\int p^alpha q^(1-\\alpha)\n\n    If the inner integral is less than min_val (default ``np.spacing(1)``),\n    uses the log of min_val instead.\n\n    If clamp (the default), enforces that the estimates are nonnegative by\n    replacing any negative estimates with 0.\n\n    Returns an array of shape (num_alphas, num_Ks).", "id": "f14617:m13"}
{"signature": "def get_pymata_version(self):", "body": "return ['<STR_LIT:2>', '<STR_LIT>']<EOL>", "docstring": "Returns the PyMata version number in a list: [Major Number, Minor Number]\n\n:return:", "id": "f9311:c0:m23"}
{"signature": "def construct_end_message(self):", "body": "app_count = self.dfk.task_count<EOL>site_count = len([x for x in self.dfk.config.executors if x.managed])<EOL>app_fails = len([t for t in self.dfk.tasks if<EOL>self.dfk.tasks[t]['<STR_LIT:status>'] in FINAL_FAILURE_STATES])<EOL>message = {'<STR_LIT>': self.uuid,<EOL>'<STR_LIT:end>': time.time(),<EOL>'<STR_LIT>': app_count,<EOL>'<STR_LIT>': site_count,<EOL>'<STR_LIT>': None,<EOL>'<STR_LIT>': app_fails,<EOL>'<STR_LIT:test>': self.test_mode,<EOL>}<EOL>return json.dumps(message)<EOL>", "docstring": "Collect the final run information at the time of DFK cleanup.\n\n        Returns:\n             - Message dict dumped as json string, ready for UDP", "id": "f2771:c0:m3"}
{"signature": "def TWU_a_alpha_common(T, Tc, omega, a, full=True, quick=True, method='<STR_LIT>'):", "body": "Tr = T/Tc<EOL>if method == '<STR_LIT>':<EOL><INDENT>if Tr < <NUM_LIT:1>:<EOL><INDENT>L0, M0, N0 = <NUM_LIT>, <NUM_LIT>, <NUM_LIT><EOL>L1, M1, N1 = <NUM_LIT>, <NUM_LIT>, <NUM_LIT><EOL><DEDENT>else:<EOL><INDENT>L0, M0, N0 = <NUM_LIT>, <NUM_LIT>, -<NUM_LIT><EOL>L1, M1, N1 = <NUM_LIT>, <NUM_LIT>, -<NUM_LIT>  <EOL><DEDENT><DEDENT>elif method == '<STR_LIT>':<EOL><INDENT>if Tr < <NUM_LIT:1>:<EOL><INDENT>L0, M0, N0 = <NUM_LIT>, <NUM_LIT>, <NUM_LIT><EOL>L1, M1, N1 = <NUM_LIT>, <NUM_LIT>, <NUM_LIT><EOL><DEDENT>else:<EOL><INDENT>L0, M0, N0 = <NUM_LIT>, <NUM_LIT>, -<NUM_LIT><EOL>L1, M1, N1 = <NUM_LIT>,  <NUM_LIT>, -<NUM_LIT><EOL><DEDENT><DEDENT>else:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT>if not full:<EOL><INDENT>alpha0 = Tr**(N0*(M0-<NUM_LIT:1.>))*exp(L0*(<NUM_LIT:1.>-Tr**(N0*M0)))<EOL>alpha1 = Tr**(N1*(M1-<NUM_LIT:1.>))*exp(L1*(<NUM_LIT:1.>-Tr**(N1*M1)))<EOL>alpha = alpha0 + omega*(alpha1 - alpha0)<EOL>return a*alpha<EOL><DEDENT>else:<EOL><INDENT>if quick:<EOL><INDENT>x0 = T/Tc<EOL>x1 = M0 - <NUM_LIT:1><EOL>x2 = N0*x1<EOL>x3 = x0**x2<EOL>x4 = M0*N0<EOL>x5 = x0**x4<EOL>x6 = exp(-L0*(x5 - <NUM_LIT:1.>))<EOL>x7 = x3*x6<EOL>x8 = M1 - <NUM_LIT:1.><EOL>x9 = N1*x8<EOL>x10 = x0**x9<EOL>x11 = M1*N1<EOL>x12 = x0**x11<EOL>x13 = x2*x7<EOL>x14 = L0*M0*N0*x3*x5*x6<EOL>x15 = x13 - x14<EOL>x16 = exp(-L1*(x12 - <NUM_LIT:1>))<EOL>x17 = -L1*M1*N1*x10*x12*x16 + x10*x16*x9 - x13 + x14<EOL>x18 = N0*N0<EOL>x19 = x18*x3*x6<EOL>x20 = x1**<NUM_LIT:2>*x19<EOL>x21 = M0**<NUM_LIT:2><EOL>x22 = L0*x18*x3*x5*x6<EOL>x23 = x21*x22<EOL>x24 = <NUM_LIT:2>*M0*x1*x22<EOL>x25 = L0**<NUM_LIT:2>*x0**(<NUM_LIT:2>*x4)*x19*x21<EOL>x26 = N1**<NUM_LIT:2><EOL>x27 = x10*x16*x26<EOL>x28 = M1**<NUM_LIT:2><EOL>x29 = L1*x10*x12*x16*x26<EOL>a_alpha = a*(-omega*(-x10*exp(L1*(-x12 + <NUM_LIT:1>)) + x3*exp(L0*(-x5 + <NUM_LIT:1>))) + x7)<EOL>da_alpha_dT = a*(omega*x17 + x15)/T<EOL>d2a_alpha_dT2 = a*(-(omega*(-L1**<NUM_LIT:2>*x0**(<NUM_LIT>*x11)*x27*x28 + <NUM_LIT>*M1*x29*x8 + x17 + x20 - x23 - x24 + x25 - x27*x8**<NUM_LIT:2> + x28*x29) + x15 - x20 + x23 + x24 - x25)/T**<NUM_LIT:2>)<EOL><DEDENT>else:<EOL><INDENT>a_alpha = TWU_a_alpha_common(T=T, Tc=Tc, omega=omega, a=a, full=False, quick=quick, method=method)<EOL>da_alpha_dT = a*(-L0*M0*N0*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(L0*(-(T/Tc)**(M0*N0) + <NUM_LIT:1>))/T + N0*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)*exp(L0*(-(T/Tc)**(M0*N0) + <NUM_LIT:1>))/T + omega*(L0*M0*N0*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(L0*(-(T/Tc)**(M0*N0) + <NUM_LIT:1>))/T - L1*M1*N1*(T/Tc)**(M1*N1)*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*exp(L1*(-(T/Tc)**(M1*N1) + <NUM_LIT:1>))/T - N0*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)*exp(L0*(-(T/Tc)**(M0*N0) + <NUM_LIT:1>))/T + N1*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*(M1 - <NUM_LIT:1>)*exp(L1*(-(T/Tc)**(M1*N1) + <NUM_LIT:1>))/T))<EOL>d2a_alpha_dT2 = a*((L0**<NUM_LIT:2>*M0**<NUM_LIT:2>*N0**<NUM_LIT:2>*(T/Tc)**(<NUM_LIT:2>*M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - L0*M0**<NUM_LIT:2>*N0**<NUM_LIT:2>*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - <NUM_LIT:2>*L0*M0*N0**<NUM_LIT:2>*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) + L0*M0*N0*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) + N0**<NUM_LIT:2>*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)**<NUM_LIT:2>*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - N0*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - omega*(L0**<NUM_LIT:2>*M0**<NUM_LIT:2>*N0**<NUM_LIT:2>*(T/Tc)**(<NUM_LIT:2>*M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - L0*M0**<NUM_LIT:2>*N0**<NUM_LIT:2>*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - <NUM_LIT:2>*L0*M0*N0**<NUM_LIT:2>*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) + L0*M0*N0*(T/Tc)**(M0*N0)*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - L1**<NUM_LIT:2>*M1**<NUM_LIT:2>*N1**<NUM_LIT:2>*(T/Tc)**(<NUM_LIT:2>*M1*N1)*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*exp(-L1*((T/Tc)**(M1*N1) - <NUM_LIT:1>)) + L1*M1**<NUM_LIT:2>*N1**<NUM_LIT:2>*(T/Tc)**(M1*N1)*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*exp(-L1*((T/Tc)**(M1*N1) - <NUM_LIT:1>)) + <NUM_LIT:2>*L1*M1*N1**<NUM_LIT:2>*(T/Tc)**(M1*N1)*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*(M1 - <NUM_LIT:1>)*exp(-L1*((T/Tc)**(M1*N1) - <NUM_LIT:1>)) - L1*M1*N1*(T/Tc)**(M1*N1)*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*exp(-L1*((T/Tc)**(M1*N1) - <NUM_LIT:1>)) + N0**<NUM_LIT:2>*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)**<NUM_LIT:2>*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - N0*(T/Tc)**(N0*(M0 - <NUM_LIT:1>))*(M0 - <NUM_LIT:1>)*exp(-L0*((T/Tc)**(M0*N0) - <NUM_LIT:1>)) - N1**<NUM_LIT:2>*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*(M1 - <NUM_LIT:1>)**<NUM_LIT:2>*exp(-L1*((T/Tc)**(M1*N1) - <NUM_LIT:1>)) + N1*(T/Tc)**(N1*(M1 - <NUM_LIT:1>))*(M1 - <NUM_LIT:1>)*exp(-L1*((T/Tc)**(M1*N1) - <NUM_LIT:1>))))/T**<NUM_LIT:2>)<EOL><DEDENT>return a_alpha, da_alpha_dT, d2a_alpha_dT2<EOL><DEDENT>", "docstring": "r'''Function to calculate `a_alpha` and optionally its first and second\n    derivatives for the TWUPR or TWUSRK EOS. Returns 'a_alpha', and \n    optionally 'da_alpha_dT' and 'd2a_alpha_dT2'.\n    Used by `TWUPR` and `TWUSRK`; has little purpose on its own.\n    See either class for the correct reference, and examples of using the EOS.\n\n    Parameters\n    ----------\n    T : float\n        Temperature, [K]\n    Tc : float\n        Critical temperature, [K]\n    omega : float\n        Acentric factor, [-]\n    a : float\n        Coefficient calculated by EOS-specific method, [J^2/mol^2/Pa]\n    full : float\n        Whether or not to return its first and second derivatives\n    quick : bool, optional\n        Whether to use a SymPy cse-derived expression (3x faster) or \n        individual formulas\n    method : str\n        Either 'PR' or 'SRK'\n\n    Notes\n    -----\n    The derivatives are somewhat long and are not described here for \n    brevity; they are obtainable from the following SymPy expression.\n\n    >>> from sympy import *\n    >>> T, Tc, omega, N1, N0, M1, M0, L1, L0 = symbols('T, Tc, omega, N1, N0, M1, M0, L1, L0')\n    >>> Tr = T/Tc\n    >>> alpha0 = Tr**(N0*(M0-1))*exp(L0*(1-Tr**(N0*M0)))\n    >>> alpha1 = Tr**(N1*(M1-1))*exp(L1*(1-Tr**(N1*M1)))\n    >>> alpha = alpha0 + omega*(alpha1-alpha0)\n    >>> # diff(alpha, T)\n    >>> # diff(alpha, T, T)", "id": "f15780:m0"}
{"signature": "def get_community_by_name(self, name, token=None):", "body": "parameters = dict()<EOL>parameters['<STR_LIT:name>'] = name<EOL>if token:<EOL><INDENT>parameters['<STR_LIT>'] = token<EOL><DEDENT>response = self.request('<STR_LIT>', parameters)<EOL>return response<EOL>", "docstring": "Get a community based on its name.\n\n:param name: The name of the target community.\n:type name: string\n:param token: (optional) A valid token for the user in question.\n:type token: None | string\n:returns: The requested community.\n:rtype: dict", "id": "f8357:c1:m10"}
{"signature": "def encode_unicode_bytes(my_string):", "body": "if not isinstance(my_string, basestring):<EOL><INDENT>my_string = repr(my_string)<EOL><DEDENT>if PYTHON_MAJOR_VERSION == <NUM_LIT:2>:<EOL><INDENT>if isinstance(my_string, str):<EOL><INDENT>return my_string<EOL><DEDENT>elif isinstance(my_string, unicode):<EOL><INDENT>return my_string.encode('<STR_LIT:utf-8>')<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if isinstance(my_string, str):<EOL><INDENT>return my_string.encode('<STR_LIT:utf-8>')<EOL><DEDENT>elif isinstance(my_string, bytes):<EOL><INDENT>return my_string<EOL><DEDENT><DEDENT>", "docstring": "Shim function, converts Unicode to UTF-8 encoded bytes regardless of the source format\n        Intended for python 3 compatibility mode, and b/c PyCurl only takes raw bytes", "id": "f241:m0"}
{"signature": "def add_payload(self, payload):", "body": "if self._payload is None:<EOL><INDENT>self.decode_payload()<EOL><DEDENT>if isinstance(payload, ElementClass):<EOL><INDENT>self._payload.append(XMLPayload(payload))<EOL><DEDENT>elif isinstance(payload, StanzaPayload):<EOL><INDENT>self._payload.append(payload)<EOL><DEDENT>else:<EOL><INDENT>raise TypeError(\"<STR_LIT>\")<EOL><DEDENT>self._dirty = True<EOL>", "docstring": "Add new the stanza payload.\n\n        Marks the stanza dirty.\n\n        :Parameters:\n            - `payload`: XML element or stanza payload object to add\n        :Types:\n            - `payload`: :etree:`ElementTree.Element` or `StanzaPayload`", "id": "f15259:c0:m21"}
{"signature": "def logon(self, username, password):", "body": "if self._token:<EOL><INDENT>self.logoff()<EOL><DEDENT>try:<EOL><INDENT>response = self.__makerequest(<EOL>'<STR_LIT>', email=username, password=password)<EOL><DEDENT>except FogBugzAPIError:<EOL><INDENT>e = sys.exc_info()[<NUM_LIT:1>]<EOL>raise FogBugzLogonError(e)<EOL><DEDENT>self._token = response.token.string<EOL>if type(self._token) == CData:<EOL><INDENT>self._token = self._token.encode('<STR_LIT:utf-8>')<EOL><DEDENT>", "docstring": "Logs the user on to FogBugz.\n\nReturns None for a successful login.", "id": "f10557:c4:m1"}
{"signature": "def _evaluate_if_headers(self, res, environ):", "body": "<EOL>if \"<STR_LIT>\" not in environ:<EOL><INDENT>util.parse_if_header_dict(environ)<EOL><DEDENT>if res is None:<EOL><INDENT>return<EOL><DEDENT>ifDict = environ[\"<STR_LIT>\"]<EOL>last_modified = -<NUM_LIT:1>  <EOL>entitytag = \"<STR_LIT>\"  <EOL>if res.get_last_modified() is not None:<EOL><INDENT>last_modified = res.get_last_modified()<EOL><DEDENT>if res.get_etag() is not None:<EOL><INDENT>entitytag = res.get_etag()<EOL><DEDENT>if (<EOL>\"<STR_LIT>\" in environ<EOL>or \"<STR_LIT>\" in environ<EOL>or \"<STR_LIT>\" in environ<EOL>or \"<STR_LIT>\" in environ<EOL>):<EOL><INDENT>util.evaluate_http_conditionals(res, last_modified, entitytag, environ)<EOL><DEDENT>if \"<STR_LIT>\" not in environ:<EOL><INDENT>return<EOL><DEDENT>refUrl = res.get_ref_url()<EOL>lockMan = self._davProvider.lock_manager<EOL>locktokenlist = []<EOL>if lockMan:<EOL><INDENT>lockList = lockMan.get_indirect_url_lock_list(<EOL>refUrl, environ[\"<STR_LIT>\"]<EOL>)<EOL>for lock in lockList:<EOL><INDENT>locktokenlist.append(lock[\"<STR_LIT>\"])<EOL><DEDENT><DEDENT>if not util.test_if_header_dict(res, ifDict, refUrl, locktokenlist, entitytag):<EOL><INDENT>self._fail(HTTP_PRECONDITION_FAILED, \"<STR_LIT>\")<EOL><DEDENT>return<EOL>", "docstring": "Apply HTTP headers on <path>, raising DAVError if conditions fail.\n\n        Add environ['wsgidav.conditions.if'] and environ['wsgidav.ifLockTokenList'].\n        Handle these headers:\n\n          - If-Match, If-Modified-Since, If-None-Match, If-Unmodified-Since:\n            Raising HTTP_PRECONDITION_FAILED or HTTP_NOT_MODIFIED\n          - If:\n            Raising HTTP_PRECONDITION_FAILED\n\n        @see http://www.webdav.org/specs/rfc4918.html#HEADER_If\n        @see util.evaluate_http_conditionals", "id": "f8595:c0:m6"}
{"signature": "def delete(self, endpoint, data, url_data=None, parameters=None):", "body": "return self.request_handler.request(<EOL>self._url(endpoint, url_data, parameters),<EOL>method=Api._method['<STR_LIT>'],<EOL>body=urllib.urlencode(data)<EOL>)<EOL>", "docstring": "Returns the response and body for a delete request\n            endpoints = 'users'  # resource to access\n            data = {'username': 'blah, 'password': blah}  # DELETE body\n            url_data = {}, ()  # Used to modularize endpoints, see __init__\n            parameters = {}, ((),()) # URL paramters, ex: google.com?q=a&f=b", "id": "f14195:c0:m6"}
{"signature": "@property<EOL><INDENT>def kg(self):<DEDENT>", "body": "return self.ThermalConductivityGas(self.T, self.P)<EOL>", "docstring": "r'''Thermal conductivity of the chemical in the gas phase at its\n        current temperature and pressure, in units of [W/m/K].\n\n        For calculation of this property at other temperatures and pressures,\n        or specifying manually the method used to calculate it, and more - see\n        the object oriented interface\n        :obj:`thermo.thermal_conductivity.ThermalConductivityGas`; each\n        Chemical instance creates one to actually perform the calculations.\n\n        Examples\n        --------\n        >>> Chemical('water', T=320).kg\n        0.021273128263091207", "id": "f15812:c0:m77"}
{"signature": "def create_two_particle_state(imsize, radius=<NUM_LIT>, delta=<NUM_LIT:1.0>, seed=None, axis='<STR_LIT:x>', **kwargs):", "body": "_seed_or_not(seed)<EOL>imsize = _toarr(imsize)<EOL>comp = {'<STR_LIT:x>': <NUM_LIT:2>, '<STR_LIT:y>': <NUM_LIT:1>, '<STR_LIT:z>': <NUM_LIT:0>}<EOL>t = float(radius)+float(delta)/<NUM_LIT:2><EOL>d = np.array([<NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:0.0>])<EOL>d[comp[axis]] = t<EOL>pos = np.array([imsize/<NUM_LIT> - d, imsize/<NUM_LIT> + d]).reshape(-<NUM_LIT:1>,<NUM_LIT:3>)<EOL>rad = np.array([radius, radius])<EOL>return create_state(util.NullImage(shape=imsize), pos, rad, **kwargs)<EOL>", "docstring": "Creates a two particle state\n\nParameters:\n-----------\nimsize : tuple, array_like, or integer\n    the unpadded image size to fill with particles\n\nradius : float\n    radius of particles to add\n\ndelta : float\n    separation between the two particles\n\nseed : integer\n    set the seed if desired\n\n*args, **kwargs : see create_state", "id": "f5764:m5"}
{"signature": "def __iter__(self):", "body": "for page_addr in sorted(self._page2map.keys()):<EOL><INDENT>start = page_addr * self.page_size<EOL>end = start + self.page_size<EOL>for addr in range(start, end):<EOL><INDENT>yield addr<EOL><DEDENT><DEDENT>", "docstring": "Iterate all valid addresses", "id": "f16973:c10:m33"}
{"signature": "def _check_shape(placeholder_shape, data_shape):", "body": "return True<EOL>squeezed_placeholder_shape = _squeeze_shape(placeholder_shape)<EOL>squeezed_data_shape = _squeeze_shape(data_shape)<EOL>for i, s_data in enumerate(squeezed_data_shape):<EOL><INDENT>s_placeholder = squeezed_placeholder_shape[i]<EOL>if s_placeholder != -<NUM_LIT:1> and s_data != s_placeholder:<EOL><INDENT>return False<EOL><DEDENT><DEDENT>return True<EOL>", "docstring": "check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)", "id": "f1363:m25"}
{"signature": "def from_db_value(self, value, expression, connection, context):", "body": "return self.to_python(value)<EOL>", "docstring": "Overrides ``models.Field`` method. This converts the value\n        returned from the database to an instance of this class.", "id": "f2453:c0:m2"}
{"signature": "def _wildcard_to_dec(nm, check=False):", "body": "if check and not is_wildcard_nm(nm):<EOL><INDENT>raise ValueError('<STR_LIT>' % nm)<EOL><DEDENT>return <NUM_LIT> - _dot_to_dec(nm, check=False)<EOL>", "docstring": "Wildcard bits to decimal conversion.", "id": "f3601:m28"}
{"signature": "def clean_strings(iterable):", "body": "retval = []<EOL>for val in iterable:<EOL><INDENT>try:<EOL><INDENT>retval.append(val.strip())<EOL><DEDENT>except(AttributeError):<EOL><INDENT>retval.append(val)<EOL><DEDENT><DEDENT>return retval<EOL>", "docstring": "Take a list of strings and clear whitespace \non each one. If a value in the list is not a \nstring pass it through untouched.\n\nArgs:\n    iterable: mixed list\n\nReturns: \n    mixed list", "id": "f1549:m12"}
{"signature": "def extended_analog(self, pin, data):", "body": "analog_data = [pin, data & <NUM_LIT>, (data >> <NUM_LIT:7>) & <NUM_LIT>, (data >> <NUM_LIT>) & <NUM_LIT>]<EOL>self._command_handler.send_sysex(self._command_handler.EXTENDED_ANALOG, analog_data)<EOL>", "docstring": "This method will send an extended data analog output command to the selected pin\n\n:param pin: 0 - 127\n\n:param data: 0 - 0xfffff", "id": "f9311:c0:m13"}
{"signature": "def indexes_seqs(ol,value,seqs):", "body": "seqs = list(seqs)<EOL>length = ol.__len__()<EOL>indexes =[]<EOL>seq = -<NUM_LIT:1><EOL>for i in range(<NUM_LIT:0>,length):<EOL><INDENT>if(value == ol[i]):<EOL><INDENT>seq = seq + <NUM_LIT:1><EOL>if(seq in seqs):<EOL><INDENT>indexes.append(i)<EOL><DEDENT>else:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>else:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>return(indexes)<EOL>", "docstring": "from elist.elist import *\nol = [1,'a',3,'a',4,'a',5]\nindexes_seqs(ol,'a',{0,2})\nindexes_seqs(ol,'a',{0,1})\nindexes_seqs(ol,'a',{1,2})\nindexes_seqs(ol,'a',{3,4})", "id": "f1599:m91"}
{"signature": "def _crawl_oxford_and_find_main_xml(self):", "body": "self.found_articles = []<EOL>def visit(arg, dirname, names):<EOL><INDENT>files = [filename for filename in names if \"<STR_LIT>\" in filename]<EOL>if files:<EOL><INDENT>try:<EOL><INDENT>for f in files:<EOL><INDENT>self.found_articles.append(join(dirname, f))<EOL><DEDENT><DEDENT>except Exception as err:<EOL><INDENT>register_exception()<EOL>print(\"<STR_LIT>\" % (dirname, err),<EOL>file=sys.stderr)<EOL><DEDENT><DEDENT><DEDENT>if hasattr(self, '<STR_LIT>'):<EOL><INDENT>walk(self.path_unpacked, visit, None)<EOL><DEDENT>elif self.path:<EOL><INDENT>walk(self.path, visit, None)<EOL><DEDENT>else:<EOL><INDENT>self.logger.info(\"<STR_LIT>\")<EOL><DEDENT>", "docstring": "A package contains several subdirectory corresponding to each article.\nAn article is actually identified by the existence of a main.pdf and\na main.xml in a given directory.", "id": "f7911:c0:m6"}
{"signature": "@contextlib.contextmanager<EOL>def tempfile_context(*args, **kwargs):", "body": "fd, filename = tempfile.mkstemp(*args, **kwargs)<EOL>os.close(fd)<EOL>try:<EOL><INDENT>yield filename<EOL><DEDENT>finally:<EOL><INDENT>os.remove(filename)<EOL><DEDENT>", "docstring": "A wrapper around tempfile.mkstemp to create the file in a context and\ndelete it after.", "id": "f4708:m8"}
{"signature": "def find_package_data(where='<STR_LIT:.>', package='<STR_LIT>',<EOL>exclude=standard_exclude,<EOL>exclude_directories=standard_exclude_directories,<EOL>only_in_packages=True,<EOL>show_ignored=False):", "body": "out = {}<EOL>stack = [(convert_path(where), '<STR_LIT>', package, only_in_packages)]<EOL>while stack:<EOL><INDENT>where, prefix, package, only_in_packages = stack.pop(<NUM_LIT:0>)<EOL>for name in os.listdir(where):<EOL><INDENT>fn = os.path.join(where, name)<EOL>if os.path.isdir(fn):<EOL><INDENT>bad_name = False<EOL>for pattern in exclude_directories:<EOL><INDENT>if (fnmatchcase(name, pattern)<EOL>or fn.lower() == pattern.lower()):<EOL><INDENT>bad_name = True<EOL>if show_ignored:<EOL><INDENT>sys.stderr.write(<EOL>\"<STR_LIT>\"<EOL>% (fn, pattern))<EOL><DEDENT>break<EOL><DEDENT><DEDENT>if bad_name:<EOL><INDENT>continue<EOL><DEDENT>if os.path.isfile(os.path.join(fn, '<STR_LIT>')):<EOL><INDENT>if not package:<EOL><INDENT>new_package = name<EOL><DEDENT>else:<EOL><INDENT>new_package = package + '<STR_LIT:.>' + name<EOL><DEDENT>stack.append((fn, '<STR_LIT>', new_package, False))<EOL><DEDENT>else:<EOL><INDENT>stack.append(<EOL>(fn, prefix + name + '<STR_LIT:/>', package, only_in_packages)<EOL>)<EOL><DEDENT><DEDENT>elif package or not only_in_packages:<EOL><INDENT>bad_name = False<EOL>for pattern in exclude:<EOL><INDENT>if (fnmatchcase(name, pattern)<EOL>or fn.lower() == pattern.lower()):<EOL><INDENT>bad_name = True<EOL>if show_ignored:<EOL><INDENT>sys.stderr.write(<EOL>\"<STR_LIT>\"<EOL>% (fn, pattern))<EOL><DEDENT>break<EOL><DEDENT><DEDENT>if bad_name:<EOL><INDENT>continue<EOL><DEDENT>out.setdefault(package, []).append(prefix + name)<EOL><DEDENT><DEDENT><DEDENT>return out<EOL>", "docstring": "Return a dictionary suitable for use in ``package_data``\nin a distutils ``setup.py`` file.\n\nThe dictionary looks like::\n\n    {'package': [files]}\n\nWhere ``files`` is a list of all the files in that package that\ndon't match anything in ``exclude``.\n\nIf ``only_in_packages`` is true, then top-level directories that\nare not packages won't be included (but directories under packages\nwill).\n\nDirectories matching any pattern in ``exclude_directories`` will\nbe ignored; by default directories with leading ``.``, ``CVS``,\nand ``_darcs`` will be ignored.\n\nIf ``show_ignored`` is true, then all the files that aren't\nincluded in package data are shown on stderr (for debugging\npurposes).\n\nNote patterns use wildcards, or can be exact paths (including\nleading ``./``), and all searching is case-insensitive.\n\nThis function is by Ian Bicking.", "id": "f7802:m2"}
{"signature": "@register.simple_tag(takes_context=True)<EOL>def render_editor(context, config):", "body": "quill_config = getattr(quill_app, config)<EOL>t = template.loader.get_template(quill_config['<STR_LIT>'])<EOL>return t.render(context)<EOL>", "docstring": "Render the editor for the given config.", "id": "f5473:m3"}
{"signature": "def sendCommands(comPort, commands):", "body": "mutex.acquire()<EOL>try:<EOL><INDENT>try:<EOL><INDENT>port = serial.Serial(port=comPort)<EOL>header = '<STR_LIT>'<EOL>footer = '<STR_LIT>'<EOL>for command in _translateCommands(commands):<EOL><INDENT>_sendBinaryData(port, header + command + footer)<EOL><DEDENT><DEDENT>except serial.SerialException:<EOL><INDENT>print('<STR_LIT>' % comPort)<EOL>print('<STR_LIT>')<EOL>raise<EOL><DEDENT><DEDENT>finally:<EOL><INDENT>mutex.release()<EOL><DEDENT>", "docstring": "Send X10 commands using the FireCracker on comPort\n\n    comPort should be the name of a serial port on the host platform. On\n    Windows, for example, 'com1'.\n\n    commands should be a string consisting of X10 commands separated by\n    commas. For example. 'A1 On, A Dim, A Dim, A Dim, A Lamps Off'. The\n    letter is a house code (A-P) and the number is the device number (1-16).\n    Possible commands for a house code / device number combination are\n    'On' and 'Off'. The commands 'Bright' and 'Dim' should be used with a\n    house code alone after sending an On command to a specific device. The\n    'All On', 'All Off', 'Lamps On', and 'Lamps Off' commands should also\n    be used with a house code alone.\n\n    # Turn on module A1\n    >>> sendCommands('com1', 'A1 On')\n\n    # Turn all modules with house code A off\n    >>> sendCommands('com1', 'A All Off')\n\n    # Turn all lamp modules with house code B on\n    >>> sendCommands('com1', 'B Lamps On')\n\n    # Turn on module A1 and dim it 3 steps, then brighten it 1 step\n    >>> sendCommands('com1', 'A1 On, A Dim, A Dim, A Dim, A Bright')", "id": "f102:m6"}
{"signature": "def __init__(self,  byhour=None, interval=<NUM_LIT:1>, tz=None):", "body": "if byhour is None: byhour=list(range(<NUM_LIT>))<EOL>rule = rrulewrapper(HOURLY, byhour=byhour, interval=interval,<EOL>byminute=<NUM_LIT:0>, bysecond=<NUM_LIT:0>)<EOL>RRuleLocator.__init__(self, rule, tz)<EOL>", "docstring": "Mark every hour in *byhour*; *byhour* can be an int or sequence.\nDefault is to tick every hour: ``byhour=range(24)``\n\n*interval* is the interval between each iteration.  For\nexample, if ``interval=2``, mark every second occurrence.", "id": "f17189:c12:m0"}
{"signature": "def _localize_inputs_recursive_command(self, task_dir, inputs):", "body": "data_dir = os.path.join(task_dir, _DATA_SUBDIR)<EOL>provider_commands = [<EOL>providers_util.build_recursive_localize_command(data_dir, inputs,<EOL>file_provider)<EOL>for file_provider in _SUPPORTED_INPUT_PROVIDERS<EOL>]<EOL>return '<STR_LIT:\\n>'.join(provider_commands)<EOL>", "docstring": "Returns a command that will stage recursive inputs.", "id": "f6202:c0:m23"}
{"signature": "def generate_pair(algorithm, bit_size=None, curve=None):", "body": "if algorithm not in set(['<STR_LIT>', '<STR_LIT>', '<STR_LIT>']):<EOL><INDENT>raise ValueError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>repr(algorithm)<EOL>))<EOL><DEDENT>if algorithm == '<STR_LIT>':<EOL><INDENT>if bit_size not in set([<NUM_LIT>, <NUM_LIT>, <NUM_LIT>, <NUM_LIT>]):<EOL><INDENT>raise ValueError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>repr(bit_size)<EOL>))<EOL><DEDENT><DEDENT>elif algorithm == '<STR_LIT>':<EOL><INDENT>if libcrypto_version_info < (<NUM_LIT:1>,):<EOL><INDENT>if bit_size != <NUM_LIT>:<EOL><INDENT>raise ValueError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>repr(bit_size)<EOL>))<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if bit_size not in set([<NUM_LIT>, <NUM_LIT>, <NUM_LIT>]):<EOL><INDENT>raise ValueError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>repr(bit_size)<EOL>))<EOL><DEDENT><DEDENT><DEDENT>elif algorithm == '<STR_LIT>':<EOL><INDENT>if curve not in set(['<STR_LIT>', '<STR_LIT>', '<STR_LIT>']):<EOL><INDENT>raise ValueError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>repr(curve)<EOL>))<EOL><DEDENT><DEDENT>if algorithm == '<STR_LIT>':<EOL><INDENT>rsa = None<EOL>exponent = None<EOL>try:<EOL><INDENT>rsa = libcrypto.RSA_new()<EOL>if is_null(rsa):<EOL><INDENT>handle_openssl_error(<NUM_LIT:0>)<EOL><DEDENT>exponent_pointer = new(libcrypto, '<STR_LIT>')<EOL>result = libcrypto.BN_dec2bn(exponent_pointer, b'<STR_LIT>')<EOL>handle_openssl_error(result)<EOL>exponent = unwrap(exponent_pointer)<EOL>result = libcrypto.RSA_generate_key_ex(rsa, bit_size, exponent, null())<EOL>handle_openssl_error(result)<EOL>buffer_length = libcrypto.i2d_RSAPublicKey(rsa, null())<EOL>if buffer_length < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(buffer_length)<EOL><DEDENT>buffer = buffer_from_bytes(buffer_length)<EOL>result = libcrypto.i2d_RSAPublicKey(rsa, buffer_pointer(buffer))<EOL>if result < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(result)<EOL><DEDENT>public_key_bytes = bytes_from_buffer(buffer, buffer_length)<EOL>buffer_length = libcrypto.i2d_RSAPrivateKey(rsa, null())<EOL>if buffer_length < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(buffer_length)<EOL><DEDENT>buffer = buffer_from_bytes(buffer_length)<EOL>result = libcrypto.i2d_RSAPrivateKey(rsa, buffer_pointer(buffer))<EOL>if result < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(result)<EOL><DEDENT>private_key_bytes = bytes_from_buffer(buffer, buffer_length)<EOL><DEDENT>finally:<EOL><INDENT>if rsa:<EOL><INDENT>libcrypto.RSA_free(rsa)<EOL><DEDENT>if exponent:<EOL><INDENT>libcrypto.BN_free(exponent)<EOL><DEDENT><DEDENT><DEDENT>elif algorithm == '<STR_LIT>':<EOL><INDENT>dsa = None<EOL>try:<EOL><INDENT>dsa = libcrypto.DSA_new()<EOL>if is_null(dsa):<EOL><INDENT>handle_openssl_error(<NUM_LIT:0>)<EOL><DEDENT>result = libcrypto.DSA_generate_parameters_ex(dsa, bit_size, null(), <NUM_LIT:0>, null(), null(), null())<EOL>handle_openssl_error(result)<EOL>result = libcrypto.DSA_generate_key(dsa)<EOL>handle_openssl_error(result)<EOL>buffer_length = libcrypto.i2d_DSA_PUBKEY(dsa, null())<EOL>if buffer_length < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(buffer_length)<EOL><DEDENT>buffer = buffer_from_bytes(buffer_length)<EOL>result = libcrypto.i2d_DSA_PUBKEY(dsa, buffer_pointer(buffer))<EOL>if result < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(result)<EOL><DEDENT>public_key_bytes = bytes_from_buffer(buffer, buffer_length)<EOL>buffer_length = libcrypto.i2d_DSAPrivateKey(dsa, null())<EOL>if buffer_length < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(buffer_length)<EOL><DEDENT>buffer = buffer_from_bytes(buffer_length)<EOL>result = libcrypto.i2d_DSAPrivateKey(dsa, buffer_pointer(buffer))<EOL>if result < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(result)<EOL><DEDENT>private_key_bytes = bytes_from_buffer(buffer, buffer_length)<EOL><DEDENT>finally:<EOL><INDENT>if dsa:<EOL><INDENT>libcrypto.DSA_free(dsa)<EOL><DEDENT><DEDENT><DEDENT>elif algorithm == '<STR_LIT>':<EOL><INDENT>ec_key = None<EOL>try:<EOL><INDENT>curve_id = {<EOL>'<STR_LIT>': LibcryptoConst.NID_X9_62_prime256v1,<EOL>'<STR_LIT>': LibcryptoConst.NID_secp384r1,<EOL>'<STR_LIT>': LibcryptoConst.NID_secp521r1,<EOL>}[curve]<EOL>ec_key = libcrypto.EC_KEY_new_by_curve_name(curve_id)<EOL>if is_null(ec_key):<EOL><INDENT>handle_openssl_error(<NUM_LIT:0>)<EOL><DEDENT>result = libcrypto.EC_KEY_generate_key(ec_key)<EOL>handle_openssl_error(result)<EOL>libcrypto.EC_KEY_set_asn1_flag(ec_key, LibcryptoConst.OPENSSL_EC_NAMED_CURVE)<EOL>buffer_length = libcrypto.i2o_ECPublicKey(ec_key, null())<EOL>if buffer_length < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(buffer_length)<EOL><DEDENT>buffer = buffer_from_bytes(buffer_length)<EOL>result = libcrypto.i2o_ECPublicKey(ec_key, buffer_pointer(buffer))<EOL>if result < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(result)<EOL><DEDENT>public_key_point_bytes = bytes_from_buffer(buffer, buffer_length)<EOL>public_key = keys.PublicKeyInfo({<EOL>'<STR_LIT>': keys.PublicKeyAlgorithm({<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': keys.ECDomainParameters(<EOL>name='<STR_LIT>',<EOL>value=curve<EOL>)<EOL>}),<EOL>'<STR_LIT>': public_key_point_bytes<EOL>})<EOL>public_key_bytes = public_key.dump()<EOL>buffer_length = libcrypto.i2d_ECPrivateKey(ec_key, null())<EOL>if buffer_length < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(buffer_length)<EOL><DEDENT>buffer = buffer_from_bytes(buffer_length)<EOL>result = libcrypto.i2d_ECPrivateKey(ec_key, buffer_pointer(buffer))<EOL>if result < <NUM_LIT:0>:<EOL><INDENT>handle_openssl_error(result)<EOL><DEDENT>private_key_bytes = bytes_from_buffer(buffer, buffer_length)<EOL><DEDENT>finally:<EOL><INDENT>if ec_key:<EOL><INDENT>libcrypto.EC_KEY_free(ec_key)<EOL><DEDENT><DEDENT><DEDENT>return (load_public_key(public_key_bytes), load_private_key(private_key_bytes))<EOL>", "docstring": "Generates a public/private key pair\n\n:param algorithm:\n    The key algorithm - \"rsa\", \"dsa\" or \"ec\"\n\n:param bit_size:\n    An integer - used for \"rsa\" and \"dsa\". For \"rsa\" the value maye be 1024,\n    2048, 3072 or 4096. For \"dsa\" the value may be 1024, plus 2048 or 3072\n    if OpenSSL 1.0.0 or newer is available.\n\n:param curve:\n    A unicode string - used for \"ec\" keys. Valid values include \"secp256r1\",\n    \"secp384r1\" and \"secp521r1\".\n\n:raises:\n    ValueError - when any of the parameters contain an invalid value\n    TypeError - when any of the parameters are of the wrong type\n    OSError - when an error is returned by the OS crypto library\n\n:return:\n    A 2-element tuple of (PublicKey, PrivateKey). The contents of each key\n    may be saved by calling .asn1.dump().", "id": "f9533:m0"}
{"signature": "def on_packet(packet):", "body": "print(\"<STR_LIT>\".format(packet.framenumber))<EOL>header, markers = packet.get_3d_markers()<EOL>print(\"<STR_LIT>\".format(header))<EOL>for marker in markers:<EOL><INDENT>print(\"<STR_LIT:\\t>\", marker)<EOL><DEDENT>", "docstring": "Callback function that is called everytime a data packet arrives from QTM", "id": "f8797:m0"}
{"signature": "def _get_connection(self):", "body": "if self._connection is None:<EOL><INDENT>self._connection = self._get_new_connection() <EOL><DEDENT>return self._connection<EOL>", "docstring": "_get_connection - Maybe get a new connection, or reuse if passed in.\n        Will share a connection with a model\ninternal", "id": "f4156:c2:m3"}
{"signature": "def _processSegmentUpdates(self, activeColumns):", "body": "<EOL>removeKeys = []<EOL>trimSegments = []<EOL>for key, updateList in self.segmentUpdates.items():<EOL><INDENT>c, i = key[<NUM_LIT:0>], key[<NUM_LIT:1>]<EOL>if c in activeColumns:<EOL><INDENT>action = '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>if self.doPooling and self.lrnPredictedState['<STR_LIT:t>'][c, i] == <NUM_LIT:1>:<EOL><INDENT>action = '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>action = '<STR_LIT>'<EOL><DEDENT><DEDENT>updateListKeep = []<EOL>if action != '<STR_LIT>':<EOL><INDENT>for (createDate, segUpdate) in updateList:<EOL><INDENT>if self.verbosity >= <NUM_LIT:4>:<EOL><INDENT>print(\"<STR_LIT>\", self.lrnIterationIdx, end='<STR_LIT:U+0020>')<EOL>print(segUpdate)<EOL><DEDENT>if self.lrnIterationIdx - createDate > self.segUpdateValidDuration:<EOL><INDENT>continue<EOL><DEDENT>if action == '<STR_LIT>':<EOL><INDENT>trimSegment = self._adaptSegment(segUpdate)<EOL>if trimSegment:<EOL><INDENT>trimSegments.append((segUpdate.columnIdx, segUpdate.cellIdx,<EOL>segUpdate.segment))<EOL><DEDENT><DEDENT>else:<EOL><INDENT>updateListKeep.append((createDate, segUpdate))<EOL><DEDENT><DEDENT><DEDENT>self.segmentUpdates[key] = updateListKeep<EOL>if len(updateListKeep) == <NUM_LIT:0>:<EOL><INDENT>removeKeys.append(key)<EOL><DEDENT><DEDENT>for key in removeKeys:<EOL><INDENT>self.segmentUpdates.pop(key)<EOL><DEDENT>for (c, i, segment) in trimSegments:<EOL><INDENT>self._trimSegmentsInCell(c, i, [segment], minPermanence = <NUM_LIT>,<EOL>minNumSyns = <NUM_LIT:0>)<EOL><DEDENT>", "docstring": "Go through the list of accumulated segment updates and process them\nas follows:\n\nif the segment update is too old, remove the update\nelse if the cell received bottom-up, update its permanences\nelse if it's still being predicted, leave it in the queue\nelse remove it.\n\n:param activeColumns TODO: document", "id": "f17565:c0:m77"}
{"signature": "def summarized_name(self, name):", "body": "components = name.split('<STR_LIT:.>')<EOL>prefix = '<STR_LIT:.>'.join(c[<NUM_LIT:0>] for c in components[:-<NUM_LIT:1>])<EOL>return f'<STR_LIT>'<EOL>", "docstring": "Produce a summarized record name\n  i.e. manticore.core.executor -> m.c.executor", "id": "f17010:c0:m0"}
{"signature": "def tiles_exist(self, process_tile=None, output_tile=None):", "body": "if process_tile and output_tile:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if process_tile:<EOL><INDENT>return any(<EOL>path_exists(self.get_path(tile))<EOL>for tile in self.pyramid.intersecting(process_tile)<EOL>)<EOL><DEDENT>if output_tile:<EOL><INDENT>return path_exists(self.get_path(output_tile))<EOL><DEDENT>", "docstring": "Check whether output tiles of a tile (either process or output) exists.\n\nParameters\n----------\nprocess_tile : ``BufferedTile``\n    must be member of process ``TilePyramid``\noutput_tile : ``BufferedTile``\n    must be member of output ``TilePyramid``\n\nReturns\n-------\nexists : bool", "id": "f12806:c2:m3"}
{"signature": "def constrain(self):", "body": "dx = self.w * <NUM_LIT:0.1><EOL>dy = self.h * <NUM_LIT:0.1> <EOL>for b in self:<EOL><INDENT>if b.x < self.x-dx: b.vx += _ctx.random(dx)<EOL>if b.y < self.y-dy: b.vy += _ctx.random(dy)<EOL>if b.x > self.x+self.w+dx: b.vx -= _ctx.random(dx)<EOL>if b.y > self.y+self.h+dy: b.vy -= _ctx.random(dy)<EOL>if b.z < <NUM_LIT:0>: b.vz += <NUM_LIT:10><EOL>if b.z > <NUM_LIT:100>: b.vz -= <NUM_LIT:10><EOL>if b.y > self._perch_y and _ctx.random() < self._perch:<EOL><INDENT>b.y = self._perch_y<EOL>b.vy = -abs(b.vy) * <NUM_LIT><EOL>b.is_perching = True<EOL>try:<EOL><INDENT>b._perch_t = self._perch_t()<EOL><DEDENT>except:<EOL><INDENT>b._perch_t = self._perch_t<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Cages the flock inside the x, y, w, h area.\n\n        The actual cage is a bit larger,\n        so boids don't seem to bounce of invisible walls\n        (they are rather \"encouraged\" to stay in the area).\n\n        If a boid touches the ground level,\n        it may decide to perch there for a while.", "id": "f11568:c1:m8"}
{"signature": "def _get_node_names(h5file, h5path='<STR_LIT:/>', node_type=h5py.Dataset):", "body": "if isinstance(h5file, str):<EOL><INDENT>_h5file = get_h5file(h5file, mode='<STR_LIT:r>')<EOL><DEDENT>else:<EOL><INDENT>_h5file = h5file<EOL><DEDENT>if not h5path.startswith('<STR_LIT:/>'):<EOL><INDENT>h5path = '<STR_LIT:/>' + h5path<EOL><DEDENT>names = []<EOL>try:<EOL><INDENT>h5group = _h5file.require_group(h5path)<EOL>for node in _hdf5_walk(h5group, node_type=node_type):<EOL><INDENT>names.append(node.name)<EOL><DEDENT><DEDENT>except:<EOL><INDENT>raise RuntimeError('<STR_LIT>'.format(_h5file.filename, h5path))<EOL><DEDENT>finally:<EOL><INDENT>if isinstance(h5file, str):<EOL><INDENT>_h5file.close()<EOL><DEDENT><DEDENT>return names<EOL>", "docstring": "Return the node of type node_type names within h5path of h5file.\n\n    Parameters\n    ----------\n    h5file: h5py.File\n        HDF5 file object\n\n    h5path: str\n        HDF5 group path to get the group names from\n\n    node_type: h5py object type\n        HDF5 object type\n\n    Returns\n    -------\n    names: list of str\n        List of names", "id": "f4066:m7"}
{"signature": "@staticmethod<EOL><INDENT>def validate_string_list(value):<DEDENT>", "body": "try:<EOL><INDENT>if sys.version_info.major < <NUM_LIT:3>:<EOL><INDENT>from locale import getpreferredencoding<EOL>encoding = getpreferredencoding()<EOL>value = value.decode(encoding)<EOL><DEDENT>return [x.strip() for x in value.split(u\"<STR_LIT:U+002C>\")]<EOL><DEDENT>except (AttributeError, TypeError, UnicodeError):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>", "docstring": "Validator for string lists to be used with `add_setting`.", "id": "f15311:c1:m12"}
{"signature": "@pipe.func<EOL>def resplit(prev, pattern, *args, **kw):", "body": "maxsplit = <NUM_LIT:0> if '<STR_LIT>' not in kw else kw.pop('<STR_LIT>')<EOL>pattern_obj = re.compile(pattern, *args, **kw)<EOL>for s in prev:<EOL><INDENT>yield pattern_obj.split(s, maxsplit=maxsplit)<EOL><DEDENT>", "docstring": "The resplit pipe split previous pipe input by regular expression.\n\n    Use 'maxsplit' keyword argument to limit the number of split.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to split string.\n    :type pattern: str|unicode", "id": "f2259:m15"}
{"signature": "def serialize(self, data, format=None):", "body": "return self._resource.serialize(data, response=self, format=format)<EOL>", "docstring": "Serializes the data into this response using a serializer.\n\n        @param[in] data\n            The data to be serialized.\n\n        @param[in] format\n            A specific format to serialize in; if provided, no detection is\n            done. If not provided, the accept header (as well as the URL\n            extension) is looked at to determine an appropriate serializer.\n\n        @returns\n            A tuple of the serialized text and an instance of the\n            serializer used.", "id": "f10144:c1:m15"}
{"signature": "@property<EOL><INDENT>def uuid(self):<DEDENT>", "body": "return uuid.UUID(str(self._props.Get(_DESCRIPTOR_INTERFACE, '<STR_LIT>')))<EOL>", "docstring": "Return the UUID of this GATT descriptor.", "id": "f9600:c2:m1"}
{"signature": "def p_declarations(self, p):", "body": "n = len(p)<EOL>if n == <NUM_LIT:3>:<EOL><INDENT>p[<NUM_LIT:0>] = p[<NUM_LIT:1>] + [p[<NUM_LIT:2>]]<EOL><DEDENT>elif n == <NUM_LIT:2>:<EOL><INDENT>p[<NUM_LIT:0>] = [p[<NUM_LIT:1>]]<EOL><DEDENT>", "docstring": "declarations : declarations declaration\n                        | declaration", "id": "f5672:c0:m19"}
{"signature": "def get_device_id(self, dev):", "body": "com, code, ok = io.send_packet(CMDTYPE.GETID, <NUM_LIT:0>, dev, self.baudrate, <NUM_LIT:5>)<EOL>if code is None:<EOL><INDENT>self.error(action='<STR_LIT>')<EOL><DEDENT>return code<EOL>", "docstring": "Get device ID at given address/path.\n\n        :param str dev: Serial device address/path\n        :param baudrate: Baudrate to use when connecting (optional)", "id": "f2090:c0:m5"}
{"signature": "def Edalat(T, Tc, Pc, omega):", "body": "tau = <NUM_LIT:1.> - T/Tc<EOL>a = -<NUM_LIT> - <NUM_LIT>*omega<EOL>c = -<NUM_LIT> - <NUM_LIT>*omega<EOL>d = <NUM_LIT:1.>/(-<NUM_LIT> - <NUM_LIT>*omega + <NUM_LIT>*omega**<NUM_LIT:2>)<EOL>b = <NUM_LIT> - <NUM_LIT>*omega - <NUM_LIT>*d<EOL>lnPr = (a*tau + b*tau**<NUM_LIT> + c*tau**<NUM_LIT:3> + d*tau**<NUM_LIT:6>)/(<NUM_LIT:1.>-tau)<EOL>return exp(lnPr)*Pc<EOL>", "docstring": "r'''Calculates vapor pressure of a fluid at arbitrary temperatures using a\n    CSP relationship by [1]_. Requires a chemical's critical temperature,\n    pressure, and acentric factor. Claimed to have a higher accuracy than the\n    Lee-Kesler CSP relationship.\n\n    The vapor pressure of a chemical at `T` is given by:\n\n    .. math::\n        \\ln(P^{sat}/P_c) = \\frac{a\\tau + b\\tau^{1.5} + c\\tau^3 + d\\tau^6}\n        {1-\\tau}\n\n        a = -6.1559 - 4.0855\\omega\n\n        b = 1.5737 - 1.0540\\omega - 4.4365\\times 10^{-3} d\n\n        c = -0.8747 - 7.8874\\omega\n\n        d = \\frac{1}{-0.4893 - 0.9912\\omega + 3.1551\\omega^2}\n\n        \\tau = 1 - \\frac{T}{T_c}\n\n    Parameters\n    ----------\n    T : float\n        Temperature of fluid [K]\n    Tc : float\n        Critical temperature of fluid [K]\n    Pc : float\n        Critical pressure of fluid [Pa]\n    omega : float\n        Acentric factor [-]\n\n    Returns\n    -------\n    Psat : float\n        Vapor pressure, [Pa]\n\n    Notes\n    -----\n    [1]_ found an average error of 6.06% on 94 compounds and 1106 data points.\n\n    Examples\n    --------\n    >>> Edalat(347.2, 617.1, 36E5, 0.299)\n    13461.273080743307\n\n    References\n    ----------\n    .. [1] Edalat, M., R. B. Bozar-Jomehri, and G. A. Mansoori. \"Generalized \n       Equation Predicts Vapor Pressure of Hydrocarbons.\" Oil and Gas Journal; \n       91:5 (February 1, 1993).", "id": "f15789:m8"}
{"signature": "def download_file_insecure(url, target):", "body": "try:<EOL><INDENT>from urllib.request import urlopen<EOL><DEDENT>except ImportError:<EOL><INDENT>from urllib2 import urlopen<EOL><DEDENT>src = dst = None<EOL>try:<EOL><INDENT>src = urlopen(url)<EOL>data = src.read()<EOL>dst = open(target, \"<STR_LIT:wb>\")<EOL>dst.write(data)<EOL><DEDENT>finally:<EOL><INDENT>if src:<EOL><INDENT>src.close()<EOL><DEDENT>if dst:<EOL><INDENT>dst.close()<EOL><DEDENT><DEDENT>", "docstring": "Use Python to download the file, even though it cannot authenticate the\nconnection.", "id": "f4276:m14"}
{"signature": "def p_MandatoryPart(self, p):", "body": "if p[<NUM_LIT:1>]:<EOL><INDENT>p[<NUM_LIT:0>] = p[<NUM_LIT:3>]<EOL><DEDENT>", "docstring": "MandatoryPart : MANDATORY_GROUPS '{' MandatoryGroups '}'\n                         | empty", "id": "f5672:c0:m118"}
{"signature": "def delete_datacenter(self, datacenter_id):", "body": "response = self._perform_request(<EOL>url='<STR_LIT>' % (datacenter_id),<EOL>method='<STR_LIT>')<EOL>return response<EOL>", "docstring": "Removes the data center and all its components such as servers, NICs,\nload balancers, volumes.\n\n:param      datacenter_id: The unique ID of the data center.\n:type       datacenter_id: ``str``", "id": "f811:c0:m9"}
{"signature": "def load_npz(path='<STR_LIT>', name='<STR_LIT>'):", "body": "d = np.load(os.path.join(path, name))<EOL>return d['<STR_LIT>']<EOL>", "docstring": "Load the parameters of a Model saved by tl.files.save_npz().\n\n    Parameters\n    ----------\n    path : str\n        Folder path to `.npz` file.\n    name : str\n        The name of the `.npz` file.\n\n    Returns\n    --------\n    list of array\n        A list of parameters in order.\n\n    Examples\n    --------\n    - See ``tl.files.save_npz``\n\n    References\n    ----------\n    - `Saving dictionary using numpy <http://stackoverflow.com/questions/22315595/saving-dictionary-of-header-information-using-numpy-savez>`__", "id": "f11137:m18"}
{"signature": "def step(self, x, y, *args, **kwargs):", "body": "where = kwargs.pop('<STR_LIT>', '<STR_LIT>')<EOL>if where not in ('<STR_LIT>', '<STR_LIT>', '<STR_LIT>'):<EOL><INDENT>raise ValueError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL><DEDENT>kwargs['<STR_LIT>'] = '<STR_LIT>' + where<EOL>return self.plot(x, y, *args, **kwargs)<EOL>", "docstring": "call signature::\n\n  step(x, y, *args, **kwargs)\n\nMake a step plot. Additional keyword args to :func:`step` are the same\nas those for :func:`~matplotlib.pyplot.plot`.\n\n*x* and *y* must be 1-D sequences, and it is assumed, but not checked,\nthat *x* is uniformly increasing.\n\nKeyword arguments:\n\n*where*: [ 'pre' | 'post' | 'mid'  ]\n  If 'pre', the interval from x[i] to x[i+1] has level y[i]\n\n  If 'post', that interval has level y[i+1]\n\n  If 'mid', the jumps in *y* occur half-way between the\n  *x*-values.", "id": "f17238:c1:m146"}
{"signature": "@instruction<EOL><INDENT>def PUNPCKLQDQ(cpu, dest, src):<DEDENT>", "body": "cpu._PUNPCKL(dest, src, <NUM_LIT:64>)<EOL>", "docstring": "Interleaves the low-order quad-words of the source and destination operands.\n\nUnpacks and interleaves the low-order data elements (bytes, words, doublewords, and quadwords)\nof the destination operand (first operand) and source operand (second operand) into the\ndestination operand.\n\n:param cpu: current CPU.\n:param dest: destination operand.\n:param src: source operand.", "id": "f16975:c2:m198"}
{"signature": "def __init__(self, *values, **kwargs):", "body": "super(Q, self).__init__(*values, **kwargs)<EOL>self.escape = kwargs.setdefault(\"<STR_LIT>\", self.escape)<EOL>self.html_js_escape = kwargs.setdefault(\"<STR_LIT>\", self.html_js_escape)<EOL>self.quote = kwargs.setdefault(\"<STR_LIT>\", self.quote)<EOL>", "docstring": "Create the new ``Quote`` instance\n\n        :param bool escape: Whether or not quoted data should be escaped (default=``False``)\n        :param bool html_js_escape: Whether or not quoted data should be html-javascript escaped (default=``False``)\n        :param str quote: The quote character to be used if ``escape`` and ``html_js_escape`` are ``False``", "id": "f756:c9:m0"}
{"signature": "def print_success(string):", "body": "print_line('<STR_LIT>'.format(string))<EOL>", "docstring": "Prints a green [+] before the message", "id": "f7511:m4"}
{"signature": "def get_times(self):", "body": "self.ensureDetection()<EOL>times=[]<EOL>for ap in self.APs:<EOL><INDENT>times.append(ap[\"<STR_LIT:T>\"])<EOL><DEDENT>return np.array(sorted(times))<EOL>", "docstring": "return an array of times (in sec) of all APs.", "id": "f11352:c0:m5"}
{"signature": "def add_acl_method(self, method_name):", "body": "if isinstance(self.allowed_methods, set):<EOL><INDENT>self.allowed_methods.add(method_name)<EOL><DEDENT>else:<EOL><INDENT>self.allowed_methods = set([method_name])<EOL><DEDENT>", "docstring": "ACL system: make the method_name accessible to the current socket", "id": "f11816:c0:m2"}
{"signature": "def p_prj_home_art_1(self, p):", "body": "try:<EOL><INDENT>self.builder.set_file_atrificat_of_project(self.document, '<STR_LIT>', p[<NUM_LIT:2>])<EOL><DEDENT>except OrderError:<EOL><INDENT>self.order_error('<STR_LIT>', '<STR_LIT>', p.lineno(<NUM_LIT:1>))<EOL><DEDENT>", "docstring": "prj_home_art : ART_PRJ_HOME LINE", "id": "f3753:c0:m25"}
{"signature": "def setMinPctOverlapDutyCycles(self, minPctOverlapDutyCycles):", "body": "self._minPctOverlapDutyCycles = minPctOverlapDutyCycles<EOL>", "docstring": "Sets the minimum tolerated activity duty cycle, given as percent of\nneighbors' activity duty cycle.\n\n:param minPctOverlapDutyCycles: (float) value to set.", "id": "f17561:c4:m42"}
{"signature": "def train(self, debug=True, force=False, single_thread=False, timeout=<NUM_LIT:20>):", "body": "if not self.must_train and not force:<EOL><INDENT>return<EOL><DEDENT>self.padaos.compile()<EOL>self.train_thread = Thread(target=self._train, kwargs=dict(<EOL>debug=debug,<EOL>single_thread=single_thread,<EOL>timeout=timeout<EOL>), daemon=True)<EOL>self.train_thread.start()<EOL>self.train_thread.join(timeout)<EOL>self.must_train = False<EOL>return not self.train_thread.is_alive()<EOL>", "docstring": "Trains all the loaded intents that need to be updated\nIf a cache file exists with the same hash as the intent file,\nthe intent will not be trained and just loaded from file\n\nArgs:\n    debug (bool): Whether to print a message to stdout each time a new intent is trained\n    force (bool): Whether to force training if already finished\n    single_thread (bool): Whether to force running in a single thread\n    timeout (float): Seconds before cancelling training\nReturns:\n    bool: True if training succeeded without timeout", "id": "f9734:c0:m10"}
{"signature": "def cohere(x, y, NFFT=<NUM_LIT>, Fs=<NUM_LIT:2>, detrend=detrend_none, window=window_hanning,<EOL>noverlap=<NUM_LIT:0>, pad_to=None, sides='<STR_LIT:default>', scale_by_freq=None):", "body": "if len(x)<<NUM_LIT:2>*NFFT:<EOL><INDENT>raise ValueError(_coh_error)<EOL><DEDENT>Pxx, f = psd(x, NFFT, Fs, detrend, window, noverlap, pad_to, sides,<EOL>scale_by_freq)<EOL>Pyy, f = psd(y, NFFT, Fs, detrend, window, noverlap, pad_to, sides,<EOL>scale_by_freq)<EOL>Pxy, f = csd(x, y, NFFT, Fs, detrend, window, noverlap, pad_to, sides,<EOL>scale_by_freq)<EOL>Cxy = np.divide(np.absolute(Pxy)**<NUM_LIT:2>, Pxx*Pyy)<EOL>Cxy.shape = (len(f),)<EOL>return Cxy, f<EOL>", "docstring": "The coherence between *x* and *y*.  Coherence is the normalized\ncross spectral density:\n\n.. math::\n\n    C_{xy} = \\\\frac{|P_{xy}|^2}{P_{xx}P_{yy}}\n\n*x*, *y*\n    Array or sequence containing the data\n%(PSD)s\nThe return value is the tuple (*Cxy*, *f*), where *f* are the\nfrequencies of the coherence vector. For cohere, scaling the\nindividual densities by the sampling frequency has no effect, since\nthe factors cancel out.\n\n.. seealso::\n    :func:`psd` and :func:`csd`:\n        For information about the methods used to compute\n        :math:`P_{xy}`, :math:`P_{xx}` and :math:`P_{yy}`.", "id": "f17237:m17"}
{"signature": "def check(self, query):", "body": "if query.get_type() in {Keyword.LIST, Keyword.DROP}:<EOL><INDENT>series = query.series_stmt<EOL><DEDENT>else:<EOL><INDENT>series = query.from_stmt<EOL><DEDENT>if len(series) >= self.min_series_name_length:<EOL><INDENT>return Ok(True)<EOL><DEDENT>return Err(\"<STR_LIT>\")<EOL>", "docstring": ":param query:", "id": "f1432:c0:m3"}
{"signature": "def set_fontsize(self, fontsize):", "body": "return self.set_size(fontsize)<EOL>", "docstring": "alias for set_size", "id": "f17190:c0:m53"}
{"signature": "def get_corners(self):", "body": "return self.__max_corner, self.__min_corner<EOL>", "docstring": "!\n        @brief Return spatial description of current block.\n\n        @return (tuple) Pair of maximum and minimum corners (max_corner, min_corner).", "id": "f15588:c3:m3"}
{"signature": "def report(function, *args, **kwds):", "body": "try:<EOL><INDENT>function(*args, **kwds)<EOL><DEDENT>except Exception:<EOL><INDENT>traceback.print_exc()<EOL><DEDENT>", "docstring": "Run a function, catch, report and discard exceptions", "id": "f2007:m1"}
{"signature": "def valuestodict(key):", "body": "dout = {}<EOL>size = winreg.QueryInfoKey(key)[<NUM_LIT:1>]<EOL>tz_res = None<EOL>for i in range(size):<EOL><INDENT>key_name, value, dtype = winreg.EnumValue(key, i)<EOL>if dtype == winreg.REG_DWORD or dtype == winreg.REG_DWORD_LITTLE_ENDIAN:<EOL><INDENT>if value & (<NUM_LIT:1> << <NUM_LIT>):<EOL><INDENT>value = value - (<NUM_LIT:1> << <NUM_LIT:32>)<EOL><DEDENT><DEDENT>elif dtype == winreg.REG_SZ:<EOL><INDENT>if value.startswith('<STR_LIT>'):<EOL><INDENT>tz_res = tz_res or tzres()<EOL>value = tz_res.name_from_string(value)<EOL><DEDENT>value = value.rstrip('<STR_LIT:\\x00>')    <EOL><DEDENT>dout[key_name] = value<EOL><DEDENT>return dout<EOL>", "docstring": "Convert a registry key's values to a dictionary.", "id": "f15162:m2"}
{"signature": "def calculate(self, T, P, zs, ws, method):", "body": "if method == SIMPLE:<EOL><INDENT>ks = [i(T, P) for i in self.ThermalConductivityLiquids]<EOL>return mixing_simple(zs, ks)<EOL><DEDENT>elif method == DIPPR_9H:<EOL><INDENT>ks = [i(T, P) for i in self.ThermalConductivityLiquids]<EOL>return DIPPR9H(ws, ks)<EOL><DEDENT>elif method == FILIPPOV:<EOL><INDENT>ks = [i(T, P) for i in self.ThermalConductivityLiquids]<EOL>return Filippov(ws, ks)<EOL><DEDENT>elif method == MAGOMEDOV:<EOL><INDENT>k_w = self.ThermalConductivityLiquids[self.index_w](T, P)<EOL>ws = list(ws) ; ws.pop(self.index_w)<EOL>return thermal_conductivity_Magomedov(T, P, ws, self.wCASs, k_w)<EOL><DEDENT>else:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT>", "docstring": "r'''Method to calculate thermal conductivity of a liquid mixture at \n        temperature `T`, pressure `P`, mole fractions `zs` and weight fractions\n        `ws` with a given method.\n\n        This method has no exception handling; see `mixture_property`\n        for that.\n\n        Parameters\n        ----------\n        T : float\n            Temperature at which to calculate the property, [K]\n        P : float\n            Pressure at which to calculate the property, [Pa]\n        zs : list[float]\n            Mole fractions of all species in the mixture, [-]\n        ws : list[float]\n            Weight fractions of all species in the mixture, [-]\n        method : str\n            Name of the method to use\n\n        Returns\n        -------\n        k : float\n            Thermal conductivity of the liquid mixture, [W/m/K]", "id": "f15790:c1:m2"}
{"signature": "def modify_image(self, image, results):", "body": "return image<EOL>", "docstring": "Customize an lens_data. e.g. removing lens light.\n\nParameters\n----------\nimage: scaled_array.ScaledSquarePixelArray\n    An lens_data that has been masked\nresults: autofit.tools.pipeline.ResultsCollection\n    The result of the previous lens\n\nReturns\n-------\nlens_data: scaled_array.ScaledSquarePixelArray\n    The modified image (not changed by default)", "id": "f5967:c3:m1"}
{"signature": "def hierarchy(ref, est, **kwargs):", "body": "namespace = '<STR_LIT>'<EOL>ref = coerce_annotation(ref, namespace)<EOL>est = coerce_annotation(est, namespace)<EOL>ref_hier, ref_hier_lab = hierarchy_flatten(ref)<EOL>est_hier, est_hier_lab = hierarchy_flatten(est)<EOL>return mir_eval.hierarchy.evaluate(ref_hier, ref_hier_lab,<EOL>est_hier, est_hier_lab,<EOL>**kwargs)<EOL>", "docstring": "r'''Multi-level segmentation evaluation\n\n    Parameters\n    ----------\n    ref : jams.Annotation\n        Reference annotation object\n    est : jams.Annotation\n        Estimated annotation object\n    kwargs\n        Additional keyword arguments\n\n    Returns\n    -------\n    scores : dict\n        Dictionary of scores, where the key is the metric name (str) and\n        the value is the (float) score achieved.\n\n    See Also\n    --------\n    mir_eval.hierarchy.evaluate\n\n    Examples\n    --------\n    >>> # Load in the JAMS objects\n    >>> ref_jam = jams.load('reference.jams')\n    >>> est_jam = jams.load('estimated.jams')\n    >>> # Select the first relevant annotations\n    >>> ref_ann = ref_jam.search(namespace='multi_segment')[0]\n    >>> est_ann = est_jam.search(namespace='multi_segment')[0]\n    >>> scores = jams.eval.hierarchy(ref_ann, est_ann)", "id": "f11236:m6"}
{"signature": "def get_grey(self, tex, fontsize=None, dpi=None):", "body": "key = tex, self.get_font_config(), fontsize, dpi<EOL>alpha = self.grey_arrayd.get(key)<EOL>if alpha is None:<EOL><INDENT>pngfile = self.make_png(tex, fontsize, dpi)<EOL>X = read_png(os.path.join(self.texcache, pngfile))<EOL>if rcParams['<STR_LIT>'] is not None:<EOL><INDENT>hack = rcParams['<STR_LIT>']<EOL><DEDENT>else:<EOL><INDENT>hack = self._dvipng_hack_alpha<EOL><DEDENT>if hack:<EOL><INDENT>alpha = <NUM_LIT:1>-X[:,:,<NUM_LIT:0>]<EOL><DEDENT>else:<EOL><INDENT>alpha = X[:,:,-<NUM_LIT:1>]<EOL><DEDENT>self.grey_arrayd[key] = alpha<EOL><DEDENT>return alpha<EOL>", "docstring": "returns the alpha channel", "id": "f17185:c0:m11"}
{"signature": "def encode(self, word, max_length=-<NUM_LIT:1>):", "body": "def _foersvensker(lokal_ordet):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT:F>')<EOL>for i in self._harde_vokaler:<EOL><INDENT>lokal_ordet = lokal_ordet.replace(i + '<STR_LIT>', i + '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace(i + '<STR_LIT:Y>', i + '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace(i + '<STR_LIT:I>', i + '<STR_LIT>')<EOL><DEDENT>for i in self._mjuka_vokaler:<EOL><INDENT>lokal_ordet = lokal_ordet.replace(i + '<STR_LIT>', i + '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace(i + '<STR_LIT:Y>', i + '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace(i + '<STR_LIT:I>', i + '<STR_LIT>')<EOL><DEDENT>if '<STR_LIT:H>' in lokal_ordet:<EOL><INDENT>for i in self._uc_c_set:<EOL><INDENT>lokal_ordet = lokal_ordet.replace('<STR_LIT:H>' + i, i)<EOL><DEDENT><DEDENT>lokal_ordet = lokal_ordet.translate(self._substitutions)<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>lokal_ordet = lokal_ordet.replace('<STR_LIT>', '<STR_LIT>')<EOL>return lokal_ordet<EOL><DEDENT>def _koda_foersta_ljudet(lokal_ordet):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>if (<EOL>lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] in self._mjuka_vokaler<EOL>or lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] in self._harde_vokaler<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT:$>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>elif lokal_ordet[<NUM_LIT:0>:<NUM_LIT:2>] in ('<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'):<EOL><INDENT>lokal_ordet = '<STR_LIT>' + lokal_ordet[<NUM_LIT:2>:]<EOL><DEDENT>elif (<EOL>lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] == '<STR_LIT>'<EOL>and lokal_ordet[<NUM_LIT:1>:<NUM_LIT:2>] in self._mjuka_vokaler<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>elif lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] == '<STR_LIT>':<EOL><INDENT>lokal_ordet = '<STR_LIT>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>elif lokal_ordet[<NUM_LIT:0>:<NUM_LIT:2>] == '<STR_LIT>' and lokal_ordet[<NUM_LIT:2>:<NUM_LIT:3>] in frozenset(<EOL>self._mjuka_vokaler | self._harde_vokaler<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT:#>' + lokal_ordet[<NUM_LIT:2>:]<EOL><DEDENT>elif (<EOL>lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] == '<STR_LIT:C>'<EOL>and lokal_ordet[<NUM_LIT:1>:<NUM_LIT:2>] in self._harde_vokaler<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>elif (<EOL>lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] == '<STR_LIT:C>' and lokal_ordet[<NUM_LIT:1>:<NUM_LIT:2>] in self._uc_c_set<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>elif lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] == '<STR_LIT:X>':<EOL><INDENT>lokal_ordet = '<STR_LIT:S>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>elif (<EOL>lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] == '<STR_LIT:C>'<EOL>and lokal_ordet[<NUM_LIT:1>:<NUM_LIT:2>] in self._mjuka_vokaler<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT:S>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>elif lokal_ordet[<NUM_LIT:0>:<NUM_LIT:3>] in ('<STR_LIT>', '<STR_LIT>', '<STR_LIT>'):<EOL><INDENT>lokal_ordet = '<STR_LIT:#>' + lokal_ordet[<NUM_LIT:3>:]<EOL><DEDENT>elif lokal_ordet[<NUM_LIT:0>:<NUM_LIT:2>] in ('<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'):<EOL><INDENT>lokal_ordet = '<STR_LIT:#>' + lokal_ordet[<NUM_LIT:2>:]<EOL><DEDENT>elif (<EOL>lokal_ordet[<NUM_LIT:0>:<NUM_LIT:2>] == '<STR_LIT>'<EOL>and lokal_ordet[<NUM_LIT:2>:<NUM_LIT:3>] in self._mjuka_vokaler<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT:#>' + lokal_ordet[<NUM_LIT:2>:]<EOL><DEDENT>elif (<EOL>lokal_ordet[<NUM_LIT:0>:<NUM_LIT:1>] == '<STR_LIT>'<EOL>and lokal_ordet[<NUM_LIT:1>:<NUM_LIT:2>] in self._mjuka_vokaler<EOL>):<EOL><INDENT>lokal_ordet = '<STR_LIT:#>' + lokal_ordet[<NUM_LIT:1>:]<EOL><DEDENT>return lokal_ordet<EOL><DEDENT>word = unicode_normalize('<STR_LIT>', text_type(word.upper()))<EOL>word = word.replace('<STR_LIT>', '<STR_LIT>')<EOL>word = word.replace('<STR_LIT:->', '<STR_LIT:U+0020>')<EOL>for adelstitel in self._adelstitler:<EOL><INDENT>while adelstitel in word:<EOL><INDENT>word = word.replace(adelstitel, '<STR_LIT:U+0020>')<EOL><DEDENT>if word.startswith(adelstitel[<NUM_LIT:1>:]):<EOL><INDENT>word = word[len(adelstitel) - <NUM_LIT:1> :]<EOL><DEDENT><DEDENT>ordlista = word.split()<EOL>ordlista = [<EOL>self._delete_consecutive_repeats(ordet) for ordet in ordlista<EOL>]<EOL>if not ordlista:<EOL><INDENT>return ('<STR_LIT>',)<EOL><DEDENT>ordlista = [_foersvensker(ordet) for ordet in ordlista]<EOL>ordlista = [<EOL>'<STR_LIT>'.join(c for c in ordet if c in self._uc_set)<EOL>for ordet in ordlista<EOL>]<EOL>ordlista = [_koda_foersta_ljudet(ordet) for ordet in ordlista]<EOL>rest = [ordet[<NUM_LIT:1>:] for ordet in ordlista]<EOL>rest = [ordet.replace('<STR_LIT>', '<STR_LIT:T>') for ordet in rest]<EOL>rest = [ordet.replace('<STR_LIT:X>', '<STR_LIT>') for ordet in rest]<EOL>for vokal in self._mjuka_vokaler:<EOL><INDENT>rest = [ordet.replace('<STR_LIT:C>' + vokal, '<STR_LIT>' + vokal) for ordet in rest]<EOL><DEDENT>rest = [ordet.translate(self._trans) for ordet in rest]<EOL>rest = [self._delete_consecutive_repeats(ordet) for ordet in rest]<EOL>rest = [ordet.replace('<STR_LIT>', '<STR_LIT>') for ordet in rest]<EOL>ordlista = [<EOL>'<STR_LIT>'.join(ordet) for ordet in zip((_[<NUM_LIT:0>:<NUM_LIT:1>] for _ in ordlista), rest)<EOL>]<EOL>if max_length > <NUM_LIT:0>:<EOL><INDENT>ordlista = [ordet[:max_length] for ordet in ordlista]<EOL><DEDENT>return tuple(ordlista)<EOL>", "docstring": "Return the SfinxBis code for a word.\n\n        Parameters\n        ----------\n        word : str\n            The word to transform\n        max_length : int\n            The length of the code returned (defaults to unlimited)\n\n        Returns\n        -------\n        tuple\n            The SfinxBis value\n\n        Examples\n        --------\n        >>> pe = SfinxBis()\n        >>> pe.encode('Christopher')\n        ('K68376',)\n        >>> pe.encode('Niall')\n        ('N4',)\n        >>> pe.encode('Smith')\n        ('S53',)\n        >>> pe.encode('Schmidt')\n        ('S53',)\n\n        >>> pe.encode('Johansson')\n        ('J585',)\n        >>> pe.encode('Sj\u00f6berg')\n        ('#162',)", "id": "f6590:c0:m0"}
{"signature": "@asyncio.coroutine<EOL><INDENT>def get_play_text(self):<DEDENT>", "body": "return (yield from self.handle_text(self.API.get('<STR_LIT:text>')))<EOL>", "docstring": "Get the text associated with the played media.", "id": "f13081:c0:m27"}
{"signature": "def confirm(self):", "body": "self.email.is_verified = True<EOL>self.email.save()<EOL>signals.email_verified.send(email=self.email, sender=self.__class__)<EOL>logger.info(\"<STR_LIT>\", self.email.email)<EOL>", "docstring": "Mark the instance's email as verified.", "id": "f4819:c1:m0"}
{"signature": "def set_solid_capstyle(self, s):", "body": "s = s.lower()<EOL>if s not in self.validCap:<EOL><INDENT>raise ValueError('<STR_LIT>' % (s,)<EOL>+ '<STR_LIT>' % (self.validCap,))<EOL><DEDENT>self._solidcapstyle = s<EOL>", "docstring": "Set the cap style for solid linestyles\n\nACCEPTS: ['butt' | 'round' |  'projecting']", "id": "f17180:c0:m103"}
{"signature": "def stop(self):", "body": "self._quit = True<EOL>", "docstring": "Request the thread to stop.", "id": "f15264:c4:m3"}
{"signature": "def split_at_single(text, sep, not_before=[], not_after=[]):", "body": "n = <NUM_LIT:0><EOL>lt, s = len(text), len(sep)<EOL>last = <NUM_LIT:0><EOL>while n < lt:<EOL><INDENT>if not s + n > lt:<EOL><INDENT>if sep == text[n:n + s]:<EOL><INDENT>if any(text[last:n].endswith(e) for e in not_before):<EOL><INDENT>pass<EOL><DEDENT>elif any(text[n + s:].startswith(e) for e in not_after):<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>yield text[last:n]<EOL>last = n + s<EOL>n += s - <NUM_LIT:1><EOL><DEDENT><DEDENT><DEDENT>n += <NUM_LIT:1><EOL><DEDENT>yield text[last:]<EOL>", "docstring": "Works like text.split(sep) but separated fragments\n    cant end with not_before or start with not_after", "id": "f11658:m14"}
{"signature": "def add_cpinfo_to_lclist(<EOL>checkplots,  <EOL>initial_lc_catalog,<EOL>magcol,  <EOL>outfile,<EOL>checkplotglob='<STR_LIT>',<EOL>infokeys=CPINFO_DEFAULTKEYS,<EOL>nworkers=NCPUS<EOL>):", "body": "<EOL>if not isinstance(checkplots, list) and os.path.exists(checkplots):<EOL><INDENT>checkplots = sorted(glob.glob(os.path.join(checkplots, checkplotglob)))<EOL><DEDENT>tasklist = [(cpf, infokeys) for cpf in checkplots]<EOL>with ProcessPoolExecutor(max_workers=nworkers) as executor:<EOL><INDENT>resultfutures = executor.map(_cpinfo_key_worker, tasklist)<EOL><DEDENT>results = [x for x in resultfutures]<EOL>executor.shutdown()<EOL>with open(initial_lc_catalog,'<STR_LIT:rb>') as infd:<EOL><INDENT>lc_catalog = pickle.load(infd)<EOL><DEDENT>catalog_objectids = np.array(lc_catalog['<STR_LIT>']['<STR_LIT>'])<EOL>checkplot_objectids = np.array([x[<NUM_LIT:0>] for x in results])<EOL>extrainfokeys = []<EOL>actualkeys = []<EOL>for keyspec in infokeys:<EOL><INDENT>key, dtype, firstlevel, overwrite_append, nonesub, nansub = keyspec<EOL>if firstlevel:<EOL><INDENT>eik = key<EOL><DEDENT>else:<EOL><INDENT>eik = '<STR_LIT>' % (magcol, key)<EOL><DEDENT>extrainfokeys.append(eik)<EOL>eactual = eik.split('<STR_LIT:.>')<EOL>if not eactual[-<NUM_LIT:1>].isdigit():<EOL><INDENT>if not firstlevel:<EOL><INDENT>eactual = '<STR_LIT:.>'.join([eactual[<NUM_LIT:0>], eactual[-<NUM_LIT:1>]])<EOL><DEDENT>else:<EOL><INDENT>eactual = eactual[-<NUM_LIT:1>]<EOL><DEDENT><DEDENT>else:<EOL><INDENT>elastkey = eactual[-<NUM_LIT:2>]<EOL>if elastkey.endswith('<STR_LIT>'):<EOL><INDENT>elastkey = elastkey[:-<NUM_LIT:2>]<EOL><DEDENT>elif elastkey.endswith('<STR_LIT:s>'):<EOL><INDENT>elastkey = elastkey[:-<NUM_LIT:1>]<EOL><DEDENT>if not firstlevel:<EOL><INDENT>eactual = '<STR_LIT:.>'.join([eactual[<NUM_LIT:0>], elastkey])<EOL><DEDENT>else:<EOL><INDENT>eactual = elastkey<EOL><DEDENT><DEDENT>actualkeys.append(eactual)<EOL>if eactual not in lc_catalog['<STR_LIT>']:<EOL><INDENT>lc_catalog['<STR_LIT>'].append(eactual)<EOL><DEDENT>lc_catalog['<STR_LIT>'][eactual] = []<EOL><DEDENT>for catobj in tqdm(catalog_objectids):<EOL><INDENT>cp_objind = np.where(checkplot_objectids == catobj)<EOL>if len(cp_objind[<NUM_LIT:0>]) > <NUM_LIT:0>:<EOL><INDENT>thiscpinfo = results[cp_objind[<NUM_LIT:0>][<NUM_LIT:0>]]<EOL>thiscpinfo = thiscpinfo[<NUM_LIT:1>:]<EOL>for ekind, ek in enumerate(actualkeys):<EOL><INDENT>lc_catalog['<STR_LIT>'][ek].append(<EOL>thiscpinfo[ekind]<EOL>)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>for ekind, ek in enumerate(actualkeys):<EOL><INDENT>thiskeyspec = infokeys[ekind]<EOL>nonesub = thiskeyspec[-<NUM_LIT:2>]<EOL>lc_catalog['<STR_LIT>'][ek].append(<EOL>nonesub<EOL>)<EOL><DEDENT><DEDENT><DEDENT>for ek in actualkeys:<EOL><INDENT>lc_catalog['<STR_LIT>'][ek] = np.array(<EOL>lc_catalog['<STR_LIT>'][ek]<EOL>)<EOL><DEDENT>if '<STR_LIT>' in lc_catalog:<EOL><INDENT>if magcol not in lc_catalog['<STR_LIT>']:<EOL><INDENT>lc_catalog['<STR_LIT>'].append(magcol)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>lc_catalog['<STR_LIT>'] = [magcol]<EOL><DEDENT>with open(outfile, '<STR_LIT:wb>') as outfd:<EOL><INDENT>pickle.dump(lc_catalog, outfd, protocol=pickle.HIGHEST_PROTOCOL)<EOL><DEDENT>return outfile<EOL>", "docstring": "This adds checkplot info to the initial light curve catalogs generated by\n    `make_lclist`.\n\n    This is used to incorporate all the extra info checkplots can have for\n    objects back into columns in the light curve catalog produced by\n    `make_lclist`. Objects are matched between the checkplots and the light\n    curve catalog using their `objectid`. This then allows one to search this\n    'augmented' light curve catalog by these extra columns. The 'augmented'\n    light curve catalog also forms the basis for search interface provided by\n    the LCC-Server.\n\n    The default list of keys that will be extracted from a checkplot and added\n    as columns in the initial light curve catalog is listed above in the\n    `CPINFO_DEFAULTKEYS` list.\n\n    Parameters\n    ----------\n\n    checkplots : str or list\n        If this is a str, is interpreted as a directory which will be searched\n        for checkplot pickle files using `checkplotglob`. If this is a list, it\n        will be interpreted as a list of checkplot pickle files to process.\n\n    initial_lc_catalog : str\n        This is the path to the light curve catalog pickle made by\n        `make_lclist`.\n\n    magcol : str\n        This is used to indicate the light curve magnitude column to extract\n        magnitude column specific information. For example, Stetson variability\n        indices can be generated using magnitude measurements in separate\n        photometric apertures, which appear in separate `magcols` in the\n        checkplot. To associate each such feature of the object with its\n        specific `magcol`, pass that `magcol` in here. This `magcol` will then\n        be added as a prefix to the resulting column in the 'augmented' LC\n        catalog, e.g. Stetson J will appear as `magcol1_stetsonj` and\n        `magcol2_stetsonj` for two separate magcols.\n\n    outfile : str\n        This is the file name of the output 'augmented' light curve catalog\n        pickle file that will be written.\n\n    infokeys : list of tuples\n\n        This is a list of keys to extract from the checkplot and some info on\n        how this extraction is to be done. Each key entry is a six-element\n        tuple of the following form:\n\n        - key name in the checkplot\n        - numpy dtype of the value of this key\n        - False if key is associated with a magcol or True otherwise\n        - False if subsequent updates to the same column name will append to\n          existing key values in the output augmented light curve catalog or\n          True if these will overwrite the existing key value\n        - character to use to substitute a None value of the key in the\n          checkplot in the output light curve catalog column\n        - character to use to substitute a nan value of the key in the\n          checkplot in the output light curve catalog column\n\n        See the `CPFINFO_DEFAULTKEYS` list above for examples.\n\n    nworkers : int\n        The number of parallel workers to launch to extract checkplot\n        information.\n\n    Returns\n    -------\n\n    str\n        Returns the path to the generated 'augmented' light curve catalog pickle\n        file.", "id": "f14709:m5"}
{"signature": "def gradient(self):", "body": "grad = {}<EOL>for i, f in enumerate(self._covariances):<EOL><INDENT>for varname, g in f.gradient().items():<EOL><INDENT>grad[f\"<STR_LIT>\"] = g<EOL><DEDENT><DEDENT>return grad<EOL>", "docstring": "Sum of covariance function derivatives.\n\nReturns\n-------\ndict\n    \u2202K\u2080 + \u2202K\u2081 + \u22ef", "id": "f13607:c0:m2"}
{"signature": "def integral(A=None,dF=None,F=None,axis = <NUM_LIT:0>,trapez = False,cumulative = False):", "body": "ndim = max(v.ndim for v in (A,dF,F) if v is not None)<EOL>def broadcast(x):<EOL><INDENT>new_shape = [<NUM_LIT:1>]*ndim<EOL>new_shape[axis] = -<NUM_LIT:1><EOL>return np.reshape(x,new_shape)<EOL><DEDENT>if F is not None:<EOL><INDENT>assert(dF is None)<EOL>if F.ndim<ndim:<EOL><INDENT>F = broadcast(F)<EOL><DEDENT>N = F.shape[axis]<EOL>dF = F.take(indices = range(<NUM_LIT:1>,N),axis = axis)-F.take(indices = range(N-<NUM_LIT:1>),axis = axis)<EOL><DEDENT>elif dF is not None:<EOL><INDENT>if dF.ndim<ndim:<EOL><INDENT>dF = broadcast(dF)<EOL><DEDENT>N = dF.shape[axis]+<NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>if A.ndim<ndim:<EOL><INDENT>A = broadcast(A)<EOL><DEDENT>N = A.shape[axis]<EOL><DEDENT>if A is not None:<EOL><INDENT>if trapez:<EOL><INDENT>midA = (A.take(indices = range(<NUM_LIT:1>,N),axis = axis)+A.take(indices = range(N-<NUM_LIT:1>),axis = axis))/<NUM_LIT:2><EOL><DEDENT>else:<EOL><INDENT>midA = A.take(indices=range(N-<NUM_LIT:1>),axis=axis)<EOL><DEDENT>if dF is not None:<EOL><INDENT>dY = midA*dF<EOL><DEDENT>else:<EOL><INDENT>dY = midA<EOL><DEDENT><DEDENT>else:<EOL><INDENT>dY = dF<EOL><DEDENT>pad_shape = list(dY.shape)<EOL>pad_shape[axis] = <NUM_LIT:1><EOL>pad = np.zeros(pad_shape)<EOL>if cumulative:<EOL><INDENT>return np.concatenate((pad,np.cumsum(dY,axis = axis)),axis = axis)<EOL><DEDENT>else:<EOL><INDENT>return np.sum(dY,axis = axis)<EOL><DEDENT>", "docstring": "Turns an array A of length N (the function values in N points)\nand an array dF of length N-1 (the masses of the N-1 intervals)\ninto an array of length N (the integral \\int A dF at N points, with first entry 0)\n\n:param A: Integrand (optional, default ones, length N)\n:param dF: Integrator (optional, default ones, length N-1)\n:param F: Alternative to dF (optional, length N)\n:param trapez: Use trapezoidal rule (else left point)", "id": "f10484:m6"}
{"signature": "def djeffify_html(rendered_string):", "body": "parser = DjeffParser()<EOL>parser.feed(rendered_string)<EOL>return parser.djhtml<EOL>", "docstring": "This function contains the core logic for a\nmiddleware, template tag or Template engine approach", "id": "f4011:m1"}
{"signature": "def set_greenlet_uncaught_exception_handler(func):", "body": "global _uncaught_exception_handler<EOL>prev_handler, _uncaught_exception_handler = _uncaught_exception_handler, func<EOL>return prev_handler<EOL>", "docstring": "Sets a global greenlet uncaught exception handler that will get called if a greenlet spawned by one of this module's\nwrappers raises an uncaught exception.\n:param func: exception handler function that will receive the exc_info tuple as an argument\n:returns: previous exception handler function or None", "id": "f5291:m0"}
{"signature": "def cond_pop(ol,index,**kwargs):", "body": "cond_func = kwargs['<STR_LIT>']<EOL>cond_func_args = kwargs['<STR_LIT>']<EOL>index = uniform_index(index,ol.__len__())<EOL>if('<STR_LIT>' in kwargs):<EOL><INDENT>mode = kwargs[\"<STR_LIT>\"]<EOL><DEDENT>else:<EOL><INDENT>mode = \"<STR_LIT>\"<EOL><DEDENT>value = ol[index]<EOL>cond = cond_func(index,value,*cond_func_args)<EOL>if(mode == \"<STR_LIT>\"):<EOL><INDENT>new = copy.deepcopy(ol)<EOL>if(cond):<EOL><INDENT>popped = new.pop(index)<EOL><DEDENT>else:<EOL><INDENT>popped = new<EOL><DEDENT>return({'<STR_LIT>':popped,'<STR_LIT:list>':new})<EOL><DEDENT>else:<EOL><INDENT>if(cond):<EOL><INDENT>popped = ol.pop(index)<EOL><DEDENT>else:<EOL><INDENT>popped = ol<EOL><DEDENT>return(popped)<EOL><DEDENT>", "docstring": "from elist.jprint import pobj\nfrom elist.elist import *\nol = [{'data':0;'type':'number'},{'data':'x';'type':'str'},{'data':'y';'type':'str'},4]\n#cond_func_args is a array\ndef cond_func(index,value,cond_func_args):", "id": "f1599:m107"}
{"signature": "def cons(collection, value):", "body": "if isinstance(value, collections.Mapping):<EOL><INDENT>if collection is None:<EOL><INDENT>collection = {}<EOL><DEDENT>collection.update(**value)<EOL><DEDENT>elif isinstance(value, six.string_types):<EOL><INDENT>if collection is None:<EOL><INDENT>collection = []<EOL><DEDENT>collection.append(value)<EOL><DEDENT>elif isinstance(value, collections.Iterable):<EOL><INDENT>if collection is None:<EOL><INDENT>collection = []<EOL><DEDENT>collection.extend(value)<EOL><DEDENT>else:<EOL><INDENT>if collection is None:<EOL><INDENT>collection = []<EOL><DEDENT>collection.append(value)<EOL><DEDENT>return collection<EOL>", "docstring": "Extends a collection with a value.", "id": "f10154:m0"}
{"signature": "def _descend_folder_for_id(parsed_path, folder_id):", "body": "if len(parsed_path) == <NUM_LIT:0>:<EOL><INDENT>return folder_id<EOL><DEDENT>session.token = verify_credentials()<EOL>base_folder = session.communicator.folder_get(session.token,<EOL>folder_id)<EOL>cur_folder_id = -<NUM_LIT:1><EOL>for path_part in parsed_path:<EOL><INDENT>cur_folder_id = base_folder['<STR_LIT>']<EOL>cur_children = session.communicator.folder_children(<EOL>session.token, cur_folder_id)<EOL>for inner_folder in cur_children['<STR_LIT>']:<EOL><INDENT>if inner_folder['<STR_LIT:name>'] == path_part:<EOL><INDENT>base_folder = session.communicator.folder_get(<EOL>session.token, inner_folder['<STR_LIT>'])<EOL>cur_folder_id = base_folder['<STR_LIT>']<EOL>break<EOL><DEDENT><DEDENT>else:<EOL><INDENT>return -<NUM_LIT:1><EOL><DEDENT><DEDENT>return cur_folder_id<EOL>", "docstring": "Descend a path to return a folder id starting from the given folder id.\n\n:param parsed_path: a list of folders from top to bottom of a hierarchy\n:type parsed_path: list[string]\n:param folder_id: The id of the folder from which to start the descent\n:type folder_id: int | long\n:returns: The id of the found folder or -1\n:rtype: int | long", "id": "f8359:m17"}
{"signature": "def set_weight(self, weight):", "body": "self._fontproperties.set_weight(weight)<EOL>", "docstring": "Set the font weight.\n\nACCEPTS: [ a numeric value in range 0-1000 | 'ultralight' | 'light' | 'normal' | 'regular' | 'book' | 'medium' | 'roman' | 'semibold' | 'demibold' | 'demi' | 'bold' | 'heavy' | 'extra bold' | 'black' ]", "id": "f17190:c0:m54"}
{"signature": "def render_pep440(pieces):", "body": "if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered = pieces[\"<STR_LIT>\"]<EOL>if pieces[\"<STR_LIT>\"] or pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered += plus_or_dot(pieces)<EOL>rendered += \"<STR_LIT>\" % (pieces[\"<STR_LIT>\"], pieces[\"<STR_LIT>\"])<EOL>if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered += \"<STR_LIT>\"<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>rendered = \"<STR_LIT>\" % (pieces[\"<STR_LIT>\"],<EOL>pieces[\"<STR_LIT>\"])<EOL>if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered += \"<STR_LIT>\"<EOL><DEDENT><DEDENT>return rendered<EOL>", "docstring": "Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]", "id": "f3617:m9"}
{"signature": "def __init__(self,config=None,rules=None):", "body": "self._rules = rules or []<EOL>self._dict = config or {}<EOL>self.enforce_rules()<EOL>", "docstring": ":param config: Old configuration\n:type config: behaving like dict or Config", "id": "f10465:c0:m0"}
{"signature": "def __getitem__(self, sound):", "body": "return self.resolve_sound(sound)<EOL>", "docstring": "Return a Sound instance matching the specification.", "id": "f10437:c0:m2"}
{"signature": "def to_int64(a):", "body": "<EOL>def promote_i4(typestr):<EOL><INDENT>if typestr[<NUM_LIT:1>:] == '<STR_LIT>':<EOL><INDENT>typestr = typestr[<NUM_LIT:0>]+'<STR_LIT>'<EOL><DEDENT>return typestr<EOL><DEDENT>dtype = [(name, promote_i4(typestr)) for name,typestr in a.dtype.descr]<EOL>return a.astype(dtype)<EOL>", "docstring": "Return view of the recarray with all int32 cast to int64.", "id": "f6857:m2"}
{"signature": "def main():", "body": "<EOL>args = parse_command_line()<EOL>print(HEADER.format(ip.__version__))<EOL>np.random.seed(args.rseed)<EOL>if os.path.exists(ip.__debugflag__):<EOL><INDENT>os.remove(ip.__debugflag__)<EOL><DEDENT>if args.debug:<EOL><INDENT>print(\"<STR_LIT>\")<EOL>ip._debug_on()<EOL><DEDENT>if args.json:<EOL><INDENT>data = ipa.tetrad(name=args.name, workdir=args.workdir, load=True)<EOL>if args.force:<EOL><INDENT>data._refresh()<EOL><DEDENT><DEDENT>else:<EOL><INDENT>newjson = os.path.join(args.workdir, args.name+'<STR_LIT>')<EOL>print(\"<STR_LIT>\".format(args.name))<EOL>if (not os.path.exists(newjson)) or args.force:<EOL><INDENT>if args.force:<EOL><INDENT>ipa.tetrad(name=args.name, <EOL>workdir=args.workdir, <EOL>data=args.seq, <EOL>initarr=False, <EOL>save_invariants=args.invariants,<EOL>cli=True,<EOL>quiet=True)._refresh()<EOL><DEDENT>data = ipa.tetrad(name=args.name, <EOL>workdir=args.workdir, <EOL>method=args.method, <EOL>data=args.seq, <EOL>resolve=args.resolve,<EOL>mapfile=args.map, <EOL>guidetree=args.tree, <EOL>nboots=args.boots, <EOL>nquartets=args.nquartets, <EOL>cli=True,<EOL>save_invariants=args.invariants,<EOL>)<EOL><DEDENT>else:<EOL><INDENT>raise SystemExit(QUARTET_EXISTS.format(args.name, args.workdir, args.workdir, args.name, args.name))<EOL><DEDENT><DEDENT>if args.boots:<EOL><INDENT>data.params.nboots = int(args.boots)<EOL><DEDENT>if args.ipcluster:<EOL><INDENT>ipyclient = ipp.Client(profile=args.ipcluster)<EOL>data._ipcluster[\"<STR_LIT>\"] = len(ipyclient)<EOL><DEDENT>else:<EOL><INDENT>ipyclient = None<EOL>data._ipcluster[\"<STR_LIT>\"] = args.cores if args.cores else detect_cpus()<EOL>data._ipcluster[\"<STR_LIT>\"] = \"<STR_LIT>\"<EOL>if args.MPI:<EOL><INDENT>data._ipcluster[\"<STR_LIT>\"] = \"<STR_LIT>\"<EOL>if not args.cores:<EOL><INDENT>raise IPyradWarningExit(\"<STR_LIT>\")<EOL><DEDENT><DEDENT>data = register_ipcluster(data)<EOL><DEDENT>if data.checkpoint.boots:<EOL><INDENT>print(LOADING_MESSAGE.format(<EOL>data.name, data.params.method, data.checkpoint.boots))<EOL><DEDENT>data.run(force=args.force, ipyclient=ipyclient)<EOL>", "docstring": "main function", "id": "f5350:m1"}
{"signature": "def if_(*args):", "body": "for i in range(<NUM_LIT:0>, len(args) - <NUM_LIT:1>, <NUM_LIT:2>):<EOL><INDENT>if args[i]:<EOL><INDENT>return args[i + <NUM_LIT:1>]<EOL><DEDENT><DEDENT>if len(args) % <NUM_LIT:2>:<EOL><INDENT>return args[-<NUM_LIT:1>]<EOL><DEDENT>else:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Implements the 'if' operator with support for multiple elseif-s.", "id": "f5516:m0"}
{"signature": "def log_entity_deletion(entity, params=None):", "body": "p = {'<STR_LIT>': entity}<EOL>if params:<EOL><INDENT>p['<STR_LIT>'] = params<EOL><DEDENT>_log(TYPE_CODES.DELETE, p)<EOL>", "docstring": "Logs an entity creation", "id": "f4461:m1"}
{"signature": "def _AddHasFieldMethod(message_descriptor, cls):", "body": "is_proto3 = (message_descriptor.syntax == \"<STR_LIT>\")<EOL>error_msg = _Proto3HasError if is_proto3 else _Proto2HasError<EOL>hassable_fields = {}<EOL>for field in message_descriptor.fields:<EOL><INDENT>if field.label == _FieldDescriptor.LABEL_REPEATED:<EOL><INDENT>continue<EOL><DEDENT>if (is_proto3 and field.cpp_type != _FieldDescriptor.CPPTYPE_MESSAGE and<EOL>not field.containing_oneof):<EOL><INDENT>continue<EOL><DEDENT>hassable_fields[field.name] = field<EOL><DEDENT>if not is_proto3:<EOL><INDENT>for oneof in message_descriptor.oneofs:<EOL><INDENT>hassable_fields[oneof.name] = oneof<EOL><DEDENT><DEDENT>def HasField(self, field_name):<EOL><INDENT>try:<EOL><INDENT>field = hassable_fields[field_name]<EOL><DEDENT>except KeyError:<EOL><INDENT>raise ValueError(error_msg % field_name)<EOL><DEDENT>if isinstance(field, descriptor_mod.OneofDescriptor):<EOL><INDENT>try:<EOL><INDENT>return HasField(self, self._oneofs[field].name)<EOL><DEDENT>except KeyError:<EOL><INDENT>return False<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if field.cpp_type == _FieldDescriptor.CPPTYPE_MESSAGE:<EOL><INDENT>value = self._fields.get(field)<EOL>return value is not None and value._is_present_in_parent<EOL><DEDENT>else:<EOL><INDENT>return field in self._fields<EOL><DEDENT><DEDENT><DEDENT>cls.HasField = HasField<EOL>", "docstring": "Helper for _AddMessageMethods().", "id": "f8655:m23"}
{"signature": "def scale_out(self, blocks=<NUM_LIT:1>):", "body": "r = []<EOL>for i in range(blocks):<EOL><INDENT>if self.provider:<EOL><INDENT>block = self.provider.submit(self.launch_cmd, <NUM_LIT:1>, self.workers_per_node)<EOL>logger.debug(\"<STR_LIT>\".format(i, block))<EOL>if not block:<EOL><INDENT>raise(ScalingFailed(self.provider.label,<EOL>\"<STR_LIT>\"))<EOL><DEDENT>self.engines.extend([block])<EOL>r.extend([block])<EOL><DEDENT><DEDENT>else:<EOL><INDENT>logger.error(\"<STR_LIT>\")<EOL>r = None<EOL><DEDENT>return r<EOL>", "docstring": "Scales out the number of active workers by 1.\n\n        This method is notImplemented for threads and will raise the error if called.\n\n        Parameters:\n            blocks : int\n               Number of blocks to be provisioned.", "id": "f2813:c0:m8"}
{"signature": "def pull_tasks(self, kill_event):", "body": "logger.info(\"<STR_LIT>\")<EOL>poller = zmq.Poller()<EOL>poller.register(self.task_incoming, zmq.POLLIN)<EOL>msg = self.create_reg_message()<EOL>logger.debug(\"<STR_LIT>\".format(msg))<EOL>self.task_incoming.send(msg)<EOL>last_beat = time.time()<EOL>last_interchange_contact = time.time()<EOL>task_recv_counter = <NUM_LIT:0><EOL>poll_timer = <NUM_LIT:1><EOL>while not kill_event.is_set():<EOL><INDENT>time.sleep(LOOP_SLOWDOWN)<EOL>ready_worker_count = self.ready_worker_queue.qsize()<EOL>pending_task_count = self.pending_task_queue.qsize()<EOL>logger.debug(\"<STR_LIT>\".format(ready_worker_count,<EOL>pending_task_count))<EOL>if time.time() > last_beat + self.heartbeat_period:<EOL><INDENT>self.heartbeat()<EOL>last_beat = time.time()<EOL><DEDENT>if pending_task_count < self.max_queue_size and ready_worker_count > <NUM_LIT:0>:<EOL><INDENT>logger.debug(\"<STR_LIT>\".format(ready_worker_count))<EOL>msg = ((ready_worker_count).to_bytes(<NUM_LIT:4>, \"<STR_LIT>\"))<EOL>self.task_incoming.send(msg)<EOL><DEDENT>socks = dict(poller.poll(timeout=poll_timer))<EOL>if self.task_incoming in socks and socks[self.task_incoming] == zmq.POLLIN:<EOL><INDENT>_, pkl_msg = self.task_incoming.recv_multipart()<EOL>tasks = pickle.loads(pkl_msg)<EOL>last_interchange_contact = time.time()<EOL>if tasks == '<STR_LIT>':<EOL><INDENT>logger.critical(\"<STR_LIT>\")<EOL>kill_event.set()<EOL>break<EOL><DEDENT>elif tasks == HEARTBEAT_CODE:<EOL><INDENT>logger.debug(\"<STR_LIT>\")<EOL><DEDENT>else:<EOL><INDENT>poll_timer = <NUM_LIT:1><EOL>task_recv_counter += len(tasks)<EOL>logger.debug(\"<STR_LIT>\".format([t['<STR_LIT>'] for t in tasks],<EOL>task_recv_counter))<EOL>for task in tasks:<EOL><INDENT>self.pending_task_queue.put(task)<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>logger.debug(\"<STR_LIT>\")<EOL>poll_timer = min(self.heartbeat_period * <NUM_LIT:1000>, poll_timer * <NUM_LIT:2>)<EOL>if time.time() > last_interchange_contact + self.heartbeat_threshold:<EOL><INDENT>logger.critical(\"<STR_LIT>\")<EOL>kill_event.set()<EOL>logger.critical(\"<STR_LIT>\")<EOL>break<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Pulls tasks from the incoming tasks 0mq pipe onto the internal\n        pending task queue\n\n        Parameters:\n        -----------\n        kill_event : threading.Event\n              Event to let the thread know when it is time to die.", "id": "f2823:c0:m5"}
{"signature": "def _get_node(name: str, args: str):", "body": "obj = get_object(name)<EOL>args = ast.literal_eval(args)<EOL>if not isinstance(args, tuple):<EOL><INDENT>args = (args,)<EOL><DEDENT>return obj.node(*args)<EOL>", "docstring": "Get node from object name and arg string\n\n    Not Used. Left for future reference purpose.", "id": "f13986:m10"}
{"signature": "def tolist(self):", "body": "return self[:]<EOL>", "docstring": "Return contents as a simple list.", "id": "f6849:c0:m2"}
{"signature": "def get_log(name=None):", "body": "if name is None:<EOL><INDENT>name = '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>name = '<STR_LIT>' + name<EOL><DEDENT>log = logging.getLogger(name)<EOL>log.setLevel(logging.INFO)<EOL>return log<EOL>", "docstring": "Return a console logger.\n\n    Output may be sent to the logger using the `debug`, `info`, `warning`,\n    `error` and `critical` methods.\n\n    Parameters\n    ----------\n    name : str\n        Name of the log.\n\n    References\n    ----------\n    .. [1] Logging facility for Python,\n           http://docs.python.org/library/logging.html", "id": "f424:m0"}
{"signature": "def new_driver_object(name):", "body": "driver = __locate_driver_named(name)<EOL>return driver.Driver() if driver else None<EOL>", "docstring": "Instantiates a new object of the named driver.\n\n    The API used by the returned object can be seen in\n    `ahio.abstract_driver.AbstractDriver`\n\n    @returns a Driver object from the required type of None if it's not\n    available", "id": "f10526:m5"}
{"signature": "def walk_directories_info(self, relativePath=\"<STR_LIT>\", fullPath=False, recursive=False):", "body": "assert isinstance(fullPath, bool), \"<STR_LIT>\"<EOL>assert isinstance(recursive, bool), \"<STR_LIT>\"<EOL>relativePath = self.to_repo_relative_path(path=relativePath, split=False)<EOL>for dpath in self.walk_directories_path(relativePath=relativePath, fullPath=False, recursive=recursive):<EOL><INDENT>dirInfoPath = os.path.join(self.__path,dpath,self.__dirInfo)<EOL>if os.path.isfile(dirInfoPath):<EOL><INDENT>with open(dirInfoPath, '<STR_LIT:rb>') as fd:<EOL><INDENT>info = pickle.load(fd)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>info = None<EOL><DEDENT>if fullPath:<EOL><INDENT>yield (os.path.join(self.__path, dpath), info)<EOL><DEDENT>else:<EOL><INDENT>yield (dpath, info)<EOL><DEDENT><DEDENT>", "docstring": "Walk the repository relative path and yield tuple of two items where\nfirst item is directory relative/full path and second item is directory\ninfo. If directory file info is not found on disk, second item will be None.\n\n:parameters:\n    #. relativePath (string): The relative path from which start the walk.\n    #. fullPath (boolean): Whether to return full or relative path.\n    #. recursive (boolean): Whether walk all directories files recursively.", "id": "f13917:c1:m32"}
{"signature": "def decompose_matrix(matrix):", "body": "M = numpy.array(matrix, dtype=numpy.float64, copy=True).T<EOL>if abs(M[<NUM_LIT:3>, <NUM_LIT:3>]) < _EPS:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>M /= M[<NUM_LIT:3>, <NUM_LIT:3>]<EOL>P = M.copy()<EOL>P[:, <NUM_LIT:3>] = <NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:1.0><EOL>if not numpy.linalg.det(P):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>scale = numpy.zeros((<NUM_LIT:3>, ))<EOL>shear = [<NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:0.0>]<EOL>angles = [<NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:0.0>]<EOL>if any(abs(M[:<NUM_LIT:3>, <NUM_LIT:3>]) > _EPS):<EOL><INDENT>perspective = numpy.dot(M[:, <NUM_LIT:3>], numpy.linalg.inv(P.T))<EOL>M[:, <NUM_LIT:3>] = <NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:1.0><EOL><DEDENT>else:<EOL><INDENT>perspective = numpy.array([<NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:0.0>, <NUM_LIT:1.0>])<EOL><DEDENT>translate = M[<NUM_LIT:3>, :<NUM_LIT:3>].copy()<EOL>M[<NUM_LIT:3>, :<NUM_LIT:3>] = <NUM_LIT:0.0><EOL>row = M[:<NUM_LIT:3>, :<NUM_LIT:3>].copy()<EOL>scale[<NUM_LIT:0>] = vector_norm(row[<NUM_LIT:0>])<EOL>row[<NUM_LIT:0>] /= scale[<NUM_LIT:0>]<EOL>shear[<NUM_LIT:0>] = numpy.dot(row[<NUM_LIT:0>], row[<NUM_LIT:1>])<EOL>row[<NUM_LIT:1>] -= row[<NUM_LIT:0>] * shear[<NUM_LIT:0>]<EOL>scale[<NUM_LIT:1>] = vector_norm(row[<NUM_LIT:1>])<EOL>row[<NUM_LIT:1>] /= scale[<NUM_LIT:1>]<EOL>shear[<NUM_LIT:0>] /= scale[<NUM_LIT:1>]<EOL>shear[<NUM_LIT:1>] = numpy.dot(row[<NUM_LIT:0>], row[<NUM_LIT:2>])<EOL>row[<NUM_LIT:2>] -= row[<NUM_LIT:0>] * shear[<NUM_LIT:1>]<EOL>shear[<NUM_LIT:2>] = numpy.dot(row[<NUM_LIT:1>], row[<NUM_LIT:2>])<EOL>row[<NUM_LIT:2>] -= row[<NUM_LIT:1>] * shear[<NUM_LIT:2>]<EOL>scale[<NUM_LIT:2>] = vector_norm(row[<NUM_LIT:2>])<EOL>row[<NUM_LIT:2>] /= scale[<NUM_LIT:2>]<EOL>shear[<NUM_LIT:1>:] /= scale[<NUM_LIT:2>]<EOL>if numpy.dot(row[<NUM_LIT:0>], numpy.cross(row[<NUM_LIT:1>], row[<NUM_LIT:2>])) < <NUM_LIT:0>:<EOL><INDENT>numpy.negative(scale, scale)<EOL>numpy.negative(row, row)<EOL><DEDENT>angles[<NUM_LIT:1>] = math.asin(-row[<NUM_LIT:0>, <NUM_LIT:2>])<EOL>if math.cos(angles[<NUM_LIT:1>]):<EOL><INDENT>angles[<NUM_LIT:0>] = math.atan2(row[<NUM_LIT:1>, <NUM_LIT:2>], row[<NUM_LIT:2>, <NUM_LIT:2>])<EOL>angles[<NUM_LIT:2>] = math.atan2(row[<NUM_LIT:0>, <NUM_LIT:1>], row[<NUM_LIT:0>, <NUM_LIT:0>])<EOL><DEDENT>else:<EOL><INDENT>angles[<NUM_LIT:0>] = math.atan2(-row[<NUM_LIT:2>, <NUM_LIT:1>], row[<NUM_LIT:1>, <NUM_LIT:1>])<EOL>angles[<NUM_LIT:2>] = <NUM_LIT:0.0><EOL><DEDENT>return scale, shear, angles, translate, perspective<EOL>", "docstring": "Return sequence of transformations from transformation matrix.\n\n    matrix : array_like\n        Non-degenerative homogeneous transformation matrix\n\n    Return tuple of:\n        scale : vector of 3 scaling factors\n        shear : list of shear factors for x-y, x-z, y-z axes\n        angles : list of Euler angles about static x, y, z axes\n        translate : translation vector along x, y, z axes\n        perspective : perspective partition of matrix\n\n    Raise ValueError if matrix is of wrong type or degenerative.\n\n    >>> T0 = translation_matrix([1, 2, 3])\n    >>> scale, shear, angles, trans, persp = decompose_matrix(T0)\n    >>> T1 = translation_matrix(trans)\n    >>> numpy.allclose(T0, T1)\n    True\n    >>> S = scale_matrix(0.123)\n    >>> scale, shear, angles, trans, persp = decompose_matrix(S)\n    >>> scale[0]\n    0.123\n    >>> R0 = euler_matrix(1, 2, 3)\n    >>> scale, shear, angles, trans, persp = decompose_matrix(R0)\n    >>> R1 = euler_matrix(*angles)\n    >>> numpy.allclose(R0, R1)\n    True", "id": "f15133:m14"}
{"signature": "@instruction<EOL><INDENT>def SETZ(cpu, dest):<DEDENT>", "body": "dest.write(Operators.ITEBV(dest.size, cpu.ZF, <NUM_LIT:1>, <NUM_LIT:0>))<EOL>", "docstring": "Sets byte if zero.\n\n:param cpu: current CPU.\n:param dest: destination operand.", "id": "f16975:c2:m99"}
{"signature": "def get_view_interval(self):", "body": "raise NotImplementedError('<STR_LIT>')<EOL>", "docstring": "return the view Interval instance for the axis this tick is ticking", "id": "f17198:c0:m17"}
{"signature": "def get_cells(self):", "body": "return self.__cells<EOL>", "docstring": "!\n        @brief Returns CLIQUE blocks that are formed during clustering process.\n        @details CLIQUE blocks can be used for visualization purposes. Each CLIQUE block contain its logical location\n                  in grid, spatial location in data space and points that belong to block.\n\n        @return (list) List of CLIQUE blocks.", "id": "f15466:c4:m4"}
{"signature": "def platform_detect():", "body": "<EOL>pi = pi_version()<EOL>if pi is not None:<EOL><INDENT>return RASPBERRY_PI<EOL><DEDENT>plat = platform.platform()<EOL>if plat.lower().find('<STR_LIT>') > -<NUM_LIT:1>:<EOL><INDENT>return BEAGLEBONE_BLACK<EOL><DEDENT>elif plat.lower().find('<STR_LIT>') > -<NUM_LIT:1>:<EOL><INDENT>return BEAGLEBONE_BLACK<EOL><DEDENT>elif plat.lower().find('<STR_LIT>') > -<NUM_LIT:1>:<EOL><INDENT>return BEAGLEBONE_BLACK<EOL><DEDENT>elif plat.lower().find('<STR_LIT>') > -<NUM_LIT:1>:<EOL><INDENT>return JETSON_NANO<EOL><DEDENT>try: <EOL><INDENT>import mraa <EOL>if mraa.getPlatformName()=='<STR_LIT>':<EOL><INDENT>return MINNOWBOARD<EOL><DEDENT><DEDENT>except ImportError:<EOL><INDENT>pass<EOL><DEDENT>return UNKNOWN<EOL>", "docstring": "Detect if running on the Raspberry Pi or Beaglebone Black and return the\n    platform type.  Will return RASPBERRY_PI, BEAGLEBONE_BLACK, or UNKNOWN.", "id": "f8000:m0"}
{"signature": "@property<EOL><INDENT>def color(self):<DEDENT>", "body": "return self.get(COLOR)<EOL>", "docstring": "The color of the row.\n\n        :return: the color of the row as specified or :obj:`None`", "id": "f535:c0:m9"}
{"signature": "@instruction<EOL><INDENT>def POP(cpu, dest):<DEDENT>", "body": "dest.write(cpu.pop(dest.size))<EOL>", "docstring": "Pops a value from the stack.\n\nLoads the value from the top of the stack to the location specified\nwith the destination operand and then increments the stack pointer.\n\n:param cpu: current CPU.\n:param dest: destination operand.", "id": "f16975:c2:m102"}
{"signature": "def lcsseq(self, src, tar):", "body": "lengths = np_zeros((len(src) + <NUM_LIT:1>, len(tar) + <NUM_LIT:1>), dtype=np_int)<EOL>for i, src_char in enumerate(src):<EOL><INDENT>for j, tar_char in enumerate(tar):<EOL><INDENT>if src_char == tar_char:<EOL><INDENT>lengths[i + <NUM_LIT:1>, j + <NUM_LIT:1>] = lengths[i, j] + <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>lengths[i + <NUM_LIT:1>, j + <NUM_LIT:1>] = max(<EOL>lengths[i + <NUM_LIT:1>, j], lengths[i, j + <NUM_LIT:1>]<EOL>)<EOL><DEDENT><DEDENT><DEDENT>result = '<STR_LIT>'<EOL>i, j = len(src), len(tar)<EOL>while i != <NUM_LIT:0> and j != <NUM_LIT:0>:<EOL><INDENT>if lengths[i, j] == lengths[i - <NUM_LIT:1>, j]:<EOL><INDENT>i -= <NUM_LIT:1><EOL><DEDENT>elif lengths[i, j] == lengths[i, j - <NUM_LIT:1>]:<EOL><INDENT>j -= <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>result = src[i - <NUM_LIT:1>] + result<EOL>i -= <NUM_LIT:1><EOL>j -= <NUM_LIT:1><EOL><DEDENT><DEDENT>return result<EOL>", "docstring": "Return the longest common subsequence of two strings.\n\n        Based on the dynamic programming algorithm from\n        http://rosettacode.org/wiki/Longest_common_subsequence\n        :cite:`rosettacode:2018b`. This is licensed GFDL 1.2.\n\n        Modifications include:\n            conversion to a numpy array in place of a list of lists\n\n        Parameters\n        ----------\n        src : str\n            Source string for comparison\n        tar : str\n            Target string for comparison\n\n        Returns\n        -------\n        str\n            The longest common subsequence\n\n        Examples\n        --------\n        >>> sseq = LCSseq()\n        >>> sseq.lcsseq('cat', 'hat')\n        'at'\n        >>> sseq.lcsseq('Niall', 'Neil')\n        'Nil'\n        >>> sseq.lcsseq('aluminum', 'Catalan')\n        'aln'\n        >>> sseq.lcsseq('ATCG', 'TAGC')\n        'AC'", "id": "f6627:c0:m0"}
{"signature": "def setup_databases(self, **kwargs):", "body": "pass<EOL>", "docstring": "Override the database creation defined in parent class", "id": "f8713:c0:m1"}
{"signature": "def __init__(self, text, segment_type=None,<EOL>is_bold=False, is_italic=False, is_strikethrough=False,<EOL>is_underline=False, link_target=None):", "body": "if segment_type is not None:<EOL><INDENT>self.type_ = segment_type<EOL><DEDENT>elif link_target is not None:<EOL><INDENT>self.type_ = hangouts_pb2.SEGMENT_TYPE_LINK<EOL><DEDENT>else:<EOL><INDENT>self.type_ = hangouts_pb2.SEGMENT_TYPE_TEXT<EOL><DEDENT>self.text = text<EOL>self.is_bold = is_bold<EOL>self.is_italic = is_italic<EOL>self.is_strikethrough = is_strikethrough<EOL>self.is_underline = is_underline<EOL>self.link_target = link_target<EOL>", "docstring": "Create a new chat message segment.", "id": "f10039:c1:m0"}
{"signature": "def jam_pack(jam, **kwargs):", "body": "if not hasattr(jam.sandbox, '<STR_LIT>'):<EOL><INDENT>jam.sandbox.muda = jams.Sandbox(history=[],<EOL>state=[],<EOL>version=dict(muda=version,<EOL>librosa=librosa.__version__,<EOL>jams=jams.__version__,<EOL>pysoundfile=psf.__version__))<EOL><DEDENT>elif not isinstance(jam.sandbox.muda, jams.Sandbox):<EOL><INDENT>jam.sandbox.muda = jams.Sandbox(**jam.sandbox.muda)<EOL><DEDENT>jam.sandbox.muda.update(**kwargs)<EOL>return jam<EOL>", "docstring": "Pack data into a jams sandbox.\n\n    If not already present, this creates a `muda` field within `jam.sandbox`,\n    along with `history`, `state`, and version arrays which are populated by\n    deformation objects.\n\n    Any additional fields can be added to the `muda` sandbox by supplying\n    keyword arguments.\n\n    Parameters\n    ----------\n    jam : jams.JAMS\n        A JAMS object\n\n    Returns\n    -------\n    jam : jams.JAMS\n        The updated JAMS object\n\n    Examples\n    --------\n    >>> jam = jams.JAMS()\n    >>> muda.jam_pack(jam, my_data=dict(foo=5, bar=None))\n    >>> jam.sandbox\n    <Sandbox: muda>\n    >>> jam.sandbox.muda\n    <Sandbox: state, version, my_data, history>\n    >>> jam.sandbox.muda.my_data\n    {'foo': 5, 'bar': None}", "id": "f5854:m0"}
{"signature": "def safe_load(stream):", "body": "return load(stream, SafeLoader)<EOL>", "docstring": "Parse the first YAML document in a stream\nand produce the corresponding Python object.\nResolve only basic YAML tags.", "id": "f8321:m6"}
{"signature": "@classmethod<EOL><INDENT>def consumeArgs(cls):<DEDENT>", "body": "return cls._processArgs()[<NUM_LIT:1>]<EOL>", "docstring": "Consumes the test arguments and returns the remaining arguments meant\n        for unittest.man", "id": "f17408:c6:m2"}
{"signature": "def _dec_to_bin(ip):", "body": "bits = []<EOL>while ip:<EOL><INDENT>bits.append(_BYTES_TO_BITS[ip & <NUM_LIT:255>])<EOL>ip >>= <NUM_LIT:8><EOL><DEDENT>bits.reverse()<EOL>return '<STR_LIT>'.join(bits) or <NUM_LIT:32>*'<STR_LIT:0>'<EOL>", "docstring": "Decimal to binary conversion.", "id": "f3601:m23"}
{"signature": "def list_products(self):", "body": "url = self.base_url<EOL>obdata = self.connection.make_get(url)<EOL>return ProductList(obdata, self)<EOL>", "docstring": "Lists all products in the system, returns ProductList you can iterate over.\n\n        Holvi API does not currently support server-side filtering so you will have to use Pythons filter() function as usual.", "id": "f6518:c5:m1"}
{"signature": "def IIR_lpf(f_pass, f_stop, Ripple_pass, Atten_stop, <EOL>fs = <NUM_LIT>, ftype = '<STR_LIT>'):", "body": "b,a = signal.iirdesign(<NUM_LIT:2>*float(f_pass)/fs, <NUM_LIT:2>*float(f_stop)/fs,<EOL>Ripple_pass, Atten_stop,<EOL>ftype = ftype, output='<STR_LIT>')<EOL>sos = signal.iirdesign(<NUM_LIT:2>*float(f_pass)/fs, <NUM_LIT:2>*float(f_stop)/fs,<EOL>Ripple_pass, Atten_stop,<EOL>ftype = ftype, output='<STR_LIT>')<EOL>tag = '<STR_LIT>' + ftype + '<STR_LIT>'<EOL>print('<STR_LIT>' % (tag,len(a)-<NUM_LIT:1>))<EOL>return b, a, sos<EOL>", "docstring": "Design an IIR lowpass filter using scipy.signal.iirdesign. \nThe filter order is determined based on \nf_pass Hz, f_stop Hz, and the desired stopband attenuation\nd_stop in dB, all relative to a sampling rate of fs Hz.\n\nParameters\n----------\nf_pass : Passband critical frequency in Hz\nf_stop : Stopband critical frequency in Hz\nRipple_pass : Filter gain in dB at f_pass\nAtten_stop : Filter attenuation in dB at f_stop\nfs : Sampling rate in Hz\nftype : Analog prototype from 'butter' 'cheby1', 'cheby2',\n        'ellip', and 'bessel'\n\nReturns\n-------\nb : ndarray of the numerator coefficients\na : ndarray of the denominator coefficients\nsos : 2D ndarray of second-order section coefficients\n\nNotes\n-----\nAdditionally a text string telling the user the filter order is\nwritten to the console, e.g., IIR cheby1 order = 8.\n\nExamples\n--------\n>>> fs = 48000\n>>> f_pass = 5000\n>>> f_stop = 8000\n>>> b_but,a_but,sos_but = IIR_lpf(f_pass,f_stop,0.5,60,fs,'butter')\n>>> b_cheb1,a_cheb1,sos_cheb1 = IIR_lpf(f_pass,f_stop,0.5,60,fs,'cheby1')\n>>> b_cheb2,a_cheb2,sos_cheb2 = IIR_lpf(f_pass,f_stop,0.5,60,fs,'cheby2')\n>>> b_elli,a_elli,sos_elli = IIR_lpf(f_pass,f_stop,0.5,60,fs,'ellip')\n\n\nMark Wickert October 2016", "id": "f14906:m0"}
{"signature": "def disjuncts(s):", "body": "return dissociate('<STR_LIT:|>', [s])<EOL>", "docstring": "Return a list of the disjuncts in the sentence s.\n    >>> disjuncts(A | B)\n    [A, B]\n    >>> disjuncts(A & B)\n    [(A & B)]", "id": "f1683:m20"}
{"signature": "def get_mass(self):", "body": "return self.size_class_masses.sum()<EOL>", "docstring": "Determine the mass of self.\n\nreturns: [kg] The mass of self.", "id": "f15827:c1:m8"}
{"signature": "def prevent_locking(self):", "body": "return False<EOL>", "docstring": "Return True, to prevent locking.\n\n        This default implementation returns ``False``, so standard processing\n        takes place: locking (and refreshing of locks) is implemented using\n        the lock manager, if one is configured.", "id": "f8596:c0:m26"}
{"signature": "def get_output(self):", "body": "if self.process.poll() is not None:<EOL><INDENT>self.close()<EOL>yield None, None<EOL><DEDENT>while not (self.stdout_queue.empty() and self.stderr_queue.empty()):<EOL><INDENT>if not self.stdout_queue.empty():<EOL><INDENT>line = self.stdout_queue.get().decode('<STR_LIT:utf-8>')<EOL>yield line, None<EOL><DEDENT>if not self.stderr_queue.empty():<EOL><INDENT>line = self.stderr_queue.get().decode('<STR_LIT:utf-8>')<EOL>yield None, line<EOL><DEDENT><DEDENT>", "docstring": ":yield: stdout_line, stderr_line, running\n\nGenerator that outputs lines captured from stdout and stderr\n\nThese can be consumed to output on a widget in an IDE", "id": "f11493:c2:m6"}
{"signature": "@property<EOL><INDENT>def pipelines(self):<DEDENT>", "body": "if not self.response:<EOL><INDENT>return set()<EOL><DEDENT>elif self._pipelines is None and self.response:<EOL><INDENT>self._pipelines = set()<EOL>for group in self.response.payload:<EOL><INDENT>for pipeline in group['<STR_LIT>']:<EOL><INDENT>self._pipelines.add(pipeline['<STR_LIT:name>'])<EOL><DEDENT><DEDENT><DEDENT>return self._pipelines<EOL>", "docstring": "Returns a set of all pipelines from the last response\n\n        Returns:\n          set: Response success: all the pipelines available in the response\n               Response failure: an empty set", "id": "f1622:c0:m3"}
{"signature": "def __eq__(self, other):", "body": "return self._opcode == other._opcode andself._name == other._name andself._operand == other._operand andself._operand_size == other._operand_size andself._pops == other._pops andself._pushes == other._pushes andself._fee == other._fee andself._pc == other._pc andself._description == other._description<EOL>", "docstring": "Instructions are equal if all features match", "id": "f13733:c4:m1"}
{"signature": "@abstractmethod<EOL><INDENT>def getPatternMachine(self):<DEDENT>", "body": "", "docstring": "Implement this method to provide the pattern machine.", "id": "f17638:c0:m1"}
{"signature": "@property<EOL><INDENT>def layers(self):<DEDENT>", "body": "layers = [self._layer_def(style) for style in self.styles]<EOL>return layers<EOL>", "docstring": "Renders the list of layers to add to the map.\n\n            Returns:\n                layers (list): list of layer entries suitable for use in mapbox-gl 'map.addLayer()' call", "id": "f7107:c0:m4"}
{"signature": "def otsu_threshold(data, bins=<NUM_LIT:255>):", "body": "h0, x0 = np.histogram(data.ravel(), bins=bins)<EOL>h = h0.astype('<STR_LIT:float>') / h0.sum()  <EOL>x = <NUM_LIT:0.5>*(x0[<NUM_LIT:1>:] + x0[:-<NUM_LIT:1>])  <EOL>wk = np.array([h[:i+<NUM_LIT:1>].sum() for i in range(h.size)])  <EOL>mk = np.array([sum(x[:i+<NUM_LIT:1>]*h[:i+<NUM_LIT:1>]) for i in range(h.size)])  <EOL>mt = mk[-<NUM_LIT:1>]  <EOL>sb = (mt*wk - mk)**<NUM_LIT:2> / (wk*(<NUM_LIT:1>-wk) + <NUM_LIT>)  <EOL>ind = sb.argmax()<EOL>return <NUM_LIT:0.5>*(x0[ind] + x0[ind+<NUM_LIT:1>])<EOL>", "docstring": "Otsu threshold on data.\n\nOtsu thresholding [1]_is a method for selecting an intensity value\nfor thresholding an image into foreground and background. The sel-\nected intensity threshold maximizes the inter-class variance.\n\nParameters\n----------\n    data : numpy.ndarray\n        The data to threshold\n    bins : Int or numpy.ndarray, optional\n        Bin edges, as passed to numpy.histogram\n\nReturns\n-------\n    numpy.float\n        The value of the threshold which maximizes the inter-class\n        variance.\n\nNotes\n-----\n    This could be generalized to more than 2 classes.\nReferences\n----------\n    ..[1] N. Otsu, \"A Threshold Selection Method from Gray-level\n        Histograms,\" IEEE Trans. Syst., Man, Cybern., Syst., 9, 1,\n        62-66 (1979)", "id": "f5748:m12"}
{"signature": "def add_prefix(multicodec, bytes_):", "body": "prefix = get_prefix(multicodec)<EOL>return b'<STR_LIT>'.join([prefix, bytes_])<EOL>", "docstring": "Adds multicodec prefix to the given bytes input\n\n:param str multicodec: multicodec to use for prefixing\n:param bytes bytes_: data to prefix\n:return: prefixed byte data\n:rtype: bytes", "id": "f15407:m2"}
{"signature": "def _qsub_collate_and_launch(self, output_dir, error_dir, job_names):", "body": "job_name = \"<STR_LIT>\" % (self.batch_name,<EOL>self.job_timestamp,<EOL>self.collate_count)<EOL>overrides = [(\"<STR_LIT>\",error_dir), ('<STR_LIT>',job_name), (\"<STR_LIT>\",output_dir),<EOL>('<STR_LIT>','<STR_LIT:U+002C>'.join(job_names))]<EOL>resume_cmds =[\"<STR_LIT>\",<EOL>(\"<STR_LIT>\"<EOL>% self.root_directory),<EOL>\"<STR_LIT>\",<EOL>\"<STR_LIT>\"]<EOL>cmd_args = [self.command.executable,<EOL>'<STR_LIT:-c>', '<STR_LIT:;>'.join(resume_cmds)]<EOL>popen_args = self._qsub_args(overrides, cmd_args)<EOL>p = subprocess.Popen(popen_args, stdout=subprocess.PIPE)<EOL>(stdout, stderr) = p.communicate()<EOL>self.debug(stdout)<EOL>if p.poll() != <NUM_LIT:0>:<EOL><INDENT>raise EnvironmentError(\"<STR_LIT>\" % p.poll())<EOL><DEDENT>self.collate_count += <NUM_LIT:1><EOL>self.message(\"<STR_LIT>\")<EOL>return job_name<EOL>", "docstring": "The method that actually runs qsub to invoke the python\nprocess with the necessary commands to trigger the next\ncollation step and next block of jobs.", "id": "f12319:c4:m4"}
{"signature": "def set_alpha(self, alpha):", "body": "self._alpha = alpha<EOL>", "docstring": "Set the alpha value used for blending - not supported on\nall backends", "id": "f17207:c1:m14"}
{"signature": "@property<EOL><INDENT>def last_human_transaction(self):<DEDENT>", "body": "for tx in reversed(self.transactions):<EOL><INDENT>if tx.depth == <NUM_LIT:0>:<EOL><INDENT>return tx<EOL><DEDENT><DEDENT>return None<EOL>", "docstring": "Last completed human transaction", "id": "f17019:c17:m15"}
{"signature": "def _find_tails(self, mag, rounding=True, half=<NUM_LIT:5>, full=<NUM_LIT:10>, flag=<NUM_LIT:50>):", "body": "<EOL>if rounding:<EOL><INDENT>mag = half * (mag / half + <NUM_LIT:0.5>).astype(np.int)<EOL><DEDENT>num_flags = np.floor(mag / flag).astype(np.int)<EOL>mag = np.mod(mag, flag)<EOL>num_barb = np.floor(mag / full).astype(np.int)<EOL>mag = np.mod(mag, full)<EOL>half_flag = mag >= half<EOL>empty_flag = ~(half_flag | (num_flags > <NUM_LIT:0>) | (num_barb > <NUM_LIT:0>))<EOL>return num_flags, num_barb, half_flag, empty_flag<EOL>", "docstring": "Find how many of each of the tail pieces is necessary.  Flag\nspecifies the increment for a flag, barb for a full barb, and half for\nhalf a barb. Mag should be the magnitude of a vector (ie. >= 0).\n\nThis returns a tuple of:\n\n    (*number of flags*, *number of barbs*, *half_flag*, *empty_flag*)\n\n*half_flag* is a boolean whether half of a barb is needed,\nsince there should only ever be one half on a given\nbarb. *empty_flag* flag is an array of flags to easily tell if\na barb is empty (too low to plot any barbs/flags.", "id": "f17167:c2:m1"}
{"signature": "@classmethod<EOL><INDENT>def circular(cls, shape, pixel_scale, radius_arcsec, centre=(<NUM_LIT:0.>, <NUM_LIT:0.>), invert=False):<DEDENT>", "body": "mask = mask_util.mask_circular_from_shape_pixel_scale_and_radius(shape, pixel_scale, radius_arcsec,<EOL>centre)<EOL>if invert: mask = np.invert(mask)<EOL>return cls(array=mask.astype('<STR_LIT:bool>'), pixel_scale=pixel_scale)<EOL>", "docstring": "Setup a mask where unmasked pixels are within a circle of an input arc second radius and centre.\n\n        Parameters\n        ----------\n        shape: (int, int)\n            The (y,x) shape of the mask in units of pixels.\n        pixel_scale: float\n            The arc-second to pixel conversion factor of each pixel.\n        radius_arcsec : float\n            The radius (in arc seconds) of the circle within which pixels unmasked.\n        centre: (float, float)\n            The centre of the circle used to mask pixels.", "id": "f5991:c0:m4"}
{"signature": "def get_cluster_encoding(self):", "body": "return type_encoding.CLUSTER_INDEX_LIST_SEPARATION<EOL>", "docstring": "!\n        @brief Returns clustering result representation type that indicate how clusters are encoded.\n\n        @return (type_encoding) Clustering result representation.\n\n        @see get_clusters()", "id": "f15546:c0:m3"}
{"signature": "def resolve_conflicts(inputs, outputs):", "body": "data = {}<EOL>for inp, out in zip(inputs, outputs):<EOL><INDENT>tup = tuple(inp)<EOL>if tup in data:<EOL><INDENT>data[tup].append(out)<EOL><DEDENT>else:<EOL><INDENT>data[tup] = [out]<EOL><DEDENT><DEDENT>inputs, outputs = [], []<EOL>for inp, outs in data.items():<EOL><INDENT>inputs.append(list(inp))<EOL>combined = [<NUM_LIT:0>] * len(outs[<NUM_LIT:0>])<EOL>for i in range(len(combined)):<EOL><INDENT>combined[i] = max(j[i] for j in outs)<EOL><DEDENT>outputs.append(combined)<EOL><DEDENT>return inputs, outputs<EOL>", "docstring": "Checks for duplicate inputs and if there are any,\nremove one and set the output to the max of the two outputs\nArgs:\n    inputs (list<list<float>>): Array of input vectors\n    outputs (list<list<float>>): Array of output vectors\nReturns:\n    tuple<inputs, outputs>: The modified inputs and outputs", "id": "f9728:m4"}
{"signature": "def calculate_concordance_probability_by_annotation(graph, annotation, key, cutoff=None, permutations=None,<EOL>percentage=None,<EOL>use_ambiguous=False):", "body": "result = [<EOL>(value, calculate_concordance_probability(<EOL>subgraph,<EOL>key,<EOL>cutoff=cutoff,<EOL>permutations=permutations,<EOL>percentage=percentage,<EOL>use_ambiguous=use_ambiguous,<EOL>))<EOL>for value, subgraph in get_subgraphs_by_annotation(graph, annotation).items()<EOL>]<EOL>return dict(result)<EOL>", "docstring": "Returns the results of concordance analysis on each subgraph, stratified by the given annotation.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param str annotation: The annotation to group by.\n    :param str key: The node data dictionary key storing the logFC\n    :param float cutoff: The optional logFC cutoff for significance\n    :param int permutations: The number of random permutations to test. Defaults to 500\n    :param float percentage: The percentage of the graph's edges to maintain. Defaults to 0.9\n    :param bool use_ambiguous: Compare to ambiguous edges as well\n    :rtype: dict[str,tuple]", "id": "f9416:m7"}
{"signature": "def create_branch(self, name):", "body": "self._git.branch(name)<EOL>", "docstring": "Creates a branch", "id": "f8774:c0:m14"}
{"signature": "def run(command, data=None, timeout=None, kill_timeout=None, env=None, cwd=None):", "body": "command = expand_args(command)<EOL>history = []<EOL>for c in command:<EOL><INDENT>if len(history):<EOL><INDENT>data = history[-<NUM_LIT:1>].std_out[<NUM_LIT:0>:<NUM_LIT:10>*<NUM_LIT>]<EOL><DEDENT>cmd = Command(c)<EOL>try:<EOL><INDENT>out, err = cmd.run(data, timeout, kill_timeout, env, cwd)<EOL>status_code = cmd.returncode<EOL><DEDENT>except OSError as e:<EOL><INDENT>out, err = '<STR_LIT>', u\"<STR_LIT:\\n>\".join([e.strerror, traceback.format_exc()])<EOL>status_code = <NUM_LIT><EOL><DEDENT>r = Response(process=cmd)<EOL>r.command = c<EOL>r.std_out = out<EOL>r.std_err = err<EOL>r.status_code = status_code<EOL>history.append(r)<EOL><DEDENT>r = history.pop()<EOL>r.history = history<EOL>return r<EOL>", "docstring": "Executes a given commmand and returns Response.\n\n    Blocks until process is complete, or timeout is reached.", "id": "f4246:m4"}
{"signature": "def all_edges_consistent(graph):", "body": "return all(<EOL>is_edge_consistent(graph, u, v)<EOL>for u, v in graph.edges()<EOL>)<EOL>", "docstring": "Return if all edges are consistent in a graph. Wraps :func:`pybel_tools.utils.is_edge_consistent`.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :return: Are all edges consistent\n    :rtype: bool", "id": "f9372:m1"}
{"signature": "def plot_monthly_returns_dist(returns, ax=None, **kwargs):", "body": "if ax is None:<EOL><INDENT>ax = plt.gca()<EOL><DEDENT>x_axis_formatter = FuncFormatter(utils.percentage)<EOL>ax.xaxis.set_major_formatter(FuncFormatter(x_axis_formatter))<EOL>ax.tick_params(axis='<STR_LIT:x>', which='<STR_LIT>')<EOL>monthly_ret_table = ep.aggregate_returns(returns, '<STR_LIT>')<EOL>ax.hist(<EOL><NUM_LIT:100> * monthly_ret_table,<EOL>color='<STR_LIT>',<EOL>alpha=<NUM_LIT>,<EOL>bins=<NUM_LIT:20>,<EOL>**kwargs)<EOL>ax.axvline(<EOL><NUM_LIT:100> * monthly_ret_table.mean(),<EOL>color='<STR_LIT>',<EOL>linestyle='<STR_LIT>',<EOL>lw=<NUM_LIT:4>,<EOL>alpha=<NUM_LIT:1.0>)<EOL>ax.axvline(<NUM_LIT:0.0>, color='<STR_LIT>', linestyle='<STR_LIT:->', lw=<NUM_LIT:3>, alpha=<NUM_LIT>)<EOL>ax.legend(['<STR_LIT>'], frameon=True, framealpha=<NUM_LIT:0.5>)<EOL>ax.set_ylabel('<STR_LIT>')<EOL>ax.set_xlabel('<STR_LIT>')<EOL>ax.set_title(\"<STR_LIT>\")<EOL>return ax<EOL>", "docstring": "Plots a distribution of monthly returns.\n\nParameters\n----------\nreturns : pd.Series\n    Daily returns of the strategy, noncumulative.\n     - See full explanation in tears.create_full_tear_sheet.\nax : matplotlib.Axes, optional\n    Axes upon which to plot.\n**kwargs, optional\n    Passed to plotting function.\n\nReturns\n-------\nax : matplotlib.Axes\n    The axes that were plotted on.", "id": "f12210:m5"}
{"signature": "def create_parser(subparsers):", "body": "return cli_helper.create_parser(subparsers, '<STR_LIT>', '<STR_LIT>')<EOL>", "docstring": ":param subparsers:\n:return:", "id": "f7381:m0"}
{"signature": "def load_configs(self):", "body": "self.statemgr_config.set_state_locations(self.configs[STATEMGRS_KEY])<EOL>if EXTRA_LINKS_KEY in self.configs:<EOL><INDENT>for extra_link in self.configs[EXTRA_LINKS_KEY]:<EOL><INDENT>self.extra_links.append(self.validate_extra_link(extra_link))<EOL><DEDENT><DEDENT>", "docstring": "load config files", "id": "f7323:c0:m1"}
{"signature": "def attribute_as_str(path, name):", "body": "matches = re.findall('<STR_LIT>' + name.upper() + '<STR_LIT>', path)<EOL>if matches:<EOL><INDENT>return matches[-<NUM_LIT:1>]<EOL><DEDENT>else:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Returns the two numbers found behind --[A-Z] in path. If several matches\n    are found, the last one is returned.\n\n    Parameters\n    ----------\n    path : string\n        String with path of file/folder to get attribute from.\n    name : string\n        Name of attribute to get. Should be A-Z or a-z (implicit converted to\n        uppercase).\n\n    Returns\n    -------\n    string\n        Returns two digit number found in path behind --name.", "id": "f4440:m6"}
{"signature": "def sys_fsync(self, fd):", "body": "ret = <NUM_LIT:0><EOL>try:<EOL><INDENT>self.files[fd].sync()<EOL><DEDENT>except IndexError:<EOL><INDENT>ret = -errno.EBADF<EOL><DEDENT>except FdError:<EOL><INDENT>ret = -errno.EINVAL<EOL><DEDENT>return ret<EOL>", "docstring": "Synchronize a file's in-core state with that on disk.", "id": "f17024:c9:m42"}
{"signature": "def set_directory(path=None):", "body": "old_path = get_directory()<EOL>terminate_server()<EOL>cache.clear()<EOL>if path:<EOL><INDENT>cache['<STR_LIT>'] = path<EOL>try:<EOL><INDENT>get_jar_info()<EOL><DEDENT>except Error:<EOL><INDENT>cache['<STR_LIT>'] = old_path<EOL>raise<EOL><DEDENT><DEDENT>", "docstring": "Set LanguageTool directory.", "id": "f4271:m9"}
{"signature": "def stroke(self, *args):", "body": "if args is not None:<EOL><INDENT>self._canvas.strokecolor = self.color(*args)<EOL><DEDENT>return self._canvas.strokecolor<EOL>", "docstring": "Set a stroke color, applying it to new paths.\n\n        :param args: color in supported format", "id": "f11505:c0:m41"}
{"signature": "def speriodogram(x, NFFT=None, detrend=True, sampling=<NUM_LIT:1.>,<EOL>scale_by_freq=True, window='<STR_LIT>', axis=<NUM_LIT:0>):", "body": "x = np.array(x)<EOL>if x.ndim == <NUM_LIT:1>:<EOL><INDENT>axis = <NUM_LIT:0><EOL>r = x.shape[<NUM_LIT:0>]<EOL>w = Window(r, window)   <EOL>w = w.data<EOL><DEDENT>elif x.ndim == <NUM_LIT:2>:<EOL><INDENT>logging.debug('<STR_LIT>')<EOL>[r, c] = x.shape<EOL>w = np.array([Window(r, window).data for this in range(c)]).reshape(r,c) <EOL><DEDENT>if NFFT is None:<EOL><INDENT>NFFT = len(x)<EOL><DEDENT>isreal = np.isrealobj(x)<EOL>if detrend == True:<EOL><INDENT>m = np.mean(x, axis=axis)<EOL><DEDENT>else:<EOL><INDENT>m = <NUM_LIT:0><EOL><DEDENT>if isreal == True:<EOL><INDENT>if x.ndim == <NUM_LIT:2>:<EOL><INDENT>res =  (abs (rfft (x*w - m, NFFT, axis=<NUM_LIT:0>))) ** <NUM_LIT> / r<EOL><DEDENT>else:<EOL><INDENT>res =  (abs (rfft (x*w - m, NFFT, axis=-<NUM_LIT:1>))) ** <NUM_LIT> / r<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if x.ndim == <NUM_LIT:2>:<EOL><INDENT>res =  (abs (fft (x*w - m, NFFT, axis=<NUM_LIT:0>))) ** <NUM_LIT> / r<EOL><DEDENT>else:<EOL><INDENT>res =  (abs (fft (x*w - m, NFFT, axis=-<NUM_LIT:1>))) ** <NUM_LIT> / r<EOL><DEDENT><DEDENT>if scale_by_freq is True:<EOL><INDENT>df = sampling / float(NFFT)<EOL>res*= <NUM_LIT:2> * np.pi / df<EOL><DEDENT>if x.ndim == <NUM_LIT:1>:<EOL><INDENT>return res.transpose()<EOL><DEDENT>else:<EOL><INDENT>return res<EOL><DEDENT>", "docstring": "Simple periodogram, but matrices accepted.\n\n    :param x: an array or matrix of data samples.\n    :param NFFT: length of the data before FFT is computed (zero padding)\n    :param bool detrend: detrend the data before co,puteing the FFT\n    :param float sampling: sampling frequency of the input :attr:`data`.\n\n    :param scale_by_freq:\n    :param str window:\n\n    :return: 2-sided PSD if complex data, 1-sided if real.\n\n    if a matrix is provided (using numpy.matrix), then a periodogram\n    is computed for each row. The returned matrix has the same shape as the input\n    matrix.\n\n    The mean of the input data is also removed from the data before computing\n    the psd.\n\n    .. plot::\n        :width: 80%\n        :include-source:\n\n        from pylab import grid, semilogy\n        from spectrum import data_cosine, speriodogram\n        data = data_cosine(N=1024, A=0.1, sampling=1024, freq=200)\n        semilogy(speriodogram(data, detrend=False, sampling=1024), marker='o')\n        grid(True)\n\n\n    .. plot::\n        :width: 80%\n        :include-source:\n\n        import numpy\n        from spectrum import speriodogram, data_cosine\n        from pylab import figure, semilogy, figure ,imshow\n        # create N data sets and make the frequency dependent on the time\n        N = 100\n        m = numpy.concatenate([data_cosine(N=1024, A=0.1, sampling=1024, freq=x) \n            for x in range(1, N)]);\n        m.resize(N, 1024)\n        res = speriodogram(m)\n        figure(1)\n        semilogy(res)\n        figure(2)\n        imshow(res.transpose(), aspect='auto')\n\n    .. todo:: a proper spectrogram class/function that takes care of normalisation", "id": "f10929:m0"}
{"signature": "def append_records(self, records):", "body": "for obs in records:<EOL><INDENT>if isinstance(obs, Observation):<EOL><INDENT>self.append(**obs._asdict())<EOL><DEDENT>else:<EOL><INDENT>self.append(**obs)<EOL><DEDENT><DEDENT>", "docstring": "Add observations from row-major storage.\n\n        This is primarily useful for deserializing sparsely packed data.\n\n        Parameters\n        ----------\n        records : iterable of dicts or Observations\n            Each element of `records` corresponds to one observation.", "id": "f11239:c2:m3"}
{"signature": "@api_v1.route(\"<STR_LIT>\", methods=['<STR_LIT:POST>'])<EOL>def create_user():", "body": "return jsonify(success=True)<EOL>", "docstring": "Since the path matches the regular expression r'/api/*', this resource\nautomatically has CORS headers set.\n\nBrowsers will first make a preflight request to verify that the resource\nallows cross-origin POSTs with a JSON Content-Type, which can be simulated\nas:\n$ curl --include -X OPTIONS http://127.0.0.1:5000/api/v1/users/create \\\n    --header Access-Control-Request-Method:POST \\\n    --header Access-Control-Request-Headers:Content-Type \\\n    --header Origin:www.examplesite.com\n>> HTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nAllow: POST, OPTIONS\nAccess-Control-Allow-Origin: *\nAccess-Control-Allow-Headers: Content-Type\nAccess-Control-Allow-Methods: DELETE, GET, HEAD, OPTIONS, PATCH, POST, PUT\nContent-Length: 0\nServer: Werkzeug/0.9.6 Python/2.7.9\nDate: Sat, 31 Jan 2015 22:25:22 GMT\n\n\n$ curl --include -X POST http://127.0.0.1:5000/api/v1/users/create \\\n    --header Content-Type:application/json \\\n    --header Origin:www.examplesite.com\n\n\n>> HTTP/1.0 200 OK\nContent-Type: application/json\nContent-Length: 21\nAccess-Control-Allow-Origin: *\nServer: Werkzeug/0.9.6 Python/2.7.9\nDate: Sat, 31 Jan 2015 22:25:04 GMT\n\n{\n  \"success\": true\n}", "id": "f13108:m1"}
{"signature": "def matrix(self):", "body": "return quaternion_matrix(self._qnow)<EOL>", "docstring": "Return homogeneous rotation matrix.", "id": "f15133:c0:m8"}
{"signature": "def delete_execution_state(self, topologyName):", "body": "pass<EOL>", "docstring": "Delete path is currently not supported in file based state manager.", "id": "f7422:c0:m14"}
{"signature": "def mcmc(transform, loglikelihood, parameter_names, nsteps=<NUM_LIT>, nburn=<NUM_LIT>, <EOL>stdevs=<NUM_LIT:0.1>, start = <NUM_LIT:0.5>, **problem):", "body": "if '<STR_LIT>' in problem:<EOL><INDENT>numpy.random.seed(problem['<STR_LIT>'])<EOL><DEDENT>n_params = len(parameter_names)<EOL>def like(cube):<EOL><INDENT>cube = numpy.array(cube)<EOL>if (cube <= <NUM_LIT>).any() or (cube >= <NUM_LIT:1>-<NUM_LIT>).any():<EOL><INDENT>return -<NUM_LIT><EOL><DEDENT>params = transform(cube)<EOL>return loglikelihood(params)<EOL><DEDENT>start = start + numpy.zeros(n_params)<EOL>stdevs = stdevs + numpy.zeros(n_params)<EOL>def compute_stepwidths(chain):<EOL><INDENT>return numpy.std(chain, axis=<NUM_LIT:0>) / <NUM_LIT:3><EOL><DEDENT>import matplotlib.pyplot as plt<EOL>plt.figure(figsize=(<NUM_LIT:7>, <NUM_LIT:7>))<EOL>steps = numpy.array([<NUM_LIT:0.1>]*(n_params))<EOL>print('<STR_LIT>')<EOL>chain, prob, _, steps_ = mcmc_advance(start, steps, like, nsteps=nburn / <NUM_LIT:2>, adapt=True)<EOL>steps = compute_stepwidths(chain)<EOL>print('<STR_LIT>')<EOL>chain, prob, _, steps_ = mcmc_advance(chain[-<NUM_LIT:1>], steps, like, nsteps=nburn / <NUM_LIT:2>, adapt=True)<EOL>steps = compute_stepwidths(chain)<EOL>print('<STR_LIT>')<EOL>chain, prob, _, steps_ = mcmc_advance(chain[-<NUM_LIT:1>], steps, like, nsteps=nsteps)<EOL>chain = numpy.array(chain)<EOL>i = numpy.argmax(prob)<EOL>final = chain[-<NUM_LIT:1>]<EOL>print('<STR_LIT>')<EOL>chain = numpy.array([transform(params) for params in chain])<EOL>return dict(start=chain[-<NUM_LIT:1>], maximum=chain[i], seeds=[final, chain[i]], chain=chain, method='<STR_LIT>')<EOL>", "docstring": "**Metropolis Hastings MCMC**\n\nwith automatic step width adaption. \nBurnin period is also used to guess steps.\n\n:param nburn: number of burnin steps\n:param stdevs: step widths to start with", "id": "f14125:m1"}
{"signature": "def all(self, get_all=False, **queryparams):", "body": "self.folder_id = None<EOL>if get_all:<EOL><INDENT>return self._iterate(url=self._build_path(), **queryparams)<EOL><DEDENT>else:<EOL><INDENT>return self._mc_client._get(url=self._build_path(), **queryparams)<EOL><DEDENT>", "docstring": "Get all folders used to organize templates.\n\n:param get_all: Should the query get all results\n:type get_all: :py:class:`bool`\n:param queryparams: The query string parameters\nqueryparams['fields'] = []\nqueryparams['exclude_fields'] = []\nqueryparams['count'] = integer\nqueryparams['offset'] = integer", "id": "f248:c0:m2"}
{"signature": "def draw_boxes_and_labels_to_image(<EOL>image, classes, coords, scores, classes_list, is_center=True, is_rescale=True, save_name=None<EOL>):", "body": "if len(coords) != len(classes):<EOL><INDENT>raise AssertionError(\"<STR_LIT>\")<EOL><DEDENT>if len(scores) > <NUM_LIT:0> and len(scores) != len(classes):<EOL><INDENT>raise AssertionError(\"<STR_LIT>\")<EOL><DEDENT>image = image.copy()<EOL>imh, imw = image.shape[<NUM_LIT:0>:<NUM_LIT:2>]<EOL>thick = int((imh + imw) // <NUM_LIT>)<EOL>for i, _v in enumerate(coords):<EOL><INDENT>if is_center:<EOL><INDENT>x, y, x2, y2 = tl.prepro.obj_box_coord_centroid_to_upleft_butright(coords[i])<EOL><DEDENT>else:<EOL><INDENT>x, y, x2, y2 = coords[i]<EOL><DEDENT>if is_rescale:  <EOL><INDENT>x, y, x2, y2 = tl.prepro.obj_box_coord_scale_to_pixelunit([x, y, x2, y2], (imh, imw))<EOL><DEDENT>cv2.rectangle(<EOL>image,<EOL>(int(x), int(y)),<EOL>(int(x2), int(y2)),  <EOL>[<NUM_LIT:0>, <NUM_LIT:255>, <NUM_LIT:0>],<EOL>thick<EOL>)<EOL>cv2.putText(<EOL>image,<EOL>classes_list[classes[i]] + ((\"<STR_LIT>\" % (scores[i])) if (len(scores) != <NUM_LIT:0>) else \"<STR_LIT:U+0020>\"),<EOL>(int(x), int(y)),  <EOL><NUM_LIT:0>,<EOL><NUM_LIT> * imh,  <EOL>[<NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT>],  <EOL>int(thick / <NUM_LIT:2>) + <NUM_LIT:1><EOL>)  <EOL><DEDENT>if save_name is not None:<EOL><INDENT>save_image(image, save_name)<EOL><DEDENT>return image<EOL>", "docstring": "Draw bboxes and class labels on image. Return or save the image with bboxes, example in the docs of ``tl.prepro``.\n\n    Parameters\n    -----------\n    image : numpy.array\n        The RGB image [height, width, channel].\n    classes : list of int\n        A list of class ID (int).\n    coords : list of int\n        A list of list for coordinates.\n            - Should be [x, y, x2, y2] (up-left and botton-right format)\n            - If [x_center, y_center, w, h] (set is_center to True).\n    scores : list of float\n        A list of score (float). (Optional)\n    classes_list : list of str\n        for converting ID to string on image.\n    is_center : boolean\n        Whether the coordinates is [x_center, y_center, w, h]\n            - If coordinates are [x_center, y_center, w, h], set it to True for converting it to [x, y, x2, y2] (up-left and botton-right) internally.\n            - If coordinates are [x1, x2, y1, y2], set it to False.\n    is_rescale : boolean\n        Whether to rescale the coordinates from pixel-unit format to ratio format.\n            - If True, the input coordinates are the portion of width and high, this API will scale the coordinates to pixel unit internally.\n            - If False, feed the coordinates with pixel unit format.\n    save_name : None or str\n        The name of image file (i.e. image.png), if None, not to save image.\n\n    Returns\n    -------\n    numpy.array\n        The saved image.\n\n    References\n    -----------\n    - OpenCV rectangle and putText.\n    - `scikit-image <http://scikit-image.org/docs/dev/api/skimage.draw.html#skimage.draw.rectangle>`__.", "id": "f11206:m4"}
{"signature": "def create_blueprint(endpoints):", "body": "blueprint = Blueprint(<EOL>'<STR_LIT>',<EOL>__name__,<EOL>url_prefix='<STR_LIT>',<EOL>)<EOL>create_error_handlers(blueprint)<EOL>for endpoint, options in (endpoints or {}).items():<EOL><INDENT>options = deepcopy(options)<EOL>if '<STR_LIT>' in options:<EOL><INDENT>files_serializers = options.get('<STR_LIT>')<EOL>files_serializers = {mime: obj_or_import_string(func)<EOL>for mime, func in files_serializers.items()}<EOL>del options['<STR_LIT>']<EOL><DEDENT>else:<EOL><INDENT>files_serializers = {}<EOL><DEDENT>if '<STR_LIT>' in options:<EOL><INDENT>serializers = options.get('<STR_LIT>')<EOL>serializers = {mime: obj_or_import_string(func)<EOL>for mime, func in serializers.items()}<EOL><DEDENT>else:<EOL><INDENT>serializers = {}<EOL><DEDENT>file_list_route = options.pop(<EOL>'<STR_LIT>',<EOL>'<STR_LIT>'.format(options['<STR_LIT>'])<EOL>)<EOL>file_item_route = options.pop(<EOL>'<STR_LIT>',<EOL>'<STR_LIT>'.format(options['<STR_LIT>'])<EOL>)<EOL>options.setdefault('<STR_LIT>', DepositSearch)<EOL>search_class = obj_or_import_string(options['<STR_LIT>'])<EOL>options.setdefault('<STR_LIT>', Deposit)<EOL>record_class = obj_or_import_string(options['<STR_LIT>'])<EOL>options.setdefault('<STR_LIT>', None)<EOL>for rule in records_rest_url_rules(endpoint, **options):<EOL><INDENT>blueprint.add_url_rule(**rule)<EOL><DEDENT>search_class_kwargs = {}<EOL>if options.get('<STR_LIT>'):<EOL><INDENT>search_class_kwargs['<STR_LIT:index>'] = options['<STR_LIT>']<EOL><DEDENT>if options.get('<STR_LIT>'):<EOL><INDENT>search_class_kwargs['<STR_LIT>'] = options['<STR_LIT>']<EOL><DEDENT>ctx = dict(<EOL>read_permission_factory=obj_or_import_string(<EOL>options.get('<STR_LIT>')<EOL>),<EOL>create_permission_factory=obj_or_import_string(<EOL>options.get('<STR_LIT>')<EOL>),<EOL>update_permission_factory=obj_or_import_string(<EOL>options.get('<STR_LIT>')<EOL>),<EOL>delete_permission_factory=obj_or_import_string(<EOL>options.get('<STR_LIT>')<EOL>),<EOL>record_class=record_class,<EOL>search_class=partial(search_class, **search_class_kwargs),<EOL>default_media_type=options.get('<STR_LIT>'),<EOL>)<EOL>deposit_actions = DepositActionResource.as_view(<EOL>DepositActionResource.view_name.format(endpoint),<EOL>serializers=serializers,<EOL>pid_type=options['<STR_LIT>'],<EOL>ctx=ctx,<EOL>)<EOL>blueprint.add_url_rule(<EOL>'<STR_LIT>'.format(<EOL>options['<STR_LIT>'],<EOL>'<STR_LIT:U+002C>'.join(extract_actions_from_class(record_class)),<EOL>),<EOL>view_func=deposit_actions,<EOL>methods=['<STR_LIT:POST>'],<EOL>)<EOL>deposit_files = DepositFilesResource.as_view(<EOL>DepositFilesResource.view_name.format(endpoint),<EOL>serializers=files_serializers,<EOL>pid_type=options['<STR_LIT>'],<EOL>ctx=ctx,<EOL>)<EOL>blueprint.add_url_rule(<EOL>file_list_route,<EOL>view_func=deposit_files,<EOL>methods=['<STR_LIT:GET>', '<STR_LIT:POST>', '<STR_LIT>'],<EOL>)<EOL>deposit_file = DepositFileResource.as_view(<EOL>DepositFileResource.view_name.format(endpoint),<EOL>serializers=files_serializers,<EOL>pid_type=options['<STR_LIT>'],<EOL>ctx=ctx,<EOL>)<EOL>blueprint.add_url_rule(<EOL>file_item_route,<EOL>view_func=deposit_file,<EOL>methods=['<STR_LIT:GET>', '<STR_LIT>', '<STR_LIT>'],<EOL>)<EOL><DEDENT>return blueprint<EOL>", "docstring": "Create Invenio-Deposit-REST blueprint.\n\n    See: :data:`invenio_deposit.config.DEPOSIT_REST_ENDPOINTS`.\n\n    :param endpoints: List of endpoints configuration.\n    :returns: The configured blueprint.", "id": "f9217:m1"}
{"signature": "@property<EOL><INDENT>def reset_in(self):<DEDENT>", "body": "return max(self.reset - time(), <NUM_LIT:0>)<EOL>", "docstring": "Time in seconds until the limit will be reset\n\nReturns\n-------\nint\n    Time in seconds until the limit will be reset", "id": "f4747:c10:m1"}
{"signature": "def atari_arg_parser():", "body": "print('<STR_LIT>')<EOL>return common_arg_parser()<EOL>", "docstring": "Create an argparse.ArgumentParser for run_atari.py.", "id": "f1342:m5"}
{"signature": "def create_dataset(n_rows=<NUM_LIT:1000>, extras=False, has_none=True):", "body": "random.seed(<NUM_LIT>)<EOL>ds = [[<EOL>random.random(),                                                                        <EOL>random.choice([float('<STR_LIT>'), float('<STR_LIT>'), float('<STR_LIT>'), -<NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT:1>, -<NUM_LIT:1>, math.pi]),      <EOL>row,                                                                                    <EOL>str(row),                                                                               <EOL>random.choice(['<STR_LIT:A>', '<STR_LIT:B>']) if extras else '<STR_LIT:A>',                                           <EOL>random.choice(['<STR_LIT:A>', '<STR_LIT>', '<STR_LIT>']),                                                   <EOL>random.choice(['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>', None]) if has_none else random.choice(['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>']),   <EOL>random.choice(['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>', '<STR_LIT:D>']) if extras else random.choice(['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>']),      <EOL>random.choice([<NUM_LIT:12>, <NUM_LIT>, -<NUM_LIT:32>]),                                                           <EOL>random.choice(['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>']),                                                         <EOL>random.choice(['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>', np.nan])                                                  <EOL>] for row in range(n_rows)]<EOL>df = pd.DataFrame(ds, columns=['<STR_LIT:float>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT:none>', '<STR_LIT>', <NUM_LIT>, '<STR_LIT>', '<STR_LIT>'])<EOL>df['<STR_LIT>'] = pd.Categorical(df['<STR_LIT>'], categories=['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>'])<EOL>df['<STR_LIT>'] = pd.Categorical(df['<STR_LIT>'], categories=['<STR_LIT:A>', '<STR_LIT:B>', '<STR_LIT:C>'])<EOL>return df<EOL>", "docstring": "Creates a dataset with some categorical variables.", "id": "f3981:m2"}
{"signature": "def cmd_partition(opts):", "body": "config = load_config(opts.config)<EOL>b = get_blockade(config, opts)<EOL>if opts.random:<EOL><INDENT>if opts.partitions:<EOL><INDENT>raise BlockadeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL><DEDENT>b.random_partition()<EOL><DEDENT>else:<EOL><INDENT>partitions = []<EOL>for partition in opts.partitions:<EOL><INDENT>names = []<EOL>for name in partition.split(\"<STR_LIT:U+002C>\"):<EOL><INDENT>name = name.strip()<EOL>if name:<EOL><INDENT>names.append(name)<EOL><DEDENT><DEDENT>partitions.append(names)<EOL><DEDENT>if not partitions:<EOL><INDENT>raise BlockadeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL><DEDENT>b.partition(partitions)<EOL><DEDENT>", "docstring": "Partition the network between containers\n\n    Replaces any existing partitions outright. Any containers NOT specified\n    in arguments will be globbed into a single implicit partition. For\n    example if you have three containers: c1, c2, and c3 and you run:\n\n        blockade partition c1\n\n    The result will be a partition with just c1 and another partition with\n    c2 and c3.\n\n    Alternatively, --random may be specified, and zero or more random\n    partitions will be generated by blockade.", "id": "f342:m20"}
{"signature": "def set_adjustable(self, adjustable):", "body": "if adjustable in ('<STR_LIT>', '<STR_LIT>'):<EOL><INDENT>if self in self._shared_x_axes or self in self._shared_y_axes:<EOL><INDENT>if adjustable == '<STR_LIT>':<EOL><INDENT>raise ValueError(<EOL>'<STR_LIT>')<EOL><DEDENT><DEDENT>self._adjustable = adjustable<EOL><DEDENT>else:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>", "docstring": "ACCEPTS: [ 'box' | 'datalim' ]", "id": "f17238:c1:m26"}
{"signature": "def send(self, campaign_id):", "body": "self.campaign_id = campaign_id<EOL>return self._mc_client._post(url=self._build_path(campaign_id, '<STR_LIT>'))<EOL>", "docstring": "Send a MailChimp campaign. For RSS Campaigns, the campaign will send\naccording to its schedule. All other campaigns will send immediately.\n\n:param campaign_id: The unique id for the campaign.\n:type campaign_id: :py:class:`str`", "id": "f289:c0:m6"}
{"signature": "@staticmethod<EOL><INDENT>def save_variables(filename, variables):<DEDENT>", "body": "ext = get_extension(filename).lower()<EOL>out_exts = {'<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'}<EOL>output_file = filename<EOL>if not ext in out_exts:<EOL><INDENT>output_file = add_extension_if_needed(filename, '<STR_LIT>')<EOL>ext = get_extension(filename)<EOL><DEDENT>if ext == '<STR_LIT>' or ext == '<STR_LIT>':<EOL><INDENT>save_variables_to_shelve(output_file, variables)<EOL><DEDENT>elif ext == '<STR_LIT>':<EOL><INDENT>save_variables_to_mat(output_file, variables)<EOL><DEDENT>elif ext == '<STR_LIT>' or ext == '<STR_LIT>':<EOL><INDENT>from .hdf5 import save_variables_to_hdf5<EOL>save_variables_to_hdf5(output_file, variables)<EOL><DEDENT>else:<EOL><INDENT>raise ValueError('<STR_LIT>'.format(ext))<EOL><DEDENT>", "docstring": "Save given variables in a file.\n        Valid extensions: '.pyshelf' or '.shelf' (Python shelve)\n                          '.mat' (Matlab archive),\n                          '.hdf5' or '.h5' (HDF5 file)\n\n        Parameters\n        ----------\n        filename: str\n            Output file path.\n\n        variables: dict\n            Dictionary varname -> variable\n\n        Raises\n        ------\n        ValueError: if the extension of the filesname is not recognized.", "id": "f4065:c0:m1"}
{"signature": "def _listOfOnTimesInVec(vector):", "body": "<EOL>durations = []<EOL>numOnTimes   = <NUM_LIT:0><EOL>totalOnTime = <NUM_LIT:0><EOL>nonzeros = numpy.array(vector).nonzero()[<NUM_LIT:0>]<EOL>if len(nonzeros) == <NUM_LIT:0>:<EOL><INDENT>return (<NUM_LIT:0>, <NUM_LIT:0>, [])<EOL><DEDENT>if len(nonzeros) == <NUM_LIT:1>:<EOL><INDENT>return (<NUM_LIT:1>, <NUM_LIT:1>, [<NUM_LIT:1>])<EOL><DEDENT>prev = nonzeros[<NUM_LIT:0>]<EOL>onTime = <NUM_LIT:1><EOL>endIdx = nonzeros[-<NUM_LIT:1>]<EOL>for idx in nonzeros[<NUM_LIT:1>:]:<EOL><INDENT>if idx != prev+<NUM_LIT:1>:<EOL><INDENT>totalOnTime += onTime<EOL>numOnTimes  += <NUM_LIT:1><EOL>durations.append(onTime)<EOL>onTime       = <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>onTime += <NUM_LIT:1><EOL><DEDENT>prev = idx<EOL><DEDENT>totalOnTime += onTime<EOL>numOnTimes  += <NUM_LIT:1><EOL>durations.append(onTime)<EOL>return (totalOnTime, numOnTimes, durations)<EOL>", "docstring": "Returns 3 things for a vector:\n  * the total on time\n  * the number of runs\n  * a list of the durations of each run.\n\nParameters:\n-----------------------------------------------\ninput stream: 11100000001100000000011111100000\nreturn value: (11, 3, [3, 2, 6])", "id": "f17557:m19"}
{"signature": "def set(self, key, value):", "body": "if self.in_memory:<EOL><INDENT>self._memory_db[key] = value<EOL><DEDENT>else:<EOL><INDENT>db = self._read_file()<EOL>db[key] = value<EOL>with open(self.db_path, '<STR_LIT:w>') as f:<EOL><INDENT>f.write(json.dumps(db, ensure_ascii=False, indent=<NUM_LIT:2>))<EOL><DEDENT><DEDENT>", "docstring": "Set key value", "id": "f7934:c0:m3"}
{"signature": "def _get_request_type(self):", "body": "return self._get_param(param=\"<STR_LIT>\", allowed_values=allowed_request_types[self.params['<STR_LIT>']])<EOL>", "docstring": "Find requested request type in GET request.", "id": "f13851:c2:m3"}
{"signature": "def __init__(self, ccd_data, mask, hyper_model_image, hyper_galaxy_images, hyper_minimum_values, sub_grid_size=<NUM_LIT:2>,<EOL>image_psf_shape=None, inversion_psf_shape=None, positions=None, interp_pixel_scale=None):", "body": "super().__init__(ccd_data=ccd_data, mask=mask, sub_grid_size=sub_grid_size, image_psf_shape=image_psf_shape,<EOL>inversion_psf_shape=inversion_psf_shape, positions=positions,<EOL>interp_pixel_scale=interp_pixel_scale)<EOL>self.hyper_model_image = hyper_model_image<EOL>self.hyper_galaxy_images = hyper_galaxy_images<EOL>self.hyper_minimum_values = hyper_minimum_values<EOL>self.hyper_model_image_1d = mask.map_2d_array_to_masked_1d_array(array_2d=hyper_model_image)<EOL>self.hyper_galaxy_images_1d = list(map(lambda hyper_galaxy_image :<EOL>mask.map_2d_array_to_masked_1d_array(hyper_galaxy_image),<EOL>hyper_galaxy_images))<EOL>", "docstring": "The lens data is the collection of data (image, noise-map, PSF), a mask, grid_stack, convolver \\\nand other utilities that are used for modeling and fitting an image of a strong lens.\n\nWhilst the image, noise-map, etc. are loaded in 2D, the lens data creates reduced 1D arrays of each \\\nfor lensing calculations.\n\nLens hyper-data includes the hyper-images necessary for changing different aspects of the data that is fitted \\\nin a Bayesian framework, for example the background-sky subtraction and noise-map.\n\nParameters\n----------\nccd_data: im.CCD\n    The ccd data all in 2D (the image, noise-map, PSF, etc.)\nmask: msk.Mask\n    The 2D mask that is applied to the image.\nsub_grid_size : int\n    The size of the sub-grid used for each lens SubGrid. E.g. a value of 2 grid_stack each image-pixel on a 2x2 \\\n    sub-grid.\nimage_psf_shape : (int, int)\n    The shape of the PSF used for convolving model image generated using analytic light profiles. A smaller \\\n    shape will trim the PSF relative to the input image PSF, giving a faster analysis run-time.\ninversion_psf_shape : (int, int)\n    The shape of the PSF used for convolving the inversion mapping matrix. A smaller \\\n    shape will trim the PSF relative to the input image PSF, giving a faster analysis run-time.\npositions : [[]]\n    Lists of image-pixel coordinates (arc-seconds) that mappers close to one another in the source-plane(s), used \\\n    to speed up the non-linear sampling.\ninterp_pixel_scale : float\n    If *True*, expensive to compute mass profile deflection angles will be computed on a sparse grid and \\\n    interpolated to the regular, sub and blurring grids.", "id": "f5970:c1:m0"}
{"signature": "def element_or_none(self, using, value):", "body": "try:<EOL><INDENT>return self._execute(Command.FIND_ELEMENT, {<EOL>'<STR_LIT>': using,<EOL>'<STR_LIT:value>': value<EOL>})<EOL><DEDENT>except:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Check if an element in the current context.\n\n        Support:\n            Android iOS Web(WebView)\n\n        Args:\n            using(str): The element location strategy.\n            value(str): The value of the location strategy.\n\n        Returns:\n            Return Element if the element does exists and return None otherwise.\n\n        Raises:\n            WebDriverException.", "id": "f4426:c0:m54"}
{"signature": "def setup(self, port):", "body": "port = str(port)<EOL>self._serial = serial.Serial(port, <NUM_LIT>, timeout=<NUM_LIT:2>)<EOL>time.sleep(<NUM_LIT:2>)  <EOL>if not self._serial.is_open:<EOL><INDENT>raise RuntimeError('<STR_LIT>')<EOL><DEDENT>self._serial.write(b'<STR_LIT>')<EOL>if self._serial.read() != b'<STR_LIT>':<EOL><INDENT>raise RuntimeError('<STR_LIT>')<EOL><DEDENT>ps = [p for p in self.available_pins() if p['<STR_LIT>']['<STR_LIT>']]<EOL>for pin in ps:<EOL><INDENT>self._set_pin_direction(pin['<STR_LIT:id>'], ahio.Direction.Output)<EOL><DEDENT>", "docstring": "Connects to an Arduino UNO on serial port `port`.\n\n        @throw RuntimeError can't connect to Arduino", "id": "f10522:c1:m2"}
{"signature": "def set_labels(self, labels):", "body": "if not isinstance(labels, string_types) and len(labels) != self.n_subjs:<EOL><INDENT>raise ValueError('<STR_LIT>'<EOL>'<STR_LIT>'.format(len(labels), self.n_subjs))<EOL><DEDENT>self.labels = labels<EOL>", "docstring": "Parameters\n----------\nlabels: list of int or str\n    This list will be checked to have the same size as\n\nRaises\n------\nValueError\n    if len(labels) != self.n_subjs", "id": "f4085:c0:m9"}
{"signature": "def p_annotation_comment_1(self, p):", "body": "try:<EOL><INDENT>if six.PY2:<EOL><INDENT>value = p[<NUM_LIT:2>].decode(encoding='<STR_LIT:utf-8>')<EOL><DEDENT>else:<EOL><INDENT>value = p[<NUM_LIT:2>]<EOL><DEDENT>self.builder.add_annotation_comment(self.document, value)<EOL><DEDENT>except CardinalityError:<EOL><INDENT>self.more_than_one_error('<STR_LIT>', p.lineno(<NUM_LIT:1>))<EOL><DEDENT>except OrderError:<EOL><INDENT>self.order_error('<STR_LIT>', '<STR_LIT>', p.lineno(<NUM_LIT:1>))<EOL><DEDENT>", "docstring": "annotation_comment : ANNOTATION_COMMENT TEXT", "id": "f3753:c0:m121"}
{"signature": "def pitch_contour(annotation, **kwargs):", "body": "ax = kwargs.pop('<STR_LIT>', None)<EOL>ax = mir_eval.display.__get_axes(ax=ax)[<NUM_LIT:0>]<EOL>times, values = annotation.to_interval_values()<EOL>indices = np.unique([v['<STR_LIT:index>'] for v in values])<EOL>for idx in indices:<EOL><INDENT>rows = [i for (i, v) in enumerate(values) if v['<STR_LIT:index>'] == idx]<EOL>freqs = np.asarray([values[r]['<STR_LIT>'] for r in rows])<EOL>unvoiced = ~np.asarray([values[r]['<STR_LIT>'] for r in rows])<EOL>freqs[unvoiced] *= -<NUM_LIT:1><EOL>ax = mir_eval.display.pitch(times[rows, <NUM_LIT:0>], freqs, unvoiced=True,<EOL>ax=ax,<EOL>**kwargs)<EOL><DEDENT>return ax<EOL>", "docstring": "Plotting wrapper for pitch contours", "id": "f11235:m3"}
{"signature": "def get_global_images(self):", "body": "data = self.get_images()<EOL>images = list()<EOL>for i in data:<EOL><INDENT>if i.public:<EOL><INDENT>i.token = self.token<EOL>images.append(i)<EOL><DEDENT><DEDENT>return images<EOL>", "docstring": "This function returns a list of Image objects representing\npublic DigitalOcean images (e.g. base distribution images\nand 'One-Click' applications).", "id": "f1473:c0:m10"}
{"signature": "def request(<EOL>self,<EOL>method,<EOL>url,<EOL>data=None,<EOL>headers=None,<EOL>withhold_token=False,<EOL>client_id=None,<EOL>client_secret=None,<EOL>**kwargs<EOL>):", "body": "if not is_secure_transport(url):<EOL><INDENT>raise InsecureTransportError()<EOL><DEDENT>if self.token and not withhold_token:<EOL><INDENT>log.debug(<EOL>\"<STR_LIT>\",<EOL>len(self.compliance_hook[\"<STR_LIT>\"]),<EOL>)<EOL>for hook in self.compliance_hook[\"<STR_LIT>\"]:<EOL><INDENT>log.debug(\"<STR_LIT>\", hook)<EOL>url, headers, data = hook(url, headers, data)<EOL><DEDENT>log.debug(\"<STR_LIT>\", self.token)<EOL>try:<EOL><INDENT>url, headers, data = self._client.add_token(<EOL>url, http_method=method, body=data, headers=headers<EOL>)<EOL><DEDENT>except TokenExpiredError:<EOL><INDENT>if self.auto_refresh_url:<EOL><INDENT>log.debug(<EOL>\"<STR_LIT>\",<EOL>self.auto_refresh_url,<EOL>)<EOL>auth = kwargs.pop(\"<STR_LIT>\", None)<EOL>if client_id and client_secret and (auth is None):<EOL><INDENT>log.debug(<EOL>'<STR_LIT>',<EOL>client_id,<EOL>)<EOL>auth = requests.auth.HTTPBasicAuth(client_id, client_secret)<EOL><DEDENT>token = self.refresh_token(<EOL>self.auto_refresh_url, auth=auth, **kwargs<EOL>)<EOL>if self.token_updater:<EOL><INDENT>log.debug(<EOL>\"<STR_LIT>\", token, self.token_updater<EOL>)<EOL>self.token_updater(token)<EOL>url, headers, data = self._client.add_token(<EOL>url, http_method=method, body=data, headers=headers<EOL>)<EOL><DEDENT>else:<EOL><INDENT>raise TokenUpdated(token)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>raise<EOL><DEDENT><DEDENT><DEDENT>log.debug(\"<STR_LIT>\", url, method)<EOL>log.debug(\"<STR_LIT>\", headers, data)<EOL>log.debug(\"<STR_LIT>\", kwargs)<EOL>return super(OAuth2Session, self).request(<EOL>method, url, headers=headers, data=data, **kwargs<EOL>)<EOL>", "docstring": "Intercept all requests and add the OAuth 2 token if present.", "id": "f6413:c1:m15"}
{"signature": "def to_decimal(text):", "body": "if not isinstance(text, string_type):<EOL><INDENT>raise TypeError(\"<STR_LIT>\" % type(text))<EOL><DEDENT>if findall(r\"<STR_LIT>\", text):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>text = text.lstrip('<STR_LIT:!>')<EOL>decimal = <NUM_LIT:0><EOL>length = len(text) - <NUM_LIT:1><EOL>for i, char in enumerate(text):<EOL><INDENT>decimal += (ord(char) - <NUM_LIT>) * (<NUM_LIT> ** (length - i))<EOL><DEDENT>return decimal if text != '<STR_LIT>' else <NUM_LIT:0><EOL>", "docstring": "Takes a base91 char string and returns decimal", "id": "f12504:m0"}
{"signature": "def field_add_subfield(field, code, value):", "body": "field[<NUM_LIT:0>].append((code, value))<EOL>", "docstring": "Add a subfield to field 'field'.", "id": "f7891:m28"}
{"signature": "def __init__(self, pattern=None):", "body": "self.pattern = pattern<EOL>", "docstring": "Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nn-uiautomationclient-iuiautomationspreadsheetpattern", "id": "f1782:c62:m0"}
{"signature": "def get_after_cursor(self):", "body": "return self.after_cursor<EOL>", "docstring": "Acquires the after cursor.\n\n        :returns: the after cursor", "id": "f11267:c2:m11"}
{"signature": "def enumerate_joint(vars, e, P):", "body": "if not vars:<EOL><INDENT>return P[e]<EOL><DEDENT>Y, rest = vars[<NUM_LIT:0>], vars[<NUM_LIT:1>:]<EOL>return sum([enumerate_joint(rest, extend(e, Y, y), P)<EOL>for y in P.values(Y)])<EOL>", "docstring": "Return the sum of those entries in P consistent with e,\n    provided vars is P's remaining variables (the ones not in e).", "id": "f1686:m3"}
{"signature": "def is_not_close_to(self, other, tolerance):", "body": "self._validate_close_to_args(self.val, other, tolerance)<EOL>if self.val >= (other-tolerance) and self.val <= (other+tolerance):<EOL><INDENT>if type(self.val) is datetime.datetime:<EOL><INDENT>tolerance_seconds = tolerance.days * <NUM_LIT> + tolerance.seconds + tolerance.microseconds / <NUM_LIT><EOL>h, rem = divmod(tolerance_seconds, <NUM_LIT>)<EOL>m, s = divmod(rem, <NUM_LIT>)<EOL>self._err('<STR_LIT>' % (self.val.strftime('<STR_LIT>'), other.strftime('<STR_LIT>'), h, m, s))<EOL><DEDENT>else:<EOL><INDENT>self._err('<STR_LIT>' % (self.val, other, tolerance))<EOL><DEDENT><DEDENT>return self<EOL>", "docstring": "Asserts that val is numeric and is not close to other within tolerance.", "id": "f9307:c0:m41"}
{"signature": "def init(self):", "body": "<EOL>self.es.indices.create(index=self.params['<STR_LIT:index>'], ignore=<NUM_LIT>)<EOL>", "docstring": "Create an Elasticsearch index if necessary", "id": "f575:c5:m1"}
{"signature": "def sort_by_modified(files_or_folders: list) -> list:", "body": "return sorted(files_or_folders, key=os.path.getmtime, reverse=True)<EOL>", "docstring": "Sort files or folders by modified time\n\nArgs:\n    files_or_folders: list of files or folders\n\nReturns:\n    list", "id": "f161:m5"}
{"signature": "@abstractmethod<EOL><INDENT>def _build_this(self, builder, stage_names):<DEDENT>", "body": "raise RuntimeError(\"<STR_LIT>\")<EOL>", "docstring": "This is the method that's implemented by the operators.\n        :type builder: TopologyBuilder\n        :param builder: The operator adds in the current streamlet as a spout/bolt", "id": "f7194:c0:m21"}
{"signature": "@JwtLmsApiClient.refresh_token<EOL><INDENT>def get_course_certificate(self, course_id, username):<DEDENT>", "body": "return self.client.certificates(username).courses(course_id).get()<EOL>", "docstring": "Retrieve the certificate for the given username for the given course_id.\n\nArgs:\n* ``course_id`` (str): The string value of the course's unique identifier\n* ``username`` (str): The username ID identifying the user for which to retrieve the certificate\n\nRaises:\n\nHttpNotFoundError if no certificate found for the given user+course.\n\nReturns:\n\na dict containing:\n\n* ``username``: A string representation of an user's username passed in the request.\n* ``course_id``: A string representation of a Course ID.\n* ``certificate_type``: A string representation of the certificate type.\n* ``created_date`: Datetime the certificate was created (tz-aware).\n* ``status``: A string representation of the certificate status.\n* ``is_passing``: True if the certificate has a passing status, False if not.\n* ``download_url``: A string representation of the certificate url.\n* ``grade``: A string representation of a float for the user's course grade.", "id": "f16094:c7:m0"}
{"signature": "def push_tag(tag):", "body": "_call('<STR_LIT>' + str(tag))<EOL>", "docstring": "Pushes a tag into the upstream", "id": "f1:m7"}
{"signature": "def plot_fit_subplot_lens_plane_only(<EOL>fit, should_plot_mask=True, extract_array_from_mask=False, zoom_around_mask=False, positions=None,<EOL>should_plot_image_plane_pix=False,<EOL>units='<STR_LIT>', figsize=None, aspect='<STR_LIT>',<EOL>cmap='<STR_LIT>', norm='<STR_LIT>', norm_min=None, norm_max=None, linthresh=<NUM_LIT>, linscale=<NUM_LIT>,<EOL>cb_ticksize=<NUM_LIT:10>, cb_fraction=<NUM_LIT>, cb_pad=<NUM_LIT>, cb_tick_values=None, cb_tick_labels=None,<EOL>titlesize=<NUM_LIT:10>, xlabelsize=<NUM_LIT:10>, ylabelsize=<NUM_LIT:10>, xyticksize=<NUM_LIT:10>,<EOL>mask_pointsize=<NUM_LIT:10>, position_pointsize=<NUM_LIT:10>, grid_pointsize=<NUM_LIT:1>,<EOL>output_path=None, output_filename='<STR_LIT>', output_format='<STR_LIT>'):", "body": "rows, columns, figsize_tool = plotter_util.get_subplot_rows_columns_figsize(number_subplots=<NUM_LIT:6>)<EOL>mask = lens_plotter_util.get_mask(fit=fit, should_plot_mask=should_plot_mask)<EOL>if figsize is None:<EOL><INDENT>figsize = figsize_tool<EOL><DEDENT>plt.figure(figsize=figsize)<EOL>plt.subplot(rows, columns, <NUM_LIT:1>)<EOL>kpc_per_arcsec = fit.tracer.image_plane.kpc_per_arcsec<EOL>image_plane_pix_grid = lens_plotter_util.get_image_plane_pix_grid(should_plot_image_plane_pix, fit)<EOL>lens_plotter_util.plot_image(<EOL>fit=fit, mask=mask, extract_array_from_mask=extract_array_from_mask, zoom_around_mask=zoom_around_mask,<EOL>positions=positions, image_plane_pix_grid=image_plane_pix_grid, as_subplot=True,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>grid_pointsize=grid_pointsize, position_pointsize=position_pointsize, mask_pointsize=mask_pointsize,<EOL>output_path=output_path, output_filename='<STR_LIT>', output_format=output_format)<EOL>plt.subplot(rows, columns, <NUM_LIT:2>)<EOL>lens_plotter_util.plot_noise_map(<EOL>fit=fit, mask=mask, extract_array_from_mask=extract_array_from_mask, zoom_around_mask=zoom_around_mask,<EOL>positions=positions, as_subplot=True,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>position_pointsize=position_pointsize, mask_pointsize=mask_pointsize,<EOL>output_path=output_path, output_filename='<STR_LIT>', output_format=output_format)<EOL>plt.subplot(rows, columns, <NUM_LIT:3>)<EOL>lens_plotter_util.plot_signal_to_noise_map(<EOL>fit=fit, mask=mask, extract_array_from_mask=extract_array_from_mask, zoom_around_mask=zoom_around_mask,<EOL>positions=positions, as_subplot=True,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>position_pointsize=position_pointsize, mask_pointsize=mask_pointsize,<EOL>output_path=output_path, output_filename='<STR_LIT>', output_format=output_format)<EOL>plt.subplot(rows, columns, <NUM_LIT:4>)<EOL>lens_plotter_util.plot_model_data(<EOL>fit=fit, mask=mask, extract_array_from_mask=extract_array_from_mask, zoom_around_mask=zoom_around_mask, as_subplot=True,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>output_path=output_path, output_filename='<STR_LIT>', output_format=output_format)<EOL>plt.subplot(rows, columns, <NUM_LIT:5>)<EOL>lens_plotter_util.plot_residual_map(<EOL>fit=fit, mask=mask, extract_array_from_mask=extract_array_from_mask, zoom_around_mask=zoom_around_mask, as_subplot=True,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>output_path=output_path, output_filename='<STR_LIT>', output_format=output_format)<EOL>plt.subplot(rows, columns, <NUM_LIT:6>)<EOL>lens_plotter_util.plot_chi_squared_map(<EOL>fit=fit, mask=mask, extract_array_from_mask=extract_array_from_mask, zoom_around_mask=zoom_around_mask, as_subplot=True,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>output_path=output_path, output_filename='<STR_LIT>', output_format=output_format)<EOL>plotter_util.output_subplot_array(output_path=output_path, output_filename=output_filename,<EOL>output_format=output_format)<EOL>plt.close()<EOL>", "docstring": "Plot the model datas_ of an analysis, using the *Fitter* class object.\n\n    The visualization and output type can be fully customized.\n\n    Parameters\n    -----------\n    fit : autolens.lens.fitting.Fitter\n        Class containing fit between the model datas_ and observed lens datas_ (including residual_map, chi_squared_map etc.)\n    output_path : str\n        The path where the datas_ is output if the output_type is a file format (e.g. png, fits)\n    output_filename : str\n        The name of the file that is output, if the output_type is a file format (e.g. png, fits)\n    output_format : str\n        How the datas_ is output. File formats (e.g. png, fits) output the datas_ to harddisk. 'show' displays the datas_ \\\n        in the python interpreter window.", "id": "f5976:m1"}
{"signature": "def sort_nested(data):", "body": "if isinstance(data, dict):<EOL><INDENT>new_data = {}<EOL>for k, v in data.items():<EOL><INDENT>if isinstance(v, list):<EOL><INDENT>v = sorted(v)<EOL><DEDENT>if isinstance(v, dict):<EOL><INDENT>v = sort_nested(v)<EOL><DEDENT>new_data[k] = v<EOL><DEDENT>return new_data<EOL><DEDENT>elif isinstance(data, list):<EOL><INDENT>new_data = []<EOL>for v in sorted(data):<EOL><INDENT>if isinstance(v, list):<EOL><INDENT>v = sort_nested(v)<EOL><DEDENT>if isinstance(v, dict):<EOL><INDENT>v = sort_nested(v)<EOL><DEDENT>new_data.append(v)<EOL><DEDENT>return new_data<EOL><DEDENT>", "docstring": "Return a new dict with any nested list sorted recursively.", "id": "f3764:m4"}
{"signature": "def cleanwrap(func):", "body": "def enc(self, *args, **kwargs):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>return (func(self, item, **kwargs) for item in args)<EOL><DEDENT>return enc<EOL>", "docstring": "Wrapper for Zotero._cleanup", "id": "f5629:m2"}
{"signature": "def __init__(self, logic=None, weight=None, simple=None, simple_weight=None, dataset=None, system=None, file=None,<EOL>query=None, **kwargs):", "body": "self._logic = None<EOL>self.logic = logic<EOL>self._weight = None<EOL>self.weight = weight<EOL>self._simple = None<EOL>self.simple = simple<EOL>self._simple_weight = None<EOL>self.simple_weight = simple_weight<EOL>self._dataset = None<EOL>self.dataset = dataset<EOL>self._system = None<EOL>self.system = system<EOL>self._file = None<EOL>self.file = file<EOL>self._query = None<EOL>self.query = query<EOL>", "docstring": "Constructor.\n\n:param logic: The logic to apply to the query ('SHOULD', 'MUST', 'MUST_NOT', or 'OPTIONAL').\n:param weight: Weight for the query.\n:param simple: String with the simple search to run against all fields.\n:param simple_weight: Dictionary of relative paths to their weights for simple queries.\n:param dataset: One or more :class:`DatasetQuery` objects with queries against dataset metadata.\n:param system: One or more :class:`PifSystemQuery` objects with queries against PIF systems\n:param file: One or more :class:`FileQuery` objects with queries against file content or metadata.\n:type file: :class:`FileQuery`\n:param query: Nested list of :class:`DataQuery` objects.", "id": "f3537:c0:m0"}
{"signature": "def diff(self, end_date):", "body": "if end_date < self:<EOL><INDENT>y, m, d = BusinessDate.diff(end_date, self)<EOL>return -y, -m, -d<EOL><DEDENT>y = end_date.year - self.year<EOL>m = end_date.month - self.month<EOL>while m < <NUM_LIT:0>:<EOL><INDENT>y -= <NUM_LIT:1><EOL>m += <NUM_LIT:12><EOL><DEDENT>while m > <NUM_LIT:12>:<EOL><INDENT>y += <NUM_LIT:1><EOL>m -= <NUM_LIT:12><EOL><DEDENT>s = BusinessDate.add_years(BusinessDate.add_months(self, m), y)<EOL>d = BusinessDate.diff_in_days(s, end_date)<EOL>if d < <NUM_LIT:0>:<EOL><INDENT>m -= <NUM_LIT:1><EOL>if m < <NUM_LIT:0>:<EOL><INDENT>y -= <NUM_LIT:1><EOL>m += <NUM_LIT:12><EOL><DEDENT>s = BusinessDate.add_years(BusinessDate.add_months(self, m), y)<EOL><DEDENT>d = BusinessDate.diff_in_days(s, end_date)<EOL>return -int(y), -int(m), -int(d)<EOL>", "docstring": "difference expressed as a tuple of years, months, days\n(see also the python lib dateutils.relativedelta)\n\n:param BusinessDate start_date:\n:param BusinessDate end_date:\n:return (int, int, int):", "id": "f15441:c2:m36"}
{"signature": "@belns.command()<EOL>@connection_option<EOL>@click.option('<STR_LIT>', '<STR_LIT>', multiple=True, help='<STR_LIT>')<EOL>@click.option('<STR_LIT>', '<STR_LIT>', type=click.Path(file_okay=False, dir_okay=True), default=os.getcwd(),<EOL>help='<STR_LIT>')<EOL>@click.option('<STR_LIT>', '<STR_LIT>', is_flag=True, help='<STR_LIT>')<EOL>def write(connection, skip, directory, force):", "body": "os.makedirs(directory, exist_ok=True)<EOL>from .manager.namespace_manager import BELNamespaceManagerMixin<EOL>for idx, name, manager in _iterate_managers(connection, skip):<EOL><INDENT>if not (isinstance(manager, AbstractManager) and isinstance(manager, BELNamespaceManagerMixin)):<EOL><INDENT>continue<EOL><DEDENT>click.secho(name, fg='<STR_LIT>', bold=True)<EOL>if force:<EOL><INDENT>try:<EOL><INDENT>click.echo(f'<STR_LIT>')<EOL>manager.drop_all()<EOL>click.echo('<STR_LIT>')<EOL>clear_cache(name)<EOL>click.echo('<STR_LIT>')<EOL>manager.populate()<EOL><DEDENT>except Exception:<EOL><INDENT>click.secho(f'<STR_LIT>', fg='<STR_LIT>')<EOL>continue<EOL><DEDENT><DEDENT>try:<EOL><INDENT>r = manager.write_directory(directory)<EOL><DEDENT>except TypeError as e:<EOL><INDENT>click.secho(f'<STR_LIT>'.rstrip(), fg='<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>if not r:<EOL><INDENT>click.echo('<STR_LIT>')<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Write a BEL namespace names/identifiers to terminology store.", "id": "f1584:m9"}
{"signature": "def GetTextPattern(self) -> TextPattern:", "body": "return self.GetPattern(PatternId.TextPattern)<EOL>", "docstring": "Return `TextPattern` if it supports the pattern else None(Conditional support according to MSDN).", "id": "f1782:c113:m3"}
{"signature": "def workdir_is_clean(self, quiet=False):", "body": "<EOL>self.run('<STR_LIT>', **RUN_KWARGS)<EOL>unchanged = True<EOL>try:<EOL><INDENT>self.run('<STR_LIT>', report_error=False, **RUN_KWARGS)<EOL><DEDENT>except exceptions.Failure:<EOL><INDENT>unchanged = False<EOL>if not quiet:<EOL><INDENT>notify.warning('<STR_LIT>')<EOL>self.run('<STR_LIT>', **RUN_KWARGS)<EOL><DEDENT><DEDENT>try:<EOL><INDENT>self.run('<STR_LIT>', report_error=False, **RUN_KWARGS)<EOL><DEDENT>except exceptions.Failure:<EOL><INDENT>unchanged = False<EOL>if not quiet:<EOL><INDENT>notify.warning('<STR_LIT>')<EOL>self.run('<STR_LIT>', **RUN_KWARGS)<EOL><DEDENT><DEDENT>return unchanged<EOL>", "docstring": "Check for uncommitted changes, return `True` if everything is clean.\n\n            Inspired by http://stackoverflow.com/questions/3878624/.", "id": "f3286:c0:m0"}
{"signature": "def exit(self, code=<NUM_LIT:0>):", "body": "sys.exit(code)<EOL>", "docstring": "Exits the application with a given exit code.", "id": "f8340:c0:m15"}
{"signature": "def run(ci=False):", "body": "xml_report_path = os.path.join(package_root, '<STR_LIT>')<EOL>if os.path.exists(xml_report_path):<EOL><INDENT>os.unlink(xml_report_path)<EOL><DEDENT>cov = coverage.Coverage(include='<STR_LIT>' % package_name)<EOL>cov.start()<EOL>from .tests import run as run_tests<EOL>result = run_tests()<EOL>print()<EOL>if ci:<EOL><INDENT>suite = unittest.TestSuite()<EOL>loader = unittest.TestLoader()<EOL>for other_package in other_packages:<EOL><INDENT>for test_class in _load_package_tests(other_package):<EOL><INDENT>suite.addTest(loader.loadTestsFromTestCase(test_class))<EOL><DEDENT><DEDENT>if suite.countTestCases() > <NUM_LIT:0>:<EOL><INDENT>print('<STR_LIT>')<EOL>sys.stdout.flush()<EOL>runner_result = unittest.TextTestRunner(stream=sys.stdout, verbosity=<NUM_LIT:1>).run(suite)<EOL>result = runner_result.wasSuccessful() and result<EOL>print()<EOL>sys.stdout.flush()<EOL><DEDENT><DEDENT>cov.stop()<EOL>cov.save()<EOL>cov.report(show_missing=False)<EOL>print()<EOL>sys.stdout.flush()<EOL>if ci:<EOL><INDENT>cov.xml_report()<EOL><DEDENT>if ci and result and os.path.exists(xml_report_path):<EOL><INDENT>_codecov_submit()<EOL>print()<EOL><DEDENT>return result<EOL>", "docstring": "Runs the tests while measuring coverage\n\n:param ci:\n    If coverage is being run in a CI environment - this triggers trying to\n    run the tests for the rest of modularcrypto and uploading coverage data\n\n:return:\n    A bool - if the tests ran successfully", "id": "f9468:m0"}
{"signature": "def _load(self, config):", "body": "if isinstance(config, six.string_types):<EOL><INDENT>try:<EOL><INDENT>config = json.loads(config)<EOL><DEDENT>except ValueError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>if not isinstance(config, dict):<EOL><INDENT>raise TypeError('<STR_LIT>'<EOL>'<STR_LIT>')<EOL><DEDENT>return config<EOL>", "docstring": "Loads config from string or dict", "id": "f11894:c0:m1"}
{"signature": "@property<EOL><INDENT>def IsPassword(self) -> bool:<DEDENT>", "body": "return self.Element.CurrentIsPassword<EOL>", "docstring": "Property IsPassword.\nCall IUIAutomationElement::get_CurrentIsPassword.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationelement-get_currentispassword", "id": "f1782:c78:m28"}
{"signature": "def swap_environment_cnames(self, from_env_name, to_env_name):", "body": "self.ebs.swap_environment_cnames(source_environment_name=from_env_name,<EOL>destination_environment_name=to_env_name)<EOL>", "docstring": "Swaps cnames for an environment", "id": "f15103:c1:m1"}
{"signature": "def make_loop(self, handlers):", "body": "self.loop = SelectMainLoop(None, handlers)<EOL>", "docstring": "Create the main loop object.", "id": "f15283:c6:m2"}
{"signature": "def mock_empty_response(self, resource, *args, **kwargs):", "body": "responses.add(<EOL>responses.GET,<EOL>url=self.build_enterprise_api_url(resource, *args, **kwargs),<EOL>json={},<EOL>status=<NUM_LIT:200>,<EOL>content_type='<STR_LIT:application/json>',<EOL>)<EOL>", "docstring": "DRY function to register an empty response from some Enterprise API endpoint.", "id": "f16260:c0:m7"}
{"signature": "@db.non_transactional(allow_existing=True)<EOL><INDENT>def locked_delete(self):<DEDENT>", "body": "if self._cache:<EOL><INDENT>self._cache.delete(self._key_name)<EOL><DEDENT>self._delete_entity()<EOL>", "docstring": "Delete Credential from datastore.", "id": "f2463:c4:m6"}
{"signature": "def exec_file(self, path):", "body": "filename = os.path.basename(path)<EOL>log.info('<STR_LIT>', filename)<EOL>content = from_file(path).replace('<STR_LIT:\\r>', '<STR_LIT>').split('<STR_LIT:\\n>')<EOL>res = '<STR_LIT>'<EOL>for line in content:<EOL><INDENT>line = line.rstrip('<STR_LIT:\\n>')<EOL>retlines = (res + self.__exchange(line)).splitlines()<EOL>res = retlines.pop()<EOL>for lin in retlines:<EOL><INDENT>log.info(lin)<EOL><DEDENT><DEDENT>log.info(res)<EOL>", "docstring": "execute the lines in the local file 'path", "id": "f15366:c0:m14"}
{"signature": "@property<EOL><INDENT>def encoding(self):<DEDENT>", "body": "if self.redirect is not None:<EOL><INDENT>return self.redirect.encoding<EOL><DEDENT>else:<EOL><INDENT>return super(TeeStringIO, self).encoding<EOL><DEDENT>", "docstring": "Gets the encoding of the `redirect` IO object\n\nDoctest:\n    >>> redirect = io.StringIO()\n    >>> assert TeeStringIO(redirect).encoding is None\n    >>> assert TeeStringIO(None).encoding is None\n    >>> assert TeeStringIO(sys.stdout).encoding is sys.stdout.encoding\n    >>> redirect = io.TextIOWrapper(io.StringIO())\n    >>> assert TeeStringIO(redirect).encoding is redirect.encoding", "id": "f5132:c0:m2"}
{"signature": "def _is_api_definition(line):", "body": "return line.split('<STR_LIT:U+0020>', <NUM_LIT:1>)[<NUM_LIT:0>] in HTTP_METHODS<EOL>", "docstring": "Is a definition of a Trello endpoint.\n\n>>> _is_api_definition('GET /1/actions/[idAction]')\nTrue\n>>> _is_api_definition('action')\nFalse", "id": "f9786:m1"}
{"signature": "def binary_repr(number, max_length = <NUM_LIT>):", "body": "<EOL>shifts = list(map (operator.rshift, max_length * [number],list(range(max_length - <NUM_LIT:1>, -<NUM_LIT:1>, -<NUM_LIT:1>))))<EOL>digits = list(map (operator.mod, shifts, max_length * [<NUM_LIT:2>]))<EOL>if not digits.count (<NUM_LIT:1>): return <NUM_LIT:0><EOL>digits = digits [digits.index (<NUM_LIT:1>):]<EOL>return '<STR_LIT>'.join (map (repr, digits)).replace('<STR_LIT:L>','<STR_LIT>')<EOL>", "docstring": "Return the binary representation of the input *number* as a\nstring.\n\nThis is more efficient than using :func:`base_repr` with base 2.\n\nIncrease the value of max_length for very large numbers. Note that\non 32-bit machines, 2**1023 is the largest integer power of 2\nwhich can be converted to a Python float.", "id": "f17237:m66"}
{"signature": "def Split(self):", "body": "googlename = self.RepositoryName()<EOL>project, rest = os.path.split(googlename)<EOL>return (project,) + os.path.splitext(rest)<EOL>", "docstring": "Splits the file into the directory, basename, and extension.\n\n        For 'chrome/browser/browser.cc', Split() would\n        return ('chrome/browser', 'browser', '.cc')\n\n        Returns:\n          A tuple of (directory, basename, extension).", "id": "f7245:c4:m3"}
{"signature": "@cli.command('<STR_LIT:list>')<EOL>def list_():", "body": "environments = cpenv.get_environments()<EOL>modules = cpenv.get_modules()<EOL>click.echo(format_objects(environments + modules, children=True))<EOL>", "docstring": "List available environments and modules", "id": "f8303:m7"}
{"signature": "def links_to_text(self):", "body": "self.parser.stripTags(self.get_top_node(), '<STR_LIT:a>')<EOL>", "docstring": "\\\n        cleans up and converts any nodes that\n        should be considered text into text", "id": "f14086:c0:m6"}
{"signature": "def printOneTrainingVector(x):", "body": "print('<STR_LIT>'.join('<STR_LIT:1>' if k != <NUM_LIT:0> else '<STR_LIT:.>' for k in x))<EOL>", "docstring": "Print a single vector succinctly.", "id": "f17532:m0"}
{"signature": "@property<EOL><INDENT>def base(self):<DEDENT>", "body": "return self._fname.parent<EOL>", "docstring": "We only support data in the filesystem, thus we make sure `base` is a `pathlib.Path`.", "id": "f15003:c12:m7"}
{"signature": "def state(self):", "body": "return self._state<EOL>", "docstring": "Return current state.", "id": "f14226:c0:m16"}
{"signature": "def reposition(self, frame_no):", "body": "for label, j in self.channels.items():<EOL><INDENT>body = self.bodies[label]<EOL>body.position = self.positions[frame_no, j]<EOL>body.linear_velocity = self.velocities[frame_no, j]<EOL><DEDENT>", "docstring": "Reposition markers to a specific frame of data.\n\n        Parameters\n        ----------\n        frame_no : int\n            The frame of data where we should reposition marker bodies. Markers\n            will be positioned in the appropriate places in world coordinates.\n            In addition, linear velocities of the markers will be set according\n            to the data as long as there are no dropouts in neighboring frames.", "id": "f14892:c0:m14"}
{"signature": "def p_media_type(self, p):", "body": "p[<NUM_LIT:0>] = tuple(list(p)[<NUM_LIT:1>:])<EOL>", "docstring": "media_type                : css_media_type\n                                      | css_media_type t_ws", "id": "f12427:c2:m101"}
{"signature": "def euclidean_distance_square(point1, point2):", "body": "distance = <NUM_LIT:0.0><EOL>for i in range(len(point1)):<EOL><INDENT>distance += (point1[i] - point2[i]) ** <NUM_LIT><EOL><DEDENT>return distance<EOL>", "docstring": "!\n    @brief Calculate square Euclidean distance between two vectors.\n\n    \\f[\n    dist(a, b) = \\sum_{i=0}^{N}(a_{i} - b_{i})^{2};\n    \\f]\n\n    @param[in] point1 (array_like): The first vector.\n    @param[in] point2 (array_like): The second vector.\n\n    @return (double) Square Euclidean distance between two vectors.\n\n    @see euclidean_distance, manhattan_distance, chebyshev_distance", "id": "f15695:m2"}
{"signature": "def __del__(self):", "body": "self._stop = True<EOL>", "docstring": "Stop callback when destroying this class", "id": "f10311:c5:m1"}
{"signature": "def run_suite(suite):", "body": "sys.stdout.write('<STR_LIT>' % tests.countTestCases())<EOL>res = unittest.TestResult()<EOL>suite.run(res)<EOL>if res.wasSuccessful():<EOL><INDENT>sys.stdout.write('<STR_LIT>')<EOL>return<EOL><DEDENT>sys.stdout.write('<STR_LIT:\\n>')<EOL>for problems, kind in ((res.errors, '<STR_LIT:error>'),<EOL>(res.failures, '<STR_LIT>')):<EOL><INDENT>if len(problems):<EOL><INDENT>head = '<STR_LIT>' % (len(problems),<EOL>kind,<EOL>'<STR_LIT:s>' if len(problems) != <NUM_LIT:1> else '<STR_LIT>')<EOL>sys.stdout.write('<STR_LIT>' %<EOL>(head, '<STR_LIT>' * len(head)))<EOL><DEDENT>for problem in problems:<EOL><INDENT>func = problem[<NUM_LIT:0>]._testMethodName[<NUM_LIT:5>:]<EOL>environ = '<STR_LIT>' if isinstance(problem[<NUM_LIT:0>],<EOL>BrokenCtypesTest)else '<STR_LIT>'<EOL>sys.stdout.write(<EOL>'<STR_LIT>' %<EOL>(func, environ, '<STR_LIT:\\n>'.join(map(lambda s: '<STR_LIT:U+0020>' + s,<EOL>problem[<NUM_LIT:1>].splitlines())))<EOL>)<EOL><DEDENT><DEDENT>sys.stdout.write('<STR_LIT>' %<EOL>(res.testsRun - len(res.failures) - len(res.errors)))<EOL>", "docstring": "unittest is basically a disaster, so let's do this ourselves.", "id": "f4702:m0"}
{"signature": "def mpsse_gpio(self):", "body": "level_low = chr(self._level & <NUM_LIT>)<EOL>level_high = chr((self._level >> <NUM_LIT:8>) & <NUM_LIT>)<EOL>dir_low = chr(self._direction & <NUM_LIT>)<EOL>dir_high = chr((self._direction >> <NUM_LIT:8>) & <NUM_LIT>)<EOL>return str(bytearray((<NUM_LIT>, level_low, dir_low, <NUM_LIT>, level_high, dir_high)))<EOL>", "docstring": "Return command to update the MPSSE GPIO state to the current direction\n        and level.", "id": "f8002:c0:m9"}
{"signature": "def do_xref(self, node):", "body": "id = node.attributes[\"<STR_LIT:id>\"].value<EOL>self.parse(self.randomChildElement(self.refs[id]))<EOL>", "docstring": "handle <xref id='...'> tag\n\n        An <xref id='...'> tag is a cross-reference to a <ref id='...'>\n        tag.  <xref id='sentence'/> evaluates to a randomly chosen child of\n        <ref id='sentence'>.", "id": "f11511:c1:m14"}
{"signature": "def _update_type(self, params):", "body": "dozscale = False<EOL>particles = []<EOL>for p in listify(params):<EOL><INDENT>typ, ind = self._p2i(p)<EOL>particles.append(ind)<EOL>dozscale = dozscale or typ == '<STR_LIT>'<EOL><DEDENT>particles = set(particles)<EOL>return dozscale, particles<EOL>", "docstring": "Returns dozscale and particle list of update", "id": "f5759:c1:m16"}
{"signature": "def __rxor__(self, other ):", "body": "if isinstance( other, str ):<EOL><INDENT>other = Literal( other )<EOL><DEDENT>if not isinstance( other, ParserElement ):<EOL><INDENT>warnings.warn(\"<STR_LIT>\" % type(other),<EOL>SyntaxWarning, stacklevel=<NUM_LIT:2>)<EOL>return None<EOL><DEDENT>return other ^ self<EOL>", "docstring": "Implementation of ^ operator when left operand is not a ParserElement", "id": "f17196:c8:m32"}
{"signature": "def set_hatch(self, hatch):", "body": "hatches = {'<STR_LIT>':<NUM_LIT:0>, '<STR_LIT>':<NUM_LIT:0>, '<STR_LIT>':<NUM_LIT:0>, '<STR_LIT>':<NUM_LIT:0>}<EOL>for letter in hatch:<EOL><INDENT>if   (letter == '<STR_LIT:/>'):    hatches['<STR_LIT>'] += <NUM_LIT:1><EOL>elif (letter == '<STR_LIT:\\\\>'):   hatches['<STR_LIT>'] += <NUM_LIT:1><EOL>elif (letter == '<STR_LIT:|>'):    hatches['<STR_LIT>']  += <NUM_LIT:1><EOL>elif (letter == '<STR_LIT:->'):    hatches['<STR_LIT>'] += <NUM_LIT:1><EOL>elif (letter == '<STR_LIT:+>'):<EOL><INDENT>hatches['<STR_LIT>'] += <NUM_LIT:1><EOL>hatches['<STR_LIT>'] += <NUM_LIT:1><EOL><DEDENT>elif (letter.lower() == '<STR_LIT:x>'):<EOL><INDENT>hatches['<STR_LIT>'] += <NUM_LIT:1><EOL>hatches['<STR_LIT>'] += <NUM_LIT:1><EOL><DEDENT><DEDENT>def do_hatch(angle, density):<EOL><INDENT>if (density == <NUM_LIT:0>): return \"<STR_LIT>\"<EOL>return", "docstring": "hatch can be one of:\n    /   - diagonal hatching\n    \\   - back diagonal\n    |   - vertical\n    -   - horizontal\n    +   - crossed\n    X   - crossed diagonal\n\nletters can be combined, in which case all the specified\nhatchings are done\n\nif same letter repeats, it increases the density of hatching\nin that direction", "id": "f17234:c0:m9"}
{"signature": "@property<EOL><INDENT>def AriaProperties(self) -> str:<DEDENT>", "body": "return self.Element.CurrentAriaProperties<EOL>", "docstring": "Property AriaProperties.\nCall IUIAutomationElement::get_CurrentAriaProperties.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationelement-get_currentariaproperties", "id": "f1782:c78:m12"}
{"signature": "def add_cmd_to_checkplot(<EOL>cpx,<EOL>cmdpkl,<EOL>require_cmd_magcolor=True,<EOL>save_cmd_pngs=False<EOL>):", "body": "<EOL>if isinstance(cpx, str) and os.path.exists(cpx):<EOL><INDENT>cpdict = _read_checkplot_picklefile(cpx)<EOL><DEDENT>elif isinstance(cpx, dict):<EOL><INDENT>cpdict = cpx<EOL><DEDENT>else:<EOL><INDENT>LOGERROR('<STR_LIT>')<EOL>return None<EOL><DEDENT>if isinstance(cmdpkl, str) and os.path.exists(cmdpkl):<EOL><INDENT>with open(cmdpkl, '<STR_LIT:rb>') as infd:<EOL><INDENT>cmd = pickle.load(infd)<EOL><DEDENT><DEDENT>elif isinstance(cmdpkl, dict):<EOL><INDENT>cmd = cmdpkl<EOL><DEDENT>cpdict['<STR_LIT>'] = {}<EOL>cplist_mags = cmd['<STR_LIT>']<EOL>cplist_colors = cmd['<STR_LIT>']<EOL>for c1, c2, ym, ind in zip(cmd['<STR_LIT>'],<EOL>cmd['<STR_LIT>'],<EOL>cmd['<STR_LIT>'],<EOL>range(len(cmd['<STR_LIT>']))):<EOL><INDENT>if (c1 in cpdict['<STR_LIT>'] and<EOL>cpdict['<STR_LIT>'][c1] is not None):<EOL><INDENT>c1mag = cpdict['<STR_LIT>'][c1]<EOL><DEDENT>else:<EOL><INDENT>c1mag = np.nan<EOL><DEDENT>if (c2 in cpdict['<STR_LIT>'] and<EOL>cpdict['<STR_LIT>'][c2] is not None):<EOL><INDENT>c2mag = cpdict['<STR_LIT>'][c2]<EOL><DEDENT>else:<EOL><INDENT>c2mag = np.nan<EOL><DEDENT>if (ym in cpdict['<STR_LIT>'] and<EOL>cpdict['<STR_LIT>'][ym] is not None):<EOL><INDENT>ymmag = cpdict['<STR_LIT>'][ym]<EOL><DEDENT>else:<EOL><INDENT>ymmag = np.nan<EOL><DEDENT>if (require_cmd_magcolor and<EOL>not (np.isfinite(c1mag) and<EOL>np.isfinite(c2mag) and<EOL>np.isfinite(ymmag))):<EOL><INDENT>LOGWARNING(\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\" %<EOL>(c1, c2, ym, cpdict['<STR_LIT>']))<EOL>continue<EOL><DEDENT>try:<EOL><INDENT>thiscmd_title = r'<STR_LIT>' % (CMD_LABELS[c1],<EOL>CMD_LABELS[c2],<EOL>CMD_LABELS[ym])<EOL>fig = plt.figure(figsize=(<NUM_LIT:10>,<NUM_LIT:8>))<EOL>plt.plot(cplist_colors[:,ind],<EOL>cplist_mags[:,ind],<EOL>rasterized=True,<EOL>marker='<STR_LIT:o>',<EOL>linestyle='<STR_LIT:none>',<EOL>mew=<NUM_LIT:0>,<EOL>ms=<NUM_LIT:3>)<EOL>plt.plot([c1mag - c2mag], [ymmag],<EOL>ms=<NUM_LIT:20>,<EOL>color='<STR_LIT>',<EOL>marker='<STR_LIT:*>',<EOL>mew=<NUM_LIT:0>)<EOL>plt.xlabel(r'<STR_LIT>' % (CMD_LABELS[c1], CMD_LABELS[c2]))<EOL>plt.ylabel(r'<STR_LIT>' % CMD_LABELS[ym])<EOL>plt.title('<STR_LIT>' % (cpdict['<STR_LIT>'], thiscmd_title))<EOL>plt.gca().invert_yaxis()<EOL>cmdpng = StrIO()<EOL>plt.savefig(cmdpng, bbox_inches='<STR_LIT>',<EOL>pad_inches=<NUM_LIT:0.0>, format='<STR_LIT>')<EOL>cmdpng.seek(<NUM_LIT:0>)<EOL>cmdb64 = base64.b64encode(cmdpng.read())<EOL>cmdpng.close()<EOL>plt.close('<STR_LIT:all>')<EOL>plt.gcf().clear()<EOL>cpdict['<STR_LIT>']['<STR_LIT>' % (c1,c2,ym)] = cmdb64<EOL>if save_cmd_pngs:<EOL><INDENT>if isinstance(cpx, str):<EOL><INDENT>outpng = os.path.join(os.path.dirname(cpx),<EOL>'<STR_LIT>' %<EOL>(cpdict['<STR_LIT>'],<EOL>c1,c2,ym))<EOL><DEDENT>else:<EOL><INDENT>outpng = '<STR_LIT>' % (cpdict['<STR_LIT>'],<EOL>c1,c2,ym)<EOL><DEDENT>_base64_to_file(cmdb64, outpng)<EOL><DEDENT><DEDENT>except Exception as e:<EOL><INDENT>LOGEXCEPTION('<STR_LIT>' %<EOL>(c1, c2, ym, cmdpkl))<EOL>continue<EOL><DEDENT><DEDENT>if isinstance(cpx, str):<EOL><INDENT>cpf = _write_checkplot_picklefile(cpdict, outfile=cpx, protocol=<NUM_LIT:4>)<EOL>return cpf<EOL><DEDENT>elif isinstance(cpx, dict):<EOL><INDENT>return cpdict<EOL><DEDENT>", "docstring": "This adds CMD figures to a checkplot dict or pickle.\n\n    Looks up the CMDs in `cmdpkl`, adds the object from `cpx` as a gold(-ish)\n    star in the plot, and then saves the figure to a base64 encoded PNG, which\n    can then be read and used by the `checkplotserver`.\n\n    Parameters\n    ----------\n\n    cpx : str or dict\n        This is the input checkplot pickle or dict to add the CMD to.\n\n    cmdpkl : str or dict\n        The CMD pickle generated by the `colormagdiagram_cplist` or\n        `colormagdiagram_cpdir` functions above, or the dict produced by reading\n        this pickle in.\n\n    require_cmd_magcolor : bool\n        If this is True, a CMD plot will not be made if the color and mag keys\n        required by the CMD are not present or are nan in this checkplot's\n        objectinfo dict.\n\n    save_cmd_png : bool\n        If this is True, then will save the CMD plots that were generated and\n        added back to the checkplotdict as PNGs to the same directory as\n        `cpx`. If `cpx` is a dict, will save them to the current working\n        directory.\n\n    Returns\n    -------\n\n    str or dict\n        If `cpx` was a str filename of checkplot pickle, this will return that\n        filename to indicate that the CMD was added to the file. If `cpx` was a\n        checkplotdict, this will return the checkplotdict with a new key called\n        'colormagdiagram' containing the base64 encoded PNG binary streams of\n        all CMDs generated.", "id": "f14703:m5"}
{"signature": "def assertEqual(self, first, second, msg=None):", "body": "assertion_func = self._getAssertEqualityFunc(first, second)<EOL>assertion_func(first, second, msg=msg)<EOL>", "docstring": "Fail if the two objects are unequal as determined by the '=='\n           operator.", "id": "f16396:c5:m29"}
{"signature": "def get_provider(self, id):", "body": "if id in self.providers:<EOL><INDENT>return self.providers.get(id, False)<EOL><DEDENT>elif is_uri(id) and id in self.concept_scheme_uri_map:<EOL><INDENT>return self.providers.get(self.concept_scheme_uri_map[id], False)<EOL><DEDENT>return False<EOL>", "docstring": "Get a provider by id or :term:`uri`.\n\n:param str id: The identifier for the provider. This can either be the\n    id with which it was registered or the :term:`uri` of the conceptscheme\n    that the provider services.\n:returns: A :class:`skosprovider.providers.VocabularyProvider`\n    or `False` if the id or uri is unknown.", "id": "f13802:c1:m3"}
{"signature": "def match_complex_metapath(graph, node, complex_metapath):", "body": "raise NotImplementedError<EOL>", "docstring": "Matches a complex metapath starting at the given node\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param tuple node: A BEL node\n    :param list[str] complex_metapath: An iterable of alternating BEL nodes and relations\n    :return: An iterable over paths from the node matching the metapath\n    :rtype: iter[tuple]", "id": "f9369:m4"}
{"signature": "def _start_proc(self):", "body": "assert '<STR_LIT>' not in dir(self) or self._proc is None<EOL>try:<EOL><INDENT>self._proc = Popen(shlex.split(self._command), stdin=PIPE, stdout=PIPE, bufsize=<NUM_LIT:0>, universal_newlines=True)<EOL><DEDENT>except OSError as e:<EOL><INDENT>print(e, \"<STR_LIT>\")<EOL>raise Z3NotFoundError  <EOL><DEDENT>for cfg in self._init:<EOL><INDENT>self._send(cfg)<EOL><DEDENT>", "docstring": "Spawns z3 solver process", "id": "f16986:c1:m2"}
{"signature": "def _get_info (self, fontname, font_class, sym, fontsize, dpi):", "body": "key = fontname, sym, fontsize, dpi<EOL>tup = self.glyphd.get(key)<EOL>if tup is not None:<EOL><INDENT>return tup<EOL><DEDENT>if (fontname == '<STR_LIT>' and<EOL>(len(sym) > <NUM_LIT:1> or<EOL>not unicodedata.category(str(sym)).startswith(\"<STR_LIT:L>\"))):<EOL><INDENT>fontname = '<STR_LIT>'<EOL><DEDENT>found_symbol = False<EOL>if sym in latex_to_standard:<EOL><INDENT>fontname, num = latex_to_standard[sym]<EOL>glyph = chr(num)<EOL>found_symbol = True<EOL><DEDENT>elif len(sym) == <NUM_LIT:1>:<EOL><INDENT>glyph = sym<EOL>num = ord(glyph)<EOL>found_symbol = True<EOL><DEDENT>else:<EOL><INDENT>warn(\"<STR_LIT>\" % sym,<EOL>MathTextWarning)<EOL><DEDENT>slanted = (fontname == '<STR_LIT>')<EOL>font = self._get_font(fontname)<EOL>if found_symbol:<EOL><INDENT>try:<EOL><INDENT>symbol_name = font.get_name_char(glyph)<EOL><DEDENT>except KeyError:<EOL><INDENT>warn(\"<STR_LIT>\" %<EOL>(font.postscript_name, sym),<EOL>MathTextWarning)<EOL>found_symbol = False<EOL><DEDENT><DEDENT>if not found_symbol:<EOL><INDENT>glyph = sym = '<STR_LIT:?>'<EOL>num = ord(glyph)<EOL>symbol_name = font.get_name_char(glyph)<EOL><DEDENT>offset = <NUM_LIT:0><EOL>scale = <NUM_LIT> * fontsize<EOL>xmin, ymin, xmax, ymax = [val * scale<EOL>for val in font.get_bbox_char(glyph)]<EOL>metrics = Bunch(<EOL>advance  = font.get_width_char(glyph) * scale,<EOL>width    = font.get_width_char(glyph) * scale,<EOL>height   = font.get_height_char(glyph) * scale,<EOL>xmin = xmin,<EOL>xmax = xmax,<EOL>ymin = ymin+offset,<EOL>ymax = ymax+offset,<EOL>iceberg = ymax + offset,<EOL>slanted = slanted<EOL>)<EOL>self.glyphd[key] = Bunch(<EOL>font            = font,<EOL>fontsize        = fontsize,<EOL>postscript_name = font.get_fontname(),<EOL>metrics         = metrics,<EOL>symbol_name     = symbol_name,<EOL>num             = num,<EOL>glyph           = glyph,<EOL>offset          = offset<EOL>)<EOL>return self.glyphd[key]<EOL>", "docstring": "load the cmfont, metrics and glyph with caching", "id": "f17193:c14:m2"}
{"signature": "def perfiles_consumo_en_intervalo(t0, tf):", "body": "t_ini = pd.Timestamp(t0)<EOL>t_fin = pd.Timestamp(tf)<EOL>assert (t_fin > t_ini)<EOL>marca_fin = '<STR_LIT>'.format(t_fin)<EOL>marca_ini = '<STR_LIT>'.format(t_ini)<EOL>if marca_ini == marca_fin:<EOL><INDENT>perfiles = get_data_perfiles_finales_mes(t_ini)<EOL><DEDENT>else:<EOL><INDENT>dates = pd.DatetimeIndex(start=t_ini.replace(day=<NUM_LIT:1>),<EOL>end=t_fin.replace(day=<NUM_LIT:1>), freq='<STR_LIT>')<EOL>perfiles = pd.concat([get_data_perfiles_finales_mes(t) for t in dates])<EOL><DEDENT>return perfiles.loc[t_ini:t_fin].iloc[:-<NUM_LIT:1>]<EOL>", "docstring": "Descarga de perfiles horarios para un intervalo dado\n    Con objeto de calcular el precio medio ponderado de aplicaci\u00f3n para dicho intervalo.\n    :return: perfiles_intervalo\n    :rtype: pd.Dataframe", "id": "f14216:m3"}
