{"signature": "def handle_data(self, d):", "body": "self.fed.append(d)<EOL>", "docstring": "Return representation of pure text data.", "id": "f7888:c0:m1"}
{"signature": "def count_sequences(infile):", "body": "seq_reader = sequences.file_reader(infile)<EOL>n = <NUM_LIT:0><EOL>for seq in seq_reader:<EOL><INDENT>n += <NUM_LIT:1><EOL><DEDENT>return n<EOL>", "docstring": "Returns the number of sequences in a file", "id": "f405:m3"}
{"signature": "def __init__(self, logic=None, weight=None, simple=None, simple_weight=None, extract_as=None, extract_all=None,<EOL>extract_when_missing=None, tags=None, length=None, offset=None, doi=None, isbn=None, issn=None,<EOL>url=None, title=None, publisher=None, journal=None, volume=None, issue=None, year=None,<EOL>figure=None, table=None, pages=None, authors=None, editors=None, affiliations=None,<EOL>acknowledgements=None, references=None, query=None, **kwargs):", "body": "super(ReferenceQuery, self).__init__(<EOL>logic=logic, weight=weight, simple=simple, simple_weight=simple_weight, extract_as=extract_as,<EOL>extract_all=extract_all, extract_when_missing=extract_when_missing, tags=tags, length=length,<EOL>offset=offset, **kwargs)<EOL>self._doi = None<EOL>self.doi = doi<EOL>self._isbn = None<EOL>self.isbn = isbn<EOL>self._issn = None<EOL>self.issn = issn<EOL>self._url = None<EOL>self.url = url<EOL>self._title = None<EOL>self.title = title<EOL>self._publisher = None<EOL>self.publisher = publisher<EOL>self._journal = None<EOL>self.journal = journal<EOL>self._volume = None<EOL>self.volume = volume<EOL>self._issue = None<EOL>self.issue = issue<EOL>self._year = None<EOL>self.year = year<EOL>self._figure = None<EOL>self.figure = figure<EOL>self._table = None<EOL>self.table = table<EOL>self._pages = None<EOL>self.pages = pages<EOL>self._authors = None<EOL>self.authors = authors<EOL>self._editors = None<EOL>self.editors = editors<EOL>self._affiliations = None<EOL>self.affiliations = affiliations<EOL>self._acknowledgements = None<EOL>self.acknowledgements = acknowledgements<EOL>self._references = None<EOL>self.references = references<EOL>self._query = None<EOL>self.query = query<EOL>", "docstring": "Constructor.\n\n:param logic: Logic for this filter. Must be equal to one of \"MUST\", \"MUST_NOT\", \"SHOULD\", or \"OPTIONAL\".\n:param weight: Weight of the query.\n:param simple: String with the simple query to run against all fields.\n:param simple_weight: Dictionary of relative paths to their weights for simple queries.\n:param extract_as: String with the alias to save this field under.\n:param extract_all: Boolean setting whether all values in an array should be extracted.\n:param extract_when_missing: Any valid JSON-supported object or PIF object. This value is returned when a value is missing that should be extracted (and the overall query is still satisfied).\n:param tags: One or more :class:`FieldQuery` operations against the tags field.\n:param length: One or more :class:`FieldQuery` operations against the length field.\n:param offset: One or more :class:`FieldQuery` operations against the offset field.\n:param doi: One or more :class:`FieldQuery` operations against the doi field.\n:param isbn: One or more :class:`FieldQuery` operations against the isbn field.\n:param issn: One or more :class:`FieldQuery` operations against the issn field.\n:param url: One or more :class:`FieldQuery` operations against the url field.\n:param title: One or more :class:`FieldQuery` operations against the title field.\n:param publisher: One or more :class:`FieldQuery` operations against the publisher field.\n:param journal: One or more :class:`FieldQuery` operations against the journal field.\n:param volume: One or more :class:`FieldQuery` operations against the volume field.\n:param issue: One or more :class:`FieldQuery` operations against the issue field.\n:param year: One or more :class:`FieldQuery` operations against the year field.\n:param figure: One or more :class:`DisplayItemQuery` operations against the figure field.\n:param table: One or more :class:`DisplayItemQuery` operations against the table field.\n:param pages: One or more :class:`PagesQuery` operations against the pages field.\n:param authors: One or more :class:`NameQuery` operations against the authors field.\n:param editors: One or more :class:`NameQuery` operations against the editors field.\n:param affiliations: One or more :class:`FieldQuery` operations against the affiliations field.\n:param acknowledgements: One or more :class:`FieldQuery` operations against the acknowledgements field.\n:param references: One or more :class:`ReferenceQuery` operations against the references field.\n:param query: One or more :class:`ReferenceQuery` objects with nested queries.", "id": "f3557:c0:m0"}
{"signature": "def skipIf(condition, reason):", "body": "if condition:<EOL><INDENT>return skip(reason)<EOL><DEDENT>return _id<EOL>", "docstring": "Skip a test if the condition is true.", "id": "f16396:m2"}
{"signature": "def Realize(self, waitTime: float = OPERATION_WAIT_TIME) -> bool:", "body": "ret = self.pattern.Realize() == S_OK<EOL>time.sleep(waitTime)<EOL>return ret<EOL>", "docstring": "Call IUIAutomationVirtualizedItemPattern::Realize.\nCreate a full UI Automation element for a virtualized item.\nwaitTime: float.\nReturn bool, True if succeed otherwise False.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationvirtualizeditempattern-realize", "id": "f1782:c76:m1"}
{"signature": "def dict_format_type(d, source, formatter, include_list=True):", "body": "if not isinstance(d, dict):<EOL><INDENT>if isinstance(d, source):<EOL><INDENT>return formatter(d)<EOL><DEDENT>else:<EOL><INDENT>return d<EOL><DEDENT><DEDENT>else:<EOL><INDENT>dd = dict()<EOL>for key, value in d.items():<EOL><INDENT>if include_list and isinstance(value, list):<EOL><INDENT>dd[key] = [dict_format_type(i, source, formatter) for i in value]<EOL><DEDENT>elif isinstance(value, dict):<EOL><INDENT>dd[key] = dict_format_type(value, source, formatter)<EOL><DEDENT>elif isinstance(value, source):<EOL><INDENT>dd[key] = formatter(value)<EOL><DEDENT>else:<EOL><INDENT>dd[key] = value<EOL><DEDENT><DEDENT>return dd<EOL><DEDENT>", "docstring": "Replace the values of a dict with certain type to other values\n:param d: the dictionary\n:param source: the source type, e.g., int\n:param formatter: the formatter method, e.g., return the string format of an int\n:param include_list: whether list should be formatted, otherwise list will be considered as source type\n:return: formatted dictionary", "id": "f14774:m9"}
{"signature": "@task(help=dict(<EOL>verbose=\"<STR_LIT>\",<EOL>pypi=\"<STR_LIT>\",<EOL>))<EOL>def bump(ctx, verbose=False, pypi=False):", "body": "cfg = config.load()<EOL>scm = scm_provider(cfg.project_root, commit=False, ctx=ctx)<EOL>if not scm.workdir_is_clean():<EOL><INDENT>notify.warning(\"<STR_LIT>\")<EOL><DEDENT>pep440 = scm.pep440_dev_version(verbose=verbose, non_local=pypi)<EOL>setup_cfg = cfg.rootjoin('<STR_LIT>')<EOL>if not pep440:<EOL><INDENT>notify.info(\"<STR_LIT>\")<EOL><DEDENT>elif os.path.exists(setup_cfg):<EOL><INDENT>with io.open(setup_cfg, encoding='<STR_LIT:utf-8>') as handle:<EOL><INDENT>data = handle.readlines()<EOL><DEDENT>changed = False<EOL>for i, line in enumerate(data):<EOL><INDENT>if re.match(r\"<STR_LIT>\", line):<EOL><INDENT>verb, _ = data[i].split('<STR_LIT:=>', <NUM_LIT:1>)<EOL>data[i] = '<STR_LIT>'.format(verb, pep440)<EOL>changed = True<EOL><DEDENT><DEDENT>if changed:<EOL><INDENT>notify.info(\"<STR_LIT>\")<EOL>with io.open(setup_cfg, '<STR_LIT:w>', encoding='<STR_LIT:utf-8>') as handle:<EOL><INDENT>handle.write('<STR_LIT>'.join(data))<EOL><DEDENT><DEDENT>else:<EOL><INDENT>notify.warning(\"<STR_LIT>\")<EOL><DEDENT><DEDENT>else:<EOL><INDENT>notify.warning(\"<STR_LIT>\")<EOL><DEDENT>if os.path.exists(setup_cfg):<EOL><INDENT>egg_info = shell.capture(\"<STR_LIT>\", echo=True if verbose else None)<EOL>for line in egg_info.splitlines():<EOL><INDENT>if line.endswith('<STR_LIT>'):<EOL><INDENT>pkg_info_file = line.split(None, <NUM_LIT:1>)[<NUM_LIT:1>]<EOL>with io.open(pkg_info_file, encoding='<STR_LIT:utf-8>') as handle:<EOL><INDENT>notify.info('<STR_LIT:\\n>'.join(i for i in handle.readlines() if i.startswith('<STR_LIT>')).strip())<EOL><DEDENT><DEDENT><DEDENT>ctx.run(\"<STR_LIT>\", echo=True if verbose else None)<EOL><DEDENT>", "docstring": "Bump a development version.", "id": "f3283:m1"}
{"signature": "def ch(c):", "body": "<EOL>istdout.info(c)<EOL>", "docstring": "print one or more characters without a newline at the end\n\n    example --\n        for x in range(1000):\n            echo.ch(\".\")\n\n    c -- string -- the chars that will be output", "id": "f6248:m6"}
{"signature": "def getLocalizedName(self):", "body": "return self._getLocalizedName()<EOL>", "docstring": "Return the localized name of the application.", "id": "f10331:c1:m13"}
{"signature": "def _get_metadata(self):", "body": "self.description = '<STR_LIT>'<EOL>self.meta = {}<EOL>self.title = '<STR_LIT>'<EOL>descfile = splitext(self.src_path)[<NUM_LIT:0>] + '<STR_LIT>'<EOL>if isfile(descfile):<EOL><INDENT>meta = read_markdown(descfile)<EOL>for key, val in meta.items():<EOL><INDENT>setattr(self, key, val)<EOL><DEDENT><DEDENT>", "docstring": "Get image metadata from filename.md: title, description, meta.", "id": "f13464:c0:m7"}
{"signature": "@api.check(<NUM_LIT:2>, \"<STR_LIT>\")<EOL>def win_set_title(title, new_title, **kwargs):", "body": "text = kwargs.get(\"<STR_LIT:text>\", \"<STR_LIT>\")<EOL>ret = AUTO_IT.AU3_WinSetTitle(LPCWSTR(title), LPCWSTR(text),<EOL>LPCWSTR(new_title))<EOL>return ret<EOL>", "docstring": ":param title:\n:param new_title:\n:param kwargs:\n:return:", "id": "f5587:m37"}
{"signature": "def __get_amount_color(self, node_indexes, color_number):", "body": "color_counter = <NUM_LIT:0>;  <EOL>for index in node_indexes:<EOL><INDENT>if (self.__coloring[index] == color_number):<EOL><INDENT>color_counter += <NUM_LIT:1>;<EOL><DEDENT><DEDENT>return color_counter<EOL>", "docstring": "!\n        @brief Countes how many nodes has color 'color_number'.\n\n        @param[in] node_indexes (list): Indexes of graph nodes for checking.\n        @param[in] color_number (uint): Number of color that is searched in nodes.\n\n        @return (uint) Number found nodes with the specified color 'color_number'.", "id": "f15708:c0:m3"}
{"signature": "def delete(self, project_id):", "body": "self.logger.debug('<STR_LIT>' + project_id)<EOL>url = '<STR_LIT>' % {<EOL>'<STR_LIT>': self.base_url, '<STR_LIT>': project_id<EOL>}<EOL>r = self.gbdx_connection.delete(url)<EOL>r.raise_for_status()<EOL>", "docstring": "Deletes a project by id\n\nArgs:\n     project_id: The project id to delete\n\nReturns:\n     Nothing", "id": "f7051:c1:m3"}
{"signature": "def put(self, coro):", "body": "<EOL>assert asyncio.iscoroutine(coro)<EOL>self._queue.put_nowait(coro)<EOL>", "docstring": "Put a coroutine in the queue to be executed.", "id": "f10036:c2:m1"}
{"signature": "def appendOps(self, ops, append_to=None):", "body": "if isinstance(ops, list):<EOL><INDENT>self.ops.extend(ops)<EOL><DEDENT>else:<EOL><INDENT>self.ops.append(ops)<EOL><DEDENT>parent = self.parent<EOL>if parent:<EOL><INDENT>parent._set_require_reconstruction()<EOL><DEDENT>", "docstring": "Append op(s) to the transaction builder\n\n            :param list ops: One or a list of operations", "id": "f8254:c0:m6"}
{"signature": "def _handleAuth(fn):", "body": "@functools.wraps(fn)<EOL>def wrapped(*args, **kwargs):<EOL><INDENT>from yotta.lib import auth<EOL>interactive = globalconf.get('<STR_LIT>')<EOL>try:<EOL><INDENT>return fn(*args, **kwargs)<EOL><DEDENT>except requests.exceptions.HTTPError as e:<EOL><INDENT>if e.response.status_code == requests.codes.unauthorized: <EOL><INDENT>logger.debug('<STR_LIT>', fn)<EOL>auth.authorizeUser(provider=None, interactive=interactive)<EOL>if interactive:<EOL><INDENT>logger.debug('<STR_LIT>')<EOL>return fn(*args, **kwargs)<EOL><DEDENT><DEDENT>raise<EOL><DEDENT><DEDENT>return wrapped<EOL>", "docstring": "Decorator to re-try API calls after asking the user for authentication.", "id": "f13542:m5"}
{"signature": "@property<EOL><INDENT>def uuid(self):<DEDENT>", "body": "return uuid.UUID(str(self._props.Get(_DESCRIPTOR_INTERFACE, '<STR_LIT>')))<EOL>", "docstring": "Return the UUID of this GATT descriptor.", "id": "f9600:c2:m1"}
{"signature": "def app_login(self, username, password):", "body": "path = '<STR_LIT>'<EOL>data = {<EOL>'<STR_LIT:username>': username,<EOL>'<STR_LIT:password>': password,<EOL>}<EOL>resp = self.app.post(path, data)<EOL>return resp<EOL>", "docstring": "Logins in to the app using (a stub) accounts.", "id": "f15192:c1:m11"}
{"signature": "def float_unpack(Q, size, le):", "body": "if size == <NUM_LIT:8>:<EOL><INDENT>MIN_EXP = -<NUM_LIT>  <EOL>MAX_EXP = <NUM_LIT>   <EOL>MANT_DIG = <NUM_LIT>    <EOL>BITS = <NUM_LIT:64><EOL><DEDENT>elif size == <NUM_LIT:4>:<EOL><INDENT>MIN_EXP = -<NUM_LIT>   <EOL>MAX_EXP = <NUM_LIT>    <EOL>MANT_DIG = <NUM_LIT>    <EOL>BITS = <NUM_LIT:32><EOL><DEDENT>else:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if Q >> BITS:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>sign = Q >> BITS - <NUM_LIT:1><EOL>exp = (Q & ((<NUM_LIT:1> << BITS - <NUM_LIT:1>) - (<NUM_LIT:1> << MANT_DIG - <NUM_LIT:1>))) >> MANT_DIG - <NUM_LIT:1><EOL>mant = Q & ((<NUM_LIT:1> << MANT_DIG - <NUM_LIT:1>) - <NUM_LIT:1>)<EOL>if exp == MAX_EXP - MIN_EXP + <NUM_LIT:2>:<EOL><INDENT>result = float('<STR_LIT>') if mant else float('<STR_LIT>')<EOL><DEDENT>elif exp == <NUM_LIT:0>:<EOL><INDENT>result = math.ldexp(float(mant), MIN_EXP - MANT_DIG)<EOL><DEDENT>else:<EOL><INDENT>mant += <NUM_LIT:1> << MANT_DIG - <NUM_LIT:1><EOL>result = math.ldexp(float(mant), exp + MIN_EXP - MANT_DIG - <NUM_LIT:1>)<EOL><DEDENT>return -result if sign else result<EOL>", "docstring": "Convert a 32-bit or 64-bit integer created\n    by float_pack into a Python float.", "id": "f16488:m12"}
{"signature": "def _refresh(self, http):", "body": "try:<EOL><INDENT>self._retrieve_info(http)<EOL>self.access_token, self.token_expiry = _metadata.get_token(<EOL>http, service_account=self.service_account_email)<EOL><DEDENT>except http_client.HTTPException as err:<EOL><INDENT>raise client.HttpAccessTokenRefreshError(str(err))<EOL><DEDENT>", "docstring": "Refreshes the access token.\n\n        Skip all the storage hoops and just refresh using the API.\n\n        Args:\n            http: an object to be used to make HTTP requests.\n\n        Raises:\n            HttpAccessTokenRefreshError: When the refresh fails.", "id": "f2458:c0:m5"}
{"signature": "def add_subtract(st, max_iter=<NUM_LIT:7>, max_npart='<STR_LIT>', max_mem=<NUM_LIT>,<EOL>always_check_remove=False, **kwargs):", "body": "if max_npart == '<STR_LIT>':<EOL><INDENT>max_npart = <NUM_LIT> * st.obj_get_positions().shape[<NUM_LIT:0>]<EOL><DEDENT>total_changed = <NUM_LIT:0><EOL>_change_since_opt = <NUM_LIT:0><EOL>removed_poses = []<EOL>added_poses0 = []<EOL>added_poses = []<EOL>nr = <NUM_LIT:1>  <EOL>for _ in range(max_iter):<EOL><INDENT>if (nr != <NUM_LIT:0>) or (always_check_remove):<EOL><INDENT>nr, rposes = remove_bad_particles(st, **kwargs)<EOL><DEDENT>na, aposes = add_missing_particles(st, **kwargs)<EOL>current_changed = na + nr<EOL>removed_poses.extend(rposes)<EOL>added_poses0.extend(aposes)<EOL>total_changed += current_changed<EOL>_change_since_opt += current_changed<EOL>if current_changed == <NUM_LIT:0>:<EOL><INDENT>break<EOL><DEDENT>elif _change_since_opt > max_npart:<EOL><INDENT>_change_since_opt *= <NUM_LIT:0><EOL>CLOG.info('<STR_LIT>')<EOL>opt.do_levmarq(st, opt.name_globals(st, remove_params=st.get(<EOL>'<STR_LIT>').params), max_iter=<NUM_LIT:1>, run_length=<NUM_LIT:4>, num_eig_dirs=<NUM_LIT:3>,<EOL>max_mem=max_mem, eig_update_frequency=<NUM_LIT:2>, rz_order=<NUM_LIT:0>,<EOL>use_accel=True)<EOL>CLOG.info('<STR_LIT>'.format(st.error))<EOL><DEDENT><DEDENT>for p in added_poses0:<EOL><INDENT>i = st.obj_closest_particle(p)<EOL>opt.do_levmarq_particles(st, np.array([i]), max_iter=<NUM_LIT:2>, damping=<NUM_LIT>)<EOL>added_poses.append(st.obj_get_positions()[i])<EOL><DEDENT>return total_changed, np.array(removed_poses), np.array(added_poses)<EOL>", "docstring": "Automatically adds and subtracts missing & extra particles.\n\nOperates by removing bad particles then adding missing particles on\nrepeat, until either no particles are added/removed or after `max_iter`\nattempts.\n\nParameters\n----------\nst: :class:`peri.states.State`\n    The state to add and subtract particles to.\nmax_iter : Int, optional\n    The maximum number of add-subtract loops to use. Default is 7.\n    Terminates after either max_iter loops or when nothing has changed.\nmax_npart : Int or 'calc', optional\n    The maximum number of particles to add before optimizing the non-psf\n    globals. Default is ``'calc'``, which uses 5% of the initial number\n    of particles.\nmax_mem : Int, optional\n    The maximum memory to use for optimization after adding max_npart\n    particles. Default is 2e8.\nalways_check_remove : Bool, optional\n    Set to True to always check whether to remove particles. If ``False``,\n    only checks for removal while particles were removed on the previous\n    attempt. Default is False.\n\nOther Parameters\n----------------\ninvert : Bool, optional\n    ``True`` if the particles are dark on a bright background, ``False``\n    if they are bright on a dark background. Default is ``True``.\nmin_rad : Float, optional\n    Particles with radius below ``min_rad`` are automatically deleted.\n    Default is ``'calc'`` = median rad - 25* radius std.\nmax_rad : Float, optional\n    Particles with radius above ``max_rad`` are automatically deleted.\n    Default is ``'calc'`` = median rad + 15* radius std, but you should\n    change this for your particle sizes.\n\nmin_edge_dist : Float, optional\n    Particles closer to the edge of the padded image than this are\n    automatically deleted. Default is 2.0.\ncheck_rad_cutoff : 2-element float list.\n    Particles with ``radii < check_rad_cutoff[0]`` or ``> check...[1]``\n    are checked if they should be deleted (not automatic). Default is\n    ``[3.5, 15]``.\ncheck_outside_im : Bool, optional\n    Set to True to check whether to delete particles whose positions are\n    outside the un-padded image.\n\nrad : Float, optional\n    The initial radius for added particles; added particles radii are\n    not fit until the end of ``add_subtract``. Default is ``'calc'``,\n    which uses the median radii of active particles.\n\ntries : Int, optional\n    The number of particles to attempt to remove or add, per iteration.\n    Default is 50.\n\nim_change_frac : Float, optional\n    How good the change in error needs to be relative to the change in\n    the difference image. Default is 0.2; i.e. if the error does not\n    decrease by 20% of the change in the difference image, do not add\n    the particle.\n\nmin_derr : Float, optional\n    The minimum change in the state's error to keep a particle in the\n    image. Default is ``'3sig'`` which uses ``3*st.sigma``.\n\ndo_opt : Bool, optional\n    Set to False to avoid optimizing particle positions after adding.\nminmass : Float, optional\n    The minimum mass for a particle to be identified as a feature,\n    as used by trackpy. Defaults to a decent guess.\n\nuse_tp : Bool, optional\n    Set to True to use trackpy to find missing particles inside the\n    image. Not recommended since trackpy deliberately cuts out particles\n    at the edge of the image. Default is ``False``.\n\nReturns\n-------\ntotal_changed : Int\n    The total number of adds and subtracts done on the data. Not the\n    same as ``changed_inds.size`` since the same particle or particle\n    index can be added/subtracted multiple times.\nadded_positions : [N_added,3] numpy.ndarray\n    The positions of particles that have been added at any point in the\n    add-subtract cycle.\nremoved_positions : [N_added,3] numpy.ndarray\n    The positions of particles that have been removed at any point in\n    the add-subtract cycle.\n\nNotes\n------\nOccasionally after the intial featuring a cluster of particles is\nfeatured as 1 big particle. To fix these mistakes, it helps to set\nmax_rad to a physical value. This removes the big particle and allows\nit to be re-featured by (several passes of) the adds.\n\nThe added/removed positions returned are whether or not the position\nhas been added or removed ever. It's possible that a position is\nadded, then removed during a later iteration.", "id": "f5774:m7"}
{"signature": "def Timestamp(value, _divisor=<NUM_LIT:1.>, tz=UTC, encoding=None):", "body": "value = Float(value, encoding)<EOL>if value is not None:<EOL><INDENT>value = value / _divisor<EOL>return datetime.fromtimestamp(value, tz)<EOL><DEDENT>return None<EOL>", "docstring": "Parse a value as a POSIX timestamp in seconds.\n\n:type  value: `unicode` or `bytes`\n:param value: Text value to parse, which should be the number of seconds\n    since the epoch.\n\n:type  _divisor: `float`\n:param _divisor: Number to divide the value by.\n\n:type  tz: `tzinfo`\n:param tz: Timezone, defaults to UTC.\n\n:type  encoding: `bytes`\n:param encoding: Encoding to treat `bytes` values as, defaults to\n    ``utf-8``.\n\n:rtype: `datetime.datetime`\n:return: Parsed datetime or ``None`` if ``value`` could not be parsed.", "id": "f9836:m8"}
{"signature": "def run(command, parser, cl_args, unknown_args):", "body": "try:<EOL><INDENT>clusters = tracker_access.get_clusters()<EOL><DEDENT>except:<EOL><INDENT>Log.error(\"<STR_LIT>\", cl_args[\"<STR_LIT>\"])<EOL>return False<EOL><DEDENT>print('<STR_LIT>')<EOL>for cluster in clusters:<EOL><INDENT>print('<STR_LIT>' % cluster)<EOL><DEDENT>return True<EOL>", "docstring": "run command", "id": "f7393:m1"}
{"signature": "def tzname(self):", "body": "if self._tzinfo is None:<EOL><INDENT>return None<EOL><DEDENT>name = self._tzinfo.tzname(self)<EOL>_check_tzname(name)<EOL>return name<EOL>", "docstring": "Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.", "id": "f16496:c4:m25"}
{"signature": "def parse_comment_telemetry(text):", "body": "parsed = {}<EOL>match = re.findall(r\"<STR_LIT>\", text)<EOL>if match and len(match[<NUM_LIT:0>][<NUM_LIT:1>]) % <NUM_LIT:2> == <NUM_LIT:0>:<EOL><INDENT>text, telemetry, post = match[<NUM_LIT:0>]<EOL>text += post<EOL>temp = [<NUM_LIT:0>] * <NUM_LIT:7><EOL>for i in range(<NUM_LIT:7>):<EOL><INDENT>temp[i] = base91.to_decimal(telemetry[i*<NUM_LIT:2>:i*<NUM_LIT:2>+<NUM_LIT:2>])<EOL><DEDENT>parsed.update({<EOL>'<STR_LIT>': {<EOL>'<STR_LIT>': temp[<NUM_LIT:0>],<EOL>'<STR_LIT>': temp[<NUM_LIT:1>:<NUM_LIT:6>]<EOL>}<EOL>})<EOL>if temp[<NUM_LIT:6>] != '<STR_LIT>':<EOL><INDENT>parsed['<STR_LIT>'].update({<EOL>'<STR_LIT>': \"<STR_LIT>\".format(temp[<NUM_LIT:6>] & <NUM_LIT>)[::-<NUM_LIT:1>]<EOL>})<EOL><DEDENT><DEDENT>return (text, parsed)<EOL>", "docstring": "Looks for base91 telemetry found in comment field\nReturns [remaining_text, telemetry]", "id": "f12498:m0"}
{"signature": "def _get_key_for_index(self, indexedField, val):", "body": "<EOL>if hasattr(indexedField, '<STR_LIT>'):<EOL><INDENT>val = indexedField.toIndex(val)<EOL><DEDENT>else:<EOL><INDENT>val = self.fields[indexedField].toIndex(val)<EOL><DEDENT>return '<STR_LIT>'.join( [INDEXED_REDIS_PREFIX, self.keyName, '<STR_LIT>', indexedField, '<STR_LIT::>', val] )<EOL>", "docstring": "_get_key_for_index - Returns the key name that would hold the indexes on a value\nInternal - does not validate that indexedFields is actually indexed. Trusts you. Don't let it down.\n\n@param indexedField - string of field name\n@param val - Value of field\n\n@return - Key name string, potentially hashed.", "id": "f4156:c2:m9"}
{"signature": "def get_http_object(*args, **kwargs):", "body": "return httplib2.Http(*args, **kwargs)<EOL>", "docstring": "Return a new HTTP object.\n\n    Args:\n        *args: tuple, The positional arguments to be passed when\n               contructing a new HTTP object.\n        **kwargs: dict, The keyword arguments to be passed when\n                  contructing a new HTTP object.\n\n    Returns:\n        httplib2.Http, an HTTP object.", "id": "f2474:m1"}
{"signature": "def __init__(self, fields, target=sys.stdout):", "body": "self.fields = fields<EOL>self.target = target<EOL>", "docstring": "Create a processor that prints the requested fields' values\n\n        This is useful for strings with newlines in them. Keep in mind that the\n        fields will be popped from the event dictionary, so they will not be\n        visible to anything (other processors and the logger itself) after this\n        processor has printed them.\n\n        :param fields: An iterable specifying the fields to print\n        :param target: A file-like object to print to", "id": "f12703:c4:m0"}
{"signature": "def get_protocol_sequence(self,sweep):", "body": "self.setsweep(sweep)<EOL>return list(self.protoSeqX),list(self.protoSeqY)<EOL>", "docstring": "given a sweep, return the protocol as condensed sequence.\nThis is better for comparing similarities and determining steps.\nThere should be no duplicate numbers.", "id": "f11350:c0:m7"}
{"signature": "def p_DefValPart(self, p):", "body": "if p[<NUM_LIT:1>] and p[<NUM_LIT:3>]:<EOL><INDENT>p[<NUM_LIT:0>] = (p[<NUM_LIT:1>], p[<NUM_LIT:3>])<EOL><DEDENT>", "docstring": "DefValPart : DEFVAL '{' Value '}'\n                      | empty", "id": "f5672:c0:m86"}
{"signature": "def recall(ntp, nfn):", "body": "if (ntp+nfn) > <NUM_LIT:0>:<EOL><INDENT>return ntp/(ntp+nfn)<EOL><DEDENT>else:<EOL><INDENT>return np.nan<EOL><DEDENT>", "docstring": "This calculates recall.\n\nhttps://en.wikipedia.org/wiki/Precision_and_recall\n\nParameters\n----------\n\nntp : int\n    The number of true positives.\n\nnfn : int\n    The number of false negatives.\n\nReturns\n-------\n\nfloat\n    The precision calculated using `ntp/(ntp + nfn)`.", "id": "f14692:m4"}
{"signature": "def change_password(self, newpassword):", "body": "if not self.unlocked():<EOL><INDENT>raise WalletLocked<EOL><DEDENT>self.password = newpassword<EOL>self._save_encrypted_masterpassword()<EOL>", "docstring": "Change the password that allows to decrypt the master key", "id": "f8226:c0:m13"}
{"signature": "def set_group_ban(self, *, group_id, user_id, duration=<NUM_LIT:30> * <NUM_LIT>):", "body": "return super().__getattr__('<STR_LIT>')(group_id=group_id, user_id=user_id, duration=duration)<EOL>", "docstring": "\u7fa4\u7ec4\u5355\u4eba\u7981\u8a00\n\n------------\n\n:param int group_id: \u7fa4\u53f7\n:param int user_id: \u8981\u7981\u8a00\u7684 QQ \u53f7\n:param int duration: \u7981\u8a00\u65f6\u957f\uff0c\u5355\u4f4d\u79d2\uff0c0 \u8868\u793a\u53d6\u6d88\u7981\u8a00\n:return: None\n:rtype: None", "id": "f847:c0:m12"}
{"signature": "def overlaps(self, other):", "body": "return bool(self.poly.overlaps(other.poly))<EOL>", "docstring": "Check if two shapes overlap.\n\n        Parameters\n        ----------\n        other : |Shape|\n\n        Returns\n        -------\n        bool", "id": "f3037:c0:m26"}
{"signature": "def getcellvalue(self, window_name, object_name, row_index, column=<NUM_LIT:0>):", "body": "object_handle = self._get_object_handle(window_name, object_name)<EOL>if not object_handle.AXEnabled:<EOL><INDENT>raise LdtpServerException(u\"<STR_LIT>\" % object_name)<EOL><DEDENT>count = len(object_handle.AXRows)<EOL>if row_index < <NUM_LIT:0> or row_index > count:<EOL><INDENT>raise LdtpServerException('<STR_LIT>' % row_index)<EOL><DEDENT>cell = object_handle.AXRows[row_index]<EOL>count = len(cell.AXChildren)<EOL>if column < <NUM_LIT:0> or column > count:<EOL><INDENT>raise LdtpServerException('<STR_LIT>' % column)<EOL><DEDENT>obj = cell.AXChildren[column]<EOL>if not re.search(\"<STR_LIT>\", obj.AXRole):<EOL><INDENT>obj = cell.AXChildren[column]<EOL><DEDENT>return obj.AXValue<EOL>", "docstring": "Get cell value\n\n@param window_name: Window name to type in, either full name,\nLDTP's name convention, or a Unix glob.\n@type window_name: string\n@param object_name: Object name to type in, either full name,\nLDTP's name convention, or a Unix glob. \n@type object_name: string\n@param row_index: Row index to get\n@type row_index: integer\n@param column: Column index to get, default value 0\n@type column: integer\n\n@return: cell value on success.\n@rtype: string", "id": "f10317:c0:m8"}
{"signature": "def calculate_derivative_P(self, P, T, zs, ws, method, order=<NUM_LIT:1>):", "body": "f = lambda P: self.calculate(T, P, zs, ws, method)<EOL>return derivative(f, P, dx=<NUM_LIT>, n=order, order=<NUM_LIT:1>+order*<NUM_LIT:2>)<EOL>", "docstring": "r'''Method to calculate a derivative of a mixture property with respect \n        to pressure at constant temperature and composition\n        of a given order using a specified method. Uses SciPy's derivative \n        function, with a delta of 0.01 Pa and a number of points equal to \n        2*order + 1.\n\n        This method can be overwritten by subclasses who may perfer to add\n        analytical methods for some or all methods as this is much faster.\n\n        If the calculation does not succeed, returns the actual error\n        encountered.\n\n        Parameters\n        ----------\n        P : float\n            Pressure at which to calculate the derivative, [Pa]\n        T : float\n            Temperature at which to calculate the derivative, [K]\n        zs : list[float]\n            Mole fractions of all species in the mixture, [-]\n        ws : list[float]\n            Weight fractions of all species in the mixture, [-]\n        method : str\n            Method for which to find the derivative\n        order : int\n            Order of the derivative, >= 1\n\n        Returns\n        -------\n        d_prop_d_P_at_T : float\n            Calculated derivative property at constant temperature, \n            [`units/Pa^order`]", "id": "f15806:c2:m6"}
{"signature": "@instruction<EOL><INDENT>def SETNBE(cpu, dest):<DEDENT>", "body": "dest.write(Operators.ITEBV(dest.size, Operators.AND(cpu.CF == False, cpu.ZF == False), <NUM_LIT:1>, <NUM_LIT:0>))<EOL>", "docstring": "Sets byte if not below or equal.\n\n:param cpu: current CPU.\n:param dest: destination operand.", "id": "f16975:c2:m83"}
{"signature": "def write(obj):", "body": "import warnings<EOL>warnings.warn(\"<STR_LIT>\",<EOL>DeprecationWarning)<EOL>return dumps(obj)<EOL>", "docstring": "jsonlib, JsonUtils, python-json, json-py API compatibility hook.\nUse dumps(s) instead.", "id": "f11590:m7"}
{"signature": "def value(self):", "body": "return self._X @ self._effsizes<EOL>", "docstring": "Linear mean function.\n\nReturns\n-------\n\ud835\udc26 : (n,) ndarray\n    X\ud835\udf36.", "id": "f13596:c0:m2"}
{"signature": "def get_cells(self):", "body": "return self.__cells<EOL>", "docstring": "!\n        @brief Returns CLIQUE blocks that are formed during clustering process.\n        @details CLIQUE blocks can be used for visualization purposes. Each CLIQUE block contain its logical location\n                  in grid, spatial location in data space and points that belong to block.\n\n        @return (list) List of CLIQUE blocks.", "id": "f15466:c4:m4"}
{"signature": "def add_identity(self,item_name,item_category=None,item_type=None):", "body": "return DiscoIdentity(self,item_name,item_category,item_type)<EOL>", "docstring": "Add an identity to the `DiscoInfo` object.\n\n        :Parameters:\n            - `item_name`: name of the item.\n            - `item_category`: category of the item.\n            - `item_type`: type of the item.\n        :Types:\n            - `item_name`: `unicode`\n            - `item_category`: `unicode`\n            - `item_type`: `unicode`\n\n        :returns: the identity created.\n        :returntype: `DiscoIdentity`", "id": "f15250:c3:m14"}
{"signature": "def load_user_rights(self, user):", "body": "if user.username in self.admins:<EOL><INDENT>user.is_admin = True<EOL><DEDENT>elif not hasattr(user, '<STR_LIT>'):<EOL><INDENT>user.is_admin = False<EOL><DEDENT>", "docstring": "Sets permissions on user object", "id": "f12999:c0:m6"}
{"signature": "def validate_day_start_ut(conn):", "body": "G = GTFS(conn)<EOL>cur = conn.execute('<STR_LIT>')<EOL>for date, day_start_ut in cur:<EOL><INDENT>assert day_start_ut == G.get_day_start_ut(date)<EOL><DEDENT>", "docstring": "This validates the day_start_ut of the days table.", "id": "f12928:m1"}
{"signature": "def hidelist(self, window_name, object_name):", "body": "object_handle = self._get_object_handle(window_name, object_name)<EOL>object_handle.activate()<EOL>object_handle.sendKey(AXKeyCodeConstants.ESCAPE)<EOL>return <NUM_LIT:1><EOL>", "docstring": "Hide combo box list / menu\n\n@param window_name: Window name to type in, either full name,\nLDTP's name convention, or a Unix glob.\n@type window_name: string\n@param object_name: Object name to type in, either full name,\nLDTP's name convention, or a Unix glob. \n@type object_name: string\n\n@return: 1 on success.\n@rtype: integer", "id": "f10321:c0:m4"}
{"signature": "def original_image_extender(pipeline_index,<EOL>finder_image_urls,<EOL>extender_image_urls=[],<EOL>*args, **kwargs):", "body": "now_extender_image_urls = []<EOL>check_re = re.compile(r'<STR_LIT>', re.IGNORECASE)<EOL>search_re = re.compile(r'<STR_LIT>', re.IGNORECASE)<EOL>for image_url in finder_image_urls:<EOL><INDENT>if check_re.search(image_url):<EOL><INDENT>if search_re.search(image_url):<EOL><INDENT>extender_image_url = search_re.sub('<STR_LIT:.>', image_url)<EOL>now_extender_image_urls.append(extender_image_url)<EOL><DEDENT><DEDENT><DEDENT>output = {}<EOL>output['<STR_LIT>'] = extender_image_urls + now_extender_image_urls<EOL>return output<EOL>", "docstring": "Example:\nhttp://fashion-fever.nl/wp-content/upload/2013/09/DSC_0058-110x110.jpg\nhttp://www.wendyslookbook.com/wp-content/uploads/2013/09/Morning-Coffee-Run-7-433x650.jpg\nto\nhttp://fashion-fever.nl/wp-content/upload/2013/09/DSC_0058.jpg\nhttp://www.wendyslookbook.com/wp-content/uploads/2013/09/Morning-Coffee-Run-7.jpg", "id": "f4835:m0"}
{"signature": "def _make_regex(self):", "body": "return re.compile(\"<STR_LIT:|>\".join(map(re.escape, list(self.keys()))))<EOL>", "docstring": "Build re object based on the keys of the current dictionary", "id": "f17209:c13:m0"}
{"signature": "def delistify(a, b=None):", "body": "if isinstance(b, (tuple, list, np.ndarray)):<EOL><INDENT>if isinstance(a, (tuple, list, np.ndarray)):<EOL><INDENT>return type(b)(a)<EOL><DEDENT>return type(b)([a])<EOL><DEDENT>else:<EOL><INDENT>if isinstance(a, (tuple, list, np.ndarray)) and len(a) == <NUM_LIT:1>:<EOL><INDENT>return a[<NUM_LIT:0>]<EOL><DEDENT>return a<EOL><DEDENT>return a<EOL>", "docstring": "If a single element list, extract the element as an object, otherwise\nleave as it is.\n\nExamples\n--------\n>>> delistify('string')\n'string'\n\n>>> delistify(['string'])\n'string'\n\n>>> delistify(['string', 'other'])\n['string', 'other']\n\n>>> delistify(np.array([1.0]))\n1.0\n\n>>> delistify([1, 2, 3])\n[1, 2, 3]", "id": "f5741:m2"}
{"signature": "def create_deleted_record(self, record):", "body": "identifier = record_get_field_value(record,<EOL>tag=\"<STR_LIT>\",<EOL>code=\"<STR_LIT:a>\")<EOL>recid = identifier.split(\"<STR_LIT::>\")[-<NUM_LIT:1>]<EOL>try:<EOL><INDENT>source = identifier.split(\"<STR_LIT::>\")[<NUM_LIT:1>]<EOL><DEDENT>except IndexError:<EOL><INDENT>source = \"<STR_LIT>\"<EOL><DEDENT>record_add_field(record, \"<STR_LIT>\",<EOL>subfields=[(\"<STR_LIT>\", source), (\"<STR_LIT:a>\", recid)])<EOL>record_add_field(record, \"<STR_LIT>\",<EOL>subfields=[(\"<STR_LIT:c>\", \"<STR_LIT>\")])<EOL>return record<EOL>", "docstring": "Generate the record deletion if deleted form OAI-PMH.", "id": "f7891:c2:m3"}
{"signature": "def _finalizeSVD(self, numSVDDims=None):", "body": "if numSVDDims is not None:<EOL><INDENT>self.numSVDDims = numSVDDims<EOL><DEDENT>if self.numSVDDims==\"<STR_LIT>\":<EOL><INDENT>if self.fractionOfMax is not None:<EOL><INDENT>self.numSVDDims = self.getAdaptiveSVDDims(self._s, self.fractionOfMax)<EOL><DEDENT>else:<EOL><INDENT>self.numSVDDims = self.getAdaptiveSVDDims(self._s)<EOL><DEDENT><DEDENT>if self._vt.shape[<NUM_LIT:0>] < self.numSVDDims:<EOL><INDENT>print(\"<STR_LIT>\")<EOL>print (\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL>print(\"<STR_LIT>\", self._vt.shape[<NUM_LIT:0>])<EOL>print(\"<STR_LIT>\")<EOL>self.numSVDDims = self._vt.shape[<NUM_LIT:0>]<EOL><DEDENT>self._vt = self._vt[:self.numSVDDims]<EOL>if len(self._vt) == <NUM_LIT:0>:<EOL><INDENT>return<EOL><DEDENT>self._Memory = numpy.zeros((self._numPatterns,self.numSVDDims))<EOL>self._M = self._Memory<EOL>self.useSparseMemory = False<EOL>for i in range(self._numPatterns):<EOL><INDENT>self._Memory[i] = numpy.dot(self._vt, self._a[i])<EOL><DEDENT>self._a = None<EOL>", "docstring": "Called by finalizeLearning(). This will project all the patterns onto the\nSVD eigenvectors.\n:param numSVDDims: (int) number of egeinvectors used for projection.\n:return:", "id": "f17568:c0:m29"}
{"signature": "@abc.abstractproperty<EOL><INDENT>def advertised(self):<DEDENT>", "body": "raise NotImplementedError<EOL>", "docstring": "Return a list of UUIDs for services that are advertised by this\n        device.", "id": "f9590:c0:m4"}
{"signature": "def setup_databases(self, **kwargs):", "body": "pass<EOL>", "docstring": "Override the database creation defined in parent class", "id": "f8713:c0:m1"}
{"signature": "@property<EOL><INDENT>def certificate(self):<DEDENT>", "body": "if self._session_context is None:<EOL><INDENT>self._raise_closed()<EOL><DEDENT>if self._certificate is None:<EOL><INDENT>self._read_certificates()<EOL><DEDENT>return self._certificate<EOL>", "docstring": "An asn1crypto.x509.Certificate object of the end-entity certificate\npresented by the server", "id": "f9507:c1:m16"}
{"signature": "async def get_oauth_token(consumer_key, consumer_secret, callback_uri=\"<STR_LIT>\"):", "body": "client = BasePeonyClient(consumer_key=consumer_key,<EOL>consumer_secret=consumer_secret,<EOL>api_version=\"<STR_LIT>\",<EOL>suffix=\"<STR_LIT>\")<EOL>response = await client.api.oauth.request_token.post(<EOL>_suffix=\"<STR_LIT>\",<EOL>oauth_callback=callback_uri<EOL>)<EOL>return parse_token(response)<EOL>", "docstring": "Get a temporary oauth token\n\nParameters\n----------\nconsumer_key : str\n    Your consumer key\nconsumer_secret : str\n    Your consumer secret\ncallback_uri : str, optional\n    Callback uri, defaults to 'oob'\n\nReturns\n-------\ndict\n    Temporary tokens", "id": "f4734:m0"}
{"signature": "@property<EOL><INDENT>def is_draft(self):<DEDENT>", "body": "if not hasattr(self, '<STR_LIT>'):<EOL><INDENT>self._parse_documentclass()<EOL><DEDENT>if '<STR_LIT>' in self._document_options:<EOL><INDENT>return True<EOL><DEDENT>else:<EOL><INDENT>return False<EOL><DEDENT>", "docstring": "Document is a draft if ``'lsstdoc'`` is included in the\n        documentclass options (`bool`).", "id": "f4210:c0:m18"}
{"signature": "def pop_max(self):", "body": "item = self.max_item()<EOL>self.remove(item[<NUM_LIT:0>])<EOL>return item<EOL>", "docstring": "T.pop_max() -> (k, v), remove item with maximum key, raise ValueError\n        if T is empty.", "id": "f16266:c3:m19"}
{"signature": "def escape(self, text):", "body": "return escape(text)<EOL>", "docstring": "Rendering escape sequence.\n\n        :param text: text content.", "id": "f14379:c4:m19"}
{"signature": "def printColConfidence(self, aState, maxCols = <NUM_LIT:20>):", "body": "def formatFPRow(var):<EOL><INDENT>s = '<STR_LIT>'<EOL>for c in range(min(maxCols, self.numberOfCols)):<EOL><INDENT>if c > <NUM_LIT:0> and c % <NUM_LIT:10> == <NUM_LIT:0>:<EOL><INDENT>s += '<STR_LIT:U+0020>'<EOL><DEDENT>s += '<STR_LIT>' % var[c]<EOL><DEDENT>s += '<STR_LIT:U+0020>'<EOL>return s<EOL><DEDENT>print(formatFPRow(aState))<EOL>", "docstring": "Print up to maxCols number from a flat floating point array.\n\n:param aState: TODO: document\n:param maxCols: TODO: document", "id": "f17565:c0:m24"}
{"signature": "def populate(self):", "body": "self.manager.populate()<EOL>", "docstring": "Populate the manager.", "id": "f1575:c2:m0"}
{"signature": "def add_item_upload_callback(callback):", "body": "session.item_upload_callbacks.append(callback)<EOL>", "docstring": "Pass a function to be called when an item is created. This can be quite\nuseful for performing actions such as notifications of upload progress as\nwell as calling additional API functions.\n\n:param callback: A function that takes three arguments. The first argument\n    is the communicator object of the current pydas context, the second is\n    the currently active API token and the third is the id of the item that\n    was created to result in the callback function's invocation.\n:type callback: (Communicator, string, int) -> unknown", "id": "f8359:m3"}
{"signature": "def serialize(self, serializer):", "body": "return serializer.serialize_pathattrib(self)<EOL>", "docstring": "Serializes the instance using the provided serializer.\n\n:type  serializer: :class:`SpiffWorkflow.serializer.base.Serializer`\n:param serializer: The serializer to use.\n:rtype:  object\n:returns: The serialized object.", "id": "f7735:c2:m1"}
{"signature": "def connect(self, callback, *args, **kwargs):", "body": "if self.is_connected(callback):<EOL><INDENT>raise AttributeError('<STR_LIT>')<EOL><DEDENT>if self.hard_subscribers is None:<EOL><INDENT>self.hard_subscribers = []<EOL><DEDENT>self.hard_subscribers.append((callback, args, kwargs))<EOL>", "docstring": "Connects the event with the given callback.\nWhen the signal is emitted, the callback is invoked.\n\n.. note::\n\n    The signal handler is stored with a hard reference, so you\n    need to make sure to call :class:`disconnect()` if you want the\n    handler\n    to be garbage collected.\n\n:type  callback: object\n:param callback: The callback function.\n:type  args: tuple\n:param args: Optional arguments passed to the callback.\n:type  kwargs: dict\n:param kwargs: Optional keyword arguments passed to the callback.", "id": "f7740:c0:m2"}
{"signature": "def should_close(self) -> bool:", "body": "return self.window.has_exit<EOL>", "docstring": "returns the ``has_exit`` state in the pyglet window", "id": "f14480:c0:m7"}
{"signature": "@instruction<EOL><INDENT>def PXOR(cpu, dest, src):<DEDENT>", "body": "res = dest.write(dest.read() ^ src.read())<EOL>", "docstring": "Logical exclusive OR.\n\nPerforms a bitwise logical exclusive-OR (XOR) operation on the quadword\nsource (second) and destination (first) operands and stores the result\nin the destination operand location. The source operand can be an MMX(TM)\ntechnology register or a quadword memory location; the destination operand\nmust be an MMX register. Each bit of the result is 1 if the corresponding\nbits of the two operands are different; each bit is 0 if the corresponding\nbits of the operands are the same::\n\n    DEST  =  DEST XOR SRC;\n\n:param cpu: current CPU.\n:param dest: destination operand.\n:param src: quadword source operand.", "id": "f16975:c2:m189"}
{"signature": "@instruction<EOL><INDENT>def PREFETCHT0(cpu, arg):<DEDENT>", "body": "", "docstring": "Not implemented.\n\nPerforms no operation.", "id": "f16975:c2:m261"}
{"signature": "@typecasted<EOL><INDENT>def get_top_clanwar_clans(self, location_id='<STR_LIT>', **params: keys):<DEDENT>", "body": "url = self.api.LOCATIONS + '<STR_LIT:/>' + str(location_id) + '<STR_LIT>'<EOL>return self._get_model(url, PartialClan, **params)<EOL>", "docstring": "Get a list of top clan war clans\n\n        Parameters\n        ----------\n        location_id: Optional[str] = 'global'\n            A location ID or global\n            See https://github.com/RoyaleAPI/cr-api-data/blob/master/json/regions.json\n            for a list of acceptable location IDs\n        \\*\\*limit: Optional[int] = None\n            Limit the number of items returned in the response\n        \\*\\*timeout: Optional[int] = None\n            Custom timeout that overwrites Client.timeout", "id": "f10352:c0:m32"}
{"signature": "@staticmethod<EOL><INDENT>def cf_string_to_unicode(value):<DEDENT>", "body": "string = CoreFoundation.CFStringGetCStringPtr(<EOL>_cast_pointer_p(value),<EOL>kCFStringEncodingUTF8<EOL>)<EOL>if string is None:<EOL><INDENT>buffer = buffer_from_bytes(<NUM_LIT>)<EOL>result = CoreFoundation.CFStringGetCString(<EOL>_cast_pointer_p(value),<EOL>buffer,<EOL><NUM_LIT>,<EOL>kCFStringEncodingUTF8<EOL>)<EOL>if not result:<EOL><INDENT>raise OSError('<STR_LIT>')<EOL><DEDENT>string = byte_string_from_buffer(buffer)<EOL><DEDENT>if string is not None:<EOL><INDENT>string = string.decode('<STR_LIT:utf-8>')<EOL><DEDENT>return string<EOL>", "docstring": "Creates a python unicode string from a CFString object\n\n:param value:\n    The CFString to convert\n\n:return:\n    A python unicode string", "id": "f9504:c0:m4"}
{"signature": "def _queue_management_worker(self):", "body": "logger.debug(\"<STR_LIT>\")<EOL>while not self._executor_bad_state.is_set():<EOL><INDENT>try:<EOL><INDENT>msgs = self.incoming_q.get(timeout=<NUM_LIT:1>)<EOL><DEDENT>except queue.Empty:<EOL><INDENT>logger.debug(\"<STR_LIT>\")<EOL>pass<EOL><DEDENT>except IOError as e:<EOL><INDENT>logger.exception(\"<STR_LIT>\".format(e.errno, e))<EOL>return<EOL><DEDENT>except Exception as e:<EOL><INDENT>logger.exception(\"<STR_LIT>\".format(e))<EOL>return<EOL><DEDENT>else:<EOL><INDENT>if msgs is None:<EOL><INDENT>logger.debug(\"<STR_LIT>\")<EOL>return<EOL><DEDENT>else:<EOL><INDENT>for serialized_msg in msgs:<EOL><INDENT>try:<EOL><INDENT>msg = pickle.loads(serialized_msg)<EOL>tid = msg['<STR_LIT>']<EOL><DEDENT>except pickle.UnpicklingError:<EOL><INDENT>raise BadMessage(\"<STR_LIT>\")<EOL><DEDENT>except Exception:<EOL><INDENT>raise BadMessage(\"<STR_LIT>\")<EOL><DEDENT>if tid == -<NUM_LIT:1> and '<STR_LIT>' in msg:<EOL><INDENT>logger.warning(\"<STR_LIT>\")<EOL>self._executor_exception, _ = deserialize_object(msg['<STR_LIT>'])<EOL>logger.exception(\"<STR_LIT>\".format(self._executor_exception))<EOL>self._executor_bad_state.set()<EOL>for task in self.tasks:<EOL><INDENT>self.tasks[task].set_exception(self._executor_exception)<EOL><DEDENT>break<EOL><DEDENT>task_fut = self.tasks[tid]<EOL>if '<STR_LIT:result>' in msg:<EOL><INDENT>result, _ = deserialize_object(msg['<STR_LIT:result>'])<EOL>task_fut.set_result(result)<EOL><DEDENT>elif '<STR_LIT>' in msg:<EOL><INDENT>try:<EOL><INDENT>s, _ = deserialize_object(msg['<STR_LIT>'])<EOL>try:<EOL><INDENT>s.reraise()<EOL><DEDENT>except Exception as e:<EOL><INDENT>task_fut.set_exception(e)<EOL><DEDENT><DEDENT>except Exception as e:<EOL><INDENT>task_fut.set_exception(<EOL>DeserializationError(\"<STR_LIT>\".format(e)))<EOL><DEDENT><DEDENT>else:<EOL><INDENT>raise BadMessage(\"<STR_LIT>\")<EOL><DEDENT><DEDENT><DEDENT><DEDENT>if not self.is_alive:<EOL><INDENT>break<EOL><DEDENT><DEDENT>logger.info(\"<STR_LIT>\")<EOL>", "docstring": "Listen to the queue for task status messages and handle them.\n\n        Depending on the message, tasks will be updated with results, exceptions,\n        or updates. It expects the following messages:\n\n        .. code:: python\n\n            {\n               \"task_id\" : <task_id>\n               \"result\"  : serialized result object, if task succeeded\n               ... more tags could be added later\n            }\n\n            {\n               \"task_id\" : <task_id>\n               \"exception\" : serialized exception object, on failure\n            }\n\n        We do not support these yet, but they could be added easily.\n\n        .. code:: python\n\n            {\n               \"task_id\" : <task_id>\n               \"cpu_stat\" : <>\n               \"mem_stat\" : <>\n               \"io_stat\"  : <>\n               \"started\"  : tstamp\n            }\n\n        The `None` message is a die request.", "id": "f2822:c0:m3"}
{"signature": "def move_to(self, thing, destination):", "body": "thing.bump = self.some_things_at(destination, Obstacle)<EOL>if not thing.bump:<EOL><INDENT>thing.location = destination<EOL>for o in self.observers:<EOL><INDENT>o.thing_moved(thing)<EOL><DEDENT><DEDENT>", "docstring": "Move a thing to a new location.", "id": "f1676:c3:m6"}
{"signature": "def release(self):", "body": "<EOL>if not self.locked_status:<EOL><INDENT>raise error<EOL><DEDENT>self.locked_status = False<EOL>return True<EOL>", "docstring": "Release the dummy lock.", "id": "f16467:c1:m3"}
{"signature": "def _advapi32_sign(private_key, data, hash_algorithm, rsa_pss_padding=False):", "body": "algo = private_key.algorithm<EOL>if algo == '<STR_LIT>' and hash_algorithm == '<STR_LIT>':<EOL><INDENT>padded_data = add_pkcs1v15_signature_padding(private_key.byte_size, data)<EOL>return raw_rsa_private_crypt(private_key, padded_data)<EOL><DEDENT>if algo == '<STR_LIT>' and rsa_pss_padding:<EOL><INDENT>hash_length = {<EOL>'<STR_LIT>': <NUM_LIT:20>,<EOL>'<STR_LIT>': <NUM_LIT>,<EOL>'<STR_LIT>': <NUM_LIT:32>,<EOL>'<STR_LIT>': <NUM_LIT>,<EOL>'<STR_LIT>': <NUM_LIT:64><EOL>}.get(hash_algorithm, <NUM_LIT:0>)<EOL>padded_data = add_pss_padding(hash_algorithm, hash_length, private_key.bit_size, data)<EOL>return raw_rsa_private_crypt(private_key, padded_data)<EOL><DEDENT>if private_key.algorithm == '<STR_LIT>' and hash_algorithm == '<STR_LIT>':<EOL><INDENT>raise ValueError(pretty_message(<EOL>'''<STR_LIT>'''<EOL>))<EOL><DEDENT>hash_handle = None<EOL>try:<EOL><INDENT>alg_id = {<EOL>'<STR_LIT>': Advapi32Const.CALG_MD5,<EOL>'<STR_LIT>': Advapi32Const.CALG_SHA1,<EOL>'<STR_LIT>': Advapi32Const.CALG_SHA_256,<EOL>'<STR_LIT>': Advapi32Const.CALG_SHA_384,<EOL>'<STR_LIT>': Advapi32Const.CALG_SHA_512,<EOL>}[hash_algorithm]<EOL>hash_handle_pointer = new(advapi32, '<STR_LIT>')<EOL>res = advapi32.CryptCreateHash(<EOL>private_key.context_handle,<EOL>alg_id,<EOL>null(),<EOL><NUM_LIT:0>,<EOL>hash_handle_pointer<EOL>)<EOL>handle_error(res)<EOL>hash_handle = unwrap(hash_handle_pointer)<EOL>res = advapi32.CryptHashData(hash_handle, data, len(data), <NUM_LIT:0>)<EOL>handle_error(res)<EOL>out_len = new(advapi32, '<STR_LIT>')<EOL>res = advapi32.CryptSignHashW(<EOL>hash_handle,<EOL>Advapi32Const.AT_SIGNATURE,<EOL>null(),<EOL><NUM_LIT:0>,<EOL>null(),<EOL>out_len<EOL>)<EOL>handle_error(res)<EOL>buffer_length = deref(out_len)<EOL>buffer_ = buffer_from_bytes(buffer_length)<EOL>res = advapi32.CryptSignHashW(<EOL>hash_handle,<EOL>Advapi32Const.AT_SIGNATURE,<EOL>null(),<EOL><NUM_LIT:0>,<EOL>buffer_,<EOL>out_len<EOL>)<EOL>handle_error(res)<EOL>output = bytes_from_buffer(buffer_, deref(out_len))<EOL>output = output[::-<NUM_LIT:1>]<EOL>if algo == '<STR_LIT>':<EOL><INDENT>half_len = len(output) // <NUM_LIT:2><EOL>output = output[half_len:] + output[:half_len]<EOL>output = algos.DSASignature.from_p1363(output).dump()<EOL><DEDENT>return output<EOL><DEDENT>finally:<EOL><INDENT>if hash_handle:<EOL><INDENT>advapi32.CryptDestroyHash(hash_handle)<EOL><DEDENT><DEDENT>", "docstring": "Generates an RSA, DSA or ECDSA signature via CryptoAPI\n\n:param private_key:\n    The PrivateKey to generate the signature with\n\n:param data:\n    A byte string of the data the signature is for\n\n:param hash_algorithm:\n    A unicode string of \"md5\", \"sha1\", \"sha256\", \"sha384\", \"sha512\" or \"raw\"\n\n:param rsa_pss_padding:\n    If PSS padding should be used for RSA keys\n\n:raises:\n    ValueError - when any of the parameters contain an invalid value\n    TypeError - when any of the parameters are of the wrong type\n    OSError - when an error is returned by the OS crypto library\n\n:return:\n    A byte string of the signature", "id": "f9489:m30"}
{"signature": "def pr_qmean(self):", "body": "return qmean((self.precision(), self.recall()))<EOL>", "docstring": "r\"\"\"Return quadratic mean of precision & recall.\n\n        The quadratic mean of precision and recall is defined as:\n        :math:`\\sqrt{\\frac{precision^{2} + recall^{2}}{2}}`\n\n        Cf. https://en.wikipedia.org/wiki/Quadratic_mean\n\n        Returns\n        -------\n        float\n            The quadratic mean of the confusion table's precision & recall\n\n        Example\n        -------\n        >>> ct = ConfusionTable(120, 60, 20, 30)\n        >>> ct.pr_qmean()\n        0.8290638930598233", "id": "f6662:c0:m31"}
{"signature": "def __calculate_dataset_difference(self, amount_clusters):", "body": "dataset_differences = numpy.zeros((amount_clusters, len(self.__pointer_data)))<EOL>for index_center in range(amount_clusters):<EOL><INDENT>if self.__metric.get_type() != type_metric.USER_DEFINED:<EOL><INDENT>dataset_differences[index_center] = self.__metric(self.__pointer_data, self.__centers[index_center])<EOL><DEDENT>else:<EOL><INDENT>dataset_differences[index_center] = [ self.__metric(point, self.__centers[index_center])<EOL>for point in self.__pointer_data ]<EOL><DEDENT><DEDENT>return dataset_differences<EOL>", "docstring": "!\n        @brief Calculate distance from each point to each cluster center.", "id": "f15594:c2:m11"}
{"signature": "def _streamSSE(url, on_data=print, accrue=False):", "body": "messages = SSEClient(url)<EOL>if accrue:<EOL><INDENT>ret = []<EOL><DEDENT>for msg in messages:<EOL><INDENT>data = msg.data<EOL>on_data(json.loads(data))<EOL>if accrue:<EOL><INDENT>ret.append(msg)<EOL><DEDENT><DEDENT>return ret<EOL>", "docstring": "internal", "id": "f2338:m10"}
{"signature": "def value(self, t):", "body": "return self._v<EOL>", "docstring": "See Schedule.value", "id": "f1370:c1:m1"}
{"signature": "def __init__(self, app=None, **kwargs):", "body": "self.limiter = None<EOL>self.talisman = None<EOL>if app:<EOL><INDENT>self.init_app(app, **kwargs)<EOL><DEDENT>", "docstring": "r\"\"\"Extension initialization.\n\n        :param app: An instance of :class:`~flask.Flask`.\n        :param \\**kwargs: Keyword arguments are passed to ``init_app`` method.", "id": "f10457:c0:m0"}
{"signature": "def load_and_assign_npz_dict(name='<STR_LIT>', sess=None):", "body": "if sess is None:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if not os.path.exists(name):<EOL><INDENT>logging.error(\"<STR_LIT>\".format(name))<EOL>return False<EOL><DEDENT>params = np.load(name)<EOL>if len(params.keys()) != len(set(params.keys())):<EOL><INDENT>raise Exception(\"<STR_LIT>\" % name)<EOL><DEDENT>ops = list()<EOL>for key in params.keys():<EOL><INDENT>try:<EOL><INDENT>varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=key)<EOL>if len(varlist) > <NUM_LIT:1>:<EOL><INDENT>raise Exception(\"<STR_LIT>\" % key)<EOL><DEDENT>elif len(varlist) == <NUM_LIT:0>:<EOL><INDENT>raise KeyError<EOL><DEDENT>else:<EOL><INDENT>ops.append(varlist[<NUM_LIT:0>].assign(params[key]))<EOL>logging.info(\"<STR_LIT>\" % key)<EOL><DEDENT><DEDENT>except KeyError:<EOL><INDENT>logging.info(\"<STR_LIT>\" % key)<EOL><DEDENT><DEDENT>sess.run(ops)<EOL>logging.info(\"<STR_LIT>\" % name)<EOL>", "docstring": "Restore the parameters saved by ``tl.files.save_npz_dict()``.\n\n    Parameters\n    ----------\n    name : str\n        The name of the `.npz` file.\n    sess : Session\n        TensorFlow Session.", "id": "f11137:m22"}
{"signature": "def GetWindowPattern(self) -> WindowPattern:", "body": "return self.GetPattern(PatternId.WindowPattern)<EOL>", "docstring": "Return `WindowPattern` if it supports the pattern else None(Conditional support according to MSDN).", "id": "f1782:c117:m2"}
{"signature": "def dorun(SNR=<NUM_LIT:20>, njitters=<NUM_LIT:20>, samples=<NUM_LIT:10>, noise_samples=<NUM_LIT:10>, sweeps=<NUM_LIT:20>, burn=<NUM_LIT:10>):", "body": "jitters = np.logspace(-<NUM_LIT:6>, np.log10(<NUM_LIT:0.5>), njitters)<EOL>crbs, vals, errs, poss = [], [], [], []<EOL>for i,t in enumerate(jitters):<EOL><INDENT>print('<STR_LIT>', i, t)<EOL>for j in range(samples):<EOL><INDENT>print('<STR_LIT:image>', j, '<STR_LIT:|>', end='<STR_LIT:U+0020>') <EOL>s,im,pos = zjitter(jitter=t)<EOL>common.set_image(s, im, <NUM_LIT:1.0>/SNR)<EOL>crbs.append(crb(s))<EOL>val, err = sample(s, im, <NUM_LIT:1.0>/SNR, N=noise_samples, sweeps=sweeps, burn=burn)<EOL>poss.append(pos)<EOL>vals.append(val)<EOL>errs.append(err)<EOL><DEDENT><DEDENT>shape0 = (njitters, samples, -<NUM_LIT:1>)<EOL>shape1 = (njitters, samples, noise_samples, -<NUM_LIT:1>)<EOL>crbs = np.array(crbs).reshape(shape0)<EOL>vals = np.array(vals).reshape(shape1)<EOL>errs = np.array(errs).reshape(shape1)<EOL>poss = np.array(poss).reshape(shape0)<EOL>return  [crbs, vals, errs, poss, jitters]<EOL>", "docstring": "we want to display the errors introduced by pixelation so we plot:\n    * CRB, sampled error vs exposure time\n\na = dorun(ntimes=10, samples=5, noise_samples=5, sweeps=20, burn=8)", "id": "f5800:m1"}
{"signature": "def get_program_by_title(self, program_title):", "body": "all_programs = self._load_data(self.PROGRAMS_ENDPOINT, default=[])<EOL>matching_programs = [program for program in all_programs if program.get('<STR_LIT:title>') == program_title]<EOL>if len(matching_programs) > <NUM_LIT:1>:<EOL><INDENT>raise MultipleProgramMatchError(len(matching_programs))<EOL><DEDENT>elif len(matching_programs) == <NUM_LIT:1>:<EOL><INDENT>return matching_programs[<NUM_LIT:0>]<EOL><DEDENT>else:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Return single program by name, or None if not found.\n\nArguments:\n    program_title(string): Program title as seen by students and in Course Catalog Admin\n\nReturns:\n    dict: Program data provided by Course Catalog API", "id": "f16091:c0:m11"}
{"signature": "def _bumpUpWeakColumns(self):", "body": "weakColumns = numpy.where(self._overlapDutyCycles<EOL>< self._minOverlapDutyCycles)[<NUM_LIT:0>]<EOL>for columnIndex in weakColumns:<EOL><INDENT>perm = self._permanences[columnIndex].astype(realDType)<EOL>maskPotential = numpy.where(self._potentialPools[columnIndex] > <NUM_LIT:0>)[<NUM_LIT:0>]<EOL>perm[maskPotential] += self._synPermBelowStimulusInc<EOL>self._updatePermanencesForColumn(perm, columnIndex, raisePerm=False)<EOL><DEDENT>", "docstring": "This method increases the permanence values of synapses of columns whose\nactivity level has been too low. Such columns are identified by having an\noverlap duty cycle that drops too much below those of their peers. The\npermanence values for such columns are increased.", "id": "f17561:c4:m71"}
{"signature": "def get_result(self, errors=STRICT, **params):", "body": "service_url = self.create_session(**params)<EOL>return self.poll(service_url, errors=errors)<EOL>", "docstring": "Get all results, no filtering,\netc. by creating and polling the session.", "id": "f13321:c0:m3"}
{"signature": "def _build_endpoint_url(self, url, name=None):", "body": "if not url.endswith(self.URL_SEPERATOR):<EOL><INDENT>url = url + self.URL_SEPERATOR<EOL><DEDENT>if name is None:<EOL><INDENT>name = '<STR_LIT>'<EOL><DEDENT>return '<STR_LIT>' % (urlparse.urljoin(self.dsn, url), name,<EOL>self.NAME_EXTENSION)<EOL>", "docstring": "Method that constructs a full url with the given url and the\nsnapshot name.\n\nExample:\nfull_url = _build_endpoint_url('/users', '1')\nfull_url => 'http://firebase.localhost/users/1.json'", "id": "f9848:c2:m1"}
{"signature": "def chartDF(symbol, timeframe='<STR_LIT>', date=None, token='<STR_LIT>', version='<STR_LIT>'):", "body": "c = chart(symbol, timeframe, date, token, version)<EOL>df = pd.DataFrame(c)<EOL>_toDatetime(df)<EOL>if timeframe is not None and timeframe != '<STR_LIT>':<EOL><INDENT>_reindex(df, '<STR_LIT:date>')<EOL><DEDENT>else:<EOL><INDENT>if not df.empty:<EOL><INDENT>df.set_index(['<STR_LIT:date>', '<STR_LIT>'], inplace=True)<EOL><DEDENT>else:<EOL><INDENT>return pd.DataFrame()<EOL><DEDENT><DEDENT>return df<EOL>", "docstring": "Historical price/volume data, daily and intraday\n\n    https://iexcloud.io/docs/api/#historical-prices\n    Data Schedule\n    1d: -9:30-4pm ET Mon-Fri on regular market trading days\n        -9:30-1pm ET on early close trading days\n    All others:\n        -Prior trading day available after 4am ET Tue-Sat\n\n    Args:\n        symbol (string); Ticker to request\n        timeframe (string); Timeframe to request e.g. 1m\n        date (datetime): date, if requesting intraday\n        token (string); Access token\n        version (string); API version\n\n    Returns:\n        DataFrame: result", "id": "f2330:m13"}
{"signature": "def fetch(self, start_date, end_date):", "body": "records = []<EOL>for two_months_range in self._generate_ranges(start_date, end_date):<EOL><INDENT>log.debug(two_months_range)<EOL>for record in self._fetch_missions_for_range(two_months_range[<NUM_LIT:0>], two_months_range[<NUM_LIT:1>]):<EOL><INDENT>records.append(record)<EOL><DEDENT><DEDENT>df = pd.DataFrame(records, columns=[<EOL>'<STR_LIT>',<EOL>'<STR_LIT>',<EOL>'<STR_LIT>',<EOL>'<STR_LIT:start>',<EOL>'<STR_LIT:end>',<EOL>'<STR_LIT>',<EOL>'<STR_LIT>',<EOL>'<STR_LIT>'<EOL>])<EOL>translate_column(df, '<STR_LIT>', {<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': '<STR_LIT>'<EOL>})<EOL>translate_column(df, '<STR_LIT>', {<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': '<STR_LIT>'<EOL>})<EOL>return df.drop_duplicates()<EOL>", "docstring": "Fetches official missions within the given date range", "id": "f3895:c0:m0"}
{"signature": "@read_only<EOL><INDENT>def get_last_id(self):<DEDENT>", "body": "return max(self._tokens)<EOL>", "docstring": "Return the largest token id registered with the world.  If no tokens \nhave been added to the world, the id for the world itself (0) is \nreturned.  This means that the first \"real\" token id is 1.", "id": "f2381:c3:m8"}
{"signature": "@abstractmethod<EOL><INDENT>def put(self, key, value):<DEDENT>", "body": "pass<EOL>", "docstring": "Puts {key, value} pair into the state\n        :param key: The key to get back the value\n        :param value: The value associated with the key", "id": "f7221:c0:m0"}
{"signature": "def _dissect_roles(metadata):", "body": "for role_key in cnxepub.ATTRIBUTED_ROLE_KEYS:<EOL><INDENT>for user in metadata.get(role_key, []):<EOL><INDENT>if user['<STR_LIT:type>'] != '<STR_LIT>':<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>uid = parse_user_uri(user['<STR_LIT:id>'])<EOL>yield uid, role_key<EOL><DEDENT><DEDENT>raise StopIteration()<EOL>", "docstring": "Given a model's ``metadata``, iterate over the roles.\n    Return values are the role identifier and role type as a tuple.", "id": "f15202:m4"}
{"signature": "def register_custom_type(<EOL>self, cls: type, marshaller: Optional[Callable[[Any], Any]] = default_marshaller,<EOL>unmarshaller: Union[Callable[[Any, Any], None],<EOL>Callable[[Any], Any], None] = default_unmarshaller, *,<EOL>typename: str = None, wrap_state: bool = True) -> None:", "body": "assert check_argument_types()<EOL>typename = typename or qualified_name(cls)<EOL>if marshaller:<EOL><INDENT>self.marshallers[cls] = typename, marshaller, wrap_state<EOL>self.custom_type_codec.register_object_encoder_hook(self)<EOL><DEDENT>if unmarshaller and self.custom_type_codec is not None:<EOL><INDENT>target_cls = cls  <EOL>if len(signature(unmarshaller).parameters) == <NUM_LIT:1>:<EOL><INDENT>target_cls = None<EOL><DEDENT>self.unmarshallers[typename] = target_cls, unmarshaller<EOL>self.custom_type_codec.register_object_decoder_hook(self)<EOL><DEDENT>", "docstring": "Register a marshaller and/or unmarshaller for the given class.\n\nThe state object returned by the marshaller and passed to the unmarshaller can be any\nserializable type. Usually a dictionary mapping of attribute names to values is used.\n\n.. warning:: Registering marshallers/unmarshallers for any custom type will override any\n    serializer specific encoding/decoding hooks (respectively) already in place!\n\n:param cls: the class to register\n:param marshaller: a callable that takes the object to be marshalled as the argument and\n      returns a state object\n:param unmarshaller: a callable that either:\n\n    * takes an uninitialized instance of ``cls`` and its state object as arguments and\n      restores the state of the object\n    * takes a state object and returns a new instance of ``cls``\n:param typename: a unique identifier for the type (defaults to the ``module:varname``\n    reference to the class)\n:param wrap_state: ``True`` to wrap the marshalled state before serialization so that it\n    can be recognized later for unmarshalling, ``False`` to serialize it as is", "id": "f773:c1:m1"}
{"signature": "def get_config_keys():", "body": "global _groups<EOL>for group_name, group in _groups.items():<EOL><INDENT>for key in group:<EOL><INDENT>yield f\"<STR_LIT>\"<EOL><DEDENT><DEDENT>", "docstring": "Return an iterable covering all defined keys so far", "id": "f17008:m6"}
{"signature": "@property<EOL><INDENT>def interactions(self):<DEDENT>", "body": "interactions = []<EOL>url = PATHS['<STR_LIT>'] % self.id<EOL>response = self.api.get(url=url)<EOL>for interaction in response['<STR_LIT>']:<EOL><INDENT>interactions.append(ResponseObject(interaction))<EOL><DEDENT>self.__interactions = interactions<EOL>return self.__interactions<EOL>", "docstring": "Returns the detailed information on individual interactions with the social\nmedia update such as favorites, retweets and likes.", "id": "f4306:c0:m1"}
{"signature": "def createAndStartSwarm(client, clientInfo=\"<STR_LIT>\", clientKey=\"<STR_LIT>\", params=\"<STR_LIT>\",<EOL>minimumWorkers=None, maximumWorkers=None,<EOL>alreadyRunning=False):", "body": "if minimumWorkers is None:<EOL><INDENT>minimumWorkers = Configuration.getInt(<EOL>\"<STR_LIT>\")<EOL><DEDENT>if maximumWorkers is None:<EOL><INDENT>maximumWorkers = Configuration.getInt(<EOL>\"<STR_LIT>\")<EOL><DEDENT>return ClientJobsDAO.get().jobInsert(<EOL>client=client,<EOL>cmdLine=\"<STR_LIT>\",<EOL>clientInfo=clientInfo,<EOL>clientKey=clientKey,<EOL>alreadyRunning=alreadyRunning,<EOL>params=params,<EOL>minimumWorkers=minimumWorkers,<EOL>maximumWorkers=maximumWorkers,<EOL>jobType=ClientJobsDAO.JOB_TYPE_HS)<EOL>", "docstring": "Create and start a swarm job.\n\n    Args:\n      client - A string identifying the calling client. There is a small limit\n          for the length of the value. See ClientJobsDAO.CLIENT_MAX_LEN.\n      clientInfo - JSON encoded dict of client specific information.\n      clientKey - Foreign key. Limited in length, see ClientJobsDAO._initTables.\n      params - JSON encoded dict of the parameters for the job. This can be\n          fetched out of the database by the worker processes based on the jobID.\n      minimumWorkers - The minimum workers to allocate to the swarm. Set to None\n          to use the default.\n      maximumWorkers - The maximum workers to allocate to the swarm. Set to None\n          to use the swarm default. Set to 0 to use the maximum scheduler value.\n      alreadyRunning - Insert a job record for an already running process. Used\n          for testing.", "id": "f17600:m0"}
{"signature": "def make_loop(self, handlers):", "body": "<EOL>self.loop = PollMainLoop(None, handlers)<EOL>", "docstring": "Return a main loop object for use with this test suite.", "id": "f15283:c3:m0"}
{"signature": "def add_bpmn_files_by_glob(self, g):", "body": "self.add_bpmn_files(glob.glob(g))<EOL>", "docstring": "Add all filenames matching the provided pattern (e.g. *.bpmn) to the\nparser's set.", "id": "f7729:c0:m4"}
{"signature": "def toggle_laser(self, state):", "body": "<EOL>a = self.cnxn.xfer([<NUM_LIT>])[<NUM_LIT:0>]<EOL>sleep(<NUM_LIT>)<EOL>if state:<EOL><INDENT>b = self.cnxn.xfer([<NUM_LIT>])[<NUM_LIT:0>]<EOL><DEDENT>else:<EOL><INDENT>b = self.cnxn.xfer([<NUM_LIT>])[<NUM_LIT:0>]<EOL><DEDENT>sleep(<NUM_LIT:0.1>)<EOL>return True if a == <NUM_LIT> and b == <NUM_LIT> else False<EOL>", "docstring": "Toggle the power state of the laser.\n\n        :param state: Boolean state of the laser\n\n        :type state: boolean\n\n        :rtype: boolean\n\n        :Example:\n\n        >>> alpha.toggle_laser(True)\n        True", "id": "f4940:c1:m12"}
{"signature": "def sort(filename, key, outputFile, fields=None, watermark=<NUM_LIT> * <NUM_LIT> * <NUM_LIT:100>):", "body": "if fields is not None:<EOL><INDENT>assert set(key).issubset(set([f[<NUM_LIT:0>] for f in fields]))<EOL><DEDENT>with FileRecordStream(filename) as f:<EOL><INDENT>if fields:<EOL><INDENT>fieldNames = [ff[<NUM_LIT:0>] for ff in fields]<EOL>indices = [f.getFieldNames().index(name) for name in fieldNames]<EOL>assert len(indices) == len(fields)<EOL><DEDENT>else:<EOL><INDENT>fileds = f.getFields()<EOL>fieldNames = f.getFieldNames()<EOL>indices = None<EOL><DEDENT>key = [fieldNames.index(name) for name in key]<EOL>chunk = <NUM_LIT:0><EOL>records = []<EOL>for i, r in enumerate(f):<EOL><INDENT>if indices:<EOL><INDENT>temp = []<EOL>for i in indices:<EOL><INDENT>temp.append(r[i])<EOL><DEDENT>r = temp<EOL><DEDENT>records.append(r)<EOL>available_memory = psutil.avail_phymem()<EOL>if available_memory < watermark:<EOL><INDENT>_sortChunk(records, key, chunk, fields)<EOL>records = []<EOL>chunk += <NUM_LIT:1><EOL><DEDENT><DEDENT>if len(records) > <NUM_LIT:0>:<EOL><INDENT>_sortChunk(records, key, chunk, fields)<EOL>chunk += <NUM_LIT:1><EOL><DEDENT>_mergeFiles(key, chunk, outputFile, fields)<EOL><DEDENT>", "docstring": "Sort a potentially big file\n\n    filename - the input file (standard File format)\n    key - a list of field names to sort by\n    outputFile - the name of the output file\n    fields - a list of fields that should be included (all fields if None)\n    watermark - when available memory goes bellow the watermark create a new chunk\n\n    sort() works by reading as records from the file into memory\n    and calling _sortChunk() on each chunk. In the process it gets\n    rid of unneeded fields if any. Once all the chunks have been sorted and\n    written to chunk files it calls _merge() to merge all the chunks into a\n    single sorted file.\n\n    Note, that sort() gets a key that contains field names, which it converts\n    into field indices for _sortChunk() becuase _sortChunk() doesn't need to know\n    the field name.\n\n    sort() figures out by itself how many chunk files to use by reading records\n    from the file until the low watermark value of availabel memory is hit and\n    then it sorts the current records, generates a chunk file, clears the sorted\n    records and starts on a new chunk.\n\n    The key field names are turned into indices", "id": "f17647:m0"}
{"signature": "def _fill_pattern_collection(self, pattern_collection, values):", "body": "pattern = values.get(PATTERNS, [])<EOL>for pattern_to_parse in pattern:<EOL><INDENT>parsed_pattern = self._pattern(pattern_to_parse)<EOL>pattern_collection.append(parsed_pattern)<EOL><DEDENT>", "docstring": "Fill a pattern collection.", "id": "f560:c1:m11"}
{"signature": "def addOptionString(self, name, value, append=False):", "body": "return self.options.AddOptionString(<EOL>str_to_cppstr(name), str_to_cppstr(value), append)<EOL>", "docstring": ".. _addOptionString:\n\nAdd a string option.\n\n:param name: The name of the option.  Option names are case insensitive and must be unique.\n:type name: str\n:param value: The value of the option.\n:type value: str\n:param append: Setting append to true will cause values read from the command line\n or XML file to be concatenated into a comma delimited set.  If _append is false,\n newer values will overwrite older ones.\n:type append: boolean\n:return: The result of the operation.\n:rtype: bool\n\n:see: addOption_, addOptionBool_, addOptionInt_", "id": "f8510:c0:m7"}
{"signature": "def _read_compound_from_auxi_file_(file_name):", "body": "with open(file_name) as f:<EOL><INDENT>return json.load(f)<EOL><DEDENT>", "docstring": "Build a dictionary containing the auxi thermochemical data of a compound by\nreading the data from a file.\n\n:param file_name: Name of file to read the data from.\n\n:returns: Dictionary containing compound data.", "id": "f15866:m4"}
{"signature": "def track_enrollment(pathway, user_id, course_run_id, url_path=None):", "body": "track_event(user_id, '<STR_LIT>', {<EOL>'<STR_LIT>': pathway,<EOL>'<STR_LIT>': url_path,<EOL>'<STR_LIT>': course_run_id,<EOL>})<EOL>", "docstring": "Emit a track event for enterprise course enrollment.", "id": "f16088:m26"}
{"signature": "def EQ104(T, A, B, C, D, E, order=<NUM_LIT:0>):", "body": "if order == <NUM_LIT:0>:<EOL><INDENT>T2 = T*T<EOL>return A + (B + (C + (D + E/T)/(T2*T2*T))/T2)/T<EOL><DEDENT>elif order == <NUM_LIT:1>:<EOL><INDENT>T2 = T*T<EOL>T4 = T2*T2<EOL>return (-B + (-<NUM_LIT:3>*C + (-<NUM_LIT:8>*D - <NUM_LIT:9>*E/T)/(T4*T))/T2)/T2<EOL><DEDENT>elif order == -<NUM_LIT:1>:<EOL><INDENT>return A*T + B*log(T) - (<NUM_LIT>*C*T**<NUM_LIT:6> + <NUM_LIT:8>*D*T + <NUM_LIT:7>*E)/(<NUM_LIT>*T**<NUM_LIT:8>)<EOL><DEDENT>elif order == -<NUM_LIT>:<EOL><INDENT>return A*log(T) - (<NUM_LIT>*B*T**<NUM_LIT:8> + <NUM_LIT>*C*T**<NUM_LIT:6> + <NUM_LIT:9>*D*T + <NUM_LIT:8>*E)/(<NUM_LIT>*T**<NUM_LIT:9>)<EOL><DEDENT>else:<EOL><INDENT>raise Exception(order_not_found_msg)<EOL><DEDENT>", "docstring": "r'''DIPPR Equation #104. Often used in calculating second virial\n    coefficients of gases. All 5 parameters are required.\n    C, D, and E are normally large values.\n\n    .. math::\n        Y = A + \\frac{B}{T} + \\frac{C}{T^3} + \\frac{D}{T^8} + \\frac{E}{T^9}\n\n    Parameters\n    ----------\n    T : float\n        Temperature, [K]\n    A-E : float\n        Parameter for the equation; chemical and property specific [-]\n    order : int, optional\n        Order of the calculation. 0 for the calculation of the result itself;\n        for 1, the first derivative of the property is returned, for\n        -1, the indefinite integral of the property with respect to temperature\n        is returned; and for -1j, the indefinite integral of the property\n        divided by temperature with respect to temperature is returned. No \n        other integrals or derivatives are implemented, and an exception will \n        be raised if any other order is given.\n\n    Returns\n    -------\n    Y : float\n        Property [constant-specific; if order == 1, property/K; if order == -1,\n                  property*K; if order == -1j, unchanged from default]\n\n    Notes\n    -----\n    The derivative with respect to T, integral with respect to T, and integral\n    over T with respect to T are computed as follows. All expressions can be\n    obtained with SymPy readily.\n\n    .. math::\n        \\frac{d Y}{dT} = - \\frac{B}{T^{2}} - \\frac{3 C}{T^{4}} \n        - \\frac{8 D}{T^{9}} - \\frac{9 E}{T^{10}}\n\n    .. math::\n        \\int Y dT = A T + B \\log{\\left (T \\right )} - \\frac{1}{56 T^{8}} \n        \\left(28 C T^{6} + 8 D T + 7 E\\right)\n\n    .. math::\n        \\int \\frac{Y}{T} dT = A \\log{\\left (T \\right )} - \\frac{1}{72 T^{9}} \n        \\left(72 B T^{8} + 24 C T^{6} + 9 D T + 8 E\\right)\n\n    Examples\n    --------\n    Water second virial coefficient; DIPPR coefficients normally dimensionless.\n\n    >>> EQ104(300, 0.02222, -26.38, -16750000, -3.894E19, 3.133E21)\n    -1.1204179007265156\n\n    References\n    ----------\n    .. [1] Design Institute for Physical Properties, 1996. DIPPR Project 801\n       DIPPR/AIChE", "id": "f15808:m3"}
{"signature": "@classmethod<EOL><INDENT>def from_xml(cls, element):<DEDENT>", "body": "<EOL>items = []<EOL>jids = set()<EOL>if element.tag != QUERY_TAG:<EOL><INDENT>raise ValueError(\"<STR_LIT>\".format(element))<EOL><DEDENT>version = element.get(\"<STR_LIT>\")<EOL>for child in element:<EOL><INDENT>if child.tag != ITEM_TAG:<EOL><INDENT>logger.debug(\"<STR_LIT>\".format(child))<EOL>continue<EOL><DEDENT>item = RosterItem.from_xml(child)<EOL>if item.jid in jids:<EOL><INDENT>logger.warning(\"<STR_LIT>\".format(<EOL>item.jid))<EOL>continue<EOL><DEDENT>jids.add(item.jid)<EOL>items.append(item)<EOL><DEDENT>return cls(items, version)<EOL>", "docstring": "Create a `RosterPayload` object from an XML element.\n\n:Parameters:\n    - `element`: the XML element\n:Types:\n    - `element`: :etree:`ElementTree.Element`\n\n:return: a freshly created roster payload\n:returntype: `cls`", "id": "f15245:c4:m1"}
{"signature": "def __setitem__(self, name, value):", "body": "del self[name] <EOL>self.dict[name.lower()] = value<EOL>text = name + \"<STR_LIT>\" + value<EOL>for line in text.split(\"<STR_LIT:\\n>\"):<EOL><INDENT>self.headers.append(line + \"<STR_LIT:\\n>\")<EOL><DEDENT>", "docstring": "Set the value of a header.\n\n        Note: This is not a perfect inversion of __getitem__, because any\n        changed headers get stuck at the end of the raw-headers list rather\n        than where the altered header was.", "id": "f16460:c0:m17"}
{"signature": "def get_messages(session, query, limit=<NUM_LIT:10>, offset=<NUM_LIT:0>):", "body": "query['<STR_LIT>'] = limit<EOL>query['<STR_LIT>'] = offset<EOL>response = make_get_request(session, '<STR_LIT>', params_data=query)<EOL>json_data = response.json()<EOL>if response.status_code == <NUM_LIT:200>:<EOL><INDENT>return json_data['<STR_LIT:result>']<EOL><DEDENT>else:<EOL><INDENT>raise MessagesNotFoundException(<EOL>message=json_data['<STR_LIT:message>'],<EOL>error_code=json_data['<STR_LIT>'],<EOL>request_id=json_data['<STR_LIT>']<EOL>)<EOL><DEDENT>", "docstring": "Get one or more messages", "id": "f12295:m4"}
{"signature": "def _calcula_factura(self):", "body": "<EOL>self._dict_repr = None<EOL>self._str_repr = None<EOL>self._html_repr = None<EOL>self._html_repr_completa = None<EOL>cod_tarifa = DATOS_TIPO_PEAJE[self._tipo_peaje][<NUM_LIT:1>]<EOL>year = self._t0.year<EOL>year_f = self._tf.year<EOL>self._num_dias_factura = (self._tf - self._t0).days  <EOL>if year_f > year:<EOL><INDENT>ts_limit = pd.Timestamp('<STR_LIT>'.format(year))<EOL>days_1 = (ts_limit - self._t0).days<EOL>days_2 = (self._tf - ts_limit).days<EOL>n_days_y1 = (pd.Timestamp('<STR_LIT>'.format(year + <NUM_LIT:1>)) - pd.Timestamp('<STR_LIT>'.format(year))).days<EOL>n_days_y2 = (pd.Timestamp('<STR_LIT>'.format(year_f + <NUM_LIT:1>)) - pd.Timestamp('<STR_LIT>'.format(year_f))).days<EOL>self._periodos_fact = ((days_1, n_days_y1, year), (days_2, n_days_y2, year_f))<EOL><DEDENT>else:<EOL><INDENT>n_days_y = (pd.Timestamp('<STR_LIT>'.format(year + <NUM_LIT:1>)) - pd.Timestamp('<STR_LIT>'.format(year))).days<EOL>self._periodos_fact = ((self._num_dias_factura, n_days_y, year),)<EOL><DEDENT>if self._pvpc_data is None:<EOL><INDENT>pvpc_data = PVPC(update=True, verbose=False)<EOL>self._pvpc_data = pvpc_calc_tcu_cp_feu_d(pvpc_data.data['<STR_LIT:data>'], verbose=False, convert_kwh=True)<EOL><DEDENT>cols_tarifa = list(filter(lambda x: cod_tarifa in x, self._pvpc_data.columns))<EOL>pvpc_t_ini, pvpc_t_fin = self._t0 + pd.Timedelta('<STR_LIT>'), self._tf + pd.Timedelta('<STR_LIT>')<EOL>self._pvpc_horario = self._pvpc_data[cols_tarifa].loc[pvpc_t_ini:pvpc_t_fin].iloc[:-<NUM_LIT:1>]<EOL>self._termino_fijo, self._termino_fijo_total = [], <NUM_LIT:0><EOL>for (days_fac, days_year, year) in self._periodos_fact:<EOL><INDENT>coef_potencia = MARGEN_COMERCIALIZACI\u00d3N_EUR_KW_A\u00d1O_MCF + TERM_POT_PEAJE_ACCESO_EUR_KW_A\u00d1O_TPA[year]<EOL>coste = self._potencia_contratada * days_fac * coef_potencia / days_year<EOL>self._termino_fijo.append((coste, coef_potencia))<EOL>self._termino_fijo_total += coste<EOL><DEDENT>self._termino_fijo_total = self._round(self._termino_fijo_total)<EOL>if self._consumo is not None:<EOL><INDENT>son_totales, consumo_calc = self._consumo_numerico()<EOL>if son_totales:<EOL><INDENT>c_coef = '<STR_LIT>'.format(cod_tarifa)<EOL>hay_discr, perfs_interv = self._asigna_periodos_discr_horaria(self._pvpc_horario[c_coef])<EOL>if not hay_discr:<EOL><INDENT>self._consumo_horario = (perfs_interv * consumo_calc[<NUM_LIT:0>] / perfs_interv.sum()).rename(COL_CONSUMO)<EOL><DEDENT>else:<EOL><INDENT>consumos_horarios_periodos = []<EOL>for i, cons_periodo_i in enumerate(consumo_calc):<EOL><INDENT>c = '<STR_LIT>'.format(i + <NUM_LIT:1>)<EOL>idx = perfs_interv[perfs_interv[c]].index<EOL>consumos_horarios_periodos.append(perfs_interv.loc[idx, c_coef] * cons_periodo_i<EOL>/ perfs_interv.loc[idx, c_coef].sum())<EOL><DEDENT>self._consumo_horario = pd.Series(pd.concat(consumos_horarios_periodos)<EOL>).rename(COL_CONSUMO).sort_index()<EOL><DEDENT><DEDENT>else:<EOL><INDENT>self._consumo_horario = self._check_hourly_data(consumo_calc)<EOL>t0, tf = self._consumo_horario.index[<NUM_LIT:0>], self._consumo_horario.index[-<NUM_LIT:1>]<EOL>self._pvpc_horario = self._pvpc_horario.loc[t0:tf]<EOL><DEDENT>col_tcu = '<STR_LIT>'.format(cod_tarifa)<EOL>if len(self._periodos_fact) > <NUM_LIT:1>:<EOL><INDENT>ts_limit = pd.Timestamp('<STR_LIT>'.format(self._tf.year)).tz_localize(self._consumo_horario.index.tz)<EOL>consumo_1 = self._consumo_horario.loc[:ts_limit].iloc[:-<NUM_LIT:1>]<EOL>consumo_2 = self._consumo_horario.loc[ts_limit:]<EOL>pvpc_1 = self._pvpc_horario.loc[:ts_limit].iloc[:-<NUM_LIT:1>]<EOL>pvpc_2 = self._pvpc_horario.loc[ts_limit:]<EOL>tea_y1, tcu_y1, cons_tot_y1 = list(zip(*self._coste_tea_tcu(consumo_1, pvpc_1[col_tcu], self._t0.year)))<EOL>tea_y2, tcu_y2, cons_tot_y2 = list(zip(*self._coste_tea_tcu(consumo_2, pvpc_2[col_tcu], self._tf.year)))<EOL>self._coste_peaje_acceso_tea = (tea_y1, tea_y2)<EOL>self._coste_ponderado_energia_tcu = (tcu_y1, tcu_y2)<EOL>self._consumos_totales_por_periodo = (cons_tot_y1, cons_tot_y2)<EOL>coste_variable_tot = sum([sum(x) for x in self._coste_peaje_acceso_tea])<EOL>coste_variable_tot += sum([sum(x) for x in self._coste_ponderado_energia_tcu])<EOL><DEDENT>else:<EOL><INDENT>tea, tcu, cons_tot = list(zip(*self._coste_tea_tcu(self._consumo_horario, self._pvpc_horario[col_tcu],<EOL>self._t0.year)))<EOL>self._coste_peaje_acceso_tea = tea<EOL>self._coste_ponderado_energia_tcu = tcu<EOL>self._consumos_totales_por_periodo = cons_tot<EOL>coste_variable_tot = self._round_sum(self._coste_peaje_acceso_tea)<EOL>coste_variable_tot += self._round_sum(self._coste_ponderado_energia_tcu)<EOL><DEDENT>self._consumo = self._consumo_horario<EOL><DEDENT>else:<EOL><INDENT>self._coste_peaje_acceso_tea = (<NUM_LIT:0.>,)<EOL>self._coste_ponderado_energia_tcu = (<NUM_LIT:0.>,)<EOL>self._consumos_totales_por_periodo = (<NUM_LIT:0.>,)<EOL>coste_variable_tot = <NUM_LIT:0.><EOL><DEDENT>self._termino_variable_total = self._round(coste_variable_tot)<EOL>subt_fijo_var = self._termino_fijo_total + self._termino_variable_total<EOL>self._descuento_bono_social = <NUM_LIT:0.><EOL>if self._con_bono_social:<EOL><INDENT>self._descuento_bono_social = self._round(-<NUM_LIT> * self._round(subt_fijo_var))<EOL>subt_fijo_var += self._descuento_bono_social<EOL><DEDENT>self._termino_impuesto_electrico = self._round(self._impuesto_electrico_general * subt_fijo_var)<EOL>subt_fijo_var += self._termino_impuesto_electrico<EOL>if self.alquiler_euros is not None:<EOL><INDENT>self._termino_equipo_medida = self._round(self.alquiler_euros)<EOL><DEDENT>else:<EOL><INDENT>frac_a\u00f1o = sum([nd / dy for nd, dy, _ in self._periodos_fact])<EOL>self._termino_equipo_medida = self._round(frac_a\u00f1o * self.alquiler_euros_a\u00f1o)<EOL><DEDENT>self._calcula_iva_y_total()<EOL>", "docstring": "M\u00e9todo para regenerar el c\u00e1lculo de la factura el\u00e9ctrica.", "id": "f14218:c0:m11"}
{"signature": "def get_function(function_name):", "body": "module, basename = str(function_name).rsplit('<STR_LIT:.>', <NUM_LIT:1>)<EOL>try:<EOL><INDENT>return getattr(__import__(module, fromlist=[basename]), basename)<EOL><DEDENT>except (ImportError, AttributeError):<EOL><INDENT>raise FunctionNotFound(function_name)<EOL><DEDENT>", "docstring": "Given a Python function name, return the function it refers to.", "id": "f13910:m0"}
{"signature": "def p_conceptualTable(self, p):", "body": "p[<NUM_LIT:0>] = ('<STR_LIT>', p[<NUM_LIT:3>])<EOL>", "docstring": "conceptualTable : SEQUENCE OF row", "id": "f5672:c0:m32"}
{"signature": "def is_denied(self, role, method, resource):", "body": "return (role, method, resource) in self._denied<EOL>", "docstring": "Check wherther role is denied to access resource\n\n        :param role: Role to be checked.\n        :param method: Method to be checked.\n        :param resource: View function to be checked.", "id": "f12671:c0:m5"}
{"signature": "def get_enterprise_customer_from_catalog_id(catalog_id):", "body": "try:<EOL><INDENT>return str(EnterpriseCustomerCatalog.objects.get(pk=catalog_id).enterprise_customer.uuid)<EOL><DEDENT>except EnterpriseCustomerCatalog.DoesNotExist:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Get the enterprise customer id given an enterprise customer catalog id.", "id": "f16081:m1"}
{"signature": "def describeStats(stats, stream, title=None, details=True, totals=True, gettext=None):", "body": "from . import state<EOL>modeStringLut = dict((<EOL>(constants.SYNCTYPE_TWO_WAY,             '<STR_LIT>'),<EOL>(constants.SYNCTYPE_SLOW_SYNC,           '<STR_LIT>'),<EOL>(constants.SYNCTYPE_ONE_WAY_FROM_CLIENT, '<STR_LIT>'),<EOL>(constants.SYNCTYPE_REFRESH_FROM_CLIENT, '<STR_LIT>'),<EOL>(constants.SYNCTYPE_ONE_WAY_FROM_SERVER, '<STR_LIT>'),<EOL>(constants.SYNCTYPE_REFRESH_FROM_SERVER, '<STR_LIT>'),<EOL>))<EOL>if gettext is not None:<EOL><INDENT>_ = gettext<EOL><DEDENT>else:<EOL><INDENT>_ = lambda s: s<EOL><DEDENT>wSrc  = len(_('<STR_LIT>'))<EOL>wMode = len(_('<STR_LIT>'))<EOL>wCon  = len(_('<STR_LIT>'))<EOL>wCol  = len(_('<STR_LIT>'))<EOL>wMrg  = len(_('<STR_LIT>'))<EOL>wHereAdd = wPeerAdd = len(_('<STR_LIT>'))<EOL>wHereMod = wPeerMod = len(_('<STR_LIT>'))<EOL>wHereDel = wPeerDel = len(_('<STR_LIT>'))<EOL>wHereErr = wPeerErr = len(_('<STR_LIT>'))<EOL>totLoc = <NUM_LIT:0><EOL>totRem = <NUM_LIT:0><EOL>totErr = <NUM_LIT:0><EOL>totCol = <NUM_LIT:0><EOL>totMrg = <NUM_LIT:0><EOL>for key in stats.keys():<EOL><INDENT>wSrc  = max(wSrc, len(key))<EOL>wMode = max(wMode, len(modeStringLut.get(stats[key].mode)))<EOL>wCol  = max(wCol, len(num2str(stats[key].conflicts)))<EOL>wMrg  = max(wMrg, len(num2str(stats[key].merged)))<EOL>wHereAdd = max(wHereAdd, len(num2str(stats[key].hereAdd)))<EOL>wPeerAdd = max(wPeerAdd, len(num2str(stats[key].peerAdd)))<EOL>wHereMod = max(wHereMod, len(num2str(stats[key].hereMod)))<EOL>wPeerMod = max(wPeerMod, len(num2str(stats[key].peerMod)))<EOL>wHereDel = max(wHereDel, len(num2str(stats[key].hereDel)))<EOL>wPeerDel = max(wPeerDel, len(num2str(stats[key].peerDel)))<EOL>wHereErr = max(wHereErr, len(num2str(stats[key].hereErr)))<EOL>wPeerErr = max(wPeerErr, len(num2str(stats[key].peerErr)))<EOL>totLoc += stats[key].hereAdd + stats[key].hereMod + stats[key].hereDel<EOL>totRem += stats[key].peerAdd + stats[key].peerMod + stats[key].peerDel<EOL>totErr += stats[key].hereErr + stats[key].peerErr<EOL>totCol += stats[key].conflicts<EOL>totMrg += stats[key].merged<EOL><DEDENT>if wCon > wCol + <NUM_LIT:3> + wMrg:<EOL><INDENT>diff = wCon - ( wCol + <NUM_LIT:3> + wMrg )<EOL>wCol += diff / <NUM_LIT:2><EOL>wMrg = wCon - <NUM_LIT:3> - wCol<EOL><DEDENT>else:<EOL><INDENT>wCon = wCol + <NUM_LIT:3> + wMrg<EOL><DEDENT>if details:<EOL><INDENT>tWid = ( wSrc + <NUM_LIT:3> + wMode + <NUM_LIT:3><EOL>+ wHereAdd + wHereMod + wHereDel + wHereErr + <NUM_LIT:9> + <NUM_LIT:3><EOL>+ wPeerAdd + wPeerMod + wPeerDel + wPeerErr + <NUM_LIT:9> + <NUM_LIT:3><EOL>+ wCon )<EOL><DEDENT>else:<EOL><INDENT>if title is None:<EOL><INDENT>tWid = <NUM_LIT:0><EOL><DEDENT>else:<EOL><INDENT>tWid = len(title)<EOL><DEDENT><DEDENT>if totals:<EOL><INDENT>sumlist = []<EOL>for val, singular, plural in [<EOL>(totLoc, _('<STR_LIT>'), _('<STR_LIT>')),<EOL>(totRem, _('<STR_LIT>'), _('<STR_LIT>')),<EOL>(totErr, _('<STR_LIT:error>'), _('<STR_LIT>')),<EOL>]:<EOL><INDENT>if val == <NUM_LIT:1>:<EOL><INDENT>sumlist.append(num2str(val) + '<STR_LIT:U+0020>' + singular)<EOL><DEDENT>elif val > <NUM_LIT:1>:<EOL><INDENT>sumlist.append(num2str(val) + '<STR_LIT:U+0020>' + plural)<EOL><DEDENT><DEDENT>if len(sumlist) <= <NUM_LIT:0>:<EOL><INDENT>sumlist = _('<STR_LIT>')<EOL><DEDENT>elif len(sumlist) == <NUM_LIT:1>:<EOL><INDENT>sumlist = sumlist[<NUM_LIT:0>]<EOL><DEDENT>else:<EOL><INDENT>sumlist = '<STR_LIT:U+002CU+0020>'.join(sumlist[:-<NUM_LIT:1>]) + '<STR_LIT:U+0020>' + _('<STR_LIT>') + '<STR_LIT:U+0020>' + sumlist[-<NUM_LIT:1>]<EOL><DEDENT>if totMrg > <NUM_LIT:0> or totCol > <NUM_LIT:0>:<EOL><INDENT>sumlist += '<STR_LIT>'<EOL>if totMrg == <NUM_LIT:1>:<EOL><INDENT>sumlist += num2str(totMrg) + '<STR_LIT:U+0020>' + _('<STR_LIT>')<EOL><DEDENT>elif totMrg > <NUM_LIT:1>:<EOL><INDENT>sumlist += num2str(totMrg) + '<STR_LIT:U+0020>' + _('<STR_LIT>')<EOL><DEDENT>if totMrg > <NUM_LIT:0> and totCol > <NUM_LIT:0>:<EOL><INDENT>sumlist += '<STR_LIT:U+0020>' + _('<STR_LIT>') + '<STR_LIT:U+0020>'<EOL><DEDENT>if totCol == <NUM_LIT:1>:<EOL><INDENT>sumlist += num2str(totCol) + '<STR_LIT:U+0020>' + _('<STR_LIT>')<EOL><DEDENT>elif totCol > <NUM_LIT:1>:<EOL><INDENT>sumlist += num2str(totCol) + '<STR_LIT:U+0020>' + _('<STR_LIT>')<EOL><DEDENT><DEDENT>sumlist += '<STR_LIT:.>'<EOL>if len(sumlist) > tWid:<EOL><INDENT>wSrc += len(sumlist) - tWid<EOL>tWid = len(sumlist)<EOL><DEDENT><DEDENT>if title is not None:<EOL><INDENT>stream.write('<STR_LIT>' + '<STR_LIT:->' * tWid + '<STR_LIT>')<EOL>stream.write('<STR_LIT>'.format(title, w=tWid))<EOL>stream.write('<STR_LIT>')<EOL><DEDENT>hline = '<STR_LIT>'+ '<STR_LIT:->' * wSrc+ '<STR_LIT>'+ '<STR_LIT:->' * wMode+ '<STR_LIT>'+ '<STR_LIT:->' * ( wHereAdd + wHereMod + wHereDel + wHereErr + <NUM_LIT:9> )+ '<STR_LIT>'+ '<STR_LIT:->' * ( wPeerAdd + wPeerMod + wPeerDel + wPeerErr + <NUM_LIT:9> )+ '<STR_LIT>'+ '<STR_LIT:->' * wCon+ '<STR_LIT>'<EOL>if details:<EOL><INDENT>stream.write(hline)<EOL>stream.write('<STR_LIT>' + '<STR_LIT:U+0020>' * wSrc)<EOL>stream.write('<STR_LIT>' + '<STR_LIT:U+0020>' * wMode)<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=( wHereAdd + wHereMod + wHereDel + wHereErr + <NUM_LIT:9> )))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=( wPeerAdd + wPeerMod + wPeerDel + wPeerErr + <NUM_LIT:9> )))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wCon))<EOL>stream.write('<STR_LIT>')<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wSrc))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wMode))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wHereAdd))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wHereMod))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wHereDel))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wHereErr))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wPeerAdd))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wPeerMod))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wPeerDel))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wPeerErr))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wCol))<EOL>stream.write('<STR_LIT>'.format(_('<STR_LIT>'), w=wMrg))<EOL>stream.write('<STR_LIT>')<EOL>hsline = '<STR_LIT>' + '<STR_LIT:->' * wSrc+ '<STR_LIT>' + '<STR_LIT:->' * wMode+ '<STR_LIT>' + '<STR_LIT:->' * wHereAdd+ '<STR_LIT>' + '<STR_LIT:->' * wHereMod+ '<STR_LIT>' + '<STR_LIT:->' * wHereDel+ '<STR_LIT>' + '<STR_LIT:->' * wHereErr+ '<STR_LIT>' + '<STR_LIT:->' * wPeerAdd+ '<STR_LIT>' + '<STR_LIT:->' * wPeerMod+ '<STR_LIT>' + '<STR_LIT:->' * wPeerDel+ '<STR_LIT>' + '<STR_LIT:->' * wPeerErr+ '<STR_LIT>' + '<STR_LIT:->' * wCol+ '<STR_LIT>' + '<STR_LIT:->' * wMrg+ '<STR_LIT>'<EOL>stream.write(hsline)<EOL>def numcol(val, wid):<EOL><INDENT>if val == <NUM_LIT:0>:<EOL><INDENT>return '<STR_LIT>'.format('<STR_LIT:->', w=wid)<EOL><DEDENT>return '<STR_LIT>'.format(num2str(val), w=wid)<EOL><DEDENT>for key in sorted(stats.keys(), key=lambda k: str(k).lower()):<EOL><INDENT>stream.write('<STR_LIT>'.format(key, w=wSrc))<EOL>stream.write('<STR_LIT>'.format(modeStringLut.get(stats[key].mode), w=wMode))<EOL>stream.write(numcol(stats[key].hereAdd, wHereAdd))<EOL>stream.write(numcol(stats[key].hereMod, wHereMod))<EOL>stream.write(numcol(stats[key].hereDel, wHereDel))<EOL>stream.write(numcol(stats[key].hereErr, wHereErr))<EOL>stream.write(numcol(stats[key].peerAdd, wPeerAdd))<EOL>stream.write(numcol(stats[key].peerMod, wPeerMod))<EOL>stream.write(numcol(stats[key].peerDel, wPeerDel))<EOL>stream.write(numcol(stats[key].peerErr, wPeerErr))<EOL>stream.write(numcol(stats[key].conflicts, wCol))<EOL>stream.write(numcol(stats[key].merged, wMrg))<EOL>stream.write('<STR_LIT>')<EOL><DEDENT>stream.write(hsline)<EOL><DEDENT>if totals:<EOL><INDENT>if title is None and not details:<EOL><INDENT>stream.write('<STR_LIT>' + '<STR_LIT:->' * tWid + '<STR_LIT>')<EOL><DEDENT>stream.write('<STR_LIT>'.format(sumlist, w=tWid))<EOL>stream.write('<STR_LIT>')<EOL>stream.write('<STR_LIT>' + '<STR_LIT:->' * tWid + '<STR_LIT>')<EOL><DEDENT>return<EOL>", "docstring": "Renders an ASCII-table of the synchronization statistics `stats`,\nexample output:\n\n.. code-block::\n\n  +----------------------------------------------------------------------------------+\n  |                                      TITLE                                       |\n  +----------+------+-------------------------+--------------------------+-----------+\n  |          |      |          Local          |          Remote          | Conflicts |\n  |   Source | Mode |  Add  | Mod | Del | Err |   Add  | Mod | Del | Err | Col | Mrg |\n  +----------+------+-------+-----+-----+-----+--------+-----+-----+-----+-----+-----+\n  | contacts |  <=  |   -   |  -  |  -  |  -  | 10,387 |  -  |  -  |  -  |  -  |  -  |\n  |     note |  SS  | 1,308 |  -  |   2 |  -  |    -   |  -  |  -  |  -  |  -  |  -  |\n  +----------+------+-------+-----+-----+-----+--------+-----+-----+-----+-----+-----+\n  |                  1,310 local changes and 10,387 remote changes.                  |\n  +----------------------------------------------------------------------------------+\n\n:Parameters:\n\nstats : dict\n\n  The synchronization stats returned by a call to Adapter.sync().\n\nstream : file-like-object\n\n  An output file-like object that has at least a `write()` method,\n  e.g. ``sys.stdout`` can be used.\n\ntitle : str, optional, default: null\n\n  A title placed at the top of the table -- if omitted (the default),\n  then no title is rendered.\n\ndetails : bool, optional, default: true\n\n  If truthy, a per-datastore listing of changes will be displayed\n  (as in the above example).\n\ntotals : bool, optional, default: true\n\n  If truthy, a summary of all changes will be displayed (as in the\n  above example).\n\ngettext : callable, optional, @DEPRECATED(0.2.0), default: null\n\n  A `gettext.gettext` compatible callable used for translating\n  localized content (such as number formatting, etc.).\n\n  NOTE: this parameter is deprecated, and will be replaced with\n  a generalized i18n solution.", "id": "f12349:m12"}
{"signature": "def to_unicode(obj):", "body": "if not isinstance(obj, six.string_types):<EOL><INDENT>return obj<EOL><DEDENT>try:<EOL><INDENT>obj = six.text_type(obj)<EOL><DEDENT>except TypeError:<EOL><INDENT>pass<EOL><DEDENT>return obj<EOL>", "docstring": "Convert obj to unicode (if it can be be converted).\n\n    Conversion is only attempted if `obj` is a string type (as\n    determined by :data:`six.string_types`).\n\n    .. versionchanged:: 0.7.0\n       removed `encoding keyword argument", "id": "f6857:m0"}
{"signature": "def __divmod__(self, other):", "body": "return (np.floor_divide(self - self % other, other), self % other)<EOL>", "docstring": "x.__divmod__(y) <==> divmod(x, y)", "id": "f4853:c0:m20"}
{"signature": "def _to_alpha(self, num):", "body": "num = '<STR_LIT>'.join(c for c in text_type(num) if c in self._num_set)<EOL>return num.translate(self._num_trans)<EOL>", "docstring": "Convert a K\u00f6lner Phonetik code from numeric to alphabetic.\n\n        Parameters\n        ----------\n        num : str or int\n            A numeric K\u00f6lner Phonetik representation\n\n        Returns\n        -------\n        str\n            An alphabetic representation of the same word\n\n        Examples\n        --------\n        >>> pe = Koelner()\n        >>> pe._to_alpha('862')\n        'SNT'\n        >>> pe._to_alpha('657')\n        'NLR'\n        >>> pe._to_alpha('86766')\n        'SNRNN'", "id": "f6603:c0:m1"}
{"signature": "@api.check(<NUM_LIT:2>, \"<STR_LIT>\")<EOL>def win_set_state(title, flag=properties.SW_SHOW, **kwargs):", "body": "text = kwargs.get(\"<STR_LIT:text>\", \"<STR_LIT>\")<EOL>ret = AUTO_IT.AU3_WinSetState(LPCWSTR(title), LPCWSTR(text), INT(flag))<EOL>return ret<EOL>", "docstring": ":param title:\n:param flag: The \"show\" flag of the executed program:\n    SW_HIDE = Hide window\n    SW_SHOW = Shows a previously hidden window\n    SW_MINIMIZE = Minimize window\n    SW_MAXIMIZE = Maximize window\n    SW_RESTORE = Undoes a window minimization or maximization\n:param kwargs:\n:return:", "id": "f5587:m35"}
{"signature": "def discretize_action_space_desc(self):", "body": "<EOL>unique_list = []<EOL>for nice, record in self.action_space_desc.items():<EOL><INDENT>list_for_record = []<EOL>if record[\"<STR_LIT:type>\"] == \"<STR_LIT>\":<EOL><INDENT>head_key = record[\"<STR_LIT>\"][<NUM_LIT:0>][<NUM_LIT:0>]<EOL>head_value = record[\"<STR_LIT>\"][<NUM_LIT:0>][<NUM_LIT:1>]<EOL>list_for_record.append((head_key, <NUM_LIT:0.0>))<EOL>set_ = set()<EOL>for key_and_scale in self.action_space_desc[nice][\"<STR_LIT>\"]:<EOL><INDENT>if key_and_scale[<NUM_LIT:1>] not in set_:<EOL><INDENT>list_for_record.append((head_key, key_and_scale[<NUM_LIT:1>] / head_value))<EOL>set_.add(key_and_scale[<NUM_LIT:1>])<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>list_for_record = [(record[\"<STR_LIT>\"][<NUM_LIT:0>], False), (record[\"<STR_LIT>\"][<NUM_LIT:0>], True)]<EOL><DEDENT>unique_list.append(list_for_record)<EOL><DEDENT>def so(in_):<EOL><INDENT>st = \"<STR_LIT>\"<EOL>for i in in_:<EOL><INDENT>st += str(i[<NUM_LIT:1>])<EOL><DEDENT>return st<EOL><DEDENT>combinations = list(itertools.product(*unique_list))<EOL>combinations = list(map(lambda x: sorted(list(x), key=lambda y: y[<NUM_LIT:0>]), combinations))<EOL>combinations = sorted(combinations, key=so)<EOL>self.discretized_actions = combinations<EOL>", "docstring": "Creates a list of discrete action(-combinations) in case we want to learn with a discrete set of actions,\nbut only have action-combinations (maybe even continuous) available from the env.\nE.g. the UE4 game has the following action/axis-mappings:\n\n```javascript\n{\n'Fire':\n    {'type': 'action', 'keys': ('SpaceBar',)},\n'MoveRight':\n    {'type': 'axis', 'keys': (('Right', 1.0), ('Left', -1.0), ('A', -1.0), ('D', 1.0))},\n}\n```\n\n-> this method will discretize them into the following 6 discrete actions:\n\n```javascript\n[\n[(Right, 0.0),(SpaceBar, False)],\n[(Right, 0.0),(SpaceBar, True)]\n[(Right, -1.0),(SpaceBar, False)],\n[(Right, -1.0),(SpaceBar, True)],\n[(Right, 1.0),(SpaceBar, False)],\n[(Right, 1.0),(SpaceBar, True)],\n]\n```", "id": "f14228:c0:m10"}
{"signature": "def street_address():", "body": "return '<STR_LIT>' % (street_number(), street_name(), street_suffix())<EOL>", "docstring": "Return a random street address.\n\n    Equivalent of ``street_number() + ' ' +\n    street_name() + ' ' + street_suffix()``.", "id": "f14663:m3"}
{"signature": "def fail(self, message):", "body": "raise UsageError(message, self)<EOL>", "docstring": "Aborts the execution of the program with a specific error\n        message.\n\n        :param message: the error message to fail with.", "id": "f8340:c0:m13"}
{"signature": "def load_effects_classes(self):", "body": "self.effect_classes = []<EOL>for _, cls in inspect.getmembers(self.effect_module):<EOL><INDENT>if inspect.isclass(cls):<EOL><INDENT>if cls == Effect:<EOL><INDENT>continue<EOL><DEDENT>if issubclass(cls, Effect):<EOL><INDENT>self.effect_classes.append(cls)<EOL>self.effect_class_map[cls.__name__] = cls<EOL>cls._name = \"<STR_LIT>\".format(self.effect_module_name, cls.__name__)<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Iterate the module attributes picking out effects", "id": "f14432:c1:m8"}
{"signature": "@property<EOL><INDENT>def mu(self):<DEDENT>", "body": "return phase_select_property(phase=self.phase, l=self.mul, g=self.mug)<EOL>", "docstring": "r'''Viscosity of the chemical at its current phase, temperature, and\n        pressure in units of [Pa*s].\n\n        Utilizes the object oriented interfaces\n        :obj:`thermo.viscosity.ViscosityLiquid` and\n        :obj:`thermo.viscosity.ViscosityGas` to perform the\n        actual calculation of each property.\n\n        Examples\n        --------\n        >>> Chemical('ethanol', T=300).mu\n        0.001044526538460911\n        >>> Chemical('ethanol', T=400).mu\n        1.1853097849748217e-05", "id": "f15812:c0:m100"}
{"signature": "def offset_random_web(seed, amount=<NUM_LIT:1>):", "body": "return rgb_to_web(offset_random_rgb(seed, amount))<EOL>", "docstring": "Given a seed color, generate a specified number of random colors (1 color by default) determined by a randomized\noffset from the seed.\n\n:param seed:\n:param amount:\n:return:", "id": "f9713:m5"}
{"signature": "def generate_thumbnail(source, outname, box, delay, fit=True, options=None,<EOL>converter='<STR_LIT>'):", "body": "logger = logging.getLogger(__name__)<EOL>tmpfile = outname + \"<STR_LIT>\"<EOL>cmd = [converter, '<STR_LIT>', source, '<STR_LIT>', '<STR_LIT>', '<STR_LIT:1>',<EOL>'<STR_LIT>', delay, '<STR_LIT>', '<STR_LIT:1>', '<STR_LIT>', tmpfile]<EOL>logger.debug('<STR_LIT>', '<STR_LIT:U+0020>'.join(cmd))<EOL>check_subprocess(cmd, source, outname)<EOL>image.generate_thumbnail(tmpfile, outname, box, fit=fit, options=options)<EOL>os.unlink(tmpfile)<EOL>", "docstring": "Create a thumbnail image for the video source, based on ffmpeg.", "id": "f13463:m3"}
{"signature": "def add_spouts(parser):", "body": "parser.add_argument(<EOL>'<STR_LIT>', help='<STR_LIT>', action='<STR_LIT:store_true>')<EOL>return parser<EOL>", "docstring": "add optional argument that displays spout only", "id": "f7394:m8"}
{"signature": "def p_file_notice_2(self, p):", "body": "self.error = True<EOL>msg = ERROR_MESSAGES['<STR_LIT>'].format(p.lineno(<NUM_LIT:1>))<EOL>self.logger.log(msg)<EOL>", "docstring": "file_notice : FILE_NOTICE error", "id": "f3753:c0:m35"}
{"signature": "def get_resource(name):", "body": "module = importlib.import_module('<STR_LIT>' % name)<EOL>return module.Resource()<EOL>", "docstring": "Return an instance of the requested Resource class.\n\n    Since all of the resource classes are named `Resource`, this provides\n    a slightly cleaner interface for using these classes via. importing rather\n    than through the CLI.", "id": "f3339:m0"}
{"signature": "def _IsType(clean_lines, nesting_state, expr):", "body": "<EOL>last_word = Match(r'<STR_LIT>', expr)<EOL>if last_word:<EOL><INDENT>token = last_word.group(<NUM_LIT:1>)<EOL><DEDENT>else:<EOL><INDENT>token = expr<EOL><DEDENT>if _TYPES.match(token):<EOL><INDENT>return True<EOL><DEDENT>typename_pattern = (r'<STR_LIT>' + re.escape(token) +<EOL>r'<STR_LIT>')<EOL>block_index = len(nesting_state.stack) - <NUM_LIT:1><EOL>while block_index >= <NUM_LIT:0>:<EOL><INDENT>if isinstance(nesting_state.stack[block_index], _NamespaceInfo):<EOL><INDENT>return False<EOL><DEDENT>last_line = nesting_state.stack[block_index].starting_linenum<EOL>next_block_start = <NUM_LIT:0><EOL>if block_index > <NUM_LIT:0>:<EOL><INDENT>next_block_start = nesting_state.stack[block_index - <NUM_LIT:1>].starting_linenum<EOL><DEDENT>first_line = last_line<EOL>while first_line >= next_block_start:<EOL><INDENT>if clean_lines.elided[first_line].find('<STR_LIT>') >= <NUM_LIT:0>:<EOL><INDENT>break<EOL><DEDENT>first_line -= <NUM_LIT:1><EOL><DEDENT>if first_line < next_block_start:<EOL><INDENT>block_index -= <NUM_LIT:1><EOL>continue<EOL><DEDENT>for i in xrange(first_line, last_line + <NUM_LIT:1>, <NUM_LIT:1>):<EOL><INDENT>if Search(typename_pattern, clean_lines.elided[i]):<EOL><INDENT>return True<EOL><DEDENT><DEDENT>block_index -= <NUM_LIT:1><EOL><DEDENT>return False<EOL>", "docstring": "Check if expression looks like a type name, returns true if so.\n\n    Args:\n      clean_lines: A CleansedLines instance containing the file.\n      nesting_state: A NestingState instance which maintains information about\n                     the current stack of nested blocks being parsed.\n      expr: The expression to check.\n    Returns:\n      True, if token looks like a type.", "id": "f7245:m59"}
{"signature": "def strip_fields(self):", "body": "for tag in self.record.keys():<EOL><INDENT>if tag in self.fields_list:<EOL><INDENT>record_delete_fields(self.record, tag)<EOL><DEDENT><DEDENT>", "docstring": "Clear any fields listed in field_list.", "id": "f7924:c0:m13"}
{"signature": "def update(self, store_id, customer_id, data):", "body": "self.store_id = store_id<EOL>self.customer_id = customer_id<EOL>return self._mc_client._patch(url=self._build_path(store_id, '<STR_LIT>', customer_id), data=data)<EOL>", "docstring": "Update a customer.\n\n:param store_id: The store id.\n:type store_id: :py:class:`str`\n:param customer_id: The id for the customer of a store.\n:type customer_id: :py:class:`str`\n:param data: The request body parameters\n:type data: :py:class:`dict`", "id": "f283:c0:m4"}
{"signature": "def get_network_builder(name):", "body": "if callable(name):<EOL><INDENT>return name<EOL><DEDENT>elif name in mapping:<EOL><INDENT>return mapping[name]<EOL><DEDENT>else:<EOL><INDENT>raise ValueError('<STR_LIT>'.format(name))<EOL><DEDENT>", "docstring": "If you want to register your own network outside models.py, you just need:\n\nUsage Example:\n-------------\nfrom baselines.common.models import register\n@register(\"your_network_name\")\ndef your_network_define(**net_kwargs):\n    ...\n    return network_fn", "id": "f1373:m10"}
{"signature": "def disable_motors(self):", "body": "self.enable_motors(<NUM_LIT:0>)<EOL>", "docstring": "Disable joint motors in this skeleton.\n\n        This method sets to 0 the maximum force that joint motors are allowed to\n        apply, in addition to disabling torque feedback.", "id": "f14890:c0:m26"}
{"signature": "@classmethod<EOL><INDENT>def construct(cls, project, **desc):<DEDENT>", "body": "return cls(project.drivers, maker=project.maker, **desc)<EOL>", "docstring": "Construct a layout.\n        SHOULD BE PRIVATE", "id": "f2052:c0:m0"}
{"signature": "def get_doc():", "body": "return __onlinedoc__<EOL>", "docstring": "Get pyrep's official online documentation link.", "id": "f13924:m3"}
{"signature": "def usage_palette(parser):", "body": "parser.print_usage()<EOL>print('<STR_LIT>')<EOL>print('<STR_LIT>')<EOL>for palette in sorted(PALETTE):<EOL><INDENT>print('<STR_LIT>' % (palette,))<EOL><DEDENT>return <NUM_LIT:0><EOL>", "docstring": "Show usage and available palettes.", "id": "f3874:m3"}
{"signature": "def retype_path(<EOL>src, pyi_dir, targets, *, src_explicitly_given=False, quiet=False, hg=False<EOL>):", "body": "if src.is_dir():<EOL><INDENT>for child in src.iterdir():<EOL><INDENT>if child == pyi_dir or child == targets:<EOL><INDENT>continue<EOL><DEDENT>yield from retype_path(<EOL>child, pyi_dir / src.name, targets / src.name, quiet=quiet, hg=hg,<EOL>)<EOL><DEDENT><DEDENT>elif src.suffix == '<STR_LIT>' or src_explicitly_given:<EOL><INDENT>try:<EOL><INDENT>retype_file(src, pyi_dir, targets, quiet=quiet, hg=hg)<EOL><DEDENT>except Exception as e:<EOL><INDENT>yield (<EOL>src,<EOL>str(e),<EOL>type(e),<EOL>traceback.format_tb(e.__traceback__),<EOL>)<EOL><DEDENT><DEDENT>", "docstring": "Recursively retype files or directories given. Generate errors.", "id": "f1486:m1"}
{"signature": "def distances(self):", "body": "distances = []<EOL>for label in self.labels:<EOL><INDENT>joint = self.joints.get(label)<EOL>distances.append([np.nan, np.nan, np.nan] if joint is None else<EOL>np.array(joint.getAnchor()) - joint.getAnchor2())<EOL><DEDENT>return np.array(distances)<EOL>", "docstring": "Get a list of the distances between markers and their attachments.\n\n        Returns\n        -------\n        distances : ndarray of shape (num-markers, 3)\n            Array of distances for each marker joint in our attachment setup. If\n            a marker does not currently have an associated joint (e.g. because\n            it is not currently visible) this will contain NaN for that row.", "id": "f14892:c0:m15"}
{"signature": "def force_damp(self):", "body": "return - self.beta * self.vel<EOL>", "docstring": "Calculate the damping force -beta v", "id": "f5763:c0:m2"}
{"signature": "def eliminate_implications(s):", "body": "if not s.args or is_symbol(s.op): return s     <EOL>args = list(map(eliminate_implications, s.args))<EOL>a, b = args[<NUM_LIT:0>], args[-<NUM_LIT:1>]<EOL>if s.op == '<STR_LIT>':<EOL><INDENT>return (b | ~a)<EOL><DEDENT>elif s.op == '<STR_LIT>':<EOL><INDENT>return (a | ~b)<EOL><DEDENT>elif s.op == '<STR_LIT>':<EOL><INDENT>return (a | ~b) & (b | ~a)<EOL><DEDENT>elif s.op == '<STR_LIT>':<EOL><INDENT>assert len(args) == <NUM_LIT:2>   <EOL>return (a & ~b) | (~a & b)<EOL><DEDENT>else:<EOL><INDENT>assert s.op in ('<STR_LIT:&>', '<STR_LIT:|>', '<STR_LIT>')<EOL>return Expr(s.op, *args)<EOL><DEDENT>", "docstring": "Change >>, <<, and <=> into &, |, and ~. That is, return an Expr\n    that is equivalent to s, but has only &, |, and ~ as logical operators.\n    >>> eliminate_implications(A >> (~B << C))\n    ((~B | ~C) | ~A)\n    >>> eliminate_implications(A ^ B)\n    ((A & ~B) | (~A & B))", "id": "f1683:m14"}
{"signature": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):", "body": "assert isinstance(commands, list)<EOL>p = None<EOL>for c in commands:<EOL><INDENT>try:<EOL><INDENT>dispcmd = str([c] + args)<EOL>p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,<EOL>stderr=(subprocess.PIPE if hide_stderr<EOL>else None))<EOL>break<EOL><DEDENT>except EnvironmentError:<EOL><INDENT>e = sys.exc_info()[<NUM_LIT:1>]<EOL>if e.errno == errno.ENOENT:<EOL><INDENT>continue<EOL><DEDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % dispcmd)<EOL>print(e)<EOL><DEDENT>return None<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % (commands,))<EOL><DEDENT>return None<EOL><DEDENT>stdout = p.communicate()[<NUM_LIT:0>].strip()<EOL>if sys.version_info[<NUM_LIT:0>] >= <NUM_LIT:3>:<EOL><INDENT>stdout = stdout.decode()<EOL><DEDENT>if p.returncode != <NUM_LIT:0>:<EOL><INDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % dispcmd)<EOL><DEDENT>return None<EOL><DEDENT>return stdout<EOL>", "docstring": "Call the given command(s).", "id": "f10103:m3"}
{"signature": "def recursive_glob(base_directory, regex='<STR_LIT>'):", "body": "files = glob(op.join(base_directory, regex))<EOL>for path, dirlist, filelist in os.walk(base_directory):<EOL><INDENT>for dir_name in dirlist:<EOL><INDENT>files.extend(glob(op.join(path, dir_name, regex)))<EOL><DEDENT><DEDENT>return files<EOL>", "docstring": "Uses glob to find all files or folders that match the regex\nstarting from the base_directory.\n\nParameters\n----------\nbase_directory: str\n\nregex: str\n\nReturns\n-------\nfiles: list", "id": "f4057:m11"}
{"signature": "def register(self, obj):", "body": "self._parent = obj<EOL>", "docstring": "Registery a parent object so that communication maybe happen upwards", "id": "f5758:c2:m10"}
{"signature": "def create_user(config_data):", "body": "with chdir(os.path.abspath(config_data.project_directory)):<EOL><INDENT>env = deepcopy(dict(os.environ))<EOL>env[str('<STR_LIT>')] = str('<STR_LIT>'.format(config_data.project_name))<EOL>env[str('<STR_LIT>')] = str(os.pathsep.join(map(shlex_quote, sys.path)))<EOL>subprocess.check_call(<EOL>[sys.executable, '<STR_LIT>'], env=env, stderr=subprocess.STDOUT<EOL>)<EOL>for ext in ['<STR_LIT>', '<STR_LIT>']:<EOL><INDENT>try:<EOL><INDENT>os.remove('<STR_LIT>'.format(ext))<EOL><DEDENT>except OSError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Create admin user without user input\n\n:param config_data: configuration data", "id": "f5883:m7"}
{"signature": "def __str__(self):", "body": "return '<STR_LIT>'.format(self.user.user_email, self.course_id)<EOL>", "docstring": "Create string representation of the enrollment.", "id": "f16090:c6:m0"}
{"signature": "def get_identity(context, prefix=None, identity_func=None, user=None):", "body": "if prefix is not None:<EOL><INDENT>try:<EOL><INDENT>return context['<STR_LIT>' % prefix]<EOL><DEDENT>except KeyError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>try:<EOL><INDENT>return context['<STR_LIT>']<EOL><DEDENT>except KeyError:<EOL><INDENT>pass<EOL><DEDENT>if getattr(settings, '<STR_LIT>', True):<EOL><INDENT>try:<EOL><INDENT>if user is None:<EOL><INDENT>user = get_user_from_context(context)<EOL><DEDENT>if get_user_is_authenticated(user):<EOL><INDENT>if identity_func is not None:<EOL><INDENT>return identity_func(user)<EOL><DEDENT>else:<EOL><INDENT>return user.get_username()<EOL><DEDENT><DEDENT><DEDENT>except (KeyError, AttributeError):<EOL><INDENT>pass<EOL><DEDENT><DEDENT>return None<EOL>", "docstring": "Get the identity of a logged in user from a template context.\n\nThe `prefix` argument is used to provide different identities to\ndifferent analytics services.  The `identity_func` argument is a\nfunction that returns the identity of the user; by default the\nidentity is the username.", "id": "f14870:m3"}
{"signature": "def attributes(path):", "body": "<EOL>matches = re.findall('<STR_LIT>', path)<EOL>keys = []<EOL>values = []<EOL>for k,v in matches:<EOL><INDENT>if k in keys:<EOL><INDENT>i = keys.index(k)<EOL>del keys[i]<EOL>del values[i]<EOL><DEDENT>keys.append(k)<EOL>values.append(v)<EOL><DEDENT>lower_keys = [k.lower() for k in keys]<EOL>int_values= [int(v) for v in values]<EOL>attributes = namedtuple('<STR_LIT>', keys + lower_keys)<EOL>return attributes(*values + int_values)<EOL>", "docstring": "Get attributes from path based on format --[A-Z]. Returns namedtuple\n    with upper case attributes equal to what found in path (string) and lower\n    case as int. If path holds several occurrences of same character, only the\n    last one is kept.\n\n        >>> attrs = attributes('/folder/file--X00-X01.tif')\n        >>> print(attrs)\n        namedtuple('attributes', 'X x')('01', 1)\n        >>> print(attrs.x)\n        1\n\n    Parameters\n    ----------\n    path : string\n\n    Returns\n    -------\n    collections.namedtuple", "id": "f4440:m7"}
{"signature": "@action(Seq(Loc(\"<STR_LIT>\"), Loc(\"<STR_LIT::>\"), Rule(\"<STR_LIT>\"), Alt(try_stmt_1, try_stmt_2)))<EOL><INDENT>def try_stmt(self, try_loc, try_colon_loc, body, stmt):<DEDENT>", "body": "stmt.keyword_loc, stmt.try_colon_loc, stmt.body =try_loc, try_colon_loc, body<EOL>stmt.loc = stmt.loc.join(try_loc)<EOL>return stmt<EOL>", "docstring": "try_stmt: ('try' ':' suite\n           ((except_clause ':' suite)+\n            ['else' ':' suite]\n            ['finally' ':' suite] |\n            'finally' ':' suite))", "id": "f16502:c2:m65"}
{"signature": "def exists(self, path, environ):", "body": "return self.get_resource_inst(path, environ) is not None<EOL>", "docstring": "Return True, if path maps to an existing resource.\n\n        This method should only be used, if no other information is queried\n        for <path>. Otherwise a _DAVResource should be created first.\n\n        This method SHOULD be overridden by a more efficient implementation.", "id": "f8596:c3:m9"}
{"signature": "def log_reported(self):", "body": "self.logger_reported.debug(u\"<STR_LIT>\"<EOL>.format(self.message))<EOL>", "docstring": "Log message via the \"pyxmpp.ProtocolError.reported\" logger.", "id": "f15317:c26:m3"}
{"signature": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,<EOL>env=None):", "body": "assert isinstance(commands, list)<EOL>p = None<EOL>for c in commands:<EOL><INDENT>try:<EOL><INDENT>dispcmd = str([c] + args)<EOL>p = subprocess.Popen([c] + args, cwd=cwd, env=env,<EOL>stdout=subprocess.PIPE,<EOL>stderr=(subprocess.PIPE if hide_stderr<EOL>else None))<EOL>break<EOL><DEDENT>except EnvironmentError:<EOL><INDENT>e = sys.exc_info()[<NUM_LIT:1>]<EOL>if e.errno == errno.ENOENT:<EOL><INDENT>continue<EOL><DEDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % dispcmd)<EOL>print(e)<EOL><DEDENT>return None, None<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % (commands,))<EOL><DEDENT>return None, None<EOL><DEDENT>stdout = p.communicate()[<NUM_LIT:0>].strip()<EOL>if sys.version_info[<NUM_LIT:0>] >= <NUM_LIT:3>:<EOL><INDENT>stdout = stdout.decode()<EOL><DEDENT>if p.returncode != <NUM_LIT:0>:<EOL><INDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % dispcmd)<EOL>print(\"<STR_LIT>\" % stdout)<EOL><DEDENT>return None, p.returncode<EOL><DEDENT>return stdout, p.returncode<EOL>", "docstring": "Call the given command(s).", "id": "f6851:m3"}
{"signature": "def flare_model_residual(flareparams, times, mags, errs):", "body": "modelmags, _, _, _ = flare_model(flareparams, times, mags, errs)<EOL>return (mags - modelmags)/errs<EOL>", "docstring": "This returns the residual between model mags and the actual mags.\n\nParameters\n----------\n\nflareparams : list of float\n    This defines the flare model::\n\n        [amplitude,\n         flare_peak_time,\n         rise_gaussian_stdev,\n         decay_time_constant]\n\n    where:\n\n    `amplitude`: the maximum flare amplitude in mags or flux. If flux, then\n    amplitude should be positive. If mags, amplitude should be negative.\n\n    `flare_peak_time`: time at which the flare maximum happens.\n\n    `rise_gaussian_stdev`: the stdev of the gaussian describing the rise of\n    the flare.\n\n    `decay_time_constant`: the time constant of the exponential fall of the\n    flare.\n\ntimes,mags,errs : np.array\n    The input time-series of measurements and associated errors for which\n    the model will be generated. The times will be used to generate\n    model mags.\n\nReturns\n-------\n\nnp.array\n    The residuals between the input `mags` and generated `modelmags`,\n    weighted by the measurement errors in `errs`.", "id": "f14740:m1"}
{"signature": "def get_config_bool(name):", "body": "cli_config = CLIConfig(SF_CLI_CONFIG_DIR, SF_CLI_ENV_VAR_PREFIX)<EOL>return cli_config.getboolean('<STR_LIT>', name, False)<EOL>", "docstring": "Checks if a config value is set to a valid bool value.", "id": "f2344:m1"}
{"signature": "def _ends_in_doubled_cons(self, term):", "body": "return (<EOL>len(term) > <NUM_LIT:1><EOL>and term[-<NUM_LIT:1>] not in self._vowels<EOL>and term[-<NUM_LIT:2>] == term[-<NUM_LIT:1>]<EOL>)<EOL>", "docstring": "Return Porter helper function _ends_in_doubled_cons value.\n\n        Parameters\n        ----------\n        term : str\n            The word to check for a final doubled consonant\n\n        Returns\n        -------\n        bool\n            True iff the stem ends in a doubled consonant (as defined in the\n            Porter stemmer definition)", "id": "f6566:c0:m2"}
{"signature": "def waitforbuttonpress(self, timeout=-<NUM_LIT:1>):", "body": "blocking_input = BlockingKeyMouseInput(self)<EOL>return blocking_input(timeout=timeout)<EOL>", "docstring": "call signature::\n\n  waitforbuttonpress(self, timeout=-1)\n\nBlocking call to interact with the figure.\n\nThis will return True is a key was pressed, False if a mouse\nbutton was pressed and None if *timeout* was reached without\neither being pressed.\n\nIf *timeout* is negative, does not timeout.", "id": "f17210:c1:m46"}
{"signature": "def expandpath(path):", "body": "path = expanduser(path)<EOL>path = expandvars(path)<EOL>return path<EOL>", "docstring": "Wrapper around expanduser and expandvars.\n\nLess aggressive than truepath. Only expands environs and tilde. Does not\nchange relative paths to absolute paths.\n\nArgs:\n    path (PathLike): string representation of a path\n\nReturns:\n    PathLike : expanded path\n\nExample:\n    >>> import ubelt as ub\n    >>> assert normpath(ub.expandpath('~/foo')) == join(ub.userhome(), 'foo')\n    >>> assert ub.expandpath('foo') == 'foo'", "id": "f5133:m3"}
{"signature": "def clear(self):", "body": "def _clear(node):<EOL><INDENT>if node is not None:<EOL><INDENT>_clear(node.left)<EOL>_clear(node.right)<EOL>node.free()<EOL><DEDENT><DEDENT>_clear(self._root)<EOL>self._count = <NUM_LIT:0><EOL>self._root = None<EOL>", "docstring": "T.clear() -> None.  Remove all items from T.", "id": "f16268:c3:m1"}
{"signature": "def add_row(self, id_):", "body": "row = self._parser.new_row(id_)<EOL>self._rows.append(row)<EOL>return row<EOL>", "docstring": "Add a new row to the pattern.\n\n        :param id_: the id of the row", "id": "f508:c0:m4"}
{"signature": "def initialize(self):", "body": "pass<EOL>", "docstring": "This is called right after ``__init__``, on the initial\n        creation of a namespace so you may handle any setup job you\n        need.\n\n        Namespaces are created only when some packets arrive that ask\n        for the namespace.  They are not created altogether when a new\n        :class:`~socketio.virtsocket.Socket` connection is established,\n        so you can have many many namespaces assigned (when calling\n        :func:`~socketio.socketio_manage`) without clogging the\n        memory.\n\n        If you override this method, you probably want to initialize\n        the variables you're going to use in the events handled by this\n        namespace, setup ACLs, etc..\n\n        This method is called on all base classes following the _`method resolution order <http://docs.python.org/library/stdtypes.html?highlight=mro#class.__mro__>`\n        so you don't need to call super() to initialize the mixins or\n        other derived classes.", "id": "f11816:c0:m11"}
{"signature": "def regenerate_docker_vm_certificates():", "body": "log_to_client('<STR_LIT>')<EOL>check_call_demoted(['<STR_LIT>', '<STR_LIT>', '<STR_LIT>', constants.VM_MACHINE_NAME])<EOL>", "docstring": "Regenerate certificates for a running VM through Docker Machine.\n    This may be necessary following a restart if there were previously\n    networking issues preventing Machine from doing this as part\n    of normal startup.", "id": "f3150:m22"}
{"signature": "def try_match(request_origin, maybe_regex):", "body": "if isinstance(maybe_regex, RegexObject):<EOL><INDENT>return re.match(maybe_regex, request_origin)<EOL><DEDENT>elif probably_regex(maybe_regex):<EOL><INDENT>return re.match(maybe_regex, request_origin, flags=re.IGNORECASE)<EOL><DEDENT>else:<EOL><INDENT>try:<EOL><INDENT>return request_origin.lower() == maybe_regex.lower()<EOL><DEDENT>except AttributeError:<EOL><INDENT>return request_origin == maybe_regex<EOL><DEDENT><DEDENT>", "docstring": "Safely attempts to match a pattern or string to a request origin.", "id": "f13104:m9"}
{"signature": "def getSHA1(self, token, timestamp, nonce, encrypt):", "body": "try:<EOL><INDENT>sortlist = [token, timestamp, nonce, encrypt]<EOL>sortlist.sort()<EOL>sha = hashlib.sha1()<EOL>sha.update(\"<STR_LIT>\".join(sortlist))<EOL>return WXBizMsgCrypt_OK, sha.hexdigest()<EOL><DEDENT>except Exception:<EOL><INDENT>return WXBizMsgCrypt_ComputeSignature_Error, None<EOL><DEDENT>", "docstring": "\u7528SHA1\u7b97\u6cd5\u751f\u6210\u5b89\u5168\u7b7e\u540d\n        @param token:  \u7968\u636e\n        @param timestamp: \u65f6\u95f4\u6233\n        @param encrypt: \u5bc6\u6587\n        @param nonce: \u968f\u673a\u5b57\u7b26\u4e32\n        @return: \u5b89\u5168\u7b7e\u540d", "id": "f15109:c1:m0"}
{"signature": "def retrieve_seq_length_op2(data):", "body": "return tf.reduce_sum(tf.cast(tf.greater(data, tf.zeros_like(data)), tf.int32), <NUM_LIT:1>)<EOL>", "docstring": "An op to compute the length of a sequence, from input shape of [batch_size, n_step(max)],\n    it can be used when the features of padding (on right hand side) are all zeros.\n\n    Parameters\n    -----------\n    data : tensor\n        [batch_size, n_step(max)] with zero padding on right hand side.\n\n    Examples\n    --------\n    >>> data = [[1,2,0,0,0],\n    ...         [1,2,3,0,0],\n    ...         [1,2,6,1,0]]\n    >>> o = retrieve_seq_length_op2(data)\n    >>> sess = tf.InteractiveSession()\n    >>> tl.layers.initialize_global_variables(sess)\n    >>> print(o.eval())\n    [2 3 4]", "id": "f11199:m3"}
{"signature": "def mouserightclick(self, window_name, object_name):", "body": "object_handle = self._get_object_handle(window_name, object_name)<EOL>if not object_handle.AXEnabled:<EOL><INDENT>raise LdtpServerException(u\"<STR_LIT>\" % object_name)<EOL><DEDENT>self._grabfocus(object_handle)<EOL>x, y, width, height = self._getobjectsize(object_handle)<EOL>object_handle.clickMouseButtonRight((x + width / <NUM_LIT:2>, y + height / <NUM_LIT:2>))<EOL>return <NUM_LIT:1><EOL>", "docstring": "Mouse right click on an object.\n\n@param window_name: Window name to look for, either full name,\nLDTP's name convention, or a Unix glob.\n@type window_name: string\n@param object_name: Object name to look for, either full name,\nLDTP's name convention, or a Unix glob. Or menu heirarchy\n@type object_name: string\n\n@return: 1 on success.\n@rtype: integer", "id": "f10319:c0:m1"}
{"signature": "def init_logging(logger, log_file, log_level):", "body": "with open(log_file, '<STR_LIT:w>'):<EOL><INDENT>pass<EOL><DEDENT>numeric_level = getattr(logging, log_level.upper(), None) if log_level else logging.INFO<EOL>if not isinstance(numeric_level, int):<EOL><INDENT>raise ValueError('<STR_LIT>' % log_level)<EOL><DEDENT>logger.setLevel(logging.DEBUG)<EOL>fh = logging.FileHandler(log_file)<EOL>fh.setLevel(logging.DEBUG)<EOL>ch = logging.StreamHandler()<EOL>ch.setLevel(numeric_level)<EOL>formatter = logging.Formatter('<STR_LIT>')<EOL>fh.setFormatter(formatter)<EOL>ch.setFormatter(formatter)<EOL>logger.addHandler(fh)<EOL>logger.addHandler(ch)<EOL>return CONSTANTS.OK<EOL>", "docstring": "Initialize the naarad logger.\n:param: logger: logger object to initialize\n:param: log_file: log file name\n:param: log_level: log level (debug, info, warn, error)", "id": "f7878:m37"}
{"signature": "def _interval_metric(v1, v2, **_kwargs):", "body": "return (v1 - v2) ** <NUM_LIT:2><EOL>", "docstring": "Metric for interval data.", "id": "f11329:m2"}
{"signature": "def makedirs(path):", "body": "if not os.path.isdir(path):<EOL><INDENT>os.makedirs(path)<EOL><DEDENT>return path<EOL>", "docstring": "Create directories if they do not exist, otherwise do nothing.\n\nReturn path for convenience", "id": "f12841:m10"}
{"signature": "def flush(self):", "body": "_complain_ifclosed(self.closed)<EOL>", "docstring": "Flush the internal buffer", "id": "f16461:c0:m13"}
{"signature": "def largest_finite_distance(self):", "body": "block_start_distances = [block.distance_start for block in self._profile_blocks if<EOL>block.distance_start < float('<STR_LIT>')]<EOL>block_end_distances = [block.distance_end for block in self._profile_blocks if<EOL>block.distance_end < float('<STR_LIT>')]<EOL>distances = block_start_distances + block_end_distances<EOL>if len(distances) > <NUM_LIT:0>:<EOL><INDENT>return max(distances)<EOL><DEDENT>else:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Compute the maximum temporal distance.\n\nReturns\n-------\nmax_temporal_distance : float", "id": "f12878:c0:m6"}
{"signature": "@property<EOL><INDENT>def is_starttx(self):<DEDENT>", "body": "return self.semantics in ('<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>')<EOL>", "docstring": "True if the instruction is a transaction initiator", "id": "f13733:c4:m31"}
{"signature": "def get_content_metadata_exporter(self, user):", "body": "return SapSuccessFactorsContentMetadataExporter(user, self)<EOL>", "docstring": "Return a ``SapSuccessFactorsContentMetadataExporter`` instance.", "id": "f16198:c1:m8"}
{"signature": "def __init__(self, data):", "body": "Reply.__init__(self, data)<EOL>", "docstring": "Initialize the `Challenge` object.", "id": "f15236:c2:m0"}
{"signature": "def rename(self, target):", "body": "if self._closed:<EOL><INDENT>self._raise_closed()<EOL><DEDENT>self._accessor.rename(self, target)<EOL>", "docstring": "Rename this path to the given path.", "id": "f2883:c14:m31"}
{"signature": "def getProcessCommandLineStr(pid):", "body": "try:<EOL><INDENT>with open('<STR_LIT>' %(int(pid),), '<STR_LIT:r>') as f:<EOL><INDENT>cmdline = f.read()<EOL><DEDENT>return cmdline.replace('<STR_LIT:\\x00>', '<STR_LIT:U+0020>')<EOL><DEDENT>except:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "getProcessCommandLineStr - Gets a the commandline (program + arguments) of a given pid\n\n@param pid <int> - Process ID\n\n@return - None if process not found or can't be determined. Otherwise a string of commandline.\n\n@note Caution, args may have spaces in them, and you cannot surmise from this method. If you care (like trying to replay a command), use getProcessCommandLineList instead", "id": "f857:m2"}
{"signature": "@property<EOL><INDENT>def stadiums(self):<DEDENT>", "body": "if not self._stadiums:<EOL><INDENT>self._stadiums = stadiums()<EOL><DEDENT>return self._stadiums<EOL>", "docstring": "Return all stadiums in dict {id0: stadium0, id1: stadium1}.\n\n        :params year: Year.", "id": "f3437:c0:m11"}
{"signature": "def fit(self, X, y):", "body": "<EOL>word_vector_transformer = WordVectorTransformer(padding='<STR_LIT>')<EOL>X = word_vector_transformer.fit_transform(X)<EOL>X = LongTensor(X)<EOL>self.word_vector_transformer = word_vector_transformer<EOL>y_transformer = LabelEncoder()<EOL>y = y_transformer.fit_transform(y)<EOL>y = torch.from_numpy(y)<EOL>self.y_transformer = y_transformer<EOL>dataset = CategorizedDataset(X, y)<EOL>dataloader = DataLoader(dataset,<EOL>batch_size=self.batch_size,<EOL>shuffle=True,<EOL>num_workers=<NUM_LIT:4>)<EOL>KERNEL_SIZES = self.kernel_sizes<EOL>NUM_KERNEL = self.num_kernel<EOL>EMBEDDING_DIM = self.embedding_dim<EOL>model = TextCNN(<EOL>vocab_size=word_vector_transformer.get_vocab_size(),<EOL>embedding_dim=EMBEDDING_DIM,<EOL>output_size=len(self.y_transformer.classes_),<EOL>kernel_sizes=KERNEL_SIZES,<EOL>num_kernel=NUM_KERNEL)<EOL>if USE_CUDA:<EOL><INDENT>model = model.cuda()<EOL><DEDENT>EPOCH = self.epoch<EOL>LR = self.lr<EOL>loss_function = nn.CrossEntropyLoss()<EOL>optimizer = optim.Adam(model.parameters(), lr=LR)<EOL>for epoch in range(EPOCH):<EOL><INDENT>losses = []<EOL>for i, data in enumerate(dataloader):<EOL><INDENT>X, y = data<EOL>X, y = Variable(X), Variable(y)<EOL>optimizer.zero_grad()<EOL>model.train()<EOL>output = model(X)<EOL>loss = loss_function(output, y)<EOL>losses.append(loss.data.tolist()[<NUM_LIT:0>])<EOL>loss.backward()<EOL>optimizer.step()<EOL>if i % <NUM_LIT:100> == <NUM_LIT:0>:<EOL><INDENT>print(\"<STR_LIT>\" % (<EOL>epoch, EPOCH, np.mean(losses)))<EOL>losses = []<EOL><DEDENT><DEDENT><DEDENT>self.model = model<EOL>", "docstring": "Fit KimCNNClassifier according to X, y\n\n        Parameters\n        ----------\n        X : list of string\n            each item is a raw text\n        y : list of string\n            each item is a label", "id": "f10244:c2:m1"}
{"signature": "def record_delete_fields(rec, tag, field_positions_local=None):", "body": "if tag not in rec:<EOL><INDENT>return []<EOL><DEDENT>new_fields, deleted_fields = [], []<EOL>for position, field in enumerate(rec.get(tag, [])):<EOL><INDENT>if field_positions_local is None or position in field_positions_local:<EOL><INDENT>deleted_fields.append(field)<EOL><DEDENT>else:<EOL><INDENT>new_fields.append(field)<EOL><DEDENT><DEDENT>if new_fields:<EOL><INDENT>rec[tag] = new_fields<EOL><DEDENT>else:<EOL><INDENT>del rec[tag]<EOL><DEDENT>return deleted_fields<EOL>", "docstring": "Delete all/some fields defined with MARC tag 'tag' from record 'rec'.\n\n:param rec: a record structure.\n:type rec: tuple\n:param tag: three letter field.\n:type tag: string\n:param field_position_local: if set, it is the list of local positions\n    within all the fields with the specified tag, that should be deleted.\n    If not set all the fields with the specified tag will be deleted.\n:type field_position_local: sequence\n:return: the list of deleted fields.\n:rtype: list\n:note: the record is modified in place.", "id": "f7891:m10"}
{"signature": "def set_dashes(self, ls):", "body": "return self.set_linestyle(ls)<EOL>", "docstring": "alias for set_linestyle", "id": "f17178:c0:m21"}
{"signature": "def validate(schema_file=None, jams_files=None):", "body": "schema = load_json(schema_file)<EOL>for jams_file in jams_files:<EOL><INDENT>try:<EOL><INDENT>jams = load_json(jams_file)<EOL>jsonschema.validate(jams, schema)<EOL>print('<STR_LIT>'.format(jams_file))<EOL><DEDENT>except jsonschema.ValidationError as exc:<EOL><INDENT>print('<STR_LIT>'.format(jams_file))<EOL>print(exc)<EOL><DEDENT><DEDENT>", "docstring": "Validate a jams file against a schema", "id": "f11240:m2"}
{"signature": "def Rules(**rules):", "body": "for (lhs, rhs) in list(rules.items()):<EOL><INDENT>rules[lhs] = [alt.strip().split() for alt in rhs.split('<STR_LIT:|>')]<EOL><DEDENT>return rules<EOL>", "docstring": "Create a dictionary mapping symbols to alternative sequences.\n    >>> Rules(A = \"B C | D E\")\n    {'A': [['B', 'C'], ['D', 'E']]}", "id": "f1684:m0"}
{"signature": "def check_arrays(*arrays, **options):", "body": "sparse_format = options.pop('<STR_LIT>', None)<EOL>if sparse_format not in (None, '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'):<EOL><INDENT>raise ValueError('<STR_LIT>' % sparse_format)<EOL><DEDENT>copy = options.pop('<STR_LIT>', False)<EOL>check_ccontiguous = options.pop('<STR_LIT>', False)<EOL>dtype = options.pop('<STR_LIT>', None)<EOL>warn_nans = options.pop('<STR_LIT>', False)<EOL>replace_nans = options.pop('<STR_LIT>', False)<EOL>allow_lists = options.pop('<STR_LIT>', False)<EOL>allow_nans = options.pop('<STR_LIT>', False)<EOL>allow_nd = options.pop('<STR_LIT>', False)<EOL>if options:<EOL><INDENT>raise TypeError(\"<STR_LIT>\" % options.keys())<EOL><DEDENT>if len(arrays) == <NUM_LIT:0>:<EOL><INDENT>return None<EOL><DEDENT>n_samples = num_samples(arrays[<NUM_LIT:0>])<EOL>checked_arrays = []<EOL>for array in arrays:<EOL><INDENT>array_orig = array<EOL>if array is None:<EOL><INDENT>checked_arrays.append(array)<EOL>continue<EOL><DEDENT>size = num_samples(array)<EOL>if size != n_samples:<EOL><INDENT>raise ValueError(\"<STR_LIT>\"<EOL>% (size, n_samples))<EOL><DEDENT>if not allow_lists or hasattr(array, \"<STR_LIT>\"):<EOL><INDENT>if sp.issparse(array):<EOL><INDENT>if sparse_format == '<STR_LIT>':<EOL><INDENT>array = array.tocsr()<EOL><DEDENT>elif sparse_format == '<STR_LIT>':<EOL><INDENT>array = array.tocsc()<EOL><DEDENT>elif sparse_format == '<STR_LIT>':<EOL><INDENT>raise TypeError('<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>')<EOL><DEDENT>if check_ccontiguous:<EOL><INDENT>array.data = np.ascontiguousarray(array.data, dtype=dtype)<EOL><DEDENT>elif hasattr(array, '<STR_LIT:data>'):<EOL><INDENT>array.data = np.asarray(array.data, dtype=dtype)<EOL><DEDENT>elif array.dtype != dtype:<EOL><INDENT>array = array.astype(dtype)<EOL><DEDENT>if not allow_nans:<EOL><INDENT>if hasattr(array, '<STR_LIT:data>'):<EOL><INDENT>_assert_all_finite(array.data)<EOL><DEDENT>else:<EOL><INDENT>_assert_all_finite(array.values())<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>if check_ccontiguous:<EOL><INDENT>array = np.ascontiguousarray(array, dtype=dtype)<EOL><DEDENT>else:<EOL><INDENT>array = np.asarray(array, dtype=dtype)<EOL><DEDENT>if warn_nans:<EOL><INDENT>allow_nans = True<EOL>_warn_if_not_finite(array)<EOL><DEDENT>if replace_nans:<EOL><INDENT>array = np.nan_to_num(array)<EOL><DEDENT>if not allow_nans:<EOL><INDENT>_assert_all_finite(array)<EOL><DEDENT><DEDENT>if not allow_nd and array.ndim >= <NUM_LIT:3>:<EOL><INDENT>raise ValueError(\"<STR_LIT>\" %<EOL>array.ndim)<EOL><DEDENT><DEDENT>if copy and array is array_orig:<EOL><INDENT>array = array.copy()<EOL><DEDENT>checked_arrays.append(array)<EOL><DEDENT>return checked_arrays<EOL>", "docstring": "Check that all arrays have consistent first dimensions.\n\n    Checks whether all objects in arrays have the same shape or length.\n    By default lists and tuples are converted to numpy arrays.\n\n    It is possible to enforce certain properties, such as dtype, continguity\n    and sparse matrix format (if a sparse matrix is passed).\n\n    Converting lists to arrays can be disabled by setting ``allow_lists=True``.\n    Lists can then contain arbitrary objects and are not checked for dtype,\n    finiteness or anything else but length. Arrays are still checked\n    and possibly converted.\n\n    Parameters\n    ----------\n    *arrays : sequence of arrays or scipy.sparse matrices with same shape[0]\n        Python lists or tuples occurring in arrays are converted to 1D numpy\n        arrays, unless allow_lists is specified.\n    sparse_format : 'csr', 'csc' or 'dense', None by default\n        If not None, any scipy.sparse matrix is converted to\n        Compressed Sparse Rows or Compressed Sparse Columns representations.\n        If 'dense', an error is raised when a sparse array is\n        passed.\n    copy : boolean, False by default\n        If copy is True, ensure that returned arrays are copies of the original\n        (if not already converted to another format earlier in the process).\n    check_ccontiguous : boolean, False by default\n        Check that the arrays are C contiguous\n    dtype : a numpy dtype instance, None by default\n        Enforce a specific dtype.\n    warn_nans : boolean, False by default\n        Prints warning if nans in the arrays\n        Disables allow_nans\n    replace_nans : boolean, False by default\n        Replace nans in the arrays with zeros\n    allow_lists : bool\n        Allow lists of arbitrary objects as input, just check their length.\n        Disables\n    allow_nans : boolean, False by default\n        Allows nans in the arrays\n    allow_nd : boolean, False by default\n        Allows arrays of more than 2 dimensions.", "id": "f4510:m15"}
{"signature": "def _downloadToCache(stream, hashinfo={}, origin_info=dict()):", "body": "hash_name  = None<EOL>hash_value = None<EOL>m = None<EOL>if len(hashinfo):<EOL><INDENT>for h in ('<STR_LIT>',):<EOL><INDENT>if h in hashinfo:<EOL><INDENT>hash_name  = h<EOL>hash_value = hashinfo[h]<EOL>m = getattr(hashlib, h)()<EOL>break<EOL><DEDENT><DEDENT>if not hash_name:<EOL><INDENT>logger.warning('<STR_LIT>', hashinfo)<EOL><DEDENT><DEDENT>cache_dir = folders.cacheDirectory()<EOL>fsutils.mkDirP(cache_dir)<EOL>file_size = <NUM_LIT:0><EOL>(download_file, download_fname) = tempfile.mkstemp(dir=cache_dir, suffix='<STR_LIT>')<EOL>with os.fdopen(download_file, '<STR_LIT:wb>') as f:<EOL><INDENT>f.seek(<NUM_LIT:0>)<EOL>for chunk in stream.iter_content(<NUM_LIT>):<EOL><INDENT>f.write(chunk)<EOL>if hash_name:<EOL><INDENT>m.update(chunk)<EOL><DEDENT><DEDENT>if hash_name:<EOL><INDENT>calculated_hash = m.hexdigest()<EOL>logger.debug(<EOL>'<STR_LIT>' % (<EOL>hash_name, calculated_hash, hash_value<EOL>)<EOL>)<EOL>if hash_value and (hash_value != calculated_hash):<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT><DEDENT>file_size = f.tell()<EOL>logger.debug('<STR_LIT>', file_size, download_fname)<EOL>f.truncate()<EOL><DEDENT>extended_origin_info = {<EOL>'<STR_LIT>': hashinfo,<EOL>'<STR_LIT:size>': file_size<EOL>}<EOL>extended_origin_info.update(origin_info)<EOL>ordered_json.dump(download_fname + '<STR_LIT>', extended_origin_info)<EOL>return os.path.basename(download_fname)<EOL>", "docstring": "Download the specified stream to a temporary cache directory, and\n        returns a cache key that can be used to access/remove the file.\n        You should use either removeFromCache(cache_key) or _moveCachedFile to\n        move the downloaded file to a known key after downloading.", "id": "f13560:m7"}
{"signature": "def get_representors(self):", "body": "return self.__representors<EOL>", "docstring": "!\n        @brief Returns list of point-representors of each cluster.\n        @details Cluster index should be used for navigation between lists of point-representors.\n\n        @return (list) List of point-representors of each cluster.\n\n        @see get_clusters()\n        @see get_means()", "id": "f15541:c1:m5"}
{"signature": "@property<EOL><INDENT>def xxp(self):<DEDENT>", "body": "return _np.mean(self.x*self.xp, axis=<NUM_LIT:0>)<EOL>", "docstring": "The beam correlation :math:`\\\\langle x x' \\\\rangle`.", "id": "f12074:c0:m11"}
{"signature": "def create_thread(session, member_ids, context_type, context, message):", "body": "headers = {<EOL>'<STR_LIT:Content-Type>': '<STR_LIT>'<EOL>}<EOL>thread_data = {<EOL>'<STR_LIT>': member_ids,<EOL>'<STR_LIT>': context_type,<EOL>'<STR_LIT>': context,<EOL>'<STR_LIT:message>': message,<EOL>}<EOL>response = make_post_request(session, '<STR_LIT>', headers,<EOL>form_data=thread_data)<EOL>json_data = response.json()<EOL>if response.status_code == <NUM_LIT:200>:<EOL><INDENT>return Thread(json_data['<STR_LIT:result>'])<EOL><DEDENT>else:<EOL><INDENT>raise ThreadNotCreatedException(message=json_data['<STR_LIT:message>'],<EOL>error_code=json_data['<STR_LIT>'],<EOL>request_id=json_data['<STR_LIT>'])<EOL><DEDENT>", "docstring": "Create a thread", "id": "f12295:m0"}
{"signature": "@dimod.decorators.vartype_argument('<STR_LIT>')<EOL>def and_gate(variables, vartype=dimod.BINARY, name='<STR_LIT>'):", "body": "variables = tuple(variables)<EOL>if vartype is dimod.BINARY:<EOL><INDENT>configurations = frozenset([(<NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT:0>),<EOL>(<NUM_LIT:0>, <NUM_LIT:1>, <NUM_LIT:0>),<EOL>(<NUM_LIT:1>, <NUM_LIT:0>, <NUM_LIT:0>),<EOL>(<NUM_LIT:1>, <NUM_LIT:1>, <NUM_LIT:1>)])<EOL>def func(in1, in2, out): return (in1 and in2) == out<EOL><DEDENT>else:<EOL><INDENT>configurations = frozenset([(-<NUM_LIT:1>, -<NUM_LIT:1>, -<NUM_LIT:1>),<EOL>(-<NUM_LIT:1>, +<NUM_LIT:1>, -<NUM_LIT:1>),<EOL>(+<NUM_LIT:1>, -<NUM_LIT:1>, -<NUM_LIT:1>),<EOL>(+<NUM_LIT:1>, +<NUM_LIT:1>, +<NUM_LIT:1>)])<EOL>def func(in1, in2, out): return ((in1 > <NUM_LIT:0>) and (in2 > <NUM_LIT:0>)) == (out > <NUM_LIT:0>)<EOL><DEDENT>return Constraint(func, configurations, variables, vartype=vartype, name=name)<EOL>", "docstring": "AND gate.\n\n    Args:\n        variables (list): Variable labels for the and gate as `[in1, in2, out]`,\n            where `in1, in2` are inputs and `out` the gate's output.\n        vartype (Vartype, optional, default='BINARY'): Variable type. Accepted\n            input values:\n\n            * Vartype.SPIN, 'SPIN', {-1, 1}\n            * Vartype.BINARY, 'BINARY', {0, 1}\n        name (str, optional, default='AND'): Name for the constraint.\n\n    Returns:\n        Constraint(:obj:`.Constraint`): Constraint that is satisfied when its variables are\n        assigned values that match the valid states of an AND gate.\n\n    Examples:\n        >>> import dwavebinarycsp\n        >>> import dwavebinarycsp.factories.constraint.gates as gates\n        >>> csp = dwavebinarycsp.ConstraintSatisfactionProblem(dwavebinarycsp.BINARY)\n        >>> csp.add_constraint(gates.and_gate(['a', 'b', 'c'], name='AND1'))\n        >>> csp.check({'a': 1, 'b': 0, 'c': 0})\n        True", "id": "f11933:m0"}
{"signature": "def run(self):", "body": "pass<EOL>", "docstring": "Abstract method. This method will be executed for subclass which not implemented his own method", "id": "f1187:c0:m12"}
{"signature": "def get_citation_years(graph: BELGraph) -> List[Tuple[int, int]]:", "body": "return create_timeline(count_citation_years(graph))<EOL>", "docstring": "Create a citation timeline counter from the graph.", "id": "f9403:m15"}
{"signature": "@classmethod<EOL><INDENT>@has_storage<EOL>def remove_one(cls, which, rm=True):<DEDENT>", "body": "<EOL>obj = cls._which_to_obj(which)<EOL>rm_fwd_refs(obj)<EOL>rm_back_refs(obj)<EOL>cls._clear_caches(obj._storage_key)<EOL>if rm:<EOL><INDENT>cls.delegate(<EOL>cls._storage[<NUM_LIT:0>].remove,<EOL>False,<EOL>RawQuery(obj._primary_name, '<STR_LIT>', obj._storage_key)<EOL>)<EOL><DEDENT>obj._detached = True<EOL>", "docstring": "Remove an object, along with its references and back-references.\n        Remove the object from the cache_sandbox and sets its _detached flag to True.\n\n        :param which: Object selector: Query, StoredObject, or primary key\n        :param rm: Remove data from backend", "id": "f13215:c2:m64"}
{"signature": "def _configure_io_handler(self, handler):", "body": "if self.check_events():<EOL><INDENT>return<EOL><DEDENT>if handler in self._unprepared_handlers:<EOL><INDENT>old_fileno = self._unprepared_handlers[handler]<EOL>prepared = self._prepare_io_handler(handler)<EOL><DEDENT>else:<EOL><INDENT>old_fileno = None<EOL>prepared = True<EOL><DEDENT>fileno = handler.fileno()<EOL>if old_fileno is not None and fileno != old_fileno:<EOL><INDENT>del self._handlers[old_fileno]<EOL>try:<EOL><INDENT>self.poll.unregister(old_fileno)<EOL><DEDENT>except KeyError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>if not prepared:<EOL><INDENT>self._unprepared_handlers[handler] = fileno<EOL><DEDENT>if not fileno:<EOL><INDENT>return<EOL><DEDENT>self._handlers[fileno] = handler<EOL>events = <NUM_LIT:0><EOL>if handler.is_readable():<EOL><INDENT>logger.debug(\"<STR_LIT>\".format(handler))<EOL>events |= select.POLLIN<EOL><DEDENT>if handler.is_writable():<EOL><INDENT>logger.debug(\"<STR_LIT>\".format(handler))<EOL>events |= select.POLLOUT<EOL><DEDENT>if events:<EOL><INDENT>logger.debug(\"<STR_LIT>\"<EOL>\"<STR_LIT>\".format(handler, fileno, events))<EOL>self.poll.register(fileno, events)<EOL><DEDENT>", "docstring": "Register an io-handler at the polling object.", "id": "f15265:c0:m2"}
{"signature": "@classmethod<EOL><INDENT>def get_resources(cls):<DEDENT>", "body": "ip_controller = IpAddressesController(<EOL>directory.get_plugin())<EOL>ip_port_controller = IpAddressPortController(<EOL>directory.get_plugin())<EOL>resources = []<EOL>resources.append(extensions.ResourceExtension(<EOL>Ip_addresses.get_alias(),<EOL>ip_controller))<EOL>parent = {'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': '<STR_LIT>'}<EOL>resources.append(extensions.ResourceExtension(<EOL>'<STR_LIT>', ip_port_controller, parent=parent))<EOL>return resources<EOL>", "docstring": "Returns Ext Resources.", "id": "f10771:c2:m6"}
{"signature": "@abstractmethod<EOL><INDENT>def bolt_fail(self, bolt_fail_info):<DEDENT>", "body": "pass<EOL>", "docstring": "Called in bolt every time a tuple gets failed\n\n        :param bolt_fail_info: BoltFailInfo object", "id": "f7237:c0:m7"}
{"signature": "def __init__(<EOL>self,<EOL>optimizer,<EOL>ls_max_iterations=<NUM_LIT:10>,<EOL>ls_accept_ratio=<NUM_LIT>,<EOL>ls_mode='<STR_LIT>',<EOL>ls_parameter=<NUM_LIT:0.5>,<EOL>ls_unroll_loop=False,<EOL>scope='<STR_LIT>',<EOL>summary_labels=()<EOL>):", "body": "self.solver = LineSearch(<EOL>max_iterations=ls_max_iterations,<EOL>accept_ratio=ls_accept_ratio,<EOL>mode=ls_mode,<EOL>parameter=ls_parameter,<EOL>unroll_loop=ls_unroll_loop<EOL>)<EOL>super(OptimizedStep, self).__init__(optimizer=optimizer, scope=scope, summary_labels=summary_labels)<EOL>", "docstring": "Creates a new optimized step meta optimizer instance.\n\nArgs:\n    optimizer: The optimizer which is modified by this meta optimizer.\n    ls_max_iterations: Maximum number of line search iterations.\n    ls_accept_ratio: Line search acceptance ratio.\n    ls_mode: Line search mode, see LineSearch solver.\n    ls_parameter: Line search parameter, see LineSearch solver.\n    ls_unroll_loop: Unroll line search loop if true.", "id": "f14345:c0:m0"}
{"signature": "def render_tag(self, context, name, nodelist):", "body": "<EOL>settings = self.setting_model.objects.filter(name=name).as_dict()<EOL>try:<EOL><INDENT>value = settings[name]<EOL><DEDENT>except KeyError:<EOL><INDENT>value = settings[name] = nodelist.render(context)<EOL><DEDENT>return value<EOL>", "docstring": "Returns the value of the named setting.", "id": "f13269:c2:m0"}
{"signature": "def simple_route(config, name, url, fn):", "body": "config.add_route(name, url)<EOL>config.add_view(fn, route_name=name,<EOL>renderer=\"<STR_LIT>\" % name)<EOL>", "docstring": "Function to simplify creating routes in pyramid\nTakes the pyramid configuration, name of the route, url, and view\nfunction.", "id": "f11771:m0"}
{"signature": "def check_and_load_ssh_auth():", "body": "mac_username = get_config_value(constants.CONFIG_MAC_USERNAME_KEY)<EOL>if not mac_username:<EOL><INDENT>logging.info(\"<STR_LIT>\")<EOL>return<EOL><DEDENT>if not _running_on_mac(): <EOL><INDENT>logging.info(\"<STR_LIT>\")<EOL>return<EOL><DEDENT>if _mac_version_is_post_yosemite():<EOL><INDENT>_load_ssh_auth_post_yosemite(mac_username)<EOL><DEDENT>else:<EOL><INDENT>_load_ssh_auth_pre_yosemite()<EOL><DEDENT>", "docstring": "Will check the mac_username config value; if it is present, will load that user's\nSSH_AUTH_SOCK environment variable to the current environment.  This allows git clones\nto behave the same for the daemon as they do for the user", "id": "f3124:m16"}
{"signature": "def summary(self, solution=None, threshold=<NUM_LIT>, fva=None, names=False,<EOL>floatfmt='<STR_LIT>'):", "body": "from cobra.flux_analysis.summary import model_summary<EOL>return model_summary(self, solution=solution, threshold=threshold,<EOL>fva=fva, names=names, floatfmt=floatfmt)<EOL>", "docstring": "Print a summary of the input and output fluxes of the model.\n\nParameters\n----------\nsolution: cobra.Solution, optional\n    A previously solved model solution to use for generating the\n    summary. If none provided (default), the summary method will\n    resolve the model. Note that the solution object must match the\n    model, i.e., changes to the model such as changed bounds,\n    added or removed reactions are not taken into account by this\n    method.\nthreshold : float, optional\n    Threshold below which fluxes are not reported.\nfva : pandas.DataFrame, float or None, optional\n    Whether or not to include flux variability analysis in the output.\n    If given, fva should either be a previous FVA solution matching\n    the model or a float between 0 and 1 representing the\n    fraction of the optimum objective to be searched.\nnames : bool, optional\n    Emit reaction and metabolite names rather than identifiers (default\n    False).\nfloatfmt : string, optional\n    Format string for floats (default '.3g').", "id": "f15897:c0:m43"}
{"signature": "@classmethod<EOL><INDENT>def grid_stack_for_simulation(cls, shape, pixel_scale, psf_shape, sub_grid_size=<NUM_LIT:2>):<DEDENT>", "body": "return cls.padded_grid_stack_from_mask_sub_grid_size_and_psf_shape(mask=msk.Mask(array=np.full(shape, False),<EOL>pixel_scale=pixel_scale),<EOL>sub_grid_size=sub_grid_size,<EOL>psf_shape=psf_shape)<EOL>", "docstring": "Setup a grid-stack of grid_stack for simulating an image of a strong lens, whereby the grid's use \\\n        padded-grid_stack to ensure that the PSF blurring in the simulation routine (*ccd.PrepatoryImage.simulate*) \\\n        is not degraded due to edge effects.\n\n        Parameters\n        -----------\n        shape : (int, int)\n            The 2D shape of the array, where all pixels are used to generate the grid-stack's grid_stack.\n        pixel_scale : float\n            The size of each pixel in arc seconds.            \n        psf_shape : (int, int)\n            The shape of the PSF used in the analysis, which defines how much the grid's must be masked to mitigate \\\n            edge effects.\n        sub_grid_size : int\n            The size of a sub-pixel's sub-grid (sub_grid_size x sub_grid_size).", "id": "f5990:c0:m5"}
{"signature": "def create_ml_configuration(self, search_template, extract_as_keys, dataset_ids):", "body": "data = {<EOL>\"<STR_LIT>\":<EOL>search_template,<EOL>\"<STR_LIT>\":<EOL>extract_as_keys<EOL>}<EOL>failure_message = \"<STR_LIT>\"<EOL>config_job_id = self._get_success_json(self._post_json(<EOL>'<STR_LIT>', data, failure_message=failure_message))['<STR_LIT:data>'][<EOL>'<STR_LIT:result>']['<STR_LIT>']<EOL>while True:<EOL><INDENT>config_status = self.__get_ml_configuration_status(config_job_id)<EOL>print('<STR_LIT>', config_status)<EOL>if config_status['<STR_LIT:status>'] == '<STR_LIT>':<EOL><INDENT>ml_config = self.__convert_response_to_configuration(config_status['<STR_LIT:result>'], dataset_ids)<EOL>return ml_config<EOL><DEDENT>time.sleep(<NUM_LIT:5>)<EOL><DEDENT>", "docstring": "This method will spawn a server job to create a default ML configuration based on a search template and\nthe extract as keys.\nThis function will submit the request to build, and wait for the configuration to finish before returning.\n\n:param search_template: A search template defining the query (properties, datasets etc)\n:param extract_as_keys: Array of extract-as keys defining the descriptors\n:param dataset_ids: Array of dataset identifiers to make search template from\n:return: An identifier used to request the status of the builder job (get_ml_configuration_status)", "id": "f3579:c0:m7"}
{"signature": "def __pow__(self, other, modulo=None):", "body": "<EOL>return np.power(self, other)<EOL>", "docstring": "x.__pow__(y[, z]) <==> pow(x, y[, z])", "id": "f4853:c0:m22"}
{"signature": "def show_active(self, **kwargs):", "body": "g = self.g<EOL>for v in g.nodes():<EOL><INDENT>self.g.set_vp(v, '<STR_LIT>', [<NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT>])<EOL>is_active = False<EOL>my_iter = g.in_edges(v) if g.is_directed() else g.out_edges(v)<EOL>for e in my_iter:<EOL><INDENT>ei = g.edge_index[e]<EOL>if self.edge2queue[ei]._active:<EOL><INDENT>is_active = True<EOL>break<EOL><DEDENT><DEDENT>if is_active:<EOL><INDENT>self.g.set_vp(v, '<STR_LIT>', self.colors['<STR_LIT>'])<EOL><DEDENT>else:<EOL><INDENT>self.g.set_vp(v, '<STR_LIT>', self.colors['<STR_LIT>'])<EOL><DEDENT><DEDENT>for e in g.edges():<EOL><INDENT>ei = g.edge_index[e]<EOL>if self.edge2queue[ei]._active:<EOL><INDENT>self.g.set_ep(e, '<STR_LIT>', self.colors['<STR_LIT>'])<EOL><DEDENT>else:<EOL><INDENT>self.g.set_ep(e, '<STR_LIT>', self.colors['<STR_LIT>'])<EOL><DEDENT><DEDENT>self.draw(update_colors=False, **kwargs)<EOL>self._update_all_colors()<EOL>", "docstring": "Draws the network, highlighting active queues.\n\n        The colored vertices represent vertices that have at least one\n        queue on an in-edge that is active. Dark edges represent\n        queues that are active, light edges represent queues that are\n        inactive.\n\n        Parameters\n        ----------\n        **kwargs\n            Any additional parameters to pass to :meth:`.draw`, and\n            :meth:`.QueueNetworkDiGraph.draw_graph`.\n\n        Notes\n        -----\n        Active queues are :class:`QueueServers<.QueueServer>` that\n        accept arrivals from outside the network. The colors are\n        defined by the class attribute ``colors``. The relevant keys\n        are ``vertex_active``, ``vertex_inactive``, ``edge_active``,\n        and ``edge_inactive``.", "id": "f14533:c1:m20"}
{"signature": "def receive(self, message):", "body": "", "docstring": "Override to provide the actor behaviour.\n\n:param message: The current message.", "id": "f5561:c0:m0"}
{"signature": "def align_bulge_disk_centre_tag_from_align_bulge_disk_centre(align_bulge_disk_centre):", "body": "if not align_bulge_disk_centre:<EOL><INDENT>return '<STR_LIT>'<EOL><DEDENT>elif align_bulge_disk_centre:<EOL><INDENT>return '<STR_LIT>'<EOL><DEDENT>", "docstring": "Generate a tag for if the bulge and disk of a bulge-disk system are aligned or not, to customize phase names \\\n    based on the bulge-disk model. This changee the phase name 'phase_name' as follows:\n\n    bd_align_centres = False -> phase_name\n    bd_align_centres = True -> phase_name_bd_align_centres", "id": "f5966:m9"}
{"signature": "def _lstree(files, dirs):", "body": "for f, sha1 in files:<EOL><INDENT>yield \"<STR_LIT>\".format(sha1, f)<EOL><DEDENT>for d, sha1 in dirs:<EOL><INDENT>yield \"<STR_LIT>\".format(sha1, d)<EOL><DEDENT>", "docstring": "Make git ls-tree like output.", "id": "f14631:m1"}
{"signature": "def run(self):", "body": "config = config_creator()<EOL>debug = config.debug<EOL>branch_thread_sleep = config.branch_thread_sleep<EOL>while <NUM_LIT:1>:<EOL><INDENT>url = self.branch_queue.get()<EOL>if debug:<EOL><INDENT>print('<STR_LIT>'.format(url))<EOL><DEDENT>branch_spider = self.branch_spider(url)<EOL>sleep(random.randrange(*branch_thread_sleep))<EOL>branch_spider.request_page()<EOL>if debug:<EOL><INDENT>print('<STR_LIT>'.format(url))<EOL><DEDENT>self.branch_queue.task_done()<EOL><DEDENT>", "docstring": "run your main spider here\n        as for branch spider result data, you can return everything or do whatever with it\n        in your own code\n\n        :return: None", "id": "f867:c0:m1"}
{"signature": "@job.command()<EOL>@click.option('<STR_LIT>', '<STR_LIT>', is_flag=True, help='<STR_LIT>')<EOL>@click.pass_context<EOL>@clean_outputs<EOL>def resources(ctx, gpu):", "body": "user, project_name, _job = get_job_or_local(ctx.obj.get('<STR_LIT>'), ctx.obj.get('<STR_LIT>'))<EOL>try:<EOL><INDENT>message_handler = Printer.gpu_resources if gpu else Printer.resources<EOL>PolyaxonClient().job.resources(user,<EOL>project_name,<EOL>_job,<EOL>message_handler=message_handler)<EOL><DEDENT>except (PolyaxonHTTPError, PolyaxonShouldExitError, PolyaxonClientException) as e:<EOL><INDENT>Printer.print_error('<STR_LIT>'.format(_job))<EOL>Printer.print_error('<STR_LIT>'.format(e))<EOL>sys.exit(<NUM_LIT:1>)<EOL><DEDENT>", "docstring": "Get job resources.\n\n    Uses [Caching](/references/polyaxon-cli/#caching)\n\n    Examples:\n\n    \\b\n    ```bash\n    $ polyaxon job -j 2 resources\n    ```\n\n    For GPU resources\n\n    \\b\n    ```bash\n    $ polyaxon job -j 2 resources --gpu\n    ```", "id": "f1046:m9"}
{"signature": "def getLogin(filename, user, passwd):", "body": "if filename is None:<EOL><INDENT>return (user, passwd)<EOL><DEDENT>if os.path.exists(filename):<EOL><INDENT>print(\"<STR_LIT>\".format(filename))<EOL>with open(filename, \"<STR_LIT:r>\") as loginfile:<EOL><INDENT>encoded_cred = loginfile.read()<EOL>decoded_cred = b64decode(encoded_cred)<EOL>login = decoded_cred.split('<STR_LIT::>', <NUM_LIT:1>)<EOL>return (login[<NUM_LIT:0>], login[<NUM_LIT:1>])<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if user is None or passwd is None:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>print(\"<STR_LIT>\".format(filename))<EOL>with open(filename, \"<STR_LIT:w>\") as loginfile:<EOL><INDENT>encoded_cred = b64encode(user+\"<STR_LIT::>\"+passwd)<EOL>loginfile.write(encoded_cred)<EOL><DEDENT>return (user, passwd)<EOL><DEDENT>", "docstring": "write user/passwd to login file or get them from file.\nThis method is not Py3 safe (byte vs. str)", "id": "f828:m0"}
{"signature": "def _transmit_create(self, channel_metadata_item_map):", "body": "for chunk in chunks(channel_metadata_item_map, self.enterprise_configuration.transmission_chunk_size):<EOL><INDENT>serialized_chunk = self._serialize_items(list(chunk.values()))<EOL>try:<EOL><INDENT>self.client.create_content_metadata(serialized_chunk)<EOL><DEDENT>except ClientError as exc:<EOL><INDENT>LOGGER.error(<EOL>'<STR_LIT>',<EOL>len(chunk),<EOL>self.enterprise_configuration.enterprise_customer.name,<EOL>self.enterprise_configuration.channel_code,<EOL>)<EOL>LOGGER.error(exc)<EOL><DEDENT>else:<EOL><INDENT>self._create_transmissions(chunk)<EOL><DEDENT><DEDENT>", "docstring": "Transmit content metadata creation to integrated channel.", "id": "f16240:c0:m5"}
{"signature": "def __init__(self, *args, overlay=False, detach=True, **kwds):", "body": "super().__init__(*args, **kwds)<EOL>if detach:<EOL><INDENT>self.detach(overlay)<EOL><DEDENT>", "docstring": "If overlay is True, then preclear is set to False for everything\nother than the first animation.", "id": "f1954:c0:m0"}
{"signature": "@logExceptions(_LOGGER)<EOL><INDENT>def jobsGetFields(self, jobIDs, fields, requireAll=True):<DEDENT>", "body": "assert isinstance(jobIDs, self._SEQUENCE_TYPES)<EOL>assert len(jobIDs) >=<NUM_LIT:1><EOL>rows = self._getMatchingRowsWithRetries(<EOL>self._jobs, dict(job_id=jobIDs),<EOL>['<STR_LIT>'] + [self._jobs.pubToDBNameDict[x] for x in fields])<EOL>if requireAll and len(rows) < len(jobIDs):<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\" % (<EOL>(set(jobIDs) - set(r[<NUM_LIT:0>] for r in rows)),))<EOL><DEDENT>return [(r[<NUM_LIT:0>], list(r[<NUM_LIT:1>:])) for r in rows]<EOL>", "docstring": "Fetch the values of 1 or more fields from a sequence of job records.\n        Here, 'fields' is a sequence (list or tuple) with the names of the fields to\n        fetch. The names are the public names of the fields (camelBack, not the\n        lower_case_only form as stored in the DB).\n\n        WARNING!!!: The order of the results are NOT necessarily in the same order as\n        the order of the job IDs passed in!!!\n\n        Parameters:\n        ----------------------------------------------------------------\n        jobIDs:        A sequence of jobIDs\n        fields:        A list  of fields to return for each jobID\n\n        Returns:      A list of tuples->(jobID, [field1, field2,...])", "id": "f17555:c1:m45"}
{"signature": "def rfc2426(self):", "body": "if self.uri:<EOL><INDENT>return rfc2425encode(self.name,self.uri,{\"<STR_LIT:value>\":\"<STR_LIT>\"})<EOL><DEDENT>elif self.sound:<EOL><INDENT>return rfc2425encode(self.name,self.sound)<EOL><DEDENT>", "docstring": "RFC2426-encode the field content.\n\n        :return: the field in the RFC 2426 format.\n        :returntype: `str`", "id": "f15257:c14:m1"}
{"signature": "def get_networks_count(context, filters=None):", "body": "LOG.info(\"<STR_LIT>\" %<EOL>(context.tenant_id, filters))<EOL>return db_api.network_count_all(context)<EOL>", "docstring": "Return the number of networks.\n\n    The result depends on the identity of the user making the request\n    (as indicated by the context) as well as any filters.\n    : param context: neutron api request context\n    : param filters: a dictionary with keys that are valid keys for\n        a network as listed in the RESOURCE_ATTRIBUTE_MAP object\n        in neutron/api/v2/attributes.py.  Values in this dictiontary\n        are an iterable containing values that will be used for an exact\n        match comparison for that value.  Each result returned by this\n        function will have matched one of the values for each key in\n        filters.\n\n    NOTE: this method is optional, as it was not part of the originally\n          defined plugin API.", "id": "f10763:m5"}
{"signature": "def send_video_message(self, user_id, media_id, title=None, description=None):", "body": "video_data = {<EOL>'<STR_LIT>': media_id,<EOL>}<EOL>if title:<EOL><INDENT>video_data['<STR_LIT:title>'] = title<EOL><DEDENT>if description:<EOL><INDENT>video_data['<STR_LIT:description>'] = description<EOL><DEDENT>return self.request.post(<EOL>url='<STR_LIT>',<EOL>data={<EOL>'<STR_LIT>': user_id,<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': video_data,<EOL>}<EOL>)<EOL>", "docstring": "\u53d1\u9001\u89c6\u9891\u6d88\u606f\n\u8be6\u60c5\u8bf7\u53c2\u8003 http://mp.weixin.qq.com/wiki/7/12a5a320ae96fecdf0e15cb06123de9f.html\n:param user_id: \u7528\u6237 ID, \u5c31\u662f\u4f60\u6536\u5230\u7684 WechatMessage \u7684 source\n:param media_id: \u53d1\u9001\u7684\u89c6\u9891\u7684\u5a92\u4f53ID\u3002 \u53ef\u4ee5\u901a\u8fc7 :func:`upload_media` \u4e0a\u4f20\u3002\n:param title: \u89c6\u9891\u6d88\u606f\u7684\u6807\u9898\n:param description: \u89c6\u9891\u6d88\u606f\u7684\u63cf\u8ff0\n:return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305", "id": "f588:c0:m39"}
{"signature": "def find_elements(self, strategy, locator):", "body": "return self.driver_adapter.find_elements(strategy, locator)<EOL>", "docstring": "Finds elements on the page.\n\n        :param strategy: Location strategy to use. See :py:class:`~selenium.webdriver.common.by.By` or :py:attr:`~pypom.splinter_driver.ALLOWED_STRATEGIES`.\n        :param locator: Location of target elements.\n        :type strategy: str\n        :type locator: str\n        :return: List of :py:class:`~selenium.webdriver.remote.webelement.WebElement` or :py:class:`~splinter.element_list.ElementList`\n        :rtype: list", "id": "f11286:c0:m3"}
{"signature": "def main():", "body": "usage = \"<STR_LIT>\"<EOL>parser = optparse.OptionParser(usage=usage)<EOL>parser.add_option(<EOL>\"<STR_LIT>\", \"<STR_LIT>\",<EOL>action=\"<STR_LIT:store_true>\", dest=\"<STR_LIT>\", default=False,<EOL>help=\"<STR_LIT>\")<EOL>parser.add_option(<EOL>\"<STR_LIT>\", \"<STR_LIT>\",<EOL>action=\"<STR_LIT:store>\", type=\"<STR_LIT:string>\", dest=\"<STR_LIT>\",<EOL>default='<STR_LIT>',<EOL>help=\"<STR_LIT>\")<EOL>parser.add_option(<EOL>\"<STR_LIT>\", \"<STR_LIT>\",<EOL>action=\"<STR_LIT:store>\", type=\"<STR_LIT:string>\", dest=\"<STR_LIT>\",<EOL>default='<STR_LIT>',<EOL>help=(\"<STR_LIT>\" +<EOL>\"<STR_LIT>\"))<EOL>(options, args) = parser.parse_args()<EOL>if options.verbose:<EOL><INDENT>log_level = logging.DEBUG<EOL><DEDENT>else:<EOL><INDENT>log_level = logging.INFO<EOL><DEDENT>logging.basicConfig(level=log_level,<EOL>format=\"<STR_LIT>\")<EOL>curdir = os.getcwd()<EOL>testbinary = os.path.join(curdir, '<STR_LIT>', '<STR_LIT:test>')<EOL>if not os.path.exists(testbinary):<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\" % testbinary)<EOL><DEDENT>coveragebinary = os.path.join(curdir, '<STR_LIT>', '<STR_LIT>')<EOL>if not os.path.exists(coveragebinary):<EOL><INDENT>logger.debug(\"<STR_LIT>\")<EOL>coveragebinary = '<STR_LIT>'<EOL><DEDENT>logger.info(\"<STR_LIT>\")<EOL>parts = [coveragebinary, '<STR_LIT>', testbinary]<EOL>if options.test_args:<EOL><INDENT>parts.append(options.test_args)<EOL><DEDENT>system(\"<STR_LIT:U+0020>\".join(parts))<EOL>logger.debug(\"<STR_LIT>\")<EOL>if options.output_dir:<EOL><INDENT>coverage_dir = options.output_dir<EOL>open_in_browser = False<EOL><DEDENT>else:<EOL><INDENT>coverage_dir = '<STR_LIT>'  <EOL>open_in_browser = True<EOL><DEDENT>system(\"<STR_LIT>\" % (coveragebinary, coverage_dir))<EOL>logger.info(\"<STR_LIT>\", coverage_dir)<EOL>if open_in_browser:<EOL><INDENT>index_file = os.path.abspath(<EOL>os.path.join(coverage_dir, '<STR_LIT>'))<EOL>logger.debug(\"<STR_LIT>\", index_file)<EOL>webbrowser.open('<STR_LIT>' + index_file)<EOL>logger.info(\"<STR_LIT>\")<EOL><DEDENT>", "docstring": "Create coverage reports and open them in the browser.", "id": "f11970:m1"}
{"signature": "def get_all(self, include_archived=False):", "body": "return [conv for conv in self._conv_dict.values()<EOL>if not conv.is_archived or include_archived]<EOL>", "docstring": "Get all the conversations.\n\n        Args:\n            include_archived (bool): (optional) Whether to include archived\n                conversations. Defaults to ``False``.\n\n        Returns:\n            List of all :class:`.Conversation` objects.", "id": "f10020:c1:m1"}
{"signature": "def separation(self, r=<NUM_LIT:10>):", "body": "vx = vy = vz = <NUM_LIT:0><EOL>for b in self.boids:<EOL><INDENT>if b != self:<EOL><INDENT>if abs(self.x-b.x) < r: vx += (self.x-b.x)<EOL>if abs(self.y-b.y) < r: vy += (self.y-b.y)<EOL>if abs(self.z-b.z) < r: vz += (self.z-b.z)<EOL><DEDENT><DEDENT>return vx, vy, vz<EOL>", "docstring": "Boids keep a small distance from other boids.\n\n        Ensures that boids don't collide into each other,\n        in a smoothly accelerated motion.", "id": "f11568:c0:m3"}
{"signature": "def set_file_license_comment(self, doc, text):", "body": "if self.has_package(doc) and self.has_file(doc):<EOL><INDENT>if not self.file_license_comment_set:<EOL><INDENT>self.file_license_comment_set = True<EOL>self.file(doc).license_comment = text<EOL>return True<EOL><DEDENT>else:<EOL><INDENT>raise CardinalityError('<STR_LIT>')<EOL><DEDENT><DEDENT>else:<EOL><INDENT>raise OrderError('<STR_LIT>')<EOL><DEDENT>", "docstring": "Raises OrderError if no package or file defined.\nRaises CardinalityError if more than one per file.", "id": "f3750:c5:m2"}
{"signature": "def __from_xml(self,data):", "body": "ns=get_node_ns(data)<EOL>if ns and ns.getContent()!=VCARD_NS:<EOL><INDENT>raise ValueError(\"<STR_LIT>\" % (VCARD_NS,))<EOL><DEDENT>if data.name!=\"<STR_LIT>\":<EOL><INDENT>raise ValueError(\"<STR_LIT>\" % (data.name,))<EOL><DEDENT>n=data.children<EOL>dns=get_node_ns(data)<EOL>while n:<EOL><INDENT>if n.type!='<STR_LIT>':<EOL><INDENT>n=n.next<EOL>continue<EOL><DEDENT>ns=get_node_ns(n)<EOL>if (ns and dns and ns.getContent()!=dns.getContent()):<EOL><INDENT>n=n.next<EOL>continue<EOL><DEDENT>if not self.components.has_key(n.name):<EOL><INDENT>n=n.next<EOL>continue<EOL><DEDENT>cl,tp=self.components[n.name]<EOL>if tp in (\"<STR_LIT>\",\"<STR_LIT>\"):<EOL><INDENT>if self.content.has_key(n.name):<EOL><INDENT>raise ValueError(\"<STR_LIT>\" % (n.name,))<EOL><DEDENT>try:<EOL><INDENT>self.content[n.name]=cl(n.name,n)<EOL><DEDENT>except Empty:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>elif tp==\"<STR_LIT>\":<EOL><INDENT>if not self.content.has_key(n.name):<EOL><INDENT>self.content[n.name]=[]<EOL><DEDENT>try:<EOL><INDENT>self.content[n.name].append(cl(n.name,n))<EOL><DEDENT>except Empty:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>n=n.next<EOL><DEDENT>", "docstring": "Initialize a VCard object from XML node.\n\n        :Parameters:\n            - `data`: vcard to parse.\n        :Types:\n            - `data`: `libxml2.xmlNode`", "id": "f15257:c17:m2"}
{"signature": "def remove_organization_course(organization, course_key):", "body": "_validate_organization_data(organization)<EOL>_validate_course_key(course_key)<EOL>return data.delete_organization_course(course_key=course_key, organization=organization)<EOL>", "docstring": "Removes the specfied course from the specified organization", "id": "f4637:m10"}
{"signature": "@require_POST<EOL>def do_search(request, course_id=None):", "body": "<EOL>SearchInitializer.set_search_enviroment(request=request, course_id=course_id)<EOL>results = {<EOL>\"<STR_LIT:error>\": _(\"<STR_LIT>\")<EOL>}<EOL>status_code = <NUM_LIT><EOL>search_term = request.POST.get(\"<STR_LIT>\", None)<EOL>try:<EOL><INDENT>if not search_term:<EOL><INDENT>raise ValueError(_('<STR_LIT>'))<EOL><DEDENT>size, from_, page = _process_pagination_values(request)<EOL>track.emit(<EOL>'<STR_LIT>',<EOL>{<EOL>\"<STR_LIT>\": search_term,<EOL>\"<STR_LIT>\": size,<EOL>\"<STR_LIT>\": page,<EOL>}<EOL>)<EOL>results = perform_search(<EOL>search_term,<EOL>user=request.user,<EOL>size=size,<EOL>from_=from_,<EOL>course_id=course_id<EOL>)<EOL>status_code = <NUM_LIT:200><EOL>track.emit(<EOL>'<STR_LIT>',<EOL>{<EOL>\"<STR_LIT>\": search_term,<EOL>\"<STR_LIT>\": size,<EOL>\"<STR_LIT>\": page,<EOL>\"<STR_LIT>\": results[\"<STR_LIT>\"],<EOL>}<EOL>)<EOL><DEDENT>except ValueError as invalid_err:<EOL><INDENT>results = {<EOL>\"<STR_LIT:error>\": six.text_type(invalid_err)<EOL>}<EOL>log.debug(six.text_type(invalid_err))<EOL><DEDENT>except QueryParseError:<EOL><INDENT>results = {<EOL>\"<STR_LIT:error>\": _('<STR_LIT>')<EOL>}<EOL><DEDENT>except Exception as err:  <EOL><INDENT>results = {<EOL>\"<STR_LIT:error>\": _('<STR_LIT>').format(search_string=search_term)<EOL>}<EOL>log.exception(<EOL>'<STR_LIT>',<EOL>search_term,<EOL>request.user.id,<EOL>err<EOL>)<EOL><DEDENT>return JsonResponse(results, status=status_code)<EOL>", "docstring": "Search view for http requests\n\nArgs:\n    request (required) - django request object\n    course_id (optional) - course_id within which to restrict search\n\nReturns:\n    http json response with the following fields\n        \"took\" - how many seconds the operation took\n        \"total\" - how many results were found\n        \"max_score\" - maximum score from these results\n        \"results\" - json array of result documents\n\n        or\n\n        \"error\" - displayable information about an error that occured on the server\n\nPOST Params:\n    \"search_string\" (required) - text upon which to search\n    \"page_size\" (optional)- how many results to return per page (defaults to 20, with maximum cutoff at 100)\n    \"page_index\" (optional) - for which page (zero-indexed) to include results (defaults to 0)", "id": "f9817:m2"}
{"signature": "@classmethod<EOL><INDENT>def _bitForCoordinate(cls, coordinate, n):<DEDENT>", "body": "seed = cls._hashCoordinate(coordinate)<EOL>rng = Random(seed)<EOL>return rng.getUInt32(n)<EOL>", "docstring": "Maps the coordinate to a bit in the SDR.\n\n@param coordinate (numpy.array) Coordinate\n@param n (int) The number of available bits in the SDR\n@return (int) The index to a bit in the SDR", "id": "f17537:c0:m9"}
{"signature": "def _send_to_consumer(self, block):", "body": "self._consumer.write(block)<EOL>self._sent += len(block)<EOL>if self._callback:<EOL><INDENT>self._callback(self._sent, self.length)<EOL><DEDENT>", "docstring": "Send a block of bytes to the consumer.\n\n        Args:\n            block (str): Block of bytes", "id": "f7978:c0:m7"}
{"signature": "def clear_muc_child(self):", "body": "if self.muc_child:<EOL><INDENT>self.muc_child.free_borrowed()<EOL>self.muc_child=None<EOL><DEDENT>if not self.xmlnode.children:<EOL><INDENT>return<EOL><DEDENT>n=self.xmlnode.children<EOL>while n:<EOL><INDENT>if n.name not in (\"<STR_LIT:x>\",\"<STR_LIT>\"):<EOL><INDENT>n=n.next<EOL>continue<EOL><DEDENT>ns=n.ns()<EOL>if not ns:<EOL><INDENT>n=n.next<EOL>continue<EOL><DEDENT>ns_uri=ns.getContent()<EOL>if ns_uri in (MUC_NS,MUC_USER_NS,MUC_ADMIN_NS,MUC_OWNER_NS):<EOL><INDENT>n.unlinkNode()<EOL>n.freeNode()<EOL><DEDENT>n=n.next<EOL><DEDENT>", "docstring": "Remove the MUC specific stanza payload element.", "id": "f15255:c9:m2"}
{"signature": "def t_t_bclose(self, t):", "body": "return t<EOL>", "docstring": "r'\\}", "id": "f12428:c0:m4"}
{"signature": "@DictProperty('<STR_LIT>', '<STR_LIT>', read_only=True)<EOL><INDENT>def query(self):<DEDENT>", "body": "get = self.environ['<STR_LIT>'] = FormsDict()<EOL>pairs = _parse_qsl(self.environ.get('<STR_LIT>', '<STR_LIT>'))<EOL>for key, value in pairs:<EOL><INDENT>get[key] = value<EOL><DEDENT>return get<EOL>", "docstring": "The :attr:`query_string` parsed into a :class:`FormsDict`. These\n            values are sometimes called \"URL arguments\" or \"GET parameters\", but\n            not to be confused with \"URL wildcards\" as they are provided by the\n            :class:`Router`.", "id": "f12971:c12:m10"}
{"signature": "def waitforbuttonpress(*args, **kwargs):", "body": "return gcf().waitforbuttonpress(*args, **kwargs)<EOL>", "docstring": "Blocking call to interact with the figure.\n\nThis will wait for *n* key or mouse clicks from the user and\nreturn a list containing True's for keyboard clicks and False's\nfor mouse clicks.\n\nIf *timeout* is negative, does not timeout.", "id": "f17168:m21"}
{"signature": "def _is_root(self):", "body": "return isinstance(self._impl, RootDynamicSpaceImpl)<EOL>", "docstring": "True if ths space is a dynamic space, False otherwise.", "id": "f13977:c7:m9"}
{"signature": "def __repr__(self):", "body": "left = None<EOL>right = None<EOL>if self.left is not None:<EOL><INDENT>left = self.left.data<EOL><DEDENT>if self.right is not None:<EOL><INDENT>right = self.right.data<EOL><DEDENT>return \"<STR_LIT>\" % (self.data, left, right)<EOL>", "docstring": "!\n        @return (string) Default representation of the node.", "id": "f15715:c1:m1"}
{"signature": "def _mapPotential(self, index):", "body": "centerInput = self._mapColumn(index)<EOL>columnInputs = self._getInputNeighborhood(centerInput).astype(uintType)<EOL>numPotential = int(columnInputs.size * self._potentialPct + <NUM_LIT:0.5>)<EOL>selectedInputs = numpy.empty(numPotential, dtype=uintType)<EOL>self._random.sample(columnInputs, selectedInputs)<EOL>potential = numpy.zeros(self._numInputs, dtype=uintType)<EOL>potential[selectedInputs] = <NUM_LIT:1><EOL>return potential<EOL>", "docstring": "Maps a column to its input bits. This method encapsulates the topology of\nthe region. It takes the index of the column as an argument and determines\nwhat are the indices of the input vector that are located within the\ncolumn's potential pool. The return value is a list containing the indices\nof the input bits. The current implementation of the base class only\nsupports a 1 dimensional topology of columns with a 1 dimensional topology\nof inputs. To extend this class to support 2-D topology you will need to\noverride this method. Examples of the expected output of this method:\n* If the potentialRadius is greater than or equal to the largest input\n  dimension then each column connects to all of the inputs.\n* If the topology is one dimensional, the input space is divided up evenly\n  among the columns and each column is centered over its share of the\n  inputs.  If the potentialRadius is 5, then each column connects to the\n  input it is centered above as well as the 5 inputs to the left of that\n  input and the five inputs to the right of that input, wrapping around if\n  wrapAround=True.\n* If the topology is two dimensional, the input space is again divided up\n  evenly among the columns and each column is centered above its share of\n  the inputs.  If the potentialRadius is 5, the column connects to a square\n  that has 11 inputs on a side and is centered on the input that the column\n  is centered above.\n\nParameters:\n----------------------------\n:param index:   The index identifying a column in the permanence, potential\n                and connectivity matrices.", "id": "f17561:c4:m78"}
{"signature": "def remove_branch(self, name):", "body": "self._git.branch('<STR_LIT>', name)<EOL>", "docstring": "Removes a branch", "id": "f8774:c0:m15"}
{"signature": "def get_requires_for_build_wheel(config_settings):", "body": "backend = _build_backend()<EOL>try:<EOL><INDENT>hook = backend.get_requires_for_build_wheel<EOL><DEDENT>except AttributeError:<EOL><INDENT>return []<EOL><DEDENT>else:<EOL><INDENT>return hook(config_settings)<EOL><DEDENT>", "docstring": "Invoke the optional get_requires_for_build_wheel hook\n\n    Returns [] if the hook is not defined.", "id": "f9557:m2"}
{"signature": "def _DoubleDecoder():", "body": "local_unpack = struct.unpack<EOL>def InnerDecode(buffer, pos):<EOL><INDENT>new_pos = pos + <NUM_LIT:8><EOL>double_bytes = buffer[pos:new_pos]<EOL>if ((double_bytes[<NUM_LIT:7>:<NUM_LIT:8>] in b'<STR_LIT>')<EOL>and (double_bytes[<NUM_LIT:6>:<NUM_LIT:7>] >= b'<STR_LIT>')<EOL>and (double_bytes[<NUM_LIT:0>:<NUM_LIT:7>] != b'<STR_LIT>')):<EOL><INDENT>return (_NAN, new_pos)<EOL><DEDENT>result = local_unpack('<STR_LIT>', double_bytes)[<NUM_LIT:0>]<EOL>return (result, new_pos)<EOL><DEDENT>return _SimpleDecoder(wire_format.WIRETYPE_FIXED64, InnerDecode)<EOL>", "docstring": "Returns a decoder for a double field.\n\n    This code works around a bug in struct.unpack for not-a-number.", "id": "f8654:m7"}
{"signature": "def __unwrapResults(self):", "body": "if self.__cachedResults is None:<EOL><INDENT>if self.__rawInfo.results is not None:<EOL><INDENT>resultList = json.loads(self.__rawInfo.results)<EOL>assert len(resultList) == <NUM_LIT:2>,\"<STR_LIT>\" % (<EOL>len(resultList), resultList)<EOL>self.__cachedResults = self.ModelResults(<EOL>reportMetrics=resultList[<NUM_LIT:0>],<EOL>optimizationMetrics=resultList[<NUM_LIT:1>])<EOL><DEDENT>else:<EOL><INDENT>self.__cachedResults = self.ModelResults(<EOL>reportMetrics={},<EOL>optimizationMetrics={})<EOL><DEDENT><DEDENT>return self.__cachedResults<EOL>", "docstring": "Unwraps self.__rawInfo.results and caches it in self.__cachedResults;\n        Returns the unwrapped params\n\n        Parameters:\n        ----------------------------------------------------------------------\n        retval:         ModelResults namedtuple instance", "id": "f17598:c9:m12"}
{"signature": "def __init__(self, phase_name, tag_phases=True, phase_folders=None, galaxies=None,<EOL>optimizer_class=non_linear.MultiNest,<EOL>sub_grid_size=<NUM_LIT:2>, bin_up_factor=None, image_psf_shape=None, positions_threshold=None,<EOL>mask_function=None,<EOL>inner_mask_radii=None, cosmology=cosmo.Planck15, auto_link_priors=False):", "body": "super(MultiPlanePhase, self).__init__(phase_name=phase_name,<EOL>tag_phases=tag_phases,<EOL>phase_folders=phase_folders,<EOL>optimizer_class=optimizer_class,<EOL>sub_grid_size=sub_grid_size,<EOL>bin_up_factor=bin_up_factor,<EOL>image_psf_shape=image_psf_shape,<EOL>positions_threshold=positions_threshold,<EOL>mask_function=mask_function,<EOL>inner_mask_radii=inner_mask_radii,<EOL>cosmology=cosmology,<EOL>auto_link_priors=auto_link_priors)<EOL>self.galaxies = galaxies<EOL>", "docstring": "A phase with a simple source/lens model\n\nParameters\n----------\ngalaxies : [g.Galaxy] | [gm.GalaxyModel]\n    A galaxy that acts as a gravitational lens or is being lensed\noptimizer_class: class\n    The class of a non-linear optimizer\nsub_grid_size: int\n    The side length of the subgrid", "id": "f5967:c6:m1"}
{"signature": "def mixins(self, name):", "body": "m = self._smixins(name)<EOL>if m:<EOL><INDENT>return m<EOL><DEDENT>return self._smixins(name.replace('<STR_LIT>', '<STR_LIT:U+0020>'))<EOL>", "docstring": "Search mixins for name.\n        Allow '>' to be ignored. '.a .b()' == '.a > .b()'\n        Args:\n            name (string): Search term\n        Returns:\n            Mixin object list OR False", "id": "f12432:c0:m10"}
{"signature": "def dsa_sign(private_key, data, hash_algorithm):", "body": "if private_key.algorithm != '<STR_LIT>':<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>return _sign(private_key, data, hash_algorithm)<EOL>", "docstring": "Generates a DSA signature\n\n:param private_key:\n    The PrivateKey to generate the signature with\n\n:param data:\n    A byte string of the data the signature is for\n\n:param hash_algorithm:\n    A unicode string of \"md5\", \"sha1\", \"sha224\", \"sha256\", \"sha384\" or\n    \"sha512\"\n\n:raises:\n    ValueError - when any of the parameters contain an invalid value\n    TypeError - when any of the parameters are of the wrong type\n    OSError - when an error is returned by the OS crypto library\n\n:return:\n    A byte string of the signature", "id": "f9511:m21"}
{"signature": "def watch_files(self):", "body": "try:<EOL><INDENT>while <NUM_LIT:1>:<EOL><INDENT>sleep(<NUM_LIT:1>)  <EOL>try:<EOL><INDENT>files_stat = self.get_files_stat()<EOL><DEDENT>except SystemExit:<EOL><INDENT>logger.error(\"<STR_LIT>\")<EOL>self.shutdown_server()<EOL><DEDENT>if self.files_stat != files_stat:<EOL><INDENT>logger.info(\"<STR_LIT>\")<EOL>try:<EOL><INDENT>generator.re_generate()<EOL>global _root<EOL>_root = generator.root<EOL><DEDENT>except SystemExit:  <EOL><INDENT>logger.error(\"<STR_LIT>\")<EOL>self.shutdown_server()<EOL><DEDENT>self.files_stat = files_stat  <EOL><DEDENT><DEDENT><DEDENT>except KeyboardInterrupt:<EOL><INDENT>logger.info(\"<STR_LIT>\")<EOL>self.shutdown_watcher()<EOL><DEDENT>", "docstring": "watch files for changes, if changed, rebuild blog. this thread\n        will quit if the main process ends", "id": "f1203:c2:m3"}
{"signature": "@gcp_conn('<STR_LIT>')<EOL>def list_firewall_rules(client=None, **kwargs):", "body": "return gce_list(service=client.firewalls(),<EOL>**kwargs)<EOL>", "docstring": ":rtype: ``list``", "id": "f10866:m0"}
{"signature": "def _from_bytes(value):", "body": "result = (value.decode('<STR_LIT:utf-8>')<EOL>if isinstance(value, six.binary_type) else value)<EOL>if isinstance(result, six.text_type):<EOL><INDENT>return result<EOL><DEDENT>else:<EOL><INDENT>raise ValueError(<EOL>'<STR_LIT>'.format(value))<EOL><DEDENT>", "docstring": "Converts bytes to a string value, if necessary.\n\n    Args:\n        value: The string/bytes value to be converted.\n\n    Returns:\n        The original value converted to unicode (if bytes) or as passed in\n        if it started out as unicode.\n\n    Raises:\n        ValueError if the value could not be converted to unicode.", "id": "f2445:m10"}
{"signature": "def draw_idle(self):", "body": "DEBUG_MSG(\"<STR_LIT>\", <NUM_LIT:1>, self)<EOL>self._isDrawn = False  <EOL>if hasattr(self,'<STR_LIT>'):<EOL><INDENT>self._idletimer.Restart(IDLE_DELAY)<EOL><DEDENT>else:<EOL><INDENT>self._idletimer = wx.FutureCall(IDLE_DELAY,self._onDrawIdle)<EOL><DEDENT>", "docstring": "Delay rendering until the GUI is idle.", "id": "f17227:c3:m8"}
{"signature": "def IsMarathonEvent(event_type, **kwargs):", "body": "matching_dict = {<EOL>'<STR_LIT>': Equals(event_type),<EOL>'<STR_LIT>': After(_parse_marathon_event_timestamp,<EOL>matches_time_or_just_before(datetime.utcnow()))<EOL>}<EOL>matching_dict.update(kwargs)<EOL>return MatchesDict(matching_dict)<EOL>", "docstring": "Match a dict (deserialized from JSON) as a Marathon event. Matches the\nevent type and checks for a recent timestamp.\n\n:param event_type: The event type ('eventType' field value)\n:param kwargs: Any other matchers to apply to the dict", "id": "f13691:m3"}
{"signature": "def total_proper_motion(pmra, pmdecl, decl):", "body": "pm = np.sqrt( pmdecl*pmdecl + pmra*pmra*np.cos(np.radians(decl)) *<EOL>np.cos(np.radians(decl)) )<EOL>return pm<EOL>", "docstring": "This calculates the total proper motion of an object.\n\n    Parameters\n    ----------\n\n    pmra : float or array-like\n        The proper motion(s) in right ascension, measured in mas/yr.\n\n    pmdecl : float or array-like\n        The proper motion(s) in declination, measured in mas/yr.\n\n    decl : float or array-like\n        The declination of the object(s) in decimal degrees.\n\n    Returns\n    -------\n\n    float or array-like\n        The total proper motion(s) of the object(s) in mas/yr.", "id": "f14696:m15"}
{"signature": "def update(cursor, table, where_kv, commit=True, **field_values):", "body": "q = \"\"\"<STR_LIT>\"\"\" % table<EOL>fields = field_values.keys()<EOL>kv = '<STR_LIT:U+002C>'.join(['<STR_LIT>'.format(f) for f in fields])<EOL>where = '<STR_LIT>'.join(['<STR_LIT>'.format(f) for f in where_kv.keys()])<EOL>q = q.format(kv, where)<EOL>args = field_values.values() + where_kv.values()<EOL>cursor.execute(q, args=args)<EOL>if commit:<EOL><INDENT>cursor.connection.commit()<EOL><DEDENT>", "docstring": "db update \ucffc\ub9ac \ube4c\ub529 \ubc0f \uc2e4\ud589, \ub2e8, commit\uc740\n:param cursor: \ucee4\uc11c\n:type cursor: Cursor\n:param table: \ud14c\uc774\ube14 \uc774\ub984\n:type table: str\n:param where_kv: \uc5c5\ub370\uc774\ud2b8 where \uc870\uac74 dictionary, key:field, value:equal condition only\n:type where_kv: dict\n:param field_values: kwarg \uc5c5\ub370\uc774\ud2b8\uc6a9\n:type field_values: dict\n:param commit: \ucee4\ubc0b \uc5ec\ubd80\n:type commit: bool\n:return:", "id": "f6387:m12"}
{"signature": "def _cond_o(self, word, suffix_len):", "body": "return word[-suffix_len - <NUM_LIT:1>] in {'<STR_LIT:i>', '<STR_LIT:l>'}<EOL>", "docstring": "Return Lovins' condition O.\n\n        Parameters\n        ----------\n        word : str\n            Word to check\n        suffix_len : int\n            Suffix length\n\n        Returns\n        -------\n        bool\n            True if condition is met", "id": "f6562:c0:m13"}
{"signature": "@staticmethod<EOL><INDENT>def make_loop(handlers):<DEDENT>", "body": "return PollMainLoop(None, handlers)<EOL>", "docstring": "Return a main loop object for use with this test suite.", "id": "f15298:c1:m0"}
{"signature": "def __init__(self, app=None, **kwargs):", "body": "if app:<EOL><INDENT>self.init_app(app, **kwargs)<EOL><DEDENT>", "docstring": "Extension initialization.", "id": "f10405:c1:m0"}
{"signature": "def edit_distance(s1, s2):", "body": "d = {}<EOL>lenstr1 = len(s1)<EOL>lenstr2 = len(s2)<EOL>for i in xrange(-<NUM_LIT:1>, lenstr1 + <NUM_LIT:1>):<EOL><INDENT>d[(i, -<NUM_LIT:1>)] = i + <NUM_LIT:1><EOL><DEDENT>for j in xrange(-<NUM_LIT:1>, lenstr2 + <NUM_LIT:1>):<EOL><INDENT>d[(-<NUM_LIT:1>, j)] = j + <NUM_LIT:1><EOL><DEDENT>for i in xrange(lenstr1):<EOL><INDENT>for j in xrange(lenstr2):<EOL><INDENT>if s1[i] == s2[j]:<EOL><INDENT>cost = <NUM_LIT:0><EOL><DEDENT>else:<EOL><INDENT>cost = <NUM_LIT:1><EOL><DEDENT>d[(i, j)] = min(<EOL>d[(i - <NUM_LIT:1>, j)] + <NUM_LIT:1>, <EOL>d[(i, j - <NUM_LIT:1>)] + <NUM_LIT:1>, <EOL>d[(i - <NUM_LIT:1>, j - <NUM_LIT:1>)] + cost, <EOL>)<EOL>if i and j and s1[i] == s2[j - <NUM_LIT:1>] and s1[i - <NUM_LIT:1>] == s2[j]:<EOL><INDENT>d[(i, j)] = min(d[(i, j)], d[i - <NUM_LIT:2>, j - <NUM_LIT:2>] + cost) <EOL><DEDENT><DEDENT><DEDENT>return d[lenstr1 - <NUM_LIT:1>, lenstr2 - <NUM_LIT:1>]<EOL>", "docstring": "Calculates string edit distance between string 1 and string 2.\nDeletion, insertion, substitution, and transposition all increase edit distance.", "id": "f7968:m8"}
{"signature": "def create_doc(self):", "body": "doc_node = URIRef('<STR_LIT>')<EOL>self.graph.add((doc_node, RDF.type, self.spdx_namespace.SpdxDocument))<EOL>vers_literal = Literal(str(self.document.version))<EOL>self.graph.add((doc_node, self.spdx_namespace.specVersion, vers_literal))<EOL>data_lics = URIRef(self.document.data_license.url)<EOL>self.graph.add((doc_node, self.spdx_namespace.dataLicense, data_lics))<EOL>doc_name = URIRef(self.document.name)<EOL>self.graph.add((doc_node, self.spdx_namespace.name, doc_name))<EOL>return doc_node<EOL>", "docstring": "Add and return the root document node to graph.", "id": "f3742:c8:m1"}
{"signature": "def get_xbound(self):", "body": "left, right = self.get_xlim()<EOL>if left < right:<EOL><INDENT>return left, right<EOL><DEDENT>else:<EOL><INDENT>return right, left<EOL><DEDENT>", "docstring": "Returns the x-axis numerical bounds where::\n\n  lowerBound < upperBound", "id": "f17238:c1:m77"}
{"signature": "def touch(filepath):", "body": "with open(filepath, '<STR_LIT:a>'):<EOL><INDENT>os.utime(filepath, None)<EOL><DEDENT>", "docstring": "Touch the given filepath", "id": "f8350:m14"}
{"signature": "def set_aad_metadata(uri, resource, client):", "body": "set_config_value('<STR_LIT>', uri)<EOL>set_config_value('<STR_LIT>', resource)<EOL>set_config_value('<STR_LIT>', client)<EOL>", "docstring": "Set AAD metadata.", "id": "f2344:m14"}
{"signature": "@property<EOL><INDENT>def Lu(self):<DEDENT>", "body": "return self._Lu.value<EOL>", "docstring": "Lower-triangular, flat part of L.", "id": "f13599:c0:m8"}
{"signature": "def add(self, node_spec):", "body": "node = BayesNode(*node_spec)<EOL>assert node.variable not in self.vars<EOL>assert every(lambda parent: parent in self.vars, node.parents)<EOL>self.nodes.append(node)<EOL>self.vars.append(node.variable)<EOL>for parent in node.parents:<EOL><INDENT>self.variable_node(parent).children.append(node)<EOL><DEDENT>", "docstring": "Add a node to the net. Its parents must already be in the\n        net, and its variable must not.", "id": "f1686:c2:m1"}
{"signature": "def _createPredictionLogger(self):", "body": "<EOL>self._predictionLogger = BasicPredictionLogger(<EOL>fields=self._model.getFieldInfo(),<EOL>experimentDir=self._experimentDir,<EOL>label = \"<STR_LIT>\",<EOL>inferenceType=self._model.getInferenceType())<EOL>if self.__loggedMetricPatterns:<EOL><INDENT>metricLabels = self.__metricMgr.getMetricLabels()<EOL>loggedMetrics = matchPatterns(self.__loggedMetricPatterns, metricLabels)<EOL>self._predictionLogger.setLoggedMetrics(loggedMetrics)<EOL><DEDENT>", "docstring": "Creates the model's PredictionLogger object, which is an interface to write\nmodel results to a permanent storage location", "id": "f17593:c0:m6"}
{"signature": "def _ConvertWrapperMessage(value, message):", "body": "field = message.DESCRIPTOR.fields_by_name['<STR_LIT:value>']<EOL>setattr(message, '<STR_LIT:value>', _ConvertScalarFieldValue(value, field))<EOL>", "docstring": "Convert a JSON representation into Wrapper message.", "id": "f8658:m22"}
{"signature": "def process(self):", "body": "if self.__ccore:<EOL><INDENT>self.__process_by_ccore()<EOL><DEDENT>else:<EOL><INDENT>self.__process_by_python()<EOL><DEDENT>return self<EOL>", "docstring": "!\n        @brief Performs analysis to find out appropriate amount of clusters.\n\n        @return", "id": "f15547:c0:m1"}
{"signature": "@staticmethod<EOL><INDENT>def encapsulate_string(raw_string):<DEDENT>", "body": "raw_string.replace('<STR_LIT:\\\\>', '<STR_LIT>')<EOL>enc_string = re.sub(\"<STR_LIT>\", r\"<STR_LIT>\", raw_string)<EOL>return enc_string<EOL>", "docstring": "Encapsulate characters to make markdown look as expected.\n\n:param str raw_string: string to encapsulate\n:rtype: str\n:return: encapsulated input string", "id": "f4926:c0:m7"}
{"signature": "async def _load(self):", "body": "try:<EOL><INDENT>conv_events = await self._conversation.get_events(<EOL>self._conversation.events[<NUM_LIT:0>].id_<EOL>)<EOL><DEDENT>except (IndexError, hangups.NetworkError):<EOL><INDENT>conv_events = []<EOL><DEDENT>if not conv_events:<EOL><INDENT>self._first_loaded = True<EOL><DEDENT>if self._focus_position == self.POSITION_LOADING and conv_events:<EOL><INDENT>self.set_focus(conv_events[-<NUM_LIT:1>].id_)<EOL><DEDENT>else:<EOL><INDENT>self._modified()<EOL><DEDENT>self._refresh_watermarked_events()<EOL>self._is_loading = False<EOL>", "docstring": "Load more events for this conversation.", "id": "f10036:c14:m2"}
{"signature": "def delete(self, location):", "body": "bucket = self.info['<STR_LIT>']<EOL>prefix = self.info['<STR_LIT>']<EOL>self.logger.debug('<STR_LIT>')<EOL>s3conn = self.client <EOL>if location[<NUM_LIT:0>] == '<STR_LIT:/>':<EOL><INDENT>location = location[<NUM_LIT:1>:]<EOL><DEDENT>if location[-<NUM_LIT:1>] == '<STR_LIT:/>':<EOL><INDENT>location = location[:-<NUM_LIT:2>]<EOL><DEDENT>self.logger.debug('<STR_LIT>')<EOL>for s3key in s3conn.list_objects(Bucket=bucket, Prefix=(prefix+'<STR_LIT:/>'+location))['<STR_LIT>']:<EOL><INDENT>s3conn.delete_object(Bucket=bucket, Key=s3key['<STR_LIT>'])<EOL><DEDENT>self.logger.debug('<STR_LIT>')<EOL>", "docstring": "Delete content in bucket/prefix/location.\n           Location can be a directory or a file (e.g., my_dir or my_dir/my_image.tif)\n           If location is a directory, all files in the directory are deleted.\n           If it is a file, then that file is deleted.\n\n           Args:\n               location (str): S3 location within prefix. Can be a directory or\n                               a file (e.g., my_dir or my_dir/my_image.tif).", "id": "f7101:c0:m6"}
{"signature": "def task_context(self):", "body": "return task_context(self)<EOL>", "docstring": "Returns a context manager that sets this task as the current_task\nglobal. Similar to flask's app.request_context. This is used by the\nworkers to make the global available inside of task functions.", "id": "f3045:c2:m9"}
{"signature": "def __init__(self, default_args):", "body": "self._default_args = default_args<EOL>", "docstring": ":param default_args: default arguments\n:type default_args: string or list of string", "id": "f3777:c0:m0"}
{"signature": "@classmethod<EOL><INDENT>def get_canonical_headers(cls, req, include=None):<DEDENT>", "body": "if include is None:<EOL><INDENT>include = cls.default_include_headers<EOL><DEDENT>include = [x.lower() for x in include]<EOL>headers = req.headers.copy()<EOL>if '<STR_LIT:host>' not in headers:<EOL><INDENT>headers['<STR_LIT:host>'] = urlparse(req.url).netloc.split('<STR_LIT::>')[<NUM_LIT:0>]<EOL><DEDENT>cano_headers_dict = {}<EOL>for hdr, val in headers.items():<EOL><INDENT>hdr = hdr.strip().lower()<EOL>val = cls.amz_norm_whitespace(val).strip()<EOL>if (hdr in include or '<STR_LIT:*>' in include or<EOL>('<STR_LIT>' in include and hdr.startswith('<STR_LIT>') and not<EOL>hdr == '<STR_LIT>')):<EOL><INDENT>vals = cano_headers_dict.setdefault(hdr, [])<EOL>vals.append(val)<EOL><DEDENT><DEDENT>cano_headers = '<STR_LIT>'<EOL>signed_headers_list = []<EOL>for hdr in sorted(cano_headers_dict):<EOL><INDENT>vals = cano_headers_dict[hdr]<EOL>val = '<STR_LIT:U+002C>'.join(sorted(vals))<EOL>cano_headers += '<STR_LIT>'.format(hdr, val)<EOL>signed_headers_list.append(hdr)<EOL><DEDENT>signed_headers = '<STR_LIT:;>'.join(signed_headers_list)<EOL>return (cano_headers, signed_headers)<EOL>", "docstring": "Generate the Canonical Headers section of the Canonical Request.\n\nReturn the Canonical Headers and the Signed Headers strs as a tuple\n(canonical_headers, signed_headers).\n\nreq     -- Requests PreparedRequest object\ninclude -- List of headers to include in the canonical and signed\n           headers. It's primarily included to allow testing against\n           specific examples from Amazon. If omitted or None it\n           includes host, content-type and any header starting 'x-amz-'\n           except for x-amz-client context, which appears to break\n           mobile analytics auth if included. Except for the\n           x-amz-client-context exclusion these defaults are per the\n           AWS documentation.", "id": "f15416:c0:m8"}
{"signature": "def _get_registry(self, registry_path_or_url):", "body": "if registry_path_or_url.startswith('<STR_LIT:http>'):<EOL><INDENT>profiles = self._load_json_url(registry_path_or_url)<EOL><DEDENT>else:<EOL><INDENT>profiles = self._load_json_file(registry_path_or_url)<EOL><DEDENT>try:<EOL><INDENT>registry = {}<EOL>for profile in profiles:<EOL><INDENT>registry[profile['<STR_LIT:id>']] = profile<EOL><DEDENT>return registry<EOL><DEDENT>except KeyError as e:<EOL><INDENT>msg = (<EOL>'<STR_LIT>'<EOL>).format(path=registry_path_or_url)<EOL>six.raise_from(ValueError(msg), e)<EOL><DEDENT>", "docstring": "dict: Return the registry as dict with profiles keyed by id.", "id": "f7561:c0:m5"}
{"signature": "def dist_editex(src, tar, cost=(<NUM_LIT:0>, <NUM_LIT:1>, <NUM_LIT:2>), local=False):", "body": "return Editex().dist(src, tar, cost, local)<EOL>", "docstring": "Return the normalized Editex distance between two strings.\n\n    This is a wrapper for :py:meth:`Editex.dist`.\n\n    Parameters\n    ----------\n    src : str\n        Source string for comparison\n    tar : str\n        Target string for comparison\n    cost : tuple\n        A 3-tuple representing the cost of the four possible edits: match,\n        same-group, and mismatch respectively (by default: (0, 1, 2))\n    local : bool\n        If True, the local variant of Editex is used\n\n    Returns\n    -------\n    int\n        Normalized Editex distance\n\n    Examples\n    --------\n    >>> round(dist_editex('cat', 'hat'), 12)\n    0.333333333333\n    >>> round(dist_editex('Niall', 'Neil'), 12)\n    0.2\n    >>> dist_editex('aluminum', 'Catalan')\n    0.75\n    >>> dist_editex('ATCG', 'TAGC')\n    0.75", "id": "f6617:m1"}
{"signature": "def pick_event(self, mouseevent, artist, **kwargs):", "body": "s = '<STR_LIT>'<EOL>event = PickEvent(s, self, mouseevent, artist, **kwargs)<EOL>self.callbacks.process(s, event)<EOL>", "docstring": "This method will be called by artists who are picked and will\nfire off :class:`PickEvent` callbacks registered listeners", "id": "f17207:c10:m10"}
{"signature": "def __init__(self, first_instruction):", "body": "self._rows_in_grid = {}<EOL>self._todo = []<EOL>self._expand(first_instruction.row, Point(<NUM_LIT:0>, <NUM_LIT:0>), [])<EOL>self._walk()<EOL>", "docstring": "Start walking the knitting pattern starting from first_instruction.", "id": "f553:c3:m0"}
{"signature": "def p_declaration(self, p):", "body": "p[<NUM_LIT:0>] = p[<NUM_LIT:1>] if isinstance(p[<NUM_LIT:1>], list) else [p[<NUM_LIT:1>]]<EOL>", "docstring": "declaration                : variable_decl\n                                       | property_decl\n                                       | block_decl\n                                       | mixin_decl\n                                       | call_mixin\n                                       | import_statement", "id": "f12427:c2:m35"}
{"signature": "def permutationFilter(perm):", "body": "<EOL>return True<EOL>", "docstring": "This function can be used to selectively filter out specific permutation\n    combinations. It is called by RunPermutations for every possible permutation\n    of the variables in the permutations dict. It should return True for valid a\n    combination of permutation values and False for an invalid one.\n\n    Parameters:\n    ---------------------------------------------------------\n    perm: dict of one possible combination of name:value\n          pairs chosen from permutations.", "id": "f17399:m1"}
{"signature": "def __len__(self):", "body": "return len(self._dynamic)<EOL>", "docstring": "!\n        @brief (uint) Returns number of simulation steps that are stored in dynamic.", "id": "f15663:c0:m3"}
{"signature": "def batchzip(size, iterable=None, rest=False):", "body": "fn = ibatch(size, rest=rest) >> zipflow<EOL>return fn if iterable is None else fn(iterable)<EOL>", "docstring": "todo : add example\n:param size:\n:param iterable:\n:param rest:\n:return:", "id": "f6355:m14"}
{"signature": "@_check_player_is_active<EOL><INDENT>@_from_dbus_type<EOL>def rate(self):<DEDENT>", "body": "return self._rate<EOL>", "docstring": "Returns:\n    float: playback rate, 1 is the normal rate, 2 would be double speed.", "id": "f624:c2:m26"}
{"signature": "@classmethod<EOL><INDENT>def getSpec(cls):<DEDENT>", "body": "spec = cls.getBaseSpec()<EOL>t, o = _getAdditionalSpecs(temporalImp=gDefaultTemporalImp)<EOL>spec['<STR_LIT>'].update(t)<EOL>spec['<STR_LIT>'].update(o)<EOL>return spec<EOL>", "docstring": "Overrides :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSpec`.\n\nThe parameters collection is constructed based on the parameters specified\nby the various components (spatialSpec, temporalSpec and otherSpec)", "id": "f17618:c0:m6"}
{"signature": "def _remote_call(f, *args, **kwargs):", "body": "nargs = []<EOL>for a in args:<EOL><INDENT>if isinstance(a, Id):<EOL><INDENT>nargs.append(distob.engine[a])<EOL><DEDENT>elif (isinstance(a, collections.Sequence) and<EOL>not isinstance(a, string_types)):<EOL><INDENT>nargs.append(<EOL>[distob.engine[b] if isinstance(b, Id) else b for b in a])<EOL><DEDENT>else: nargs.append(a)<EOL><DEDENT>for k, a in kwargs.items():<EOL><INDENT>if isinstance(a, Id):<EOL><INDENT>kwargs[k] = distob.engine[a]<EOL><DEDENT>elif (isinstance(a, collections.Sequence) and<EOL>not isinstance(a, string_types)):<EOL><INDENT>kwargs[k] = [<EOL>distob.engine[b] if isinstance(b, Id) else b for b in a]<EOL><DEDENT><DEDENT>result = f(*nargs, **kwargs)<EOL>if (isinstance(result, collections.Sequence) and<EOL>not isinstance(result, string_types)):<EOL><INDENT>results = []<EOL>for subresult in result:<EOL><INDENT>if type(subresult) in distob.engine.proxy_types: <EOL><INDENT>results.append(Ref(subresult))<EOL><DEDENT>else:<EOL><INDENT>results.append(subresult)<EOL><DEDENT><DEDENT>return results<EOL><DEDENT>elif type(result) in distob.engine.proxy_types:<EOL><INDENT>return Ref(result)<EOL><DEDENT>else:<EOL><INDENT>return result<EOL><DEDENT>", "docstring": "(Executed on remote engine) convert Ids to real objects, call f", "id": "f4855:m4"}
{"signature": "def __init__(self, level=<NUM_LIT:9>):", "body": "self._level = level<EOL>", "docstring": "Initialize bzip2 compressor.\n\n        Parameters\n        ----------\n        level : int\n            The compression level (0 to 9)", "id": "f6647:c0:m0"}
{"signature": "def print_children(data_file, group='<STR_LIT:/>'):", "body": "base = data_file.get_node(group)<EOL>print ('<STR_LIT>' % base)<EOL>for node in base._f_walk_groups():<EOL><INDENT>if node is not base:<EOL><INDENT>print ('<STR_LIT>' % node)<EOL><DEDENT><DEDENT>print ('<STR_LIT>' % group)<EOL>for node in base._v_leaves.itervalues():<EOL><INDENT>info = node.shape<EOL>if len(info) == <NUM_LIT:0>:<EOL><INDENT>info = node.read()<EOL><DEDENT>print ('<STR_LIT>' % (node.name, info))<EOL>if len(node.title) > <NUM_LIT:0>:<EOL><INDENT>print ('<STR_LIT>' % node.title)<EOL><DEDENT><DEDENT>", "docstring": "Print all the sub-groups in `group` and leaf-nodes children of `group`.\n\n    Parameters:\n        data_file (pytables HDF5 file object): the data file to print\n        group (string): path name of the group to be printed.\n            Default: '/', the root node.", "id": "f15428:m1"}
{"signature": "def __init__(self):", "body": "super().__init__()<EOL>self.last_dict = {}<EOL>self.current_pc = None<EOL>self.context_key = '<STR_LIT>'<EOL>", "docstring": "Record a detailed execution trace", "id": "f16993:c2:m0"}
{"signature": "def connect(self, timeout=<NUM_LIT>):", "body": "<EOL>if self.socket:<EOL><INDENT>raise TensorForceError(\"<STR_LIT>\" +<EOL>\"<STR_LIT>\".format(self.host, self.port))<EOL><DEDENT>self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)<EOL>if timeout < <NUM_LIT:5> or timeout is None:<EOL><INDENT>timeout = <NUM_LIT:5><EOL><DEDENT>err = <NUM_LIT:0><EOL>start_time = time.time()<EOL>while time.time() - start_time < timeout:<EOL><INDENT>self.socket.settimeout(<NUM_LIT:5>)<EOL>err = self.socket.connect_ex((self.host, self.port))<EOL>if err == <NUM_LIT:0>:<EOL><INDENT>break<EOL><DEDENT>time.sleep(<NUM_LIT:1>)<EOL><DEDENT>if err != <NUM_LIT:0>:<EOL><INDENT>raise TensorForceError(\"<STR_LIT>\".<EOL>format(self.host, self.port, err, errno.errorcode[err], os.strerror(err)))<EOL><DEDENT>", "docstring": "Starts the server tcp connection on the given host:port.\n\nArgs:\n    timeout (int): The time (in seconds) for which we will attempt a connection to the remote\n        (every 5sec). After that (or if timeout is None or 0), an error is raised.", "id": "f14227:c0:m3"}
{"signature": "def is_empty(self, indexes=None):", "body": "<EOL>return not self.tile.bbox.intersects(<EOL>self.raster_file.bbox(out_crs=self.tile.crs)<EOL>)<EOL>", "docstring": "Check if there is data within this tile.\n\nReturns\n-------\nis empty : bool", "id": "f12816:c1:m2"}
{"signature": "@property<EOL><INDENT>def path(self):<DEDENT>", "body": "path = super(WindowsPath2, self).path<EOL>if path.startswith(\"<STR_LIT>\"):<EOL><INDENT>return path[<NUM_LIT:4>:]<EOL><DEDENT>return path<EOL>", "docstring": "Return the path always without the \\\\?\\ prefix.", "id": "f5406:c1:m9"}
{"signature": "def events(self, **kwargs):", "body": "return self.__api.events(query=EqualsOperator(\"<STR_LIT>\", self.hash_),<EOL>**kwargs)<EOL>", "docstring": "Get all events for this report. Additional arguments may also be\n        specified that will be passed to the query function.", "id": "f4024:c1:m4"}
{"signature": "@classmethod<EOL><INDENT>def _readConfigFile(cls, filename, path=None):<DEDENT>", "body": "outputProperties = dict()<EOL>if path is None:<EOL><INDENT>filePath = cls.findConfigFile(filename)<EOL><DEDENT>else:<EOL><INDENT>filePath = os.path.join(path, filename)<EOL><DEDENT>try:<EOL><INDENT>if filePath is not None:<EOL><INDENT>try:<EOL><INDENT>_getLogger().debug(\"<STR_LIT>\", filePath)<EOL>with open(filePath, '<STR_LIT:r>') as inp:<EOL><INDENT>contents = inp.read()<EOL><DEDENT><DEDENT>except Exception:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\" % filePath)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>try:<EOL><INDENT>contents = resource_string(\"<STR_LIT>\", filename)<EOL><DEDENT>except Exception as resourceException:<EOL><INDENT>if filename in [USER_CONFIG, CUSTOM_CONFIG]:<EOL><INDENT>contents = '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>raise resourceException<EOL><DEDENT><DEDENT><DEDENT>elements = ElementTree.XML(contents)<EOL>if elements.tag != '<STR_LIT>':<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (elements.tag))<EOL><DEDENT>propertyElements = elements.findall('<STR_LIT>')<EOL>for propertyItem in propertyElements:<EOL><INDENT>propInfo = dict()<EOL>propertyAttributes = list(propertyItem)<EOL>for propertyAttribute in propertyAttributes:<EOL><INDENT>propInfo[propertyAttribute.tag] = propertyAttribute.text<EOL><DEDENT>name = propInfo.get('<STR_LIT:name>', None)<EOL>if '<STR_LIT:value>' in propInfo and propInfo['<STR_LIT:value>'] is None:<EOL><INDENT>value = '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>value = propInfo.get('<STR_LIT:value>', None)<EOL>if value is None:<EOL><INDENT>if '<STR_LIT>' in propInfo:<EOL><INDENT>continue<EOL><DEDENT>else:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (str(propInfo)))<EOL><DEDENT><DEDENT><DEDENT>restOfValue = value<EOL>value = '<STR_LIT>'<EOL>while True:<EOL><INDENT>pos = restOfValue.find('<STR_LIT>')<EOL>if pos == -<NUM_LIT:1>:<EOL><INDENT>value += restOfValue<EOL>break<EOL><DEDENT>value += restOfValue[<NUM_LIT:0>:pos]<EOL>varTailPos = restOfValue.find('<STR_LIT:}>', pos)<EOL>if varTailPos == -<NUM_LIT:1>:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (restOfValue))<EOL><DEDENT>varname = restOfValue[pos+<NUM_LIT:6>:varTailPos]<EOL>if varname not in os.environ:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (varname))<EOL><DEDENT>envVarValue = os.environ[varname]<EOL>value += envVarValue<EOL>restOfValue = restOfValue[varTailPos+<NUM_LIT:1>:]<EOL><DEDENT>if name is None:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (str(propInfo)))<EOL><DEDENT>propInfo['<STR_LIT:value>'] = value<EOL>outputProperties[name] = propInfo<EOL><DEDENT>return outputProperties<EOL><DEDENT>except Exception:<EOL><INDENT>_getLogger().exception(\"<STR_LIT>\",<EOL>filePath)<EOL>raise<EOL><DEDENT>", "docstring": "Parse the given XML file and return a dict describing the file.\n\n        :param filename: (string) name of XML file to parse (no path)\n        :param path: (string) path of the XML file. If None, then use the standard\n               configuration search path.\n        :returns: (dict) with each property as a key and a dict of all the\n               property's attributes as value", "id": "f17641:c0:m8"}
{"signature": "def maybe_decode_header(header):", "body": "value, encoding = decode_header(header)[<NUM_LIT:0>]<EOL>if encoding:<EOL><INDENT>return value.decode(encoding)<EOL><DEDENT>else:<EOL><INDENT>return value<EOL><DEDENT>", "docstring": "Decodes an encoded 7-bit ASCII header value into it's actual value.", "id": "f25:m0"}
{"signature": "def r_first_passage(self, objectId):", "body": "collection, reffs = self.get_reffs(objectId=objectId, export_collection=True)<EOL>first, _ = reffs[<NUM_LIT:0>]<EOL>return redirect(<EOL>url_for(\"<STR_LIT>\", objectId=objectId, subreference=first, semantic=self.semantic(collection))<EOL>)<EOL>", "docstring": "Provides a redirect to the first passage of given objectId\n\n        :param objectId: Collection identifier\n        :type objectId: str\n        :return: Redirection to the first passage of given text", "id": "f9934:c0:m21"}
{"signature": "def satisfied_by_checked(self, req):", "body": "req_man = RequirementsManager([req])<EOL>return any(req_man.check(*checked) for checked in self.checked)<EOL>", "docstring": "Check if requirement is already satisfied by what was previously checked\n\n:param Requirement req: Requirement to check", "id": "f1794:c2:m8"}
{"signature": "def Velasco(T, Tc, omega):", "body": "return (<NUM_LIT> + <NUM_LIT>*omega + <NUM_LIT>*omega**<NUM_LIT:2>)*(<NUM_LIT:1>-T/Tc)**<NUM_LIT>*R*Tc<EOL>", "docstring": "r'''Calculates enthalpy of vaporization at arbitrary temperatures using a\n    the work of [1]_; requires a chemical's critical temperature and\n    acentric factor.\n\n    The enthalpy of vaporization is given by:\n\n    .. math::\n        \\Delta_{vap} H = RT_c(7.2729 + 10.4962\\omega + 0.6061\\omega^2)(1-T_r)^{0.38}\n\n    Parameters\n    ----------\n    T : float\n        Temperature of fluid [K]\n    Tc : float\n        Critical temperature of fluid [K]\n    omega : float\n        Acentric factor [-]\n\n    Returns\n    -------\n    Hvap : float\n        Enthalpy of vaporization, [J/mol]\n\n    Notes\n    -----\n    The original article has been reviewed. It is regressed from enthalpy of\n    vaporization values at 0.7Tr, from 121 fluids in REFPROP 9.1.\n    A value in the article was read to be similar, but slightly too low from\n    that calculated here.\n\n    Examples\n    --------\n    From graph, in [1]_ for perfluoro-n-heptane.\n\n    >>> Velasco(333.2, 476.0, 0.5559)\n    33299.41734936356\n\n    References\n    ----------\n    .. [1] Velasco, S., M. J. Santos, and J. A. White. \"Extended Corresponding\n       States Expressions for the Changes in Enthalpy, Compressibility Factor\n       and Constant-Volume Heat Capacity at Vaporization.\" The Journal of\n       Chemical Thermodynamics 85 (June 2015): 68-76.\n       doi:10.1016/j.jct.2015.01.011.", "id": "f15781:m6"}
{"signature": "def predict(self, X):", "body": "x = X<EOL>if not isinstance(X, list):<EOL><INDENT>x = [X]<EOL><DEDENT>y = self.estimator.predict(x)<EOL>y = [item[<NUM_LIT:0>] for item in y]<EOL>y = [self._remove_prefix(label) for label in y]<EOL>if not isinstance(X, list):<EOL><INDENT>y = y[<NUM_LIT:0>]<EOL><DEDENT>return y<EOL>", "docstring": "In order to obtain the most likely label for a list of text\n\n        Parameters\n        ----------\n        X : list of string\n            Raw texts\n\n        Returns\n        -------\n        C : list of string\n            List labels", "id": "f10241:c0:m3"}
{"signature": "def __init__(self, tasks_cls, backend_section, stopper, config, timer=<NUM_LIT:0>):", "body": "super().__init__(name=backend_section)  <EOL>self.config = config<EOL>self.tasks_cls = tasks_cls  <EOL>self.tasks = []  <EOL>self.backend_section = backend_section<EOL>self.stopper = stopper  <EOL>self.timer = timer<EOL>self.thread_id = None<EOL>", "docstring": ":tasks_cls : tasks classes to be executed using the backend\n:backend_section: perceval backend section name\n:config: config object for the manager", "id": "f9695:c0:m0"}
{"signature": "def logoNotebook(symbol, token='<STR_LIT>', version='<STR_LIT>'):", "body": "_raiseIfNotStr(symbol)<EOL>url = logo(symbol, token, version)['<STR_LIT:url>']<EOL>return ImageI(url=url)<EOL>", "docstring": "This is a helper function, but the google APIs url is standardized.\n\n    https://iexcloud.io/docs/api/#logo\n    8am UTC daily\n\n    Args:\n        symbol (string); Ticker to request\n        token (string); Access token\n        version (string); API version\n\n    Returns:\n        image: result", "id": "f2330:m54"}
{"signature": "def merge(dict_1, dict_2):", "body": "return dict((str(key), dict_1.get(key) or dict_2.get(key))<EOL>for key in set(dict_2) | set(dict_1))<EOL>", "docstring": "Merge two dictionaries.\n\n    Values that evaluate to true take priority over falsy values.\n    `dict_1` takes priority over `dict_2`.", "id": "f4080:m0"}
{"signature": "def user_line(self, frame):", "body": "self.get_stack_data(frame, None, '<STR_LIT>')<EOL>", "docstring": "This function is called when we stop or break at this line.", "id": "f4257:c0:m3"}
{"signature": "def main():", "body": "parser = argparse.ArgumentParser(description = '<STR_LIT>',<EOL>parents = [XMPPSettings.get_arg_parser()])<EOL>parser.add_argument('<STR_LIT>', metavar = '<STR_LIT>', <EOL>help = '<STR_LIT>')<EOL>parser.add_argument('<STR_LIT>',<EOL>action = '<STR_LIT>', dest = '<STR_LIT>',<EOL>const = logging.DEBUG, default = logging.INFO,<EOL>help = '<STR_LIT>')<EOL>parser.add_argument('<STR_LIT>', const = logging.ERROR,<EOL>action = '<STR_LIT>', dest = '<STR_LIT>',<EOL>help = '<STR_LIT>')<EOL>parser.add_argument('<STR_LIT>', action = '<STR_LIT:store_true>',<EOL>help = '<STR_LIT>')<EOL>args = parser.parse_args()<EOL>settings = XMPPSettings({<EOL>\"<STR_LIT>\": \"<STR_LIT>\"<EOL>})<EOL>settings.load_arguments(args)<EOL>if settings.get(\"<STR_LIT:password>\") is None:<EOL><INDENT>password = getpass(\"<STR_LIT>\".format(args.jid))<EOL>if sys.version_info.major < <NUM_LIT:3>:<EOL><INDENT>password = password.decode(\"<STR_LIT:utf-8>\")<EOL><DEDENT>settings[\"<STR_LIT:password>\"] = password<EOL><DEDENT>if sys.version_info.major < <NUM_LIT:3>:<EOL><INDENT>args.jid = args.jid.decode(\"<STR_LIT:utf-8>\")<EOL><DEDENT>logging.basicConfig(level = args.log_level)<EOL>if args.trace:<EOL><INDENT>print(\"<STR_LIT>\")<EOL>handler = logging.StreamHandler()<EOL>handler.setLevel(logging.DEBUG)<EOL>for logger in (\"<STR_LIT>\", \"<STR_LIT>\"):<EOL><INDENT>logger = logging.getLogger(logger)<EOL>logger.setLevel(logging.DEBUG)<EOL>logger.addHandler(handler)<EOL>logger.propagate = False<EOL><DEDENT><DEDENT>bot = EchoBot(JID(args.jid), settings)<EOL>try:<EOL><INDENT>bot.run()<EOL><DEDENT>except KeyboardInterrupt:<EOL><INDENT>bot.disconnect()<EOL><DEDENT>", "docstring": "Parse the command-line arguments and run the bot.", "id": "f15230:m0"}
{"signature": "def assemble_topology(self):", "body": "self.logger.debug(\"<STR_LIT>\")<EOL>top = '<STR_LIT>'<EOL>self.logger.debug(\"<STR_LIT>\")<EOL>top += self.toptemplate<EOL>top = top.replace('<STR_LIT>',       '<STR_LIT>'.join( self._make_defaults(self.system)) )<EOL>top = top.replace('<STR_LIT>',      '<STR_LIT>'.join( self._make_atomtypes(self.system)) )<EOL>top = top.replace('<STR_LIT>',  '<STR_LIT>'.join( self._make_nonbond_param(self.system)) )<EOL>top = top.replace('<STR_LIT>',      '<STR_LIT>'.join( self._make_pairtypes(self.system)) )<EOL>top = top.replace('<STR_LIT>',      '<STR_LIT>'.join( self._make_bondtypes(self.system)) )<EOL>top = top.replace('<STR_LIT>','<STR_LIT>'.join( self._make_constrainttypes(self.system)))<EOL>top = top.replace('<STR_LIT>',     '<STR_LIT>'.join( self._make_angletypes(self.system)))<EOL>top = top.replace('<STR_LIT>',  '<STR_LIT>'.join( self._make_dihedraltypes(self.system)) )<EOL>top = top.replace('<STR_LIT>',  '<STR_LIT>'.join( self._make_impropertypes(self.system)) )<EOL>top = top.replace('<STR_LIT>',      '<STR_LIT>'.join( self._make_cmaptypes(self.system)) )<EOL>for i,(molname,m) in enumerate(self.system.dict_molname_mol.items()):<EOL><INDENT>itp = self.itptemplate<EOL>itp = itp.replace('<STR_LIT>',  '<STR_LIT>'.join( self._make_moleculetype(m, molname, m.exclusion_numb))  )<EOL>itp = itp.replace('<STR_LIT>',         '<STR_LIT>'.join( self._make_atoms(m))  )<EOL>itp = itp.replace('<STR_LIT>',         '<STR_LIT>'.join( self._make_bonds(m))  )<EOL>itp = itp.replace('<STR_LIT>',         '<STR_LIT>'.join( self._make_pairs(m))  )<EOL>itp = itp.replace('<STR_LIT>',       '<STR_LIT>'.join( self._make_settles(m))  )<EOL>itp = itp.replace('<STR_LIT>','<STR_LIT>'.join( self._make_virtual_sites3(m))  )<EOL>itp = itp.replace('<STR_LIT>',    '<STR_LIT>'.join( self._make_exclusions(m))  )<EOL>itp = itp.replace('<STR_LIT>',        '<STR_LIT>'.join( self._make_angles(m)) )<EOL>itp = itp.replace('<STR_LIT>',     '<STR_LIT>'.join( self._make_dihedrals(m)) )<EOL>itp = itp.replace('<STR_LIT>',     '<STR_LIT>'.join( self._make_impropers(m)) )<EOL>itp = itp.replace('<STR_LIT>',         '<STR_LIT>'.join( self._make_cmaps(m)) )<EOL>if not self.multiple_output:<EOL><INDENT>top += itp<EOL><DEDENT>else:<EOL><INDENT>outfile = \"<STR_LIT>\".format(molname)<EOL>top += '<STR_LIT>'.format( molname )<EOL>with open(outfile, \"<STR_LIT:w>\") as f:<EOL><INDENT>f.writelines([itp])<EOL><DEDENT><DEDENT><DEDENT>top += '<STR_LIT>'<EOL>top += '<STR_LIT>'<EOL>molecules = [(\"<STR_LIT>\", <NUM_LIT:0>)]<EOL>for m in self.system.molecules:<EOL><INDENT>if (molecules[-<NUM_LIT:1>][<NUM_LIT:0>] != m.name):<EOL><INDENT>molecules.append([m.name, <NUM_LIT:0>])<EOL><DEDENT>if molecules[-<NUM_LIT:1>][<NUM_LIT:0>] == m.name:<EOL><INDENT>molecules[-<NUM_LIT:1>][<NUM_LIT:1>] += <NUM_LIT:1><EOL><DEDENT><DEDENT>for molname, n in molecules[<NUM_LIT:1>:]:<EOL><INDENT>top += '<STR_LIT>'.format(molname, n)<EOL><DEDENT>top += '<STR_LIT:\\n>'<EOL>with open(self.outfile, '<STR_LIT:w>') as f:<EOL><INDENT>f.writelines([top])<EOL><DEDENT>", "docstring": "Call the various member self._make_* functions to convert the topology object into a string", "id": "f6856:c1:m2"}
{"signature": "def spanning_2d_grid(length):", "body": "ret = nx.grid_2d_graph(length + <NUM_LIT:2>, length)<EOL>for i in range(length):<EOL><INDENT>ret.node[(<NUM_LIT:0>, i)]['<STR_LIT>'] = <NUM_LIT:0><EOL>ret[(<NUM_LIT:0>, i)][(<NUM_LIT:1>, i)]['<STR_LIT>'] = <NUM_LIT:0><EOL>ret.node[(length + <NUM_LIT:1>, i)]['<STR_LIT>'] = <NUM_LIT:1><EOL>ret[(length + <NUM_LIT:1>, i)][(length, i)]['<STR_LIT>'] = <NUM_LIT:1><EOL><DEDENT>return ret<EOL>", "docstring": "Generate a square lattice with auxiliary nodes for spanning detection\n\nParameters\n----------\n\nlength : int\n   Number of nodes in one dimension, excluding the auxiliary nodes.\n\nReturns\n-------\n\nnetworkx.Graph\n   A square lattice graph with auxiliary nodes for spanning cluster\n   detection\n\nSee Also\n--------\n\nsample_states : spanning cluster detection", "id": "f15353:m8"}
{"signature": "def end_pan(self):", "body": "del self._pan_start<EOL>", "docstring": "Called when a pan operation completes (when the mouse button\nis up.)\n\n.. note::\n    Intended to be overridden by new projection types.", "id": "f17238:c1:m114"}
{"signature": "def get_content_length(environ):", "body": "<EOL>try:<EOL><INDENT>return max(<NUM_LIT:0>, int(environ.get(\"<STR_LIT>\", <NUM_LIT:0>)))<EOL><DEDENT>except ValueError:<EOL><INDENT>return <NUM_LIT:0><EOL><DEDENT>", "docstring": "Return a positive CONTENT_LENGTH in a safe way (return 0 otherwise).", "id": "f8588:m21"}
{"signature": "def cg(f_Ax, b, cg_iters=<NUM_LIT:10>, callback=None, verbose=False, residual_tol=<NUM_LIT>):", "body": "p = b.copy()<EOL>r = b.copy()<EOL>x = np.zeros_like(b)<EOL>rdotr = r.dot(r)<EOL>fmtstr =  \"<STR_LIT>\"<EOL>titlestr =  \"<STR_LIT>\"<EOL>if verbose: print(titlestr % (\"<STR_LIT>\", \"<STR_LIT>\", \"<STR_LIT>\"))<EOL>for i in range(cg_iters):<EOL><INDENT>if callback is not None:<EOL><INDENT>callback(x)<EOL><DEDENT>if verbose: print(fmtstr % (i, rdotr, np.linalg.norm(x)))<EOL>z = f_Ax(p)<EOL>v = rdotr / p.dot(z)<EOL>x += v*p<EOL>r -= v*z<EOL>newrdotr = r.dot(r)<EOL>mu = newrdotr/rdotr<EOL>p = r + mu*p<EOL>rdotr = newrdotr<EOL>if rdotr < residual_tol:<EOL><INDENT>break<EOL><DEDENT><DEDENT>if callback is not None:<EOL><INDENT>callback(x)<EOL><DEDENT>if verbose: print(fmtstr % (i+<NUM_LIT:1>, rdotr, np.linalg.norm(x)))  <EOL>return x<EOL>", "docstring": "Demmel p 312", "id": "f1365:m0"}
{"signature": "def record_delete_subfield(rec, tag, subfield_code, ind1='<STR_LIT:U+0020>', ind2='<STR_LIT:U+0020>'):", "body": "ind1, ind2 = _wash_indicators(ind1, ind2)<EOL>for field in rec.get(tag, []):<EOL><INDENT>if field[<NUM_LIT:1>] == ind1 and field[<NUM_LIT:2>] == ind2:<EOL><INDENT>field[<NUM_LIT:0>][:] = [subfield for subfield in field[<NUM_LIT:0>]<EOL>if subfield_code != subfield[<NUM_LIT:0>]]<EOL><DEDENT><DEDENT>", "docstring": "Delete all subfields with subfield_code in the record.", "id": "f7891:m13"}
{"signature": "def sigma2fwhm(sigma):", "body": "sigma = np.asarray(sigma)<EOL>return np.sqrt(<NUM_LIT:8> * np.log(<NUM_LIT:2>)) * sigma<EOL>", "docstring": "Convert a sigma in a Gaussian kernel to a FWHM value.\n\n    Parameters\n    ----------\n    sigma: float or numpy.array\n       sigma value or values\n\n    Returns\n    -------\n    fwhm: float or numpy.array\n       fwhm values corresponding to `sigma` values", "id": "f4090:m1"}
{"signature": "def teff(cluster):", "body": "b_vs, _ = cluster.stars()<EOL>teffs = []<EOL>for b_v in b_vs:<EOL><INDENT>b_v -= cluster.eb_v<EOL>if b_v > -<NUM_LIT>:<EOL><INDENT>x = (<NUM_LIT> - b_v) / <NUM_LIT><EOL><DEDENT>else:<EOL><INDENT>x = (<NUM_LIT> - math.sqrt(<NUM_LIT> + <NUM_LIT> * b_v)) / <NUM_LIT><EOL><DEDENT>teffs.append(math.pow(<NUM_LIT:10>, x))<EOL><DEDENT>return teffs<EOL>", "docstring": "Calculate Teff for main sequence stars ranging from Teff 3500K - 8000K. Use\n[Fe/H] of the cluster, if available.\n\nReturns a list of Teff values.", "id": "f11264:m2"}
{"signature": "def revoke_all_tokens(self):", "body": "try:<EOL><INDENT>self.store.clear_tokens()<EOL><DEDENT>except Exception:<EOL><INDENT>LOGGER.exception('<STR_LIT>')<EOL>return False<EOL><DEDENT>else:<EOL><INDENT>return True<EOL><DEDENT>", "docstring": "Implementation of :meth:`twitcher.api.ITokenManager.revoke_all_tokens`.", "id": "f13850:c2:m3"}
{"signature": "def _handshake(self):", "body": "self._ssl = None<EOL>self._rbio = None<EOL>self._wbio = None<EOL>try:<EOL><INDENT>self._ssl = libssl.SSL_new(self._session._ssl_ctx)<EOL>if is_null(self._ssl):<EOL><INDENT>self._ssl = None<EOL>handle_openssl_error(<NUM_LIT:0>)<EOL><DEDENT>mem_bio = libssl.BIO_s_mem()<EOL>self._rbio = libssl.BIO_new(mem_bio)<EOL>if is_null(self._rbio):<EOL><INDENT>handle_openssl_error(<NUM_LIT:0>)<EOL><DEDENT>self._wbio = libssl.BIO_new(mem_bio)<EOL>if is_null(self._wbio):<EOL><INDENT>handle_openssl_error(<NUM_LIT:0>)<EOL><DEDENT>libssl.SSL_set_bio(self._ssl, self._rbio, self._wbio)<EOL>utf8_domain = self._hostname.encode('<STR_LIT:utf-8>')<EOL>libssl.SSL_ctrl(<EOL>self._ssl,<EOL>LibsslConst.SSL_CTRL_SET_TLSEXT_HOSTNAME,<EOL>LibsslConst.TLSEXT_NAMETYPE_host_name,<EOL>utf8_domain<EOL>)<EOL>libssl.SSL_set_connect_state(self._ssl)<EOL>if self._session._ssl_session:<EOL><INDENT>libssl.SSL_set_session(self._ssl, self._session._ssl_session)<EOL><DEDENT>self._bio_write_buffer = buffer_from_bytes(self._buffer_size)<EOL>self._read_buffer = buffer_from_bytes(self._buffer_size)<EOL>handshake_server_bytes = b'<STR_LIT>'<EOL>handshake_client_bytes = b'<STR_LIT>'<EOL>while True:<EOL><INDENT>result = libssl.SSL_do_handshake(self._ssl)<EOL>handshake_client_bytes += self._raw_write()<EOL>if result == <NUM_LIT:1>:<EOL><INDENT>break<EOL><DEDENT>error = libssl.SSL_get_error(self._ssl, result)<EOL>if error == LibsslConst.SSL_ERROR_WANT_READ:<EOL><INDENT>chunk = self._raw_read()<EOL>if chunk == b'<STR_LIT>':<EOL><INDENT>if handshake_server_bytes == b'<STR_LIT>':<EOL><INDENT>raise_disconnection()<EOL><DEDENT>if detect_client_auth_request(handshake_server_bytes):<EOL><INDENT>raise_client_auth()<EOL><DEDENT>raise_protocol_error(handshake_server_bytes)<EOL><DEDENT>handshake_server_bytes += chunk<EOL><DEDENT>elif error == LibsslConst.SSL_ERROR_WANT_WRITE:<EOL><INDENT>handshake_client_bytes += self._raw_write()<EOL><DEDENT>elif error == LibsslConst.SSL_ERROR_ZERO_RETURN:<EOL><INDENT>self._gracefully_closed = True<EOL>self._shutdown(False)<EOL>self._raise_closed()<EOL><DEDENT>else:<EOL><INDENT>info = peek_openssl_error()<EOL>if libcrypto_version_info < (<NUM_LIT:1>, <NUM_LIT:1>):<EOL><INDENT>dh_key_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_SSL3_CHECK_CERT_AND_ALGORITHM,<EOL>LibsslConst.SSL_R_DH_KEY_TOO_SMALL<EOL>)<EOL><DEDENT>else:<EOL><INDENT>dh_key_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_TLS_PROCESS_SKE_DHE,<EOL>LibsslConst.SSL_R_DH_KEY_TOO_SMALL<EOL>)<EOL><DEDENT>if info == dh_key_info:<EOL><INDENT>raise_dh_params()<EOL><DEDENT>if libcrypto_version_info < (<NUM_LIT:1>, <NUM_LIT:1>):<EOL><INDENT>unknown_protocol_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_SSL23_GET_SERVER_HELLO,<EOL>LibsslConst.SSL_R_UNKNOWN_PROTOCOL<EOL>)<EOL><DEDENT>else:<EOL><INDENT>unknown_protocol_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_SSL3_GET_RECORD,<EOL>LibsslConst.SSL_R_WRONG_VERSION_NUMBER<EOL>)<EOL><DEDENT>if info == unknown_protocol_info:<EOL><INDENT>raise_protocol_error(handshake_server_bytes)<EOL><DEDENT>tls_version_info_error = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_SSL23_GET_SERVER_HELLO,<EOL>LibsslConst.SSL_R_TLSV1_ALERT_PROTOCOL_VERSION<EOL>)<EOL>if info == tls_version_info_error:<EOL><INDENT>raise_protocol_version()<EOL><DEDENT>handshake_error_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_SSL23_GET_SERVER_HELLO,<EOL>LibsslConst.SSL_R_SSLV3_ALERT_HANDSHAKE_FAILURE<EOL>)<EOL>if info == handshake_error_info:<EOL><INDENT>raise_handshake()<EOL><DEDENT>handshake_failure_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_SSL3_READ_BYTES,<EOL>LibsslConst.SSL_R_SSLV3_ALERT_HANDSHAKE_FAILURE<EOL>)<EOL>if info == handshake_failure_info:<EOL><INDENT>raise_client_auth()<EOL><DEDENT>if libcrypto_version_info < (<NUM_LIT:1>, <NUM_LIT:1>):<EOL><INDENT>cert_verify_failed_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_SSL3_GET_SERVER_CERTIFICATE,<EOL>LibsslConst.SSL_R_CERTIFICATE_VERIFY_FAILED<EOL>)<EOL><DEDENT>else:<EOL><INDENT>cert_verify_failed_info = (<EOL><NUM_LIT:20>,<EOL>LibsslConst.SSL_F_TLS_PROCESS_SERVER_CERTIFICATE,<EOL>LibsslConst.SSL_R_CERTIFICATE_VERIFY_FAILED<EOL>)<EOL><DEDENT>if info == cert_verify_failed_info:<EOL><INDENT>verify_result = libssl.SSL_get_verify_result(self._ssl)<EOL>chain = extract_chain(handshake_server_bytes)<EOL>self_signed = False<EOL>time_invalid = False<EOL>no_issuer = False<EOL>cert = None<EOL>oscrypto_cert = None<EOL>if chain:<EOL><INDENT>cert = chain[<NUM_LIT:0>]<EOL>oscrypto_cert = load_certificate(cert)<EOL>self_signed = oscrypto_cert.self_signed<EOL>issuer_error_codes = set([<EOL>LibsslConst.X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT,<EOL>LibsslConst.X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN,<EOL>LibsslConst.X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY<EOL>])<EOL>if verify_result in issuer_error_codes:<EOL><INDENT>no_issuer = not self_signed<EOL><DEDENT>time_error_codes = set([<EOL>LibsslConst.X509_V_ERR_CERT_HAS_EXPIRED,<EOL>LibsslConst.X509_V_ERR_CERT_NOT_YET_VALID<EOL>])<EOL>time_invalid = verify_result in time_error_codes<EOL><DEDENT>if time_invalid:<EOL><INDENT>raise_expired_not_yet_valid(cert)<EOL><DEDENT>if no_issuer:<EOL><INDENT>raise_no_issuer(cert)<EOL><DEDENT>if self_signed:<EOL><INDENT>raise_self_signed(cert)<EOL><DEDENT>if oscrypto_cert and oscrypto_cert.asn1.hash_algo in set(['<STR_LIT>', '<STR_LIT>']):<EOL><INDENT>raise_weak_signature(oscrypto_cert)<EOL><DEDENT>raise_verification(cert)<EOL><DEDENT>handle_openssl_error(<NUM_LIT:0>, TLSError)<EOL><DEDENT><DEDENT>session_info = parse_session_info(<EOL>handshake_server_bytes,<EOL>handshake_client_bytes<EOL>)<EOL>self._protocol = session_info['<STR_LIT>']<EOL>self._cipher_suite = session_info['<STR_LIT>']<EOL>self._compression = session_info['<STR_LIT>']<EOL>self._session_id = session_info['<STR_LIT>']<EOL>self._session_ticket = session_info['<STR_LIT>']<EOL>if self._cipher_suite.find('<STR_LIT>') != -<NUM_LIT:1>:<EOL><INDENT>dh_params_length = get_dh_params_length(handshake_server_bytes)<EOL>if dh_params_length < <NUM_LIT>:<EOL><INDENT>self.close()<EOL>raise_dh_params()<EOL><DEDENT><DEDENT>if self._session_id == '<STR_LIT>' or self._session_ticket == '<STR_LIT>':<EOL><INDENT>if self._session._ssl_session:<EOL><INDENT>libssl.SSL_SESSION_free(self._session._ssl_session)<EOL><DEDENT>self._session._ssl_session = libssl.SSL_get1_session(self._ssl)<EOL><DEDENT>if not self._session._manual_validation:<EOL><INDENT>if self.certificate.hash_algo in set(['<STR_LIT>', '<STR_LIT>']):<EOL><INDENT>raise_weak_signature(self.certificate)<EOL><DEDENT>if not self.certificate.is_valid_domain_ip(self._hostname):<EOL><INDENT>raise_hostname(self.certificate, self._hostname)<EOL><DEDENT><DEDENT><DEDENT>except (OSError, socket_.error):<EOL><INDENT>if self._ssl:<EOL><INDENT>libssl.SSL_free(self._ssl)<EOL>self._ssl = None<EOL>self._rbio = None<EOL>self._wbio = None<EOL><DEDENT>else:<EOL><INDENT>if self._rbio:<EOL><INDENT>libssl.BIO_free(self._rbio)<EOL>self._rbio = None<EOL><DEDENT>if self._wbio:<EOL><INDENT>libssl.BIO_free(self._wbio)<EOL>self._wbio = None<EOL><DEDENT><DEDENT>self.close()<EOL>raise<EOL><DEDENT>", "docstring": "Perform an initial TLS handshake", "id": "f9532:c1:m2"}
{"signature": "def lrta_star_agent(s1):", "body": "unimplemented()<EOL>", "docstring": "[Fig. 4.24]", "id": "f1675:m17"}
{"signature": "def ifetch_single(iterable, key, default=EMPTY, getter=None):", "body": "def _getter(item):<EOL><INDENT>if getter:<EOL><INDENT>custom_getter = partial(getter, key=key)<EOL>return custom_getter(item)<EOL><DEDENT>else:<EOL><INDENT>try:<EOL><INDENT>attrgetter = operator.attrgetter(key)<EOL>return attrgetter(item)<EOL><DEDENT>except AttributeError:<EOL><INDENT>pass<EOL><DEDENT>try:<EOL><INDENT>itemgetter = operator.itemgetter(key)<EOL>return itemgetter(item)<EOL><DEDENT>except KeyError:<EOL><INDENT>pass<EOL><DEDENT>if default is not EMPTY:<EOL><INDENT>return default<EOL><DEDENT>raise ValueError('<STR_LIT>' % (item, key))<EOL><DEDENT><DEDENT>return map(_getter, iterable)<EOL>", "docstring": "getter() g(item, key):pass", "id": "f14165:m0"}
{"signature": "@staticmethod<EOL><INDENT>def from_element(root, timezone):<DEDENT>", "body": "assert root.tag == '<STR_LIT>'<EOL>if root.xpath('<STR_LIT>'):<EOL><INDENT>return _ScheduleIntervals(root, timezone)<EOL><DEDENT>elif root.xpath('<STR_LIT>'):<EOL><INDENT>return _ScheduleRecurring(root, timezone)<EOL><DEDENT>raise NotImplementedError<EOL>", "docstring": "Return a Schedule object based on an lxml Element for the <schedule>\n        tag. timezone is a tzinfo object, ideally from pytz.", "id": "f4285:c0:m1"}
{"signature": "def add_bolt(self, name, bolt_cls, par, inputs, config=None, optional_outputs=None):", "body": "assert isinstance(inputs, dict)<EOL>user_spec = bolt_cls.spec(name)<EOL>bolt_classpath = user_spec.python_class_path<EOL>if hasattr(bolt_cls, '<STR_LIT>'):<EOL><INDENT>user_outputs = bolt_cls.outputs<EOL><DEDENT>else:<EOL><INDENT>user_outputs = []<EOL><DEDENT>if optional_outputs is not None:<EOL><INDENT>user_outputs.extend(optional_outputs)<EOL><DEDENT>if config is None:<EOL><INDENT>_config = {}<EOL><DEDENT>else:<EOL><INDENT>_config = config<EOL><DEDENT>test_spec = IntegrationTestBolt.spec(name, par, inputs, _config,<EOL>user_bolt_classpath=bolt_classpath,<EOL>user_output_fields=user_outputs)<EOL>self.add_spec(test_spec)<EOL>self.bolts[name] = test_spec<EOL>return test_spec<EOL>", "docstring": "Add an integration_test bolt\n\n        Only dict based inputs is supported", "id": "f7292:c0:m2"}
{"signature": "@_check_player_is_active<EOL><INDENT>def select_subtitle(self, index):<DEDENT>", "body": "return self._player_interface.SelectSubtitle(dbus.Int32(index))<EOL>", "docstring": "Enable a subtitle specified by the index it is listed in :class:`list_subtitles`\n\nArgs:\n    index (int): index of subtitle listing returned by :class:`list_subtitles`", "id": "f624:c2:m52"}
{"signature": "def is_string_like(obj):", "body": "if isinstance(obj, str): return True<EOL>if ma.isMaskedArray(obj):<EOL><INDENT>if obj.ndim == <NUM_LIT:0> and obj.dtype.kind in '<STR_LIT>':<EOL><INDENT>return True<EOL><DEDENT>else:<EOL><INDENT>return False<EOL><DEDENT><DEDENT>try: obj + '<STR_LIT>'<EOL>except (TypeError, ValueError): return False<EOL>return True<EOL>", "docstring": "Return True if *obj* looks like a string", "id": "f17209:m4"}
{"signature": "def clear_descendants(self, source, clear_source=True):", "body": "desc = nx.descendants(self, source)<EOL>if clear_source:<EOL><INDENT>desc.add(source)<EOL><DEDENT>self.remove_nodes_from(desc)<EOL>return desc<EOL>", "docstring": "Remove all descendants of(reachable from) `source`.\n\n        Args:\n            source: Node descendants\n            clear_source(bool): Remove origin too if True.\n        Returns:\n            set: The removed nodes.", "id": "f13980:c0:m0"}
{"signature": "def get_referenced_accounts(self):", "body": "return []<EOL>", "docstring": "Retrieve the general ledger accounts referenced in this instance.\n\n:returns: The referenced accounts.", "id": "f15838:c0:m7"}
{"signature": "def activatePredictedColumn(self, column, columnActiveSegments,<EOL>columnMatchingSegments, prevActiveCells,<EOL>prevWinnerCells, learn):", "body": "return self._activatePredictedColumn(<EOL>self.connections, self._random,<EOL>columnActiveSegments, prevActiveCells, prevWinnerCells,<EOL>self.numActivePotentialSynapsesForSegment,<EOL>self.maxNewSynapseCount, self.initialPermanence,<EOL>self.permanenceIncrement, self.permanenceDecrement,<EOL>self.maxSynapsesPerSegment, learn)<EOL>", "docstring": "Determines which cells in a predicted column should be added to winner cells\nlist, and learns on the segments that correctly predicted this column.\n\n:param column: (int) Index of bursting column.\n\n:param columnActiveSegments: (iter) Active segments in this column.\n\n:param columnMatchingSegments: (iter) Matching segments in this column.\n\n:param prevActiveCells: (list) Active cells in ``t-1``.\n\n:param prevWinnerCells: (list) Winner cells in ``t-1``.\n\n:param learn: (bool) If true, grow and reinforce synapses.\n\n:returns: (list) A list of predicted cells that will be added to \n          active cells and winner cells.", "id": "f17571:c0:m6"}
{"signature": "def __load_linktype__(link_type):", "body": "try:<EOL><INDENT>filep, pathname, description = imp.find_module(link_type, sys.path)<EOL>link_type_module = imp.load_module(link_type, filep, pathname,<EOL>description)<EOL><DEDENT>except ImportError:<EOL><INDENT>return None<EOL><DEDENT>finally:<EOL><INDENT>if filep:<EOL><INDENT>filep.close()<EOL><DEDENT><DEDENT>return link_type_module<EOL>", "docstring": "Given a string for a given module, attempt to load it.", "id": "f1075:m4"}
{"signature": "@contextlib.contextmanager<EOL><INDENT>def assertWarns(self, warning, *args, **kwargs):<DEDENT>", "body": "original_filters = warnings.filters[:]<EOL>warnings.simplefilter('<STR_LIT:error>')<EOL>if len(args) == <NUM_LIT:0> and len(kwargs) == <NUM_LIT:0>:<EOL><INDENT>with self.assertRaises(warning):<EOL><INDENT>yield<EOL><DEDENT><DEDENT>else:<EOL><INDENT>self.assertRaises(warning, *args, **kwargs)<EOL><DEDENT>warnings.filters = original_filters<EOL>", "docstring": "A test that checks if a specified wanring was raised", "id": "f7142:c0:m1"}
{"signature": "def b64decode(s, altchars=None):", "body": "if altchars is not None:<EOL><INDENT>s = s.translate(string.maketrans(altchars[:<NUM_LIT:2>], '<STR_LIT>'))<EOL><DEDENT>try:<EOL><INDENT>return binascii.a2b_base64(s)<EOL><DEDENT>except binascii.Error as msg:<EOL><INDENT>raise TypeError(msg)<EOL><DEDENT>", "docstring": "Decode a Base64 encoded string.\n\n    s is the string to decode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies the\n    alternative alphabet used instead of the '+' and '/' characters.\n\n    The decoded string is returned.  A TypeError is raised if s is\n    incorrectly padded.  Characters that are neither in the normal base-64\n    alphabet nor the alternative alphabet are discarded prior to the padding\n    check.", "id": "f16403:m2"}
{"signature": "@property<EOL><INDENT>def R_specific(self):<DEDENT>", "body": "return property_molar_to_mass(R, self.MW)<EOL>", "docstring": "r'''Specific gas constant of the mixture, in units of [J/kg/K].\n\n        Examples\n        --------\n        >>> Mixture(['N2', 'O2'], zs=[0.79, .21]).R_specific\n        288.1928437986195", "id": "f15811:c0:m40"}
{"signature": "def difference_of_pandas_dfs(df_self, df_other, col_names=None):", "body": "df = pd.concat([df_self, df_other])<EOL>df = df.reset_index(drop=True)<EOL>df_gpby = df.groupby(col_names)<EOL>idx = [x[<NUM_LIT:0>] for x in list(df_gpby.groups.values()) if len(x) == <NUM_LIT:1>]<EOL>df_sym_diff = df.reindex(idx)<EOL>df_diff = pd.concat([df_other, df_sym_diff])<EOL>df_diff = df_diff.reset_index(drop=True)<EOL>df_gpby = df_diff.groupby(col_names)<EOL>idx = [x[<NUM_LIT:0>] for x in list(df_gpby.groups.values()) if len(x) == <NUM_LIT:2>]<EOL>df_diff = df_diff.reindex(idx)<EOL>return df_diff<EOL>", "docstring": "Returns a dataframe with all of df_other that are not in df_self, when considering the columns specified in col_names\n:param df_self: pandas Dataframe\n:param df_other: pandas Dataframe\n:param col_names: list of column names\n:return:", "id": "f12841:m18"}
{"signature": "def get_taus(cos_theta, n2n1=<NUM_LIT>):", "body": "return <NUM_LIT>/(<NUM_LIT:1>+csqrt(<NUM_LIT:1>+(n2n1**<NUM_LIT:2>-<NUM_LIT:1>)*cos_theta**-<NUM_LIT:2>))<EOL>", "docstring": "Calculates the Fresnel reflectivity for s-polarized light incident on an\ninterface with index ration n2n1.\n\nParameters\n----------\n    cos_theta : Float or numpy.ndarray\n        The _cosine_ of the angle of the incoming light. Float.\n    n2n1 : Float, optional\n        The ratio n2/n1 of the 2nd material's index n2 to the first's n1\n        Default is 0.95\n\nReturns\n-------\n    Float or numpy.ndarray\n        The reflectivity, in the same type (ndarray or Float) and\n        shape as cos_theta", "id": "f5753:m4"}
{"signature": "def _mask_non_positives(a):", "body": "mask = a <= <NUM_LIT:0.0><EOL>if mask.any():<EOL><INDENT>return ma.MaskedArray(a, mask=mask)<EOL><DEDENT>return a<EOL>", "docstring": "Return a Numpy masked array where all non-positive values are\nmasked.  If there are no non-positive values, the original array\nis returned.", "id": "f17187:m0"}
{"signature": "def _format_range_unified(start, stop):", "body": "<EOL>beginning = start + <NUM_LIT:1>     <EOL>length = stop - start<EOL>if length == <NUM_LIT:1>:<EOL><INDENT>return '<STR_LIT:%s>' % (beginning)<EOL><DEDENT>if not length:<EOL><INDENT>beginning -= <NUM_LIT:1>        <EOL><DEDENT>return '<STR_LIT>' % (beginning, length)<EOL>", "docstring": "Convert range to the \"ed\" format", "id": "f16407:m6"}
{"signature": "def __init__(self, document_pointer, exists, is_document):", "body": "super(InvalidDocumentPointer, self).__init__()<EOL>self._document_pointer = document_pointer<EOL>self._exists = bool(exists)<EOL>self._is_document = bool(is_document)<EOL>", "docstring": "``document_pointer`` is the DocumentPointer object that contains\n        the invalid reference.", "id": "f15206:c9:m0"}
{"signature": "def OnTaskComplete(self, info):", "body": "self.task_info = dict(<EOL>type=info.type,<EOL>result=info.result<EOL>)<EOL>", "docstring": "Task completion notification.", "id": "f1888:c0:m3"}
{"signature": "def parse(src):", "body": "rt = libparser.parse(byref(post), src)<EOL>return (<EOL>rt,<EOL>string_at(post.title, post.tsz),<EOL>string_at(post.tpic, post.tpsz),<EOL>post.body<EOL>)<EOL>", "docstring": "Note: src should be ascii string", "id": "f1208:m0"}
{"signature": "def _convert_fancy(self, field):", "body": "if self.sep is False:<EOL><INDENT>x = self._convert_singlet(field)<EOL><DEDENT>else:<EOL><INDENT>x = tuple([self._convert_singlet(s) for s in field.split(self.sep)])<EOL>if len(x) == <NUM_LIT:0>:<EOL><INDENT>x = '<STR_LIT>'<EOL><DEDENT>elif len(x) == <NUM_LIT:1>:<EOL><INDENT>x = x[<NUM_LIT:0>]<EOL><DEDENT><DEDENT>return x<EOL>", "docstring": "Convert to a list (sep != None) and convert list elements.", "id": "f6857:c0:m3"}
{"signature": "def getBucketValues(self):", "body": "if self._bucketValues is None:<EOL><INDENT>numBuckets = len(self.encoder.getBucketValues())<EOL>self._bucketValues = []<EOL>for bucketIndex in range(numBuckets):<EOL><INDENT>self._bucketValues.append(self.getBucketInfo([bucketIndex])[<NUM_LIT:0>].value)<EOL><DEDENT><DEDENT>return self._bucketValues<EOL>", "docstring": "See the function description in base.py", "id": "f17541:c0:m9"}
{"signature": "def parse_data(self, text, maxwidth, maxheight, template_dir, context,<EOL>urlize_all_links):", "body": "<EOL>replacements = {}<EOL>user_urls = set(re.findall(URL_RE, text))<EOL>for user_url in user_urls:<EOL><INDENT>try:<EOL><INDENT>resource = oembed.site.embed(user_url, maxwidth=maxwidth, maxheight=maxheight)<EOL><DEDENT>except OEmbedException:<EOL><INDENT>if urlize_all_links:<EOL><INDENT>replacements[user_url] = '<STR_LIT>' % {'<STR_LIT>': user_url}<EOL><DEDENT><DEDENT>else:<EOL><INDENT>context['<STR_LIT>'] = min(maxwidth, resource.width)<EOL>context['<STR_LIT>'] = min(maxheight, resource.height)<EOL>replacement = self.render_oembed(<EOL>resource, <EOL>user_url, <EOL>template_dir=template_dir, <EOL>context=context<EOL>)<EOL>replacements[user_url] = replacement.strip()<EOL><DEDENT><DEDENT>user_urls = re.finditer(URL_RE, text)<EOL>matches = []<EOL>for match in user_urls:<EOL><INDENT>if match.group() in replacements:<EOL><INDENT>matches.append([match.start(), match.end(), match.group()])<EOL><DEDENT><DEDENT>for indx, (start, end, user_url) in enumerate(matches):<EOL><INDENT>replacement = replacements[user_url]<EOL>difference = len(replacement) - len(user_url)<EOL>text = text[:start] + replacement + text[end:]<EOL>for j in xrange(indx + <NUM_LIT:1>, len(matches)):<EOL><INDENT>matches[j][<NUM_LIT:0>] += difference<EOL>matches[j][<NUM_LIT:1>] += difference<EOL><DEDENT><DEDENT>return mark_safe(text)<EOL>", "docstring": "Parses a block of text indiscriminately", "id": "f2947:c0:m0"}
{"signature": "def duplicate(self):", "body": "i = self.canvas.layer(self.img.copy(), self.x, self.y, self.name)<EOL>clone = self.canvas.layers[i]<EOL>clone.alpha = self.alpha<EOL>clone.blend = self.blend<EOL>", "docstring": "Creates a copy of the current layer.\n\n        This copy becomes the top layer on the canvas.", "id": "f11554:c2:m9"}
{"signature": "def disconnect(self):", "body": "with self.lock:<EOL><INDENT>if self.stream:<EOL><INDENT>if self.settings[u\"<STR_LIT>\"]:<EOL><INDENT>self.send(Presence(stanza_type = \"<STR_LIT>\"))<EOL><DEDENT>self.stream.disconnect()<EOL><DEDENT><DEDENT>", "docstring": "Gracefully disconnect from the server.", "id": "f15276:c0:m4"}
{"signature": "def versions_from_file(filename):", "body": "try:<EOL><INDENT>with open(filename) as f:<EOL><INDENT>contents = f.read()<EOL><DEDENT><DEDENT>except EnvironmentError:<EOL><INDENT>raise NotThisMethod(\"<STR_LIT>\")<EOL><DEDENT>mo = re.search(r\"<STR_LIT>\",<EOL>contents, re.M | re.S)<EOL>if not mo:<EOL><INDENT>mo = re.search(r\"<STR_LIT>\",<EOL>contents, re.M | re.S)<EOL><DEDENT>if not mo:<EOL><INDENT>raise NotThisMethod(\"<STR_LIT>\")<EOL><DEDENT>return json.loads(mo.group(<NUM_LIT:1>))<EOL>", "docstring": "Try to determine the version from _version.py if present.", "id": "f3620:m9"}
{"signature": "def _asdict(self):", "body": "return OrderedDict(list(zip(self._fields, self)))<EOL>", "docstring": "Return a new OrderedDict which maps field names to their values", "id": "f16407:c0:m3"}
{"signature": "@staticmethod<EOL><INDENT>def get_search_engine(index=None):<DEDENT>", "body": "search_engine_class = _load_class(getattr(settings, \"<STR_LIT>\", None), None)<EOL>return search_engine_class(index=index) if search_engine_class else None<EOL>", "docstring": "Returns the desired implementor (defined in settings)", "id": "f9824:c0:m6"}
{"signature": "def _sync_children(self, task_specs, state=MAYBE):", "body": "LOG.debug(\"<STR_LIT>\" % self.get_name())<EOL>if task_specs is None:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>add = task_specs[:]<EOL>remove = []<EOL>for child in self.children:<EOL><INDENT>if child.triggered:<EOL><INDENT>continue<EOL><DEDENT>if child.task_spec in add:<EOL><INDENT>add.remove(child.task_spec)<EOL>continue<EOL><DEDENT>if child._is_definite():<EOL><INDENT>raise WorkflowException(self.task_spec,<EOL>'<STR_LIT>' %<EOL>repr(child))<EOL><DEDENT>remove.append(child)<EOL><DEDENT>for child in remove:<EOL><INDENT>self.children.remove(child)<EOL><DEDENT>for task_spec in add:<EOL><INDENT>self._add_child(task_spec, state)<EOL><DEDENT>", "docstring": "This method syncs up the task's children with the given list of task\nspecs. In other words::\n\n    - Add one child for each given TaskSpec, unless that child already\n      exists.\n    - Remove all children for which there is no spec in the given list,\n      unless it is a \"triggered\" task.\n\n.. note::\n\n   It is an error if the task has a non-predicted child that is\n   not given in the TaskSpecs.\n\n:type  task_specs: list(TaskSpec)\n:param task_specs: The list of task specs that may become children.\n:type  state: integer\n:param state: The bitmask of states for the new children.", "id": "f7734:c0:m18"}
{"signature": "def recoverPubkeyParameter(message, digest, signature, pubkey):", "body": "if not isinstance(message, bytes):<EOL><INDENT>message = bytes(message, \"<STR_LIT:utf-8>\")  <EOL><DEDENT>for i in range(<NUM_LIT:0>, <NUM_LIT:4>):<EOL><INDENT>if SECP256K1_MODULE == \"<STR_LIT>\":  <EOL><INDENT>sig = pubkey.ecdsa_recoverable_deserialize(signature, i)<EOL>p = secp256k1.PublicKey(pubkey.ecdsa_recover(message, sig))<EOL>if p.serialize() == pubkey.serialize():<EOL><INDENT>return i<EOL><DEDENT><DEDENT>elif SECP256K1_MODULE == \"<STR_LIT>\" and not isinstance(pubkey, PublicKey):<EOL><INDENT>p = recover_public_key(digest, signature, i, message)<EOL>p_comp = hexlify(compressedPubkey(p))<EOL>pubkey_comp = hexlify(compressedPubkey(pubkey))<EOL>if p_comp == pubkey_comp:<EOL><INDENT>return i<EOL><DEDENT><DEDENT>else:  <EOL><INDENT>p = recover_public_key(digest, signature, i)<EOL>p_comp = hexlify(compressedPubkey(p))<EOL>p_string = hexlify(p.to_string())<EOL>if isinstance(pubkey, PublicKey):  <EOL><INDENT>pubkey_string = bytes(repr(pubkey), \"<STR_LIT:ascii>\")<EOL><DEDENT>else:  <EOL><INDENT>pubkey_string = hexlify(pubkey.to_string())<EOL><DEDENT>if p_string == pubkey_string or p_comp == pubkey_string:  <EOL><INDENT>return i<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Use to derive a number that allows to easily recover the\n        public key from the signature", "id": "f8269:m3"}
{"signature": "def xpath_eval(self,expr):", "body": "ctxt = common_doc.xpathNewContext()<EOL>ctxt.setContextNode(self.xmlnode)<EOL>ctxt.xpathRegisterNs(\"<STR_LIT>\",self.ns.getContent())<EOL>ret=ctxt.xpathEval(to_utf8(expr))<EOL>ctxt.xpathFreeContext()<EOL>return ret<EOL>", "docstring": "Evaluate XPath expression in context of `self.xmlnode`.\n\n:Parameters:\n    - `expr`: the XPath expression\n:Types:\n    - `expr`: `unicode`\n\n:return: the result of the expression evaluation.\n:returntype: list of `libxml2.xmlNode`", "id": "f15255:c0:m4"}
{"signature": "@tornado.gen.coroutine<EOL>def get_topologies():", "body": "request_url = create_url(TOPOLOGIES_URL_FMT)<EOL>raise tornado.gen.Return((yield fetch_url_as_json(request_url)))<EOL>", "docstring": "Get the list of topologies given a data center from heron tracker\n:return:", "id": "f7414:m3"}
{"signature": "def get_table_names(self):", "body": "return list(pd.read_sql(\"<STR_LIT>\", self.conn)[\"<STR_LIT:name>\"])<EOL>", "docstring": "Return a list of the underlying tables in the database.\n\nReturns\n-------\ntable_names: list[str]", "id": "f12922:c0:m12"}
{"signature": "def pad_if_need(sz_atleast, img, mode='<STR_LIT>'):<EOL>", "body": "<EOL>imsz = img.shape[:<NUM_LIT:2>]  <EOL>padneed = np.asarray((sz_atleast[<NUM_LIT:0>] - imsz[<NUM_LIT:0>], sz_atleast[<NUM_LIT:1>] - imsz[<NUM_LIT:1>]))<EOL>if np.any(padneed > <NUM_LIT:0>):<EOL><INDENT>padding = np.zeros((img.ndim, <NUM_LIT:2>), dtype='<STR_LIT>')<EOL>padneed = np.maximum(padneed, <NUM_LIT:0>)<EOL>padding[:<NUM_LIT:2>, <NUM_LIT:0>] = padneed/<NUM_LIT:2><EOL>padding[:<NUM_LIT:2>, <NUM_LIT:1>] = padneed - padneed/<NUM_LIT:2><EOL>img = np.pad(img, padding, mode=mode)<EOL><DEDENT>return img<EOL>", "docstring": "pad img if need to guarantee minumum size\n:param sz_atleast: [H,W] at least\n:param img: image np.array [H,W, ...]\n:param mode: str, padding mode\n:return: padded image or asis if enought size", "id": "f6390:m25"}
{"signature": "def _get_tuple(self, fields):", "body": "v1 = '<STR_LIT>'<EOL>v2 = '<STR_LIT>'<EOL>if len(fields) > <NUM_LIT:0>:<EOL><INDENT>v1 = fields[<NUM_LIT:0>]<EOL><DEDENT>if len(fields) > <NUM_LIT:1>:<EOL><INDENT>v2 = fields[<NUM_LIT:1>]<EOL><DEDENT>return v1, v2<EOL>", "docstring": ":param fields: a list which contains either 0,1,or 2 values\n:return: a tuple with default values of '';", "id": "f7867:c0:m1"}
{"signature": "def error(self, error_code, value, **kwargs):", "body": "code = self.error_code_map.get(error_code, error_code)<EOL>try:<EOL><INDENT>message = Template(self.error_messages[code])<EOL><DEDENT>except KeyError:<EOL><INDENT>message = Template(self.error_messages[error_code])<EOL><DEDENT>placeholders = {\"<STR_LIT:value>\": self.hidden_value if self.hidden else value}<EOL>placeholders.update(kwargs)<EOL>placeholders.update(self.message_values)<EOL>self.messages[code] = message.safe_substitute(placeholders)<EOL>", "docstring": "Helper to add error to messages field. It fills placeholder with extra call parameters\nor values from message_value map.\n\n:param error_code: Error code to use\n:rparam error_code: str\n:param value: Value checked\n:param kwargs: Map of values to use in placeholders", "id": "f13360:c1:m1"}
{"signature": "def liste_sites_prelevement(self):", "body": "_sql = \"\"\"<STR_LIT>\"\"\"<EOL>return psql.read_sql(_sql, self.conn)<EOL>", "docstring": "Liste les sites de pr\u00e9l\u00e8vements manuels", "id": "f6428:c0:m12"}
{"signature": "def t_FUNCTION(t):", "body": "t.value = ('<STR_LIT>',)<EOL>return t<EOL>", "docstring": "r\"function", "id": "f17004:m14"}
{"signature": "def disconnect(self, silent=False):", "body": "if not silent:<EOL><INDENT>packet = {\"<STR_LIT:type>\": \"<STR_LIT>\",<EOL>\"<STR_LIT>\": self.ns_name}<EOL>self.socket.send_packet(packet)<EOL><DEDENT>try:<EOL><INDENT>self.socket.remove_namespace(self.ns_name)<EOL><DEDENT>finally:<EOL><INDENT>self.kill_local_jobs()<EOL><DEDENT>", "docstring": "Send a 'disconnect' packet, so that the user knows it has been\n        disconnected (booted actually).  This will trigger an onDisconnect()\n        call on the client side.\n\n        Over here, we will kill all ``spawn``ed processes and remove the\n        namespace from the Socket object.\n\n        :param silent: do not actually send the packet (if they asked for a\n                       disconnect for example), but just kill all jobs spawned\n                       by this Namespace, and remove it from the Socket.", "id": "f11816:c0:m21"}
{"signature": "def do_escape_nl(self, arg):", "body": "if arg.lower() == '<STR_LIT>':<EOL><INDENT>self.escape_nl = False<EOL><DEDENT>else:<EOL><INDENT>self.escape_nl = True<EOL><DEDENT>", "docstring": "Escape newlines in any responses", "id": "f11497:c0:m3"}
{"signature": "def __init__(self, pattern=None):", "body": "self.pattern = pattern<EOL>", "docstring": "Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nn-uiautomationclient-iuiautomationvaluepattern", "id": "f1782:c75:m0"}
{"signature": "@property<EOL><INDENT>def torque(self):<DEDENT>", "body": "return np.array(self.ode_body.getTorque())<EOL>", "docstring": "Current net torque acting on this body (in world coordinates).", "id": "f14887:c1:m17"}
{"signature": "def _is_api_definition(line):", "body": "return line.split('<STR_LIT:U+0020>', <NUM_LIT:1>)[<NUM_LIT:0>] in HTTP_METHODS<EOL>", "docstring": "Is a definition of a Trello endpoint.\n\n>>> _is_api_definition('GET /1/actions/[idAction]')\nTrue\n>>> _is_api_definition('action')\nFalse", "id": "f9786:m1"}
{"signature": "def Pr(nu: float, alpha: float) -> float:", "body": "return nu / alpha<EOL>", "docstring": "Calculate the Prandtl number.\n\n:param nu: [m2/s] fluid kinematic viscosity / momentum diffusivity.\n:param alpha: [m2/s] fluid thermal diffusivity.\n\n:returns: float", "id": "f15861:m1"}
{"signature": "def _assert_tags(self, model, tags=None):", "body": "tags = tags if tags else []<EOL>expected_tags = (self.existing_tags + tags)<EOL>self.assertEqual(sorted(model.tags),<EOL>sorted(expected_tags))<EOL>", "docstring": "Assert given tags and already existing tags are present.", "id": "f10674:c3:m2"}
{"signature": "@property<EOL><INDENT>def big(self):<DEDENT>", "body": "if self.settings['<STR_LIT>']:<EOL><INDENT>s = self.settings<EOL>if s['<STR_LIT>']:<EOL><INDENT>return self.filename<EOL><DEDENT>orig_path = join(s['<STR_LIT>'], self.path, s['<STR_LIT>'])<EOL>check_or_create_dir(orig_path)<EOL>big_path = join(orig_path, self.src_filename)<EOL>if not isfile(big_path):<EOL><INDENT>copy(self.src_path, big_path, symlink=s['<STR_LIT>'],<EOL>rellink=self.settings['<STR_LIT>'])<EOL><DEDENT>return join(s['<STR_LIT>'], self.src_filename)<EOL><DEDENT>", "docstring": "Path to the original image, if ``keep_orig`` is set (relative to the\n        album directory). Copy the file if needed.", "id": "f13464:c0:m4"}
{"signature": "def plot_signal_to_noise_map(<EOL>ccd_data, plot_origin=True, mask=None, extract_array_from_mask=False, zoom_around_mask=False, as_subplot=False,<EOL>units='<STR_LIT>', kpc_per_arcsec=None, figsize=(<NUM_LIT:7>, <NUM_LIT:7>), aspect='<STR_LIT>',<EOL>cmap='<STR_LIT>', norm='<STR_LIT>', norm_min=None, norm_max=None, linthresh=<NUM_LIT>, linscale=<NUM_LIT>,<EOL>cb_ticksize=<NUM_LIT:10>, cb_fraction=<NUM_LIT>, cb_pad=<NUM_LIT>, cb_tick_values=None, cb_tick_labels=None,<EOL>title='<STR_LIT>', titlesize=<NUM_LIT:16>, xlabelsize=<NUM_LIT:16>, ylabelsize=<NUM_LIT:16>, xyticksize=<NUM_LIT:16>,<EOL>mask_pointsize=<NUM_LIT:10>,<EOL>output_path=None, output_format='<STR_LIT>', output_filename='<STR_LIT>'):", "body": "data_plotters.plot_signal_to_noise_map(<EOL>signal_to_noise_map=ccd_data.signal_to_noise_map, plot_origin=plot_origin, mask=mask,<EOL>extract_array_from_mask=extract_array_from_mask, zoom_around_mask=zoom_around_mask, as_subplot=as_subplot,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>title=title, titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>mask_pointsize=mask_pointsize,<EOL>output_path=output_path, output_format=output_format, output_filename=output_filename)<EOL>", "docstring": "Plot the signal-to-noise_map-map of the ccd data.\n\n    Set *autolens.data.array.plotters.array_plotters* for a description of all innput parameters not described below.\n\n    Parameters\n    -----------\n    image : data.CCDData\n        The ccd data, which includes the observed image, noise_map-map, PSF, signal-to-noise_map-map, etc.\n    plot_origin : True\n        If true, the origin of the data's coordinate system is plotted as a 'x'.", "id": "f6000:m5"}
{"signature": "def quoted(parser=any_token):", "body": "quote_char = quote()<EOL>value, _ = many_until(parser, partial(one_of, quote_char))<EOL>return build_string(value)<EOL>", "docstring": "Parses as much as possible until it encounters a matching closing quote.\n\n    By default matches any_token, but can be provided with a more specific parser if required.\n    Returns a string", "id": "f1511:m3"}
{"signature": "def _rollback(self):", "body": "last_pc, last_gas, last_instruction, last_arguments, fee, allocated = self._checkpoint_data<EOL>self._push_arguments(last_arguments)<EOL>self._gas = last_gas<EOL>self._pc = last_pc<EOL>self._allocated = allocated<EOL>self._checkpoint_data = None<EOL>", "docstring": "Revert the stack, gas, pc and memory allocation so it looks like before executing the instruction", "id": "f17019:c16:m28"}
{"signature": "def Newline(loc=None):", "body": "@llrule(loc, lambda parser: [\"<STR_LIT>\"])<EOL>def rule(parser):<EOL><INDENT>result = parser._accept(\"<STR_LIT>\")<EOL>if result is unmatched:<EOL><INDENT>return result<EOL><DEDENT>return []<EOL><DEDENT>return rule<EOL>", "docstring": "A rule that accepts token of kind ``newline`` and returns an empty list.", "id": "f16502:m14"}
{"signature": "def get_arr_int(self, background_threshold=<NUM_LIT>, background_class_id=None):", "body": "if self.input_was[<NUM_LIT:0>] in [\"<STR_LIT:bool>\", \"<STR_LIT:float>\"]:<EOL><INDENT>ia.do_assert(background_class_id is None,<EOL>\"<STR_LIT>\"<EOL>+ \"<STR_LIT>\")<EOL><DEDENT>if background_class_id is None:<EOL><INDENT>background_class_id = <NUM_LIT:0><EOL><DEDENT>channelwise_max_idx = np.argmax(self.arr, axis=<NUM_LIT:2>)<EOL>if self.input_was[<NUM_LIT:0>] in [\"<STR_LIT:bool>\", \"<STR_LIT:float>\"]:<EOL><INDENT>result = <NUM_LIT:1> + channelwise_max_idx<EOL><DEDENT>else:  <EOL><INDENT>result = channelwise_max_idx<EOL><DEDENT>if background_threshold is not None and background_threshold > <NUM_LIT:0>:<EOL><INDENT>probs = np.amax(self.arr, axis=<NUM_LIT:2>)<EOL>result[probs < background_threshold] = background_class_id<EOL><DEDENT>return result.astype(np.int32)<EOL>", "docstring": "Get the segmentation map array as an integer array of shape (H, W).\n\nEach pixel in that array contains an integer value representing the pixel's class.\nIf multiple classes overlap, the one with the highest local float value is picked.\nIf that highest local value is below `background_threshold`, the method instead uses\nthe background class id as the pixel's class value.\nBy default, class id 0 is the background class. This may only be changed if the original\ninput to the segmentation map object was an integer map.\n\nParameters\n----------\nbackground_threshold : float, optional\n    At each pixel, each class-heatmap has a value between 0.0 and 1.0. If none of the\n    class-heatmaps has a value above this threshold, the method uses the background class\n    id instead.\n\nbackground_class_id : None or int, optional\n    Class id to fall back to if no class-heatmap passes the threshold at a spatial\n    location. May only be provided if the original input was an integer mask and in these\n    cases defaults to 0. If the input were float or boolean masks, the background class id\n    may not be set as it is assumed that the background is implicitly defined\n    as 'any spatial location that has zero-like values in all masks'.\n\nReturns\n-------\nresult : (H,W) ndarray\n    Segmentation map array (int32).\n    If the original input consisted of boolean or float masks, then the highest possible\n    class id is ``1+C``, where ``C`` is the number of provided float/boolean masks. The value\n    ``0`` in the integer mask then denotes the background class.", "id": "f16283:c0:m1"}
{"signature": "def get_record(self, fileName, ref_extract_callback=None):", "body": "self.document = parse(fileName)<EOL>article_type = self._get_article_type()<EOL>if article_type not in ['<STR_LIT>',<EOL>'<STR_LIT>',<EOL>'<STR_LIT>']:<EOL><INDENT>return '<STR_LIT>'<EOL><DEDENT>rec = create_record()<EOL>title, subtitle, notes = self._get_title()<EOL>subfields = []<EOL>if subtitle:<EOL><INDENT>subfields.append(('<STR_LIT:b>', subtitle))<EOL><DEDENT>if title:<EOL><INDENT>subfields.append(('<STR_LIT:a>', title))<EOL>record_add_field(rec, '<STR_LIT>', subfields=subfields)<EOL><DEDENT>subjects = self.document.getElementsByTagName('<STR_LIT>')<EOL>subjects = map(xml_to_text, subjects)<EOL>for note_id in notes:<EOL><INDENT>note = self._get_note(note_id)<EOL>if note:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:a>', note)])<EOL><DEDENT><DEDENT>for subject in subjects:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', ind1='<STR_LIT:1>', ind2='<STR_LIT>',<EOL>subfields=[('<STR_LIT:2>', '<STR_LIT>'),<EOL>('<STR_LIT:a>', subject)])<EOL><DEDENT>keywords = self._get_keywords()<EOL>for keyword in keywords:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', ind1='<STR_LIT:1>', subfields=[('<STR_LIT:a>', keyword),<EOL>('<STR_LIT>', '<STR_LIT>')])<EOL><DEDENT>journal, volume, issue, year, date, doi, page,fpage, lpage = self._get_publication_information()<EOL>astronomy_journals = ['<STR_LIT>', '<STR_LIT>']<EOL>if journal in astronomy_journals:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', ind1='<STR_LIT:1>', ind2='<STR_LIT>',<EOL>subfields=[('<STR_LIT:2>', '<STR_LIT>'),<EOL>('<STR_LIT:a>', '<STR_LIT>')])<EOL><DEDENT>if date:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:c>', date),<EOL>('<STR_LIT:t>', '<STR_LIT>')])<EOL><DEDENT>if doi:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', ind1='<STR_LIT>', subfields=[('<STR_LIT:a>', doi),<EOL>('<STR_LIT:2>', '<STR_LIT>')])<EOL><DEDENT>abstract = self._get_abstract()<EOL>abstract = self._format_abstract(abstract)<EOL>if abstract:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:a>', abstract),<EOL>('<STR_LIT>', '<STR_LIT>')])<EOL><DEDENT>license, license_type, license_url = self._get_license()<EOL>subfields = []<EOL>if license:<EOL><INDENT>subfields.append(('<STR_LIT:a>', license))<EOL><DEDENT>if license_url:<EOL><INDENT>subfields.append(('<STR_LIT:u>', license_url))<EOL><DEDENT>if subfields:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=subfields)<EOL><DEDENT>if license_type == '<STR_LIT>':<EOL><INDENT>self._attach_fulltext(rec, doi)<EOL><DEDENT>number_of_pages = self._get_page_count()<EOL>if number_of_pages:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:a>', number_of_pages)])<EOL><DEDENT>c_holder, c_year, c_statement = self._get_copyright()<EOL>if c_holder and c_year:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:d>', c_holder),<EOL>('<STR_LIT:g>', c_year),<EOL>('<STR_LIT:e>', '<STR_LIT>')])<EOL><DEDENT>elif c_statement:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:f>', c_statement),<EOL>('<STR_LIT:e>', '<STR_LIT>')])<EOL><DEDENT>subfields = []<EOL>if journal:<EOL><INDENT>subfields.append(('<STR_LIT:p>', journal))<EOL><DEDENT>if issue:<EOL><INDENT>subfields.append(('<STR_LIT:n>', issue))<EOL><DEDENT>if volume:<EOL><INDENT>subfields.append(('<STR_LIT:v>', volume))<EOL><DEDENT>if fpage and lpage:<EOL><INDENT>subfields.append(('<STR_LIT:c>', '<STR_LIT>' % (fpage,<EOL>lpage)))<EOL><DEDENT>elif page:<EOL><INDENT>subfields.append(('<STR_LIT:c>', page))<EOL><DEDENT>if year:<EOL><INDENT>subfields.append(('<STR_LIT:y>', year))<EOL><DEDENT>record_add_field(rec, '<STR_LIT>', subfields=subfields)<EOL>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:a>', '<STR_LIT>')])<EOL>conference = '<STR_LIT>'<EOL>for tag in self.document.getElementsByTagName('<STR_LIT>'):<EOL><INDENT>conference = xml_to_text(tag)<EOL><DEDENT>if conference:<EOL><INDENT>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:a>', '<STR_LIT>')])<EOL>record_add_field(rec, '<STR_LIT>', subfields=[('<STR_LIT:a>', conference)])<EOL><DEDENT>self._add_references(rec, ref_extract_callback)<EOL>self._add_authors(rec)<EOL>try:<EOL><INDENT>return record_xml_output(rec)<EOL><DEDENT>except UnicodeDecodeError:<EOL><INDENT>message = \"<STR_LIT>\" + doi<EOL>sys.stderr.write(message)<EOL>return \"<STR_LIT>\"<EOL><DEDENT>", "docstring": "Gets the Marc xml of the files in xaml_jp directory\n\n:param fileName: the name of the file to parse.\n:type fileName: string\n:param refextract_callback: callback to be used to extract\n                            unstructured references. It should\n                            return a marcxml formated string\n                            of the reference.\n:type refextract_callback: callable\n\n:returns: a string with the marc xml version of the file.", "id": "f7892:c0:m6"}
{"signature": "def read(self, filename):", "body": "if filename is not None:<EOL><INDENT>self.filename = filename<EOL><DEDENT>with open(self.filename, '<STR_LIT:rb>') as ccp4:<EOL><INDENT>h = self.header = self._read_header(ccp4)<EOL>nentries = h['<STR_LIT>'] * h['<STR_LIT>'] * h['<STR_LIT>']<EOL>datafmt = h['<STR_LIT>'] + str(nentries) + self._data_bintype<EOL>a = np.array(struct.unpack(datafmt, ccp4.read(struct.calcsize(datafmt))))<EOL><DEDENT>self.header['<STR_LIT:filename>'] = self.filename<EOL>order = '<STR_LIT:C>' if h['<STR_LIT>'] == '<STR_LIT:z>' else '<STR_LIT:F>'<EOL>self.array = a.reshape(h['<STR_LIT>'], h['<STR_LIT>'], h['<STR_LIT>'], order=order)<EOL>self.delta = self._delta()<EOL>self.origin = np.zeros(<NUM_LIT:3>)<EOL>self.rank = <NUM_LIT:3><EOL>", "docstring": "Populate the instance from the ccp4 file *filename*.", "id": "f15452:c0:m1"}
{"signature": "def _doBottomUpCompute(self, rfInput, resetSignal):", "body": "<EOL>self._conditionalBreak()<EOL>self._spatialPoolerInput = rfInput.reshape(-<NUM_LIT:1>)<EOL>assert(rfInput.shape[<NUM_LIT:0>] == <NUM_LIT:1>)<EOL>inputVector = numpy.array(rfInput[<NUM_LIT:0>]).astype('<STR_LIT>')<EOL>outputVector = numpy.zeros(self._sfdr.getNumColumns()).astype('<STR_LIT>')<EOL>self._sfdr.compute(inputVector, self.learningMode, outputVector)<EOL>self._spatialPoolerOutput[:] = outputVector[:]<EOL>if self._fpLogSP:<EOL><INDENT>output = self._spatialPoolerOutput.reshape(-<NUM_LIT:1>)<EOL>outputNZ = output.nonzero()[<NUM_LIT:0>]<EOL>outStr = \"<STR_LIT:U+0020>\".join([\"<STR_LIT>\" % int(token) for token in outputNZ])<EOL>print(output.size, outStr, file=self._fpLogSP)<EOL><DEDENT>if self._fpLogSPInput:<EOL><INDENT>output = rfInput.reshape(-<NUM_LIT:1>)<EOL>outputNZ = output.nonzero()[<NUM_LIT:0>]<EOL>outStr = \"<STR_LIT:U+0020>\".join([\"<STR_LIT>\" % int(token) for token in outputNZ])<EOL>print(output.size, outStr, file=self._fpLogSPInput)<EOL><DEDENT>return self._spatialPoolerOutput<EOL>", "docstring": "Do one iteration of inference and/or learning and return the result\n\nParameters:\n--------------------------------------------\nrfInput:      Input vector. Shape is: (1, inputVectorLen).\nresetSignal:  True if reset is asserted", "id": "f17623:c0:m6"}
{"signature": "def _generate_packets(file_h, header, layers=<NUM_LIT:0>):", "body": "hdrp = ctypes.pointer(header)<EOL>while True:<EOL><INDENT>pkt = _read_a_packet(file_h, hdrp, layers)<EOL>if pkt:<EOL><INDENT>yield pkt<EOL><DEDENT>else:<EOL><INDENT>break<EOL><DEDENT><DEDENT>", "docstring": "Read packets one by one from the capture file. Expects the file\nhandle to point to the location immediately after the header (24\nbytes).", "id": "f1076:m6"}
{"signature": "def _clear_mapping(self, doc_type):", "body": "ElasticSearchEngine.set_mappings(self.index_name, doc_type, {})<EOL>", "docstring": "Remove the cached mappings, so that they get loaded from ES next time they are requested", "id": "f9819:c0:m5"}
{"signature": "def initialize(self):", "body": "return<EOL>", "docstring": "Called to initialize sessions, internal objects etc.", "id": "f8139:c0:m2"}
{"signature": "def state():", "body": "return random.choice(get_dictionary('<STR_LIT>')).strip()<EOL>", "docstring": "Return a random US state name.", "id": "f14663:m5"}
{"signature": "def __update_medoids(self):", "body": "medoid_indexes = [-<NUM_LIT:1>] * len(self.__clusters)<EOL>for index in range(len(self.__clusters)):<EOL><INDENT>medoid_index = medoid(self.__pointer_data, self.__clusters[index], metric=self.__metric, data_type=self.__data_type)<EOL>medoid_indexes[index] = medoid_index<EOL><DEDENT>return medoid_indexes<EOL>", "docstring": "!\n        @brief Find medoids of clusters in line with contained objects.\n\n        @return (list) list of medoids for current number of clusters.", "id": "f15591:c0:m7"}
{"signature": "def do_vcs_install(manifest_in, versionfile_source, ipy):", "body": "GITS = [\"<STR_LIT>\"]<EOL>if sys.platform == \"<STR_LIT:win32>\":<EOL><INDENT>GITS = [\"<STR_LIT>\", \"<STR_LIT>\"]<EOL><DEDENT>files = [manifest_in, versionfile_source]<EOL>if ipy:<EOL><INDENT>files.append(ipy)<EOL><DEDENT>try:<EOL><INDENT>me = __file__<EOL>if me.endswith(\"<STR_LIT>\") or me.endswith(\"<STR_LIT>\"):<EOL><INDENT>me = os.path.splitext(me)[<NUM_LIT:0>] + \"<STR_LIT>\"<EOL><DEDENT>versioneer_file = os.path.relpath(me)<EOL><DEDENT>except NameError:<EOL><INDENT>versioneer_file = \"<STR_LIT>\"<EOL><DEDENT>files.append(versioneer_file)<EOL>present = False<EOL>try:<EOL><INDENT>f = open(\"<STR_LIT>\", \"<STR_LIT:r>\")<EOL>for line in f.readlines():<EOL><INDENT>if line.strip().startswith(versionfile_source):<EOL><INDENT>if \"<STR_LIT>\" in line.strip().split()[<NUM_LIT:1>:]:<EOL><INDENT>present = True<EOL><DEDENT><DEDENT><DEDENT>f.close()<EOL><DEDENT>except EnvironmentError:<EOL><INDENT>pass<EOL><DEDENT>if not present:<EOL><INDENT>f = open(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>f.write(\"<STR_LIT>\" % versionfile_source)<EOL>f.close()<EOL>files.append(\"<STR_LIT>\")<EOL><DEDENT>run_command(GITS, [\"<STR_LIT>\", \"<STR_LIT>\"] + files)<EOL>", "docstring": "Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.", "id": "f3620:m7"}
{"signature": "def base_handlers_factory(self):", "body": "tls_handler = StreamTLSHandler(self.settings)<EOL>sasl_handler = StreamSASLHandler(self.settings)<EOL>session_handler = SessionHandler()<EOL>binding_handler = ResourceBindingHandler(self.settings)<EOL>return [tls_handler, sasl_handler, binding_handler, session_handler]<EOL>", "docstring": "Default base client handlers factory.\n\n        Subclasses can provide different behaviour by overriding this.\n\n        :Return: list of handlers", "id": "f15276:c0:m12"}
{"signature": "def warn_if_not_float(X, estimator='<STR_LIT>'):", "body": "if not isinstance(estimator, str):<EOL><INDENT>estimator = estimator.__class__.__name__<EOL><DEDENT>if X.dtype.kind != '<STR_LIT:f>':<EOL><INDENT>warnings.warn(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (estimator, X.dtype))<EOL>return True<EOL><DEDENT>return False<EOL>", "docstring": "Warning utility function to check that data type is floating point.\n\n    Returns True if a warning was raised (i.e. the input is not float) and\n    False otherwise, for easier input validation.", "id": "f4075:m10"}
{"signature": "def __new__(cls, date_value=None):", "body": "if date_value is None:<EOL><INDENT>new_date = BusinessDate(BASE_DATE)<EOL><DEDENT>elif isinstance(date_value, BaseDateFloat):<EOL><INDENT>return super(BusinessDate, cls).__new__(cls, float(date_value))<EOL><DEDENT>elif isinstance(date_value, (BaseDateDatetimeDate, BaseDateTuple)):<EOL><INDENT>return super(BusinessDate, cls).__new__(cls, date_value.year, date_value.month, date_value.day)<EOL><DEDENT>elif isinstance(date_value, (int, float)):<EOL><INDENT>if date_value >= <NUM_LIT>:<EOL><INDENT>new_date = BusinessDate.from_string(str(date_value))<EOL><DEDENT>elif <NUM_LIT:1> < date_value < <NUM_LIT:200> * <NUM_LIT>:<EOL><INDENT>new_date = BusinessDate.from_excel(date_value)<EOL><DEDENT>else:<EOL><INDENT>new_date = BusinessDate.from_ordinal(date_value)<EOL><DEDENT><DEDENT>elif isinstance(date_value, (str, unicode)):<EOL><INDENT>new_date = BusinessDate.from_string(str(date_value))<EOL><DEDENT>elif isinstance(date_value, (date, datetime)):<EOL><INDENT>new_date = BusinessDate.from_date(date_value)<EOL><DEDENT>elif isinstance(date_value, list):<EOL><INDENT>new_date = [BusinessDate(d) for d in date_value]<EOL><DEDENT>else:<EOL><INDENT>raise ValueError(\"<STR_LIT>\" % str(type(date_value)))<EOL><DEDENT>return new_date<EOL>", "docstring": "r\"\"\"\n        fundamental date class\n\n        :param date_value: input value to create BusinessDate instance\n        :type date_value: int, float, string or datetime.date\n        :return: BusinessDate\n\n        creates new BusinessDate either from `int`, `float`, `string`, `datetime.date`\n        therefore the following will create the same\n\n        .. code-block:: python\n\n            BusinessDate(datetime.date(2015, 12, 31))\n            BusinessDate(20151231)\n            BusinessDate(2015-12-31)\n            BusinessDate(31.12.2015)\n            BusinessDate(12/31/2015)\n            BusinessDate(42369)\n            BusinessDate(42369.0)\n            BusinessDate(735963)\n            BusinessDate(735963.0)\n            BusinessDate()\n\n        **caution:** recommended is the use of class methods BusinessDate.from_string, from_date etc.", "id": "f15441:c2:m0"}
{"signature": "def create_dataset(self, name=None, description=None, public=False):", "body": "data = {<EOL>\"<STR_LIT>\": _convert_bool_to_public_value(public)<EOL>}<EOL>if name:<EOL><INDENT>data[\"<STR_LIT:name>\"] = name<EOL><DEDENT>if description:<EOL><INDENT>data[\"<STR_LIT:description>\"] = description<EOL><DEDENT>dataset = {\"<STR_LIT>\": data}<EOL>failure_message = \"<STR_LIT>\"<EOL>result = self._get_success_json(self._post_json(routes.create_dataset(), dataset, failure_message=failure_message))<EOL>return _dataset_from_response_dict(result)<EOL>", "docstring": "Create a new data set.\n\n:param name: name of the dataset\n:type name: str\n:param description: description for the dataset\n:type description: str\n:param public: A boolean indicating whether or not the dataset should be public.\n:type public: bool\n:return: The newly created dataset.\n:rtype: :class:`Dataset`", "id": "f3511:c0:m9"}
{"signature": "def urlunparse(data):", "body": "scheme, netloc, url, params, query, fragment = data<EOL>if params:<EOL><INDENT>url = \"<STR_LIT>\" % (url, params)<EOL><DEDENT>return urlunsplit((scheme, netloc, url, query, fragment))<EOL>", "docstring": "Put a parsed URL back together again.  This may result in a\n    slightly different, but equivalent URL, if the URL that was parsed\n    originally had redundant delimiters, e.g. a ? with an empty query\n    (the draft states that these are equivalent).", "id": "f16401:m5"}
{"signature": "def set_parser(self,parsername):", "body": "self.__parser = self.parsers[parsername]<EOL>", "docstring": "Set parsername as the current parser.", "id": "f15445:c9:m10"}
{"signature": "def _microcanonical_average_moments(moments, alpha):", "body": "ret = dict()<EOL>runs = moments.shape[<NUM_LIT:0>]<EOL>sqrt_n = np.sqrt(runs)<EOL>moments_sample_mean = moments.mean(axis=<NUM_LIT:0>)<EOL>ret['<STR_LIT>'] = moments_sample_mean<EOL>moments_sample_std = moments.std(axis=<NUM_LIT:0>, ddof=<NUM_LIT:1>)<EOL>ret['<STR_LIT>'] = np.empty((<NUM_LIT:5>, <NUM_LIT:2>))<EOL>for k in range(<NUM_LIT:5>):<EOL><INDENT>if moments_sample_std[k]:<EOL><INDENT>old_settings = np.seterr(all='<STR_LIT>')<EOL>ret['<STR_LIT>'][k] = scipy.stats.t.interval(<EOL><NUM_LIT:1> - alpha,<EOL>df=runs - <NUM_LIT:1>,<EOL>loc=moments_sample_mean[k],<EOL>scale=moments_sample_std[k] / sqrt_n<EOL>)<EOL>np.seterr(**old_settings)<EOL><DEDENT>else:<EOL><INDENT>ret['<STR_LIT>'][k] = (<EOL>moments_sample_mean[k] * np.ones(<NUM_LIT:2>)<EOL>)<EOL><DEDENT><DEDENT>return ret<EOL>", "docstring": "Compute the average moments of the cluster size distributions\n\nHelper function for :func:`microcanonical_averages`\n\nParameters\n----------\n\nmoments : 2-D :py:class:`numpy.ndarray` of int\n    ``moments.shape[1] == 5`.\n    Each array ``moments[r, :]`` is the ``moments`` field of the output of\n    :func:`sample_states`:\n    The ``k``-th entry is the ``k``-th raw moment of the (absolute) cluster\n    size distribution.\n\nalpha: float\n    Significance level.\n\nReturns\n-------\n\nret : dict\n    Moment statistics\n\nret['moments'] : 1-D :py:class:`numpy.ndarray` of float, size 5\n    The ``k``-th entry is the average ``k``-th raw moment of the (absolute)\n    cluster size distribution, with ``k`` ranging from ``0`` to ``4``.\n\nret['moments_ci'] : 2-D :py:class:`numpy.ndarray` of float, shape (5,2)\n    ``ret['moments_ci'][k]`` are the lower and upper bounds of the normal\n    confidence interval of the average ``k``-th raw moment of the\n    (absolute) cluster size distribution, with ``k`` ranging from ``0`` to\n    ``4``.\n\nSee Also\n--------\n\nsample_states : computation of moments\n\nmicrocanonical_averages : moment statistics", "id": "f15353:m5"}
{"signature": "def _roster_set(self, item, callback, error_callback):", "body": "stanza = Iq(to_jid = self.server, stanza_type = \"<STR_LIT>\")<EOL>payload = RosterPayload([item])<EOL>stanza.set_payload(payload)<EOL>def success_cb(result_stanza):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>if callback:<EOL><INDENT>callback(item)<EOL><DEDENT><DEDENT>def error_cb(error_stanza):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>if error_callback:<EOL><INDENT>error_callback(error_stanza)<EOL><DEDENT>else:<EOL><INDENT>logger.error(\"<STR_LIT>\".format(item.jid))<EOL><DEDENT><DEDENT>processor = self.stanza_processor<EOL>processor.set_response_handlers(stanza,<EOL>success_cb, error_cb)<EOL>processor.send(stanza)<EOL>", "docstring": "Send a 'roster set' to the server.\n\n        :Parameters:\n            - `item`: the requested change\n        :Types:\n            - `item`: `RosterItem`", "id": "f15245:c6:m12"}
{"signature": "def create_collection(self, name):", "body": "assert self.is_collection<EOL>self._check_write_access()<EOL>collpath = self._getFilePath(name)<EOL>os.mkdir(collpath)<EOL>filepath = self._getFilePath(name, \"<STR_LIT>\")<EOL>f = open(filepath, \"<STR_LIT:w>\")<EOL>f.write(\"<STR_LIT>\")<EOL>f.close()<EOL>commands.add(self.provider.ui, self.provider.repo, filepath)<EOL>", "docstring": "Create a new collection as member of self.\n\n        A dummy member is created, because Mercurial doesn't handle folders.", "id": "f8623:c0:m19"}
{"signature": "def affine_horizontal_flip_matrix(prob=<NUM_LIT:0.5>):", "body": "factor = np.random.uniform(<NUM_LIT:0>, <NUM_LIT:1>)<EOL>if prob >= factor:<EOL><INDENT>filp_matrix = np.array([[ -<NUM_LIT:1.> , <NUM_LIT:0.>, <NUM_LIT:0.> ],[ <NUM_LIT:0.>, <NUM_LIT:1.>, <NUM_LIT:0.> ],[ <NUM_LIT:0.>, <NUM_LIT:0.>, <NUM_LIT:1.> ]])<EOL>return filp_matrix<EOL><DEDENT>else:<EOL><INDENT>filp_matrix = np.array([[ <NUM_LIT:1.> , <NUM_LIT:0.>, <NUM_LIT:0.> ],[ <NUM_LIT:0.>, <NUM_LIT:1.>, <NUM_LIT:0.> ],[ <NUM_LIT:0.>, <NUM_LIT:0.>, <NUM_LIT:1.> ]])<EOL>return filp_matrix<EOL><DEDENT>", "docstring": "Create an affine transformation matrix for image horizontal flipping.\n    NOTE: In OpenCV, x is width and y is height.\n\n    Parameters\n    ----------\n    prob : float\n        Probability to flip the image. 1.0 means always flip.\n\n    Returns\n    -------\n    numpy.array\n        An affine transform matrix.", "id": "f11202:m2"}
{"signature": "def __unicode__(self):", "body": "return self.as_json()<EOL>", "docstring": "Print Allele object as dict object data.", "id": "f10420:c0:m1"}
{"signature": "def main(argv=None):", "body": "if argv is None:<EOL><INDENT>argv = sys.argv<EOL><DEDENT>else:<EOL><INDENT>sys.argv.extend(argv)<EOL><DEDENT>program_name = os.path.basename(sys.argv[<NUM_LIT:0>])<EOL>program_version = \"<STR_LIT>\" % __version__<EOL>program_build_date = str(__updated__)<EOL>program_version_message = '<STR_LIT>' % (program_version, program_build_date)<EOL>program_shortdesc = __import__('<STR_LIT:__main__>').__doc__.split(\"<STR_LIT:\\n>\")[<NUM_LIT:1>]<EOL>program_license =", "docstring": "Parse command line options and dump a datacenter to snapshots and file.", "id": "f822:m10"}
{"signature": "def get_value(self, key):", "body": "try:<EOL><INDENT>return self._values[key]<EOL><DEDENT>except KeyError:<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Retrieves a predicted value.\n\n:param key: A descriptor key for a registered predicted value.\n:type key: str\n:return: The value stored at the provided descriptor key. None if no key is provided.\n:rtype: :class:`PredictedValue`", "id": "f3477:c0:m2"}
{"signature": "def raw_task_data(self):", "body": "return self._raw._asdict()<EOL>", "docstring": "Return a provider-specific representation of task data.\n\n        Returns:\n          string of task data from the provider.", "id": "f6202:c1:m1"}
{"signature": "@tornado.gen.coroutine<EOL><INDENT>def get(self):<DEDENT>", "body": "try:<EOL><INDENT>cluster = self.get_argument_cluster()<EOL>role = self.get_argument_role()<EOL>environ = self.get_argument_environ()<EOL>topology_name = self.get_argument_topology()<EOL>topology_info = self.tracker.getTopologyInfo(topology_name, cluster, role, environ)<EOL>metadata = topology_info[\"<STR_LIT>\"]<EOL>self.write_success_response(metadata)<EOL><DEDENT>except Exception as e:<EOL><INDENT>Log.error(\"<STR_LIT>\")<EOL>Log.debug(traceback.format_exc())<EOL>self.write_error_response(e)<EOL><DEDENT>", "docstring": "get method", "id": "f7348:c0:m1"}
{"signature": "@register.tag<EOL>def performable(parser, token):", "body": "bits = token.split_contents()<EOL>if len(bits) > <NUM_LIT:1>:<EOL><INDENT>raise TemplateSyntaxError(\"<STR_LIT>\" % bits[<NUM_LIT:0>])<EOL><DEDENT>return PerformableNode()<EOL>", "docstring": "Performable template tag.\n\nRenders Javascript code to set-up Performable tracking.  You must\nsupply your Performable API key in the ``PERFORMABLE_API_KEY``\nsetting.", "id": "f14851:m0"}
{"signature": "@register_vcs_handler(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>def git_versions_from_keywords(keywords, tag_prefix, verbose):", "body": "if not keywords:<EOL><INDENT>raise NotThisMethod(\"<STR_LIT>\")<EOL><DEDENT>date = keywords.get(\"<STR_LIT:date>\")<EOL>if date is not None:<EOL><INDENT>date = date.strip().replace(\"<STR_LIT:U+0020>\", \"<STR_LIT:T>\", <NUM_LIT:1>).replace(\"<STR_LIT:U+0020>\", \"<STR_LIT>\", <NUM_LIT:1>)<EOL><DEDENT>refnames = keywords[\"<STR_LIT>\"].strip()<EOL>if refnames.startswith(\"<STR_LIT>\"):<EOL><INDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\")<EOL><DEDENT>raise NotThisMethod(\"<STR_LIT>\")<EOL><DEDENT>refs = set([r.strip() for r in refnames.strip(\"<STR_LIT>\").split(\"<STR_LIT:U+002C>\")])<EOL>TAG = \"<STR_LIT>\"<EOL>tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])<EOL>if not tags:<EOL><INDENT>tags = set([r for r in refs if re.search(r'<STR_LIT>', r)])<EOL>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % \"<STR_LIT:U+002C>\".join(refs - tags))<EOL><DEDENT><DEDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % \"<STR_LIT:U+002C>\".join(sorted(tags)))<EOL><DEDENT>for ref in sorted(tags):<EOL><INDENT>if ref.startswith(tag_prefix):<EOL><INDENT>r = ref[len(tag_prefix):]<EOL>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" % r)<EOL><DEDENT>return {\"<STR_LIT:version>\": r,<EOL>\"<STR_LIT>\": keywords[\"<STR_LIT>\"].strip(),<EOL>\"<STR_LIT>\": False, \"<STR_LIT:error>\": None,<EOL>\"<STR_LIT:date>\": date}<EOL><DEDENT><DEDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\")<EOL><DEDENT>return {\"<STR_LIT:version>\": \"<STR_LIT>\",<EOL>\"<STR_LIT>\": keywords[\"<STR_LIT>\"].strip(),<EOL>\"<STR_LIT>\": False, \"<STR_LIT:error>\": \"<STR_LIT>\", \"<STR_LIT:date>\": None}<EOL>", "docstring": "Get version information from git keywords.", "id": "f6851:m6"}
{"signature": "def hex_to_rgb(hex):", "body": "hex = hex.lstrip(\"<STR_LIT:#>\")<EOL>if len(hex) < <NUM_LIT:6>:<EOL><INDENT>hex += hex[-<NUM_LIT:1>] * (<NUM_LIT:6> - len(hex))<EOL><DEDENT>r, g, b = hex[<NUM_LIT:0>:<NUM_LIT:2>], hex[<NUM_LIT:2>:<NUM_LIT:4>], hex[<NUM_LIT:4>:]<EOL>r, g, b = [int(n, <NUM_LIT:16>) / <NUM_LIT> for n in (r, g, b)]<EOL>return r, g, b<EOL>", "docstring": "Returns RGB values for a hex color string.", "id": "f11564:m1"}
{"signature": "def draw_grid(images, rows=None, cols=None):", "body": "nb_images = len(images)<EOL>do_assert(nb_images > <NUM_LIT:0>)<EOL>if is_np_array(images):<EOL><INDENT>do_assert(images.ndim == <NUM_LIT:4>)<EOL><DEDENT>else:<EOL><INDENT>do_assert(is_iterable(images) and is_np_array(images[<NUM_LIT:0>]) and images[<NUM_LIT:0>].ndim == <NUM_LIT:3>)<EOL>dts = [image.dtype.name for image in images]<EOL>nb_dtypes = len(set(dts))<EOL>do_assert(nb_dtypes == <NUM_LIT:1>, (\"<STR_LIT>\"<EOL>+ \"<STR_LIT>\") % (nb_dtypes, \"<STR_LIT:U+002CU+0020>\".join(dts)))<EOL><DEDENT>cell_height = max([image.shape[<NUM_LIT:0>] for image in images])<EOL>cell_width = max([image.shape[<NUM_LIT:1>] for image in images])<EOL>channels = set([image.shape[<NUM_LIT:2>] for image in images])<EOL>do_assert(<EOL>len(channels) == <NUM_LIT:1>,<EOL>\"<STR_LIT>\"<EOL>+ \"<STR_LIT>\" % (str(channels), len(channels))<EOL>)<EOL>nb_channels = list(channels)[<NUM_LIT:0>]<EOL>if rows is None and cols is None:<EOL><INDENT>rows = cols = int(math.ceil(math.sqrt(nb_images)))<EOL><DEDENT>elif rows is not None:<EOL><INDENT>cols = int(math.ceil(nb_images / rows))<EOL><DEDENT>elif cols is not None:<EOL><INDENT>rows = int(math.ceil(nb_images / cols))<EOL><DEDENT>do_assert(rows * cols >= nb_images)<EOL>width = cell_width * cols<EOL>height = cell_height * rows<EOL>dt = images.dtype if is_np_array(images) else images[<NUM_LIT:0>].dtype<EOL>grid = np.zeros((height, width, nb_channels), dtype=dt)<EOL>cell_idx = <NUM_LIT:0><EOL>for row_idx in sm.xrange(rows):<EOL><INDENT>for col_idx in sm.xrange(cols):<EOL><INDENT>if cell_idx < nb_images:<EOL><INDENT>image = images[cell_idx]<EOL>cell_y1 = cell_height * row_idx<EOL>cell_y2 = cell_y1 + image.shape[<NUM_LIT:0>]<EOL>cell_x1 = cell_width * col_idx<EOL>cell_x2 = cell_x1 + image.shape[<NUM_LIT:1>]<EOL>grid[cell_y1:cell_y2, cell_x1:cell_x2, :] = image<EOL><DEDENT>cell_idx += <NUM_LIT:1><EOL><DEDENT><DEDENT>return grid<EOL>", "docstring": "Converts multiple input images into a single image showing them in a grid.\n\ndtype support::\n\n    * ``uint8``: yes; fully tested\n    * ``uint16``: yes; fully tested\n    * ``uint32``: yes; fully tested\n    * ``uint64``: yes; fully tested\n    * ``int8``: yes; fully tested\n    * ``int16``: yes; fully tested\n    * ``int32``: yes; fully tested\n    * ``int64``: yes; fully tested\n    * ``float16``: yes; fully tested\n    * ``float32``: yes; fully tested\n    * ``float64``: yes; fully tested\n    * ``float128``: yes; fully tested\n    * ``bool``: yes; fully tested\n\nParameters\n----------\nimages : (N,H,W,3) ndarray or iterable of (H,W,3) array\n    The input images to convert to a grid.\n\nrows : None or int, optional\n    The number of rows to show in the grid.\n    If None, it will be automatically derived.\n\ncols : None or int, optional\n    The number of cols to show in the grid.\n    If None, it will be automatically derived.\n\nReturns\n-------\ngrid : (H',W',3) ndarray\n    Image of the generated grid.", "id": "f16269:m41"}
{"signature": "def __init__(self, protocol=None, manual_validation=False, extra_trust_roots=None):", "body": "if not isinstance(manual_validation, bool):<EOL><INDENT>raise TypeError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>type_name(manual_validation)<EOL>))<EOL><DEDENT>self._manual_validation = manual_validation<EOL>if protocol is None:<EOL><INDENT>protocol = set(['<STR_LIT>', '<STR_LIT>', '<STR_LIT>'])<EOL><DEDENT>if isinstance(protocol, str_cls):<EOL><INDENT>protocol = set([protocol])<EOL><DEDENT>elif not isinstance(protocol, set):<EOL><INDENT>raise TypeError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>type_name(protocol)<EOL>))<EOL><DEDENT>valid_protocols = set(['<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'])<EOL>unsupported_protocols = protocol - valid_protocols<EOL>if unsupported_protocols:<EOL><INDENT>raise ValueError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>repr(unsupported_protocols)<EOL>))<EOL><DEDENT>self._protocols = protocol<EOL>self._extra_trust_roots = []<EOL>if extra_trust_roots:<EOL><INDENT>for extra_trust_root in extra_trust_roots:<EOL><INDENT>if isinstance(extra_trust_root, Certificate):<EOL><INDENT>extra_trust_root = extra_trust_root.asn1<EOL><DEDENT>elif isinstance(extra_trust_root, byte_cls):<EOL><INDENT>extra_trust_root = parse_certificate(extra_trust_root)<EOL><DEDENT>elif isinstance(extra_trust_root, str_cls):<EOL><INDENT>with open(extra_trust_root, '<STR_LIT:rb>') as f:<EOL><INDENT>extra_trust_root = parse_certificate(f.read())<EOL><DEDENT><DEDENT>elif not isinstance(extra_trust_root, x509.Certificate):<EOL><INDENT>raise TypeError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>type_name(extra_trust_root)<EOL>))<EOL><DEDENT>self._extra_trust_roots.append(extra_trust_root)<EOL><DEDENT><DEDENT>ssl_ctx = None<EOL>try:<EOL><INDENT>if libcrypto_version_info < (<NUM_LIT:1>, <NUM_LIT:1>):<EOL><INDENT>method = libssl.SSLv23_method()<EOL><DEDENT>else:<EOL><INDENT>method = libssl.TLS_method()<EOL><DEDENT>ssl_ctx = libssl.SSL_CTX_new(method)<EOL>if is_null(ssl_ctx):<EOL><INDENT>handle_openssl_error(<NUM_LIT:0>)<EOL><DEDENT>self._ssl_ctx = ssl_ctx<EOL>libssl.SSL_CTX_set_timeout(ssl_ctx, <NUM_LIT>)<EOL>libssl.SSL_CTX_ctrl(<EOL>ssl_ctx,<EOL>LibsslConst.SSL_CTRL_SET_SESS_CACHE_MODE,<EOL>LibsslConst.SSL_SESS_CACHE_CLIENT,<EOL>null()<EOL>)<EOL>if sys.platform in set(['<STR_LIT:win32>', '<STR_LIT>']):<EOL><INDENT>trust_list_path = _trust_list_path<EOL>if trust_list_path is None:<EOL><INDENT>trust_list_path = get_path()<EOL><DEDENT>if sys.platform == '<STR_LIT:win32>':<EOL><INDENT>path_encoding = '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>path_encoding = '<STR_LIT:utf-8>'<EOL><DEDENT>result = libssl.SSL_CTX_load_verify_locations(<EOL>ssl_ctx,<EOL>trust_list_path.encode(path_encoding),<EOL>null()<EOL>)<EOL><DEDENT>else:<EOL><INDENT>result = libssl.SSL_CTX_set_default_verify_paths(ssl_ctx)<EOL><DEDENT>handle_openssl_error(result)<EOL>verify_mode = LibsslConst.SSL_VERIFY_NONE if manual_validation else LibsslConst.SSL_VERIFY_PEER<EOL>libssl.SSL_CTX_set_verify(ssl_ctx, verify_mode, null())<EOL>result = libssl.SSL_CTX_set_cipher_list(<EOL>ssl_ctx,<EOL>(<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>b'<STR_LIT>'<EOL>)<EOL>)<EOL>handle_openssl_error(result)<EOL>disabled_protocols = set(['<STR_LIT>'])<EOL>disabled_protocols |= (valid_protocols - self._protocols)<EOL>for disabled_protocol in disabled_protocols:<EOL><INDENT>libssl.SSL_CTX_ctrl(<EOL>ssl_ctx,<EOL>LibsslConst.SSL_CTRL_OPTIONS,<EOL>_PROTOCOL_MAP[disabled_protocol],<EOL>null()<EOL>)<EOL><DEDENT>if self._extra_trust_roots:<EOL><INDENT>x509_store = libssl.SSL_CTX_get_cert_store(ssl_ctx)<EOL>for cert in self._extra_trust_roots:<EOL><INDENT>oscrypto_cert = load_certificate(cert)<EOL>result = libssl.X509_STORE_add_cert(<EOL>x509_store,<EOL>oscrypto_cert.x509<EOL>)<EOL>handle_openssl_error(result)<EOL><DEDENT><DEDENT><DEDENT>except (Exception):<EOL><INDENT>if ssl_ctx:<EOL><INDENT>libssl.SSL_CTX_free(ssl_ctx)<EOL><DEDENT>self._ssl_ctx = None<EOL>raise<EOL><DEDENT>", "docstring": ":param protocol:\n    A unicode string or set of unicode strings representing allowable\n    protocols to negotiate with the server:\n\n     - \"TLSv1.2\"\n     - \"TLSv1.1\"\n     - \"TLSv1\"\n     - \"SSLv3\"\n\n    Default is: {\"TLSv1\", \"TLSv1.1\", \"TLSv1.2\"}\n\n:param manual_validation:\n    If certificate and certificate path validation should be skipped\n    and left to the developer to implement\n\n:param extra_trust_roots:\n    A list containing one or more certificates to be treated as trust\n    roots, in one of the following formats:\n     - A byte string of the DER encoded certificate\n     - A unicode string of the certificate filename\n     - An asn1crypto.x509.Certificate object\n     - An oscrypto.asymmetric.Certificate object\n\n:raises:\n    ValueError - when any of the parameters contain an invalid value\n    TypeError - when any of the parameters are of the wrong type\n    OSError - when an error is returned by the OS crypto library", "id": "f9532:c0:m0"}
{"signature": "def walk(self, *types):", "body": "requested = types if len(types) > <NUM_LIT:0> else [SuiteFile, ResourceFile, SuiteFolder, Testcase, Keyword]<EOL>for thing in self.robot_files:<EOL><INDENT>if thing.__class__ in requested:<EOL><INDENT>yield thing<EOL><DEDENT>if isinstance(thing, SuiteFolder):<EOL><INDENT>for child in thing.walk():<EOL><INDENT>if child.__class__ in requested:<EOL><INDENT>yield child<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>for child in thing.walk(*types):<EOL><INDENT>yield child<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Iterator which visits all suites and suite files,\nyielding test cases and keywords", "id": "f11834:c0:m1"}
{"signature": "@property<EOL><INDENT>def potential(self):<DEDENT>", "body": "if self._potential is not None and self._potential:<EOL><INDENT>return True<EOL><DEDENT>else:<EOL><INDENT>return False<EOL><DEDENT>", "docstring": "Getter for 'potential' property\n\nReturns:\n        bool: potential is required?", "id": "f1181:c0:m7"}
{"signature": "@staticmethod<EOL><INDENT>def _bind_args(sig, param_matchers, args, kwargs):<DEDENT>", "body": "<EOL>bound = sig.bind(*args, **kwargs)<EOL>if not all(param_matcher(bound.arguments[param_name])<EOL>for param_name, param_matcher in param_matchers):<EOL><INDENT>raise TypeError<EOL><DEDENT>return bound<EOL>", "docstring": "Attempt to bind the args to the type signature. First try to just bind\nto the signature, then ensure that all arguments match the parameter\ntypes.", "id": "f8679:c1:m1"}
{"signature": "def decimate_max(self, a, maxpoints, **kwargs):", "body": "return self._decimate(numkit.timeseries.max_histogrammed_function, a, maxpoints, **kwargs)<EOL>", "docstring": "Return data *a* max-decimated on *maxpoints*.\n\n        Histograms each column into *maxpoints* bins and calculates\n        the maximum in each bin as the decimated data, using\n        :func:`numkit.timeseries.max_histogrammed_function`. The coarse grained\n        time in the first column contains the centers of the histogram\n        time.\n\n        If *a* contains <= *maxpoints* then *a* is simply returned;\n        otherwise a new array of the same dimensions but with a\n        reduced number of  *maxpoints* points is returned.\n\n        .. Note::\n\n           Assumes that the first column is time.", "id": "f6858:c0:m24"}
{"signature": "def get_languages() -> set:", "body": "try:<EOL><INDENT>languages = cache['<STR_LIT>']<EOL><DEDENT>except KeyError:<EOL><INDENT>languages = LanguageTool._get_languages()<EOL>cache['<STR_LIT>'] = languages<EOL><DEDENT>return languages<EOL>", "docstring": "Get supported languages.", "id": "f4271:m7"}
{"signature": "def jhk_to_sdssz(jmag,hmag,kmag):", "body": "return convert_constants(jmag,hmag,kmag,<EOL>SDSSZ_JHK,<EOL>SDSSZ_JH, SDSSZ_JK, SDSSZ_HK,<EOL>SDSSZ_J, SDSSZ_H, SDSSZ_K)<EOL>", "docstring": "Converts given J, H, Ks mags to an SDSS z magnitude value.\n\n    Parameters\n    ----------\n\n    jmag,hmag,kmag : float\n        2MASS J, H, Ks mags of the object.\n\n    Returns\n    -------\n\n    float\n        The converted SDSS z band magnitude.", "id": "f14755:m9"}
{"signature": "def execute_no_wait(self, cmd, walltime, envs={}):", "body": "current_env = copy.deepcopy(self._envs)<EOL>current_env.update(envs)<EOL>try:<EOL><INDENT>proc = subprocess.Popen(<EOL>cmd,<EOL>stdout=subprocess.PIPE,<EOL>stderr=subprocess.PIPE,<EOL>cwd=self.userhome,<EOL>env=current_env,<EOL>shell=True,<EOL>preexec_fn=os.setpgrp<EOL>)<EOL>pid = proc.pid<EOL><DEDENT>except Exception as e:<EOL><INDENT>logger.warn(\"<STR_LIT>\", (cmd, e))<EOL>raise<EOL><DEDENT>return pid, proc<EOL>", "docstring": "Synchronously execute a commandline string on the shell.\n\n        Args:\n            - cmd (string) : Commandline string to execute\n            - walltime (int) : walltime in seconds, this is not really used now.\n\n        Returns a tuple containing:\n\n           - pid : process id\n           - proc : a subprocess.Popen object\n\n        Raises:\n         None.", "id": "f2590:c0:m2"}
{"signature": "def gen_feats(self, p_set):", "body": "if self._initialized!=True:<EOL><INDENT>error_message = \"<STR_LIT>\"<EOL>log.exception(error_message)<EOL>raise util_functions.InputError(p_set, error_message)<EOL><DEDENT>textual_features = []<EOL>for i in range(<NUM_LIT:0>,len(p_set._essay_sets)):<EOL><INDENT>textual_features.append(self._extractors[i].gen_feats(p_set._essay_sets[i]))<EOL><DEDENT>textual_matrix = numpy.concatenate(textual_features, axis=<NUM_LIT:1>)<EOL>predictor_matrix = numpy.array(p_set._numeric_features)<EOL>print(textual_matrix.shape)<EOL>print(predictor_matrix.shape)<EOL>overall_matrix = numpy.concatenate((textual_matrix, predictor_matrix), axis=<NUM_LIT:1>)<EOL>return overall_matrix.copy()<EOL>", "docstring": "Generates features based on an iput p_set\np_set - PredictorSet", "id": "f7969:c0:m2"}
{"signature": "def _encode_datetime(self, dt):", "body": "if dt.tzinfo is None:<EOL><INDENT>dt = dt.replace(tzinfo=datetime.timezone.utc)<EOL><DEDENT>dt = dt.astimezone(datetime.timezone.utc)<EOL>return dt.strftime('<STR_LIT>')<EOL>", "docstring": "Encode a datetime in the format '%Y-%m-%dT%H:%M:%SZ'.\n\n        The datetime can be naieve (doesn't have timezone info) or aware\n        (it does have a tzinfo attribute set). Regardless, the datetime\n        is transformed into UTC.", "id": "f4219:c0:m1"}
{"signature": "def add_droplets(self, droplet):", "body": "droplets = droplet<EOL>if not isinstance(droplets, list):<EOL><INDENT>droplets = [droplet]<EOL><DEDENT>resources = self.__extract_resources_from_droplets(droplets)<EOL>if len(resources) > <NUM_LIT:0>:<EOL><INDENT>return self.__add_resources(resources)<EOL><DEDENT>return False<EOL>", "docstring": "Add the Tag to a Droplet.\n\nAttributes accepted at creation time:\n    droplet: array of string or array of int, or array of Droplets.", "id": "f1481:c0:m9"}
{"signature": "@classmethod<EOL><INDENT>def from_xml(cls, element):<DEDENT>", "body": "if element.tag != ITEM_TAG:<EOL><INDENT>raise ValueError(\"<STR_LIT>\".format(element))<EOL><DEDENT>try:<EOL><INDENT>jid = JID(element.get(\"<STR_LIT>\"))<EOL><DEDENT>except ValueError:<EOL><INDENT>raise BadRequestProtocolError(\"<STR_LIT>\")<EOL><DEDENT>subscription = element.get(\"<STR_LIT>\")<EOL>ask = element.get(\"<STR_LIT>\")<EOL>name = element.get(\"<STR_LIT:name>\")<EOL>duplicate_group = False<EOL>groups = set()<EOL>for child in element:<EOL><INDENT>if child.tag != GROUP_TAG:<EOL><INDENT>continue<EOL><DEDENT>group = child.text<EOL>if group is None:<EOL><INDENT>group = \"<STR_LIT>\"<EOL><DEDENT>if group in groups:<EOL><INDENT>duplicate_group = True<EOL><DEDENT>else:<EOL><INDENT>groups.add(group)<EOL><DEDENT><DEDENT>approved = element.get(\"<STR_LIT>\")<EOL>if approved == \"<STR_LIT:true>\":<EOL><INDENT>approved = True<EOL><DEDENT>elif approved in (\"<STR_LIT:false>\", None):<EOL><INDENT>approved = False<EOL><DEDENT>else:<EOL><INDENT>logger.debug(\"<STR_LIT>\"<EOL>\"<STR_LIT>\".format(approved))<EOL>approved = False<EOL><DEDENT>result = cls(jid, name, groups, subscription, ask, approved)<EOL>result._duplicate_group = duplicate_group<EOL>return result<EOL>", "docstring": "Make a RosterItem from an XML element.\n\n        :Parameters:\n            - `element`: the XML element\n        :Types:\n            - `element`: :etree:`ElementTree.Element`\n\n        :return: a freshly created roster item\n        :returntype: `cls`", "id": "f15245:c3:m1"}
{"signature": "def post_update_data(self):", "body": "if self.data is not None:<EOL><INDENT>self.data['<STR_LIT>'] = self.busca_errores_data()<EOL><DEDENT>", "docstring": "Definici\u00f3n opcional para analizar la informaci\u00f3n descargada en busca de errores,\nque quedan almacenados en `self.data['errores']`.", "id": "f14221:c1:m3"}
{"signature": "def choose_paths(self, paths):", "body": "return [path for path in paths if self._chooses_path(path)]<EOL>", "docstring": ":return: the paths that are chosen by :meth:`chooses_path`\n        :rtype: list", "id": "f507:c0:m7"}
{"signature": "def load_pex(path_to_pex, include_deps=True):", "body": "abs_path_to_pex = os.path.abspath(path_to_pex)<EOL>Log.debug(\"<STR_LIT>\" % abs_path_to_pex)<EOL>if abs_path_to_pex not in sys.path:<EOL><INDENT>sys.path.insert(<NUM_LIT:0>, os.path.dirname(abs_path_to_pex))<EOL><DEDENT>if include_deps:<EOL><INDENT>for dep in _get_deps_list(abs_path_to_pex):<EOL><INDENT>to_join = os.path.join(os.path.dirname(abs_path_to_pex), dep)<EOL>if to_join not in sys.path:<EOL><INDENT>Log.debug(\"<STR_LIT>\" % dep)<EOL>sys.path.insert(<NUM_LIT:0>, to_join)<EOL><DEDENT><DEDENT><DEDENT>Log.debug(\"<STR_LIT>\" % str(sys.path))<EOL>", "docstring": "Loads pex file and its dependencies to the current python path", "id": "f7498:m1"}
{"signature": "def notifyAll(self):", "body": "self.notify(len(self.__waiters))<EOL>", "docstring": "Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.", "id": "f16452:c1:m9"}
{"signature": "def clear_history(self):", "body": "self.success_history.clear()<EOL>self.Q_history.clear()<EOL>", "docstring": "Clears all histories that are used for statistics", "id": "f1277:c0:m3"}
{"signature": "def filter(self, filter_function):", "body": "from heronpy.streamlet.impl.filterbolt import FilterStreamlet<EOL>filter_streamlet = FilterStreamlet(filter_function, self)<EOL>self._add_child(filter_streamlet)<EOL>return filter_streamlet<EOL>", "docstring": "Return a new Streamlet containing only the elements that satisfy filter_function", "id": "f7194:c0:m7"}
{"signature": "def set_store(self, store):", "body": "self.store = store<EOL>", "docstring": "Set the Storage for the credential.\n\n        Args:\n            store: Storage, an implementation of Storage object.\n                   This is needed to store the latest access_token if it\n                   has expired and been refreshed. This implementation uses\n                   locking to check for updates before updating the\n                   access_token.", "id": "f2472:c16:m10"}
{"signature": "def row_in_grid(self, row):", "body": "return self._walk.row_in_grid(row)<EOL>", "docstring": "The a RowInGrid for the row with position information.\n\n        :return: a row in the grid\n        :rtype: RowInGrid", "id": "f553:c5:m5"}
{"signature": "@validate_response([b\"<STR_LIT>\"])<EOL><INDENT>async def load_project(self, project_path):<DEDENT>", "body": "cmd = \"<STR_LIT>\" % project_path<EOL>return await asyncio.wait_for(<EOL>self._protocol.send_command(cmd), timeout=self._timeout<EOL>)<EOL>", "docstring": "Load a project.\n\n        :param project_path: Path to project you want to load.", "id": "f8806:c0:m19"}
{"signature": "def __init__(self, tz=None):", "body": "if tz is None: tz = _get_rc_timezone()<EOL>self.tz = tz<EOL>", "docstring": "*tz* is a :class:`tzinfo` instance.", "id": "f17189:c5:m0"}
{"signature": "def resolve_blocks(template, context):", "body": "try:<EOL><INDENT>blocks = context.render_context[BLOCK_CONTEXT_KEY]<EOL><DEDENT>except KeyError:<EOL><INDENT>blocks = context.render_context[BLOCK_CONTEXT_KEY] = BlockContext()<EOL><DEDENT>if isinstance(template, six.string_types):<EOL><INDENT>template = get_template(template)<EOL><DEDENT>template = getattr(template, '<STR_LIT>', template)<EOL>local_blocks = {<EOL>block.name: block<EOL>for block in template.nodelist.get_nodes_by_type(BlockNode)<EOL>}<EOL>blocks.add_blocks(local_blocks)<EOL>extends = template.nodelist.get_nodes_by_type(ExtendsNode)<EOL>if extends:<EOL><INDENT>extends_node = extends[<NUM_LIT:0>]<EOL>parent_template = extends_node.get_parent(context)<EOL>resolve_blocks(parent_template, context)<EOL><DEDENT>return blocks<EOL>", "docstring": "Return a BlockContext instance of all the {% block %} tags in the template.\n\nIf template is a string, it will be resolved through get_template", "id": "f5083:m0"}
{"signature": "def convert_concat(params, w_name, scope_name, inputs, layers, weights, names):", "body": "print('<STR_LIT>')<EOL>concat_nodes = [layers[i] for i in inputs]<EOL>if len(concat_nodes) == <NUM_LIT:1>:<EOL><INDENT>layers[scope_name] = concat_nodes[<NUM_LIT:0>]<EOL>return<EOL><DEDENT>if names == '<STR_LIT>':<EOL><INDENT>tf_name = '<STR_LIT>' + random_string(<NUM_LIT:5>)<EOL><DEDENT>elif names == '<STR_LIT>':<EOL><INDENT>tf_name = w_name<EOL><DEDENT>else:<EOL><INDENT>tf_name = w_name + str(random.random())<EOL><DEDENT>cat = keras.layers.Concatenate(name=tf_name, axis=params['<STR_LIT>'])<EOL>layers[scope_name] = cat(concat_nodes)<EOL>", "docstring": "Convert concatenation.\n\nArgs:\n    params: dictionary with layer parameters\n    w_name: name prefix in state_dict\n    scope_name: pytorch scope name\n    inputs: pytorch node inputs\n    layers: dictionary with keras tensors\n    weights: pytorch state_dict\n    names: use short names for keras layers", "id": "f5039:m2"}
{"signature": "def get_output(self):", "body": "if self.process.poll() is not None:<EOL><INDENT>self.close()<EOL>yield None, None<EOL><DEDENT>while not (self.stdout_queue.empty() and self.stderr_queue.empty()):<EOL><INDENT>if not self.stdout_queue.empty():<EOL><INDENT>line = self.stdout_queue.get().decode('<STR_LIT:utf-8>')<EOL>yield line, None<EOL><DEDENT>if not self.stderr_queue.empty():<EOL><INDENT>line = self.stderr_queue.get().decode('<STR_LIT:utf-8>')<EOL>yield None, line<EOL><DEDENT><DEDENT>", "docstring": ":yield: stdout_line, stderr_line, running\n\nGenerator that outputs lines captured from stdout and stderr\n\nThese can be consumed to output on a widget in an IDE", "id": "f11493:c2:m6"}
{"signature": "def parse(filename):", "body": "for event, elt in et.iterparse(filename, events= ('<STR_LIT:start>', '<STR_LIT:end>', '<STR_LIT>', '<STR_LIT>'), huge_tree=True):<EOL><INDENT>if event == '<STR_LIT:start>':<EOL><INDENT>obj = _elt2obj(elt)<EOL>obj['<STR_LIT:type>'] = ENTER<EOL>yield obj<EOL>if elt.text:<EOL><INDENT>yield {'<STR_LIT:type>': TEXT, '<STR_LIT:text>': elt.text}<EOL><DEDENT><DEDENT>elif event == '<STR_LIT:end>':<EOL><INDENT>yield {'<STR_LIT:type>': EXIT}<EOL>if elt.tail:<EOL><INDENT>yield {'<STR_LIT:type>': TEXT, '<STR_LIT:text>': elt.tail}<EOL><DEDENT>elt.clear()<EOL><DEDENT>elif event == '<STR_LIT>':<EOL><INDENT>yield {'<STR_LIT:type>': COMMENT, '<STR_LIT:text>': elt.text}<EOL><DEDENT>elif event == '<STR_LIT>':<EOL><INDENT>yield {'<STR_LIT:type>': PI, '<STR_LIT:text>': elt.text}<EOL><DEDENT>else:<EOL><INDENT>assert False, (event, elt)<EOL><DEDENT><DEDENT>", "docstring": "Parses file content into events stream", "id": "f15052:m4"}
{"signature": "def _quoteAndEscape(string):", "body": "assert type(string) in (str,)<EOL>return pprint.pformat(string)<EOL>", "docstring": "string:   input string (ascii or unicode)\n\nReturns:  a quoted string with characters that are represented in python via\n          escape sequences converted to those escape sequences", "id": "f17597:m7"}
{"signature": "def __init__(self):", "body": "curses.setupterm()<EOL>", "docstring": "Initialize curses.", "id": "f3874:c0:m0"}
{"signature": "def contains(self, other):", "body": "if isinstance(other, tuple):<EOL><INDENT>x, y = other<EOL><DEDENT>else:<EOL><INDENT>x, y = other.x, other.y<EOL><DEDENT>return self.x1 <= x <= self.x2 and self.y1 <= y <= self.y2<EOL>", "docstring": "Estimate whether the bounding box contains a point.\n\nParameters\n----------\nother : tuple of number or imgaug.Keypoint\n    Point to check for.\n\nReturns\n-------\nbool\n    True if the point is contained in the bounding box, False otherwise.", "id": "f16277:c0:m10"}
{"signature": "@staticmethod<EOL><INDENT>def _generate(num_particles, D, box, rs):<DEDENT>", "body": "X0 = rs.rand(num_particles) * (box.x2 - box.x1) + box.x1<EOL>Y0 = rs.rand(num_particles) * (box.y2 - box.y1) + box.y1<EOL>Z0 = rs.rand(num_particles) * (box.z2 - box.z1) + box.z1<EOL>return [Particle(D=D, x0=x0, y0=y0, z0=z0)<EOL>for x0, y0, z0 in zip(X0, Y0, Z0)]<EOL>", "docstring": "Generate a list of `Particle` objects.", "id": "f15419:c2:m0"}
{"signature": "def _make_file_dict(self, f):", "body": "if isinstance(f, dict):<EOL><INDENT>file_obj = f['<STR_LIT:file>']<EOL>if '<STR_LIT:filename>' in f:<EOL><INDENT>file_name = f['<STR_LIT:filename>']<EOL><DEDENT>else:<EOL><INDENT>file_name = file_obj.name<EOL><DEDENT><DEDENT>else:<EOL><INDENT>file_obj = f<EOL>file_name = f.name<EOL><DEDENT>b64_data = base64.b64encode(file_obj.read())<EOL>return {<EOL>'<STR_LIT:id>': file_name,<EOL>'<STR_LIT:data>': b64_data.decode() if six.PY3 else b64_data,<EOL>}<EOL>", "docstring": "Make a dictionary with filename and base64 file data", "id": "f13929:c0:m24"}
{"signature": "def smkdirs(dpath, mode=<NUM_LIT>):", "body": "if not os.path.exists(dpath):<EOL><INDENT>os.makedirs(dpath, mode=mode)<EOL><DEDENT>", "docstring": "Safely make a full directory path if it doesn't exist.\n\n    Parameters\n    ----------\n    dpath : str\n        Path of directory/directories to create\n\n    mode : int [default=0777]\n        Permissions for the new directories\n\n    See also\n    --------\n    os.makedirs", "id": "f11232:m2"}
{"signature": "def set_formatter(self, formatter='<STR_LIT>', handlers=None):", "body": "for h in self.get_handlers(handlers):<EOL><INDENT>h.setFormatter(logging.Formatter(formatters[formatter]))<EOL><DEDENT>", "docstring": "Set the text format of messages to one of the pre-determined forms,\none of ['quiet', 'minimal', 'standard', 'verbose']", "id": "f5783:c0:m4"}
{"signature": "def set_mode(self,mode):", "body": "if mode < <NUM_LIT:0> or mode > <NUM_LIT:3>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>self._device.mode(mode)<EOL>", "docstring": "Set SPI mode which controls clock polarity and phase.  Should be a\n        numeric value 0, 1, 2, or 3.  See wikipedia page for details on meaning:\n        http://en.wikipedia.org/wiki/Serial_Peripheral_Interface_Bus", "id": "f8003:c1:m2"}
{"signature": "@classmethod<EOL><INDENT>def consumeArgs(cls):<DEDENT>", "body": "return cls._processArgs()[<NUM_LIT:1>]<EOL>", "docstring": "Consumes the test arguments and returns the remaining arguments meant\n        for unittest.man", "id": "f17408:c6:m2"}
{"signature": "def getphraselist(self):", "body": "plist = []<EOL>while self.pos < len(self.field):<EOL><INDENT>if self.field[self.pos] in self.LWS:<EOL><INDENT>self.pos += <NUM_LIT:1><EOL><DEDENT>elif self.field[self.pos] == '<STR_LIT:\">':<EOL><INDENT>plist.append(self.getquote())<EOL><DEDENT>elif self.field[self.pos] == '<STR_LIT:(>':<EOL><INDENT>self.commentlist.append(self.getcomment())<EOL><DEDENT>elif self.field[self.pos] in self.phraseends:<EOL><INDENT>break<EOL><DEDENT>else:<EOL><INDENT>plist.append(self.getatom(self.phraseends))<EOL><DEDENT><DEDENT>return plist<EOL>", "docstring": "Parse a sequence of RFC 2822 phrases.\n\n        A phrase is a sequence of words, which are in turn either RFC 2822\n        atoms or quoted-strings.  Phrases are canonicalized by squeezing all\n        runs of continuous whitespace into one space.", "id": "f16460:c1:m12"}
{"signature": "def __merge_clusters(self, cluster1, cluster2):", "body": "merged_cluster = cure_cluster(None, None)<EOL>merged_cluster.points = cluster1.points + cluster2.points<EOL>merged_cluster.indexes = cluster1.indexes + cluster2.indexes<EOL>dimension = len(cluster1.mean)<EOL>merged_cluster.mean = [<NUM_LIT:0>] * dimension<EOL>if merged_cluster.points[<NUM_LIT:1>:] == merged_cluster.points[:-<NUM_LIT:1>]:<EOL><INDENT>merged_cluster.mean = merged_cluster.points[<NUM_LIT:0>]<EOL><DEDENT>else:<EOL><INDENT>for index in range(dimension):<EOL><INDENT>merged_cluster.mean[index] = ( len(cluster1.points) * cluster1.mean[index] + len(cluster2.points) * cluster2.mean[index] ) / ( len(cluster1.points) + len(cluster2.points) );<EOL><DEDENT><DEDENT>temporary = list()<EOL>for index in range(self.__number_represent_points):<EOL><INDENT>maximal_distance = <NUM_LIT:0><EOL>maximal_point = None<EOL>for point in merged_cluster.points:<EOL><INDENT>minimal_distance = <NUM_LIT:0><EOL>if index == <NUM_LIT:0>:<EOL><INDENT>minimal_distance = euclidean_distance_square(point, merged_cluster.mean)<EOL><DEDENT>else:<EOL><INDENT>minimal_distance = min([euclidean_distance_square(point, p) for p in temporary])<EOL><DEDENT>if minimal_distance >= maximal_distance:<EOL><INDENT>maximal_distance = minimal_distance<EOL>maximal_point = point<EOL><DEDENT><DEDENT>if maximal_point not in temporary:<EOL><INDENT>temporary.append(maximal_point)<EOL><DEDENT><DEDENT>for point in temporary:<EOL><INDENT>representative_point = [<NUM_LIT:0>] * dimension<EOL>for index in range(dimension):<EOL><INDENT>representative_point[index] = point[index] + self.__compression * (merged_cluster.mean[index] - point[index])<EOL><DEDENT>merged_cluster.rep.append(representative_point)<EOL><DEDENT>return merged_cluster<EOL>", "docstring": "!\n        @brief Merges two clusters and returns new merged cluster. Representation points and mean points are calculated for the new cluster.\n\n        @param[in] cluster1 (cure_cluster): Cluster that should be merged.\n        @param[in] cluster2 (cure_cluster): Cluster that should be merged.\n\n        @return (cure_cluster) New merged CURE cluster.", "id": "f15541:c1:m15"}
{"signature": "def annotate_metadata_platform(repo):", "body": "print(\"<STR_LIT>\")<EOL>package = repo.package<EOL>mgr = plugins_get_mgr()<EOL>repomgr = mgr.get(what='<STR_LIT>', name='<STR_LIT>')<EOL>package['<STR_LIT>'] = repomgr.get_metadata()<EOL>", "docstring": "Update metadata host information", "id": "f8147:m22"}
{"signature": "def handleLogInput(self, inputs):", "body": "if self._tapFileIn is not None:<EOL><INDENT>for input in inputs:<EOL><INDENT>for k in range(len(input)):<EOL><INDENT>print(input[k], end='<STR_LIT:U+0020>', file=self._tapFileIn)<EOL><DEDENT>print(file=self._tapFileIn)<EOL><DEDENT><DEDENT>", "docstring": "Write inputs to output tap file.\n\n:param inputs: (iter) some inputs.", "id": "f17620:c0:m19"}
{"signature": "def bond_canonical_statistics(<EOL>microcanonical_statistics,<EOL>convolution_factors,<EOL>**kwargs<EOL>):", "body": "<EOL>spanning_cluster = (<EOL>'<STR_LIT>' in microcanonical_statistics.dtype.names<EOL>)<EOL>ret = np.empty(<NUM_LIT:1>, dtype=canonical_statistics_dtype(spanning_cluster))<EOL>if spanning_cluster:<EOL><INDENT>ret['<STR_LIT>'] = np.sum(<EOL>convolution_factors *<EOL>microcanonical_statistics['<STR_LIT>']<EOL>)<EOL><DEDENT>ret['<STR_LIT>'] = np.sum(<EOL>convolution_factors *<EOL>microcanonical_statistics['<STR_LIT>']<EOL>)<EOL>ret['<STR_LIT>'] = np.sum(<EOL>convolution_factors[:, np.newaxis] *<EOL>microcanonical_statistics['<STR_LIT>'],<EOL>axis=<NUM_LIT:0>,<EOL>)<EOL>return ret<EOL>", "docstring": "canonical cluster statistics for a single run and a single probability\n\nParameters\n----------\n\nmicrocanonical_statistics : ndarray\n    Return value of `bond_microcanonical_statistics`\n\nconvolution_factors : 1-D array_like\n    The coefficients of the convolution for the given probabilty ``p``\n    and for each occupation number ``n``.\n\nReturns\n-------\nret : ndarray of size ``1``\n    Structured array with dtype as returned by\n    `canonical_statistics_dtype`\n\nret['percolation_probability'] : ndarray of float\n    The \"percolation probability\" of this run at the value of ``p``.\n    Only exists if `microcanonical_statistics` argument has the\n    ``has_spanning_cluster`` field.\n\nret['max_cluster_size'] : ndarray of int\n    Weighted size of the largest cluster (absolute number of sites)\n\nret['moments'] : 1-D :py:class:`numpy.ndarray` of float\n    Array of size ``5``.\n    The ``k``-th entry is the weighted ``k``-th raw moment of the\n    (absolute) cluster size distribution, with ``k`` ranging from ``0`` to\n    ``4``.\n\nSee Also\n--------\n\nbond_microcanonical_statistics\ncanonical_statistics_dtype", "id": "f15358:m5"}
{"signature": "def items(self):", "body": "return list(self.dict.items())<EOL>", "docstring": "Get all of a message's headers.\n\n        Returns a list of name, value tuples.", "id": "f16460:c0:m25"}
{"signature": "def startlog(self, filename, overwrite=True):", "body": "if not filename:<EOL><INDENT>return <NUM_LIT:0><EOL><DEDENT>if overwrite:<EOL><INDENT>_mode = '<STR_LIT:w>'<EOL><DEDENT>else:<EOL><INDENT>_mode = '<STR_LIT:a>'<EOL><DEDENT>self._file_logger = self.logging.FileHandler(os.path.expanduser(filename), _mode)<EOL>_formatter = self.logging.Formatter('<STR_LIT>')<EOL>self._file_logger.setFormatter(_formatter)<EOL>self.logger.addHandler(_file_logger)<EOL>if _ldtp_debug:<EOL><INDENT>self._file_logger.setLevel(logging.DEBUG)<EOL><DEDENT>else:<EOL><INDENT>self._file_logger.setLevel(logging.ERROR)<EOL><DEDENT>return <NUM_LIT:1><EOL>", "docstring": "@param filename: Start logging on the specified file\n@type filename: string\n@param overwrite: Overwrite or append\n    False - Append log to an existing file\n    True - Write log to a new file. If file already exist, \n    then erase existing file content and start log\n@type overwrite: boolean\n\n@return: 1 on success and 0 on error\n@rtype: integer", "id": "f10311:c3:m6"}
{"signature": "def get_default(self, ctx):", "body": "<EOL>if callable(self.default):<EOL><INDENT>rv = self.default()<EOL><DEDENT>else:<EOL><INDENT>rv = self.default<EOL><DEDENT>return self.type_cast_value(ctx, rv)<EOL>", "docstring": "Given a context variable this calculates the default value.", "id": "f8340:c6:m3"}
{"signature": "def _mark_log_file(self, msg):", "body": "with open('<STR_LIT>', '<STR_LIT:a>') as logfile:<EOL><INDENT>logfile.write(\"<STR_LIT:->\"*<NUM_LIT> + '<STR_LIT:\\n>')<EOL>logfile.write('<STR_LIT>' % (msg, datetime.now()))<EOL>logfile.write(\"<STR_LIT:->\"*<NUM_LIT> + '<STR_LIT:\\n>')<EOL><DEDENT>return<EOL>", "docstring": "A convenience method to mark sections of a continuous log file.", "id": "f16372:c0:m1"}
{"signature": "def description(description):", "body": "def wrapper(func):<EOL><INDENT>@wraps(func)<EOL>def wrapped(self, *args, **kwargs):<EOL><INDENT>return func(self, *args, **kwargs)<EOL><DEDENT>wrapped.description = description<EOL>return wrapped<EOL><DEDENT>return wrapper<EOL>", "docstring": "Set description to test_method", "id": "f12310:m0"}
{"signature": "def fromfunction_kw(function, dimensions, **kwargs):", "body": "warnings.warn(\"<STR_LIT>\", DeprecationWarning)<EOL>return np.fromfunction(function, dimensions, **kwargs)<EOL>", "docstring": "Drop-in replacement for :func:`numpy.fromfunction`.\n\nAllows passing keyword arguments to the desired function.\n\nCall it as (keywords are optional)::\n\n  fromfunction_kw(MyFunction, dimensions, keywords)\n\nThe function ``MyFunction`` is responsible for handling the\ndictionary of keywords it will receive.", "id": "f17237:m70"}
{"signature": "def _length(self):", "body": "self._build_chunk_headers()<EOL>length = <NUM_LIT:0><EOL>if self._data:<EOL><INDENT>for field in self._data:<EOL><INDENT>length += len(self._chunk_headers[field])<EOL>length += len(self._data[field])<EOL>length += <NUM_LIT:2><EOL><DEDENT><DEDENT>if self._files:<EOL><INDENT>for field in self._files:<EOL><INDENT>length += len(self._chunk_headers[field])<EOL>length += self._file_size(field)<EOL>length += <NUM_LIT:2><EOL><DEDENT><DEDENT>length += len(self.boundary)<EOL>length += <NUM_LIT:6><EOL>return length<EOL>", "docstring": "Returns total length for this request.\n\n        Returns:\n            int. Length", "id": "f7978:c0:m8"}
{"signature": "def formatTime(t):", "body": "if isinstance(t, (float, int)):<EOL><INDENT>return datetime.utcfromtimestamp(t).strftime(timeFormat)<EOL><DEDENT>elif isinstance(t, datetime):<EOL><INDENT>return t.strftime(timeFormat)<EOL><DEDENT>else:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>", "docstring": "Properly Format Time for permlinks", "id": "f8272:m2"}
{"signature": "def batch_normalization(x, mean, variance, offset, scale, variance_epsilon, data_format, name=None):", "body": "with ops.name_scope(name, '<STR_LIT>', [x, mean, variance, scale, offset]):<EOL><INDENT>inv = math_ops.rsqrt(variance + variance_epsilon)<EOL>if scale is not None:<EOL><INDENT>inv *= scale<EOL><DEDENT>a = math_ops.cast(inv, x.dtype)<EOL>b = math_ops.cast(offset - mean * inv if offset is not None else -mean * inv, x.dtype)<EOL>df = {'<STR_LIT>': '<STR_LIT>', '<STR_LIT>': '<STR_LIT>'}<EOL>return _bias_add(_bias_scale(x, a, df[data_format]), b, df[data_format])<EOL><DEDENT>", "docstring": "Data Format aware version of tf.nn.batch_normalization.", "id": "f11170:m3"}
{"signature": "@classmethod<EOL><INDENT>def readFromProto(cls, proto):<DEDENT>", "body": "instance = cls()<EOL>instance.encoder = MultiEncoder.read(proto.encoder)<EOL>if proto.disabledEncoder is not None:<EOL><INDENT>instance.disabledEncoder = MultiEncoder.read(proto.disabledEncoder)<EOL><DEDENT>instance.topDownMode = bool(proto.topDownMode)<EOL>instance.verbosity = proto.verbosity<EOL>instance.numCategories = proto.numCategories<EOL>return instance<EOL>", "docstring": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.readFromProto`.", "id": "f17628:c0:m15"}
{"signature": "@property<EOL><INDENT>def RowSpan(self) -> int:<DEDENT>", "body": "return self.pattern.CurrentRowSpan<EOL>", "docstring": "Property RowSpan.\nCall IUIAutomationGridItemPattern::get_CurrentRowSpan.\nReturn int, the number of rows spanned by the grid item.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationgriditempattern-get_currentrowspan", "id": "f1782:c49:m5"}
{"signature": "def _init_services(self):", "body": "logger = _get_logger(self.debug)<EOL>opened = self._session.openService('<STR_LIT>')<EOL>ev = self._session.nextEvent()<EOL>ev_name = _EVENT_DICT[ev.eventType()]<EOL>logger.info('<STR_LIT>'.format(ev_name))<EOL>for msg in ev:<EOL><INDENT>logger.info('<STR_LIT>'.format(msg))<EOL><DEDENT>if ev.eventType() != blpapi.Event.SERVICE_STATUS:<EOL><INDENT>raise RuntimeError('<STR_LIT>'<EOL>'<STR_LIT>'.format(ev_name))<EOL><DEDENT>if not opened:<EOL><INDENT>logger.warning('<STR_LIT>')<EOL>raise ConnectionError('<STR_LIT>')<EOL><DEDENT>self.refDataService = self._session.getService('<STR_LIT>')<EOL>opened = self._session.openService('<STR_LIT>')<EOL>ev = self._session.nextEvent()<EOL>ev_name = _EVENT_DICT[ev.eventType()]<EOL>logger.info('<STR_LIT>'.format(ev_name))<EOL>for msg in ev:<EOL><INDENT>logger.info('<STR_LIT>'.format(msg))<EOL><DEDENT>if ev.eventType() != blpapi.Event.SERVICE_STATUS:<EOL><INDENT>raise RuntimeError('<STR_LIT>'<EOL>'<STR_LIT>'.format(ev_name))<EOL><DEDENT>if not opened:<EOL><INDENT>logger.warning('<STR_LIT>')<EOL>raise ConnectionError('<STR_LIT>')<EOL><DEDENT>self.exrService = self._session.getService('<STR_LIT>')<EOL>return self<EOL>", "docstring": "Initialize blpapi.Session services", "id": "f5580:c0:m4"}
{"signature": "def __init__(self, home_id, tibber_control):", "body": "self._tibber_control = tibber_control<EOL>self._home_id = home_id<EOL>self._current_price_total = None<EOL>self._current_price_info = {}<EOL>self._price_info = {}<EOL>self._level_info = {}<EOL>self.sub_manager = None<EOL>self.info = {}<EOL>self._subscription_id = None<EOL>self._data = None<EOL>", "docstring": "Initialize the Tibber home class.", "id": "f12605:c1:m0"}
{"signature": "def calculate(self, **state):", "body": "T = state['<STR_LIT:T>']<EOL>y = state['<STR_LIT:y>']<EOL>x = amount_fractions(y)<EOL>return super().calculate(T=T, x=x)<EOL>", "docstring": "Calculate dynamic viscosity at the specified temperature and\ncomposition:\n\n:param T: [K] temperature\n:param y: [mass fraction] composition dictionary , e.g. \\\n{'SiO2': 0.25, 'CaO': 0.25, 'MgO': 0.25, 'FeO': 0.25}\n\n:returns: [Pa.s] dynamic viscosity\n\nThe **state parameter contains the keyword argument(s) specified above\\\nthat are used to describe the state of the material.", "id": "f15847:c1:m1"}
{"signature": "def print_exception(etype, value, tb, limit=None, file=None):", "body": "if file is None:<EOL><INDENT>file = open('<STR_LIT>', '<STR_LIT:w>')<EOL><DEDENT>if tb:<EOL><INDENT>_print(file, '<STR_LIT>')<EOL>print_tb(tb, limit, file)<EOL><DEDENT>lines = format_exception_only(etype, value)<EOL>for line in lines:<EOL><INDENT>_print(file, line, '<STR_LIT>')<EOL><DEDENT>", "docstring": "Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.", "id": "f16448:m6"}
{"signature": "def run(self, **kwargs):", "body": "run(self, **kwargs)<EOL>", "docstring": "Calls :func:`run` with the same parameters.", "id": "f12971:c11:m12"}
{"signature": "def do_xref(self, node):", "body": "id = node.attributes[\"<STR_LIT:id>\"].value<EOL>self.parse(self.randomChildElement(self.refs[id]))<EOL>", "docstring": "handle <xref id='...'> tag\n\n        An <xref id='...'> tag is a cross-reference to a <ref id='...'>\n        tag.  <xref id='sentence'/> evaluates to a randomly chosen child of\n        <ref id='sentence'>.", "id": "f11511:c1:m14"}
{"signature": "def shuffle(self, x, random=None):", "body": "if random is None:<EOL><INDENT>random = self.random<EOL><DEDENT>_int = int<EOL>for i in reversed(range(<NUM_LIT:1>, len(x))):<EOL><INDENT>j = _int(random() * (i+<NUM_LIT:1>))<EOL>x[i], x[j] = x[j], x[i]<EOL><DEDENT>", "docstring": "x, random=random.random -> shuffle list x in place; return None.\n\n        Optional arg random is a 0-argument function returning a random\n        float in [0.0, 1.0); by default, the standard random.random.", "id": "f16474:c0:m5"}
{"signature": "def write_back_register(self, reg, val):", "body": "if self.write_backs_disabled:<EOL><INDENT>return<EOL><DEDENT>if issymbolic(val):<EOL><INDENT>logger.warning(\"<STR_LIT>\")<EOL>return<EOL><DEDENT>if reg in self.flag_registers:<EOL><INDENT>self._emu.reg_write(self._to_unicorn_id('<STR_LIT>'), self._cpu.read_register('<STR_LIT>'))<EOL>return<EOL><DEDENT>self._emu.reg_write(self._to_unicorn_id(reg), val)<EOL>", "docstring": "Sync register state from Manticore -> Unicorn", "id": "f17018:c0:m16"}
{"signature": "def make_tarfile(output_filename, source_dir):", "body": "with tarfile.open(output_filename, \"<STR_LIT>\") as tar:<EOL><INDENT>tar.add(source_dir, arcname=os.path.basename(source_dir))<EOL><DEDENT>", "docstring": "Tar a directory", "id": "f7362:m23"}
{"signature": "def select_ipam_strategy(self, network_id, network_strategy, **kwargs):", "body": "LOG.info(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (network_id, network_strategy))<EOL>net_type = \"<STR_LIT>\"<EOL>if STRATEGY.is_provider_network(network_id):<EOL><INDENT>net_type = \"<STR_LIT>\"<EOL><DEDENT>strategy = self._ipam_strategies.get(net_type, {})<EOL>default = strategy.get(\"<STR_LIT:default>\")<EOL>overrides = strategy.get(\"<STR_LIT>\", {})<EOL>if network_strategy in overrides:<EOL><INDENT>LOG.info(\"<STR_LIT>\"<EOL>% (overrides[network_strategy]))<EOL>return overrides[network_strategy]<EOL><DEDENT>if default:<EOL><INDENT>LOG.info(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (default))<EOL>return default<EOL><DEDENT>LOG.info(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % (network_strategy))<EOL>return network_strategy<EOL>", "docstring": "Return relevant IPAM strategy name.\n\n        :param network_id: neutron network id.\n        :param network_strategy: default strategy for the network.\n\n        NOTE(morgabra) This feels like a hack but I can't think of a better\n        idea. The root problem is we can now attach ports to networks with\n        a different backend driver/ipam strategy than the network speficies.\n\n        We handle the the backend driver part with allowing network_plugin to\n        be specified for port objects. This works pretty well because nova or\n        whatever knows when we are hooking up an Ironic node so it can pass\n        along that key during port_create().\n\n        IPAM is a little trickier, especially in Ironic's case, because we\n        *must* use a specific IPAM for provider networks. There isn't really\n        much of an option other than involve the backend driver when selecting\n        the IPAM strategy.", "id": "f10795:c2:m5"}
{"signature": "def cluster_two_diamonds():", "body": "start_centers = [[<NUM_LIT>, <NUM_LIT>]]<EOL>template_clustering(start_centers, FCPS_SAMPLES.SAMPLE_TWO_DIAMONDS, criterion = splitting_type.BAYESIAN_INFORMATION_CRITERION)<EOL>template_clustering(start_centers, FCPS_SAMPLES.SAMPLE_TWO_DIAMONDS, criterion = splitting_type.MINIMUM_NOISELESS_DESCRIPTION_LENGTH)<EOL>", "docstring": "Start with wrong number of clusters.", "id": "f15574:m14"}
{"signature": "def printInput(self, x):", "body": "print(\"<STR_LIT>\")<EOL>for c in range(self.numberOfCols):<EOL><INDENT>print(int(x[c]), end='<STR_LIT:U+0020>')<EOL><DEDENT>print()<EOL>", "docstring": "TODO: document\n\n:param x: \n:return:", "id": "f17565:c0:m27"}
{"signature": "def unpack_from_dict(fmt, names, data, offset=<NUM_LIT:0>):", "body": "return CompiledFormatDict(fmt, names).unpack_from(data, offset)<EOL>", "docstring": "Same as :func:`~bitstruct.unpack_from_dict()`, but returns a\n    dictionary.\n\n    See :func:`~bitstruct.pack_dict()` for details on `names`.", "id": "f3652:m10"}
{"signature": "def find_root_in_path(graph, path_nodes):", "body": "path_graph = graph.subgraph(path_nodes)<EOL>node_in_degree_tuple = sorted([(n, d) for n, d in path_graph.in_degree().items()], key=itemgetter(<NUM_LIT:1>))<EOL>node_out_degree_tuple = sorted([(n, d) for n, d in path_graph.out_degree().items()], key=itemgetter(<NUM_LIT:1>),<EOL>reverse=True)<EOL>tied_root_index = <NUM_LIT:0><EOL>for i in range(<NUM_LIT:0>, (len(node_in_degree_tuple) - <NUM_LIT:1>)):<EOL><INDENT>if node_in_degree_tuple[i][<NUM_LIT:1>] < node_in_degree_tuple[i + <NUM_LIT:1>][<NUM_LIT:1>]:<EOL><INDENT>tied_root_index = i<EOL>break<EOL><DEDENT><DEDENT>if tied_root_index != <NUM_LIT:0>:<EOL><INDENT>root_tuple = max(node_out_degree_tuple[:tied_root_index], key=itemgetter(<NUM_LIT:1>))<EOL><DEDENT>else:<EOL><INDENT>root_tuple = node_in_degree_tuple[<NUM_LIT:0>]<EOL><DEDENT>return path_graph, root_tuple[<NUM_LIT:0>]<EOL>", "docstring": "Find the 'root' of the path -> The node with the lowest out degree, if multiple:\n         root is the one with the highest out degree among those with lowest out degree\n\n    :param pybel.BELGraph graph: A BEL Graph\n    :param list[tuple] path_nodes: A list of nodes in their order in a path\n    :return: A pair of the graph: graph of the path and the root node\n    :rtype: tuple[pybel.BELGraph,tuple]", "id": "f9367:m5"}
{"signature": "def plot_training_results(classifier,<EOL>classlabels,<EOL>outfile):", "body": "if isinstance(classifier,str) and os.path.exists(classifier):<EOL><INDENT>with open(classifier,'<STR_LIT:rb>') as infd:<EOL><INDENT>clfdict = pickle.load(infd)<EOL><DEDENT><DEDENT>elif isinstance(classifier, dict):<EOL><INDENT>clfdict = classifier<EOL><DEDENT>else:<EOL><INDENT>LOGERROR(\"<STR_LIT>\")<EOL>return None<EOL><DEDENT>confmatrix = clfdict['<STR_LIT>']<EOL>overall_feature_importances = clfdict[<EOL>'<STR_LIT>'<EOL>].feature_importances_<EOL>feature_importances_per_tree = np.array([<EOL>tree.feature_importances_<EOL>for tree in clfdict['<STR_LIT>'].estimators_<EOL>])<EOL>stdev_feature_importances = np.std(feature_importances_per_tree,axis=<NUM_LIT:0>)<EOL>feature_names = np.array(clfdict['<STR_LIT>'])<EOL>plt.figure(figsize=(<NUM_LIT>*<NUM_LIT>,<NUM_LIT>))<EOL>plt.subplot(<NUM_LIT>)<EOL>classes = np.array(classlabels)<EOL>plt.imshow(confmatrix, interpolation='<STR_LIT>', cmap=plt.cm.Blues)<EOL>tick_marks = np.arange(len(classes))<EOL>plt.xticks(tick_marks, classes)<EOL>plt.yticks(tick_marks, classes)<EOL>plt.title('<STR_LIT>')<EOL>plt.ylabel('<STR_LIT>')<EOL>plt.xlabel('<STR_LIT>')<EOL>thresh = confmatrix.max() / <NUM_LIT><EOL>for i, j in itertools.product(range(confmatrix.shape[<NUM_LIT:0>]),<EOL>range(confmatrix.shape[<NUM_LIT:1>])):<EOL><INDENT>plt.text(j, i, confmatrix[i, j],<EOL>horizontalalignment=\"<STR_LIT>\",<EOL>color=\"<STR_LIT>\" if confmatrix[i, j] > thresh else \"<STR_LIT>\")<EOL><DEDENT>plt.subplot(<NUM_LIT>)<EOL>features = np.array(feature_names)<EOL>sorted_ind = np.argsort(overall_feature_importances)[::-<NUM_LIT:1>]<EOL>features = features[sorted_ind]<EOL>feature_names = feature_names[sorted_ind]<EOL>overall_feature_importances = overall_feature_importances[sorted_ind]<EOL>stdev_feature_importances = stdev_feature_importances[sorted_ind]<EOL>plt.bar(np.arange(<NUM_LIT:0>,features.size),<EOL>overall_feature_importances,<EOL>yerr=stdev_feature_importances,<EOL>width=<NUM_LIT>,<EOL>color='<STR_LIT>')<EOL>plt.xticks(np.arange(<NUM_LIT:0>,features.size),<EOL>features,<EOL>rotation=<NUM_LIT>)<EOL>plt.yticks([<NUM_LIT:0.0>,<NUM_LIT:0.1>,<NUM_LIT>,<NUM_LIT>,<NUM_LIT>,<NUM_LIT:0.5>,<NUM_LIT>,<NUM_LIT>,<NUM_LIT>,<NUM_LIT>,<NUM_LIT:1.0>])<EOL>plt.xlim(-<NUM_LIT>, features.size - <NUM_LIT:1.0> + <NUM_LIT>)<EOL>plt.ylim(<NUM_LIT:0.0>,<NUM_LIT>)<EOL>plt.ylabel('<STR_LIT>')<EOL>plt.title('<STR_LIT>')<EOL>plt.subplots_adjust(wspace=<NUM_LIT:0.1>)<EOL>plt.savefig(outfile,<EOL>bbox_inches='<STR_LIT>',<EOL>dpi=<NUM_LIT:100>)<EOL>plt.close('<STR_LIT:all>')<EOL>return outfile<EOL>", "docstring": "This plots the training results from the classifier run on the training\n    set.\n\n    - plots the confusion matrix\n\n    - plots the feature importances\n\n    - FIXME: plot the learning curves too, see:\n      http://scikit-learn.org/stable/modules/learning_curve.html\n\n    Parameters\n    ----------\n\n    classifier : dict or str\n        This is the output dict or pickle created by `get_rf_classifier`\n        containing the trained classifier.\n\n    classlabels : list of str\n        This contains all of the class labels for the current classification\n        problem.\n\n    outfile : str\n        This is the filename where the plots will be written.\n\n    Returns\n    -------\n\n    str\n        The path to the generated plot file.", "id": "f14714:m4"}
{"signature": "def _onLeftButtonDown(self, evt):", "body": "x = evt.GetX()<EOL>y = self.figure.bbox.height - evt.GetY()<EOL>evt.Skip()<EOL>self.CaptureMouse()<EOL>FigureCanvasBase.button_press_event(self, x, y, <NUM_LIT:1>, guiEvent=evt)<EOL>", "docstring": "Start measuring on an axis.", "id": "f17227:c3:m34"}
{"signature": "def _create_account_(self, name, number, account_type):", "body": "new_acc = GeneralLedgerAccount(name, None, number, account_type)<EOL>self.accounts.append(new_acc)<EOL>return new_acc<EOL>", "docstring": "Create an account in the general ledger structure.\n\n:param name: The account name.\n:param number: The account number.\n:param account_type: The account type.\n\n:returns: The created account.", "id": "f15818:c4:m3"}
{"signature": "def twinx(self):", "body": "ax2 = self.figure.add_axes(self.get_position(True), sharex=self,<EOL>frameon=False)<EOL>ax2.yaxis.tick_right()<EOL>ax2.yaxis.set_label_position('<STR_LIT:right>')<EOL>self.yaxis.tick_left()<EOL>return ax2<EOL>", "docstring": "call signature::\n\n  ax = twinx()\n\ncreate a twin of Axes for generating a plot with a sharex\nx-axis but independent y axis.  The y-axis of self will have\nticks on left and the returned axes will have ticks on the\nright", "id": "f17238:c1:m171"}
{"signature": "def listfile(p):", "body": "try:<EOL><INDENT>for entry in scandir.scandir(p):<EOL><INDENT>if entry.is_file():<EOL><INDENT>yield entry.name<EOL><DEDENT><DEDENT><DEDENT>except OSError:<EOL><INDENT>return<EOL><DEDENT>", "docstring": "generator of list files in the path.\nfilenames only", "id": "f6369:m16"}
{"signature": "def gen_preds(clf, arr):", "body": "if(hasattr(clf, \"<STR_LIT>\")):<EOL><INDENT>ret = clf.predict(arr)<EOL><DEDENT>else:<EOL><INDENT>ret = clf.predict(arr)<EOL><DEDENT>return ret<EOL>", "docstring": "Generates predictions on a novel data array using a fit classifier\nclf is a classifier that has already been fit\narr is a data array identical in dimension to the array clf was trained on\nReturns the array of predictions.", "id": "f7968:m11"}
{"signature": "def _green(string):", "body": "return '<STR_LIT>'.format(string)<EOL>", "docstring": "\u5c06\u5b57\u4f53\u8f6c\u53d8\u4e3a\u7eff\u8272", "id": "f10206:m0"}
{"signature": "def create(self, name, region, size, image, ssh_keys=None,<EOL>backups=None, ipv6=None, private_networking=None, wait=True):", "body": "if ssh_keys and not isinstance(ssh_keys, (list, tuple)):<EOL><INDENT>raise TypeError(\"<STR_LIT>\")<EOL><DEDENT>resp = self.post(name=name, region=region, size=size, image=image,<EOL>ssh_keys=ssh_keys,<EOL>private_networking=private_networking,<EOL>backups=backups, ipv6=ipv6)<EOL>droplet = self.get(resp[self.singular]['<STR_LIT:id>'])<EOL>if wait:<EOL><INDENT>droplet.wait()<EOL><DEDENT>droplet = self.get(resp[self.singular]['<STR_LIT:id>'])<EOL>return droplet<EOL>", "docstring": "Create a new droplet\n\nParameters\n----------\nname: str\n    Name of new droplet\nregion: str\n    slug for region (e.g., sfo1, nyc1)\nsize: str\n    slug for droplet size (e.g., 512mb, 1024mb)\nimage: int or str\n    image id (e.g., 12352) or slug (e.g., 'ubuntu-14-04-x64')\nssh_keys: list, optional\n    default SSH keys to be added on creation\n    this is highly recommended for ssh access\nbackups: bool, optional\n    whether automated backups should be enabled for the Droplet.\n    Automated backups can only be enabled when the Droplet is created.\nipv6: bool, optional\n    whether IPv6 is enabled on the Droplet\nprivate_networking: bool, optional\n    whether private networking is enabled for the Droplet. Private\n    networking is currently only available in certain regions\nwait: bool, default True\n    if True then block until creation is complete", "id": "f4446:c0:m5"}
{"signature": "def destroy(self):", "body": "return self.get_data(\"<STR_LIT>\" % self.id, type=DELETE)<EOL>", "docstring": "Destroy the Firewall", "id": "f1468:c5:m9"}
{"signature": "def _draw_particle(self, pos, sign=<NUM_LIT:1>):", "body": "raise NotImplementedError('<STR_LIT>')<EOL>", "docstring": "Updates ``self.particles`` by drawing a particle at position ``pos``,\nwith possible additional unnamed arguments between ``pos`` and\n``sign``. If ``sign`` is -1, un-draws the particle instead.\n\nTo be able to fit this component in a model, _draw_particle must\ncreate an image that is numerically continuous as pos changes --\ni.e. the edge of the particle must alias smoothly to 0.", "id": "f5759:c0:m1"}
{"signature": "def p_package_version_2(self, p):", "body": "self.error = True<EOL>msg = ERROR_MESSAGES['<STR_LIT>'].format(p.lineno(<NUM_LIT:1>))<EOL>self.logger.log(msg)<EOL>", "docstring": "package_version : PKG_VERSION error", "id": "f3753:c0:m108"}
{"signature": "def _parse_sid_response(res):", "body": "res = json.loads(list(ChunkParser().get_chunks(res))[<NUM_LIT:0>])<EOL>sid = res[<NUM_LIT:0>][<NUM_LIT:1>][<NUM_LIT:1>]<EOL>gsessionid = res[<NUM_LIT:1>][<NUM_LIT:1>][<NUM_LIT:0>]['<STR_LIT>']<EOL>return (sid, gsessionid)<EOL>", "docstring": "Parse response format for request for new channel SID.\n\n    Example format (after parsing JS):\n    [   [0,[\"c\",\"SID_HERE\",\"\",8]],\n        [1,[{\"gsid\":\"GSESSIONID_HERE\"}]]]\n\n    Returns (SID, gsessionid) tuple.", "id": "f10018:m1"}
{"signature": "def detect(text):", "body": "if sys.version_info < (<NUM_LIT:3>, <NUM_LIT:0>):<EOL><INDENT>try:<EOL><INDENT>text = text.decode('<STR_LIT:utf-8>')<EOL><DEDENT>except UnicodeError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>for L in text:<EOL><INDENT>code = ord(L)<EOL>if code >= BRAHMIC_FIRST_CODE_POINT:<EOL><INDENT>for name, start_code in BLOCKS:<EOL><INDENT>if start_code <= code <= BRAHMIC_LAST_CODE_POINT:<EOL><INDENT>return name<EOL><DEDENT><DEDENT><DEDENT><DEDENT>if Regex.IAST_OR_KOLKATA_ONLY.search(text):<EOL><INDENT>if Regex.KOLKATA_ONLY.search(text):<EOL><INDENT>return Scheme.Kolkata<EOL><DEDENT>else:<EOL><INDENT>return Scheme.IAST<EOL><DEDENT><DEDENT>if Regex.ITRANS_ONLY.search(text):<EOL><INDENT>return Scheme.ITRANS<EOL><DEDENT>if Regex.SLP1_ONLY.search(text):<EOL><INDENT>return Scheme.SLP1<EOL><DEDENT>if Regex.VELTHUIS_ONLY.search(text):<EOL><INDENT>return Scheme.Velthuis<EOL><DEDENT>if Regex.ITRANS_OR_VELTHUIS_ONLY.search(text):<EOL><INDENT>return Scheme.ITRANS<EOL><DEDENT>return Scheme.HK<EOL>", "docstring": "Detect the input's transliteration scheme.\n\n      :param text: some text data, either a `unicode` or a `str` encoded\n                   in UTF-8.", "id": "f8756:m0"}
{"signature": "def _printOneTrainingVector(self, x):", "body": "print('<STR_LIT>'.join('<STR_LIT:1>' if k != <NUM_LIT:0> else '<STR_LIT:.>' for k in x))<EOL>", "docstring": "Print a single vector succinctly.", "id": "f17351:c0:m1"}
{"signature": "def catalog(self, node):", "body": "catalogs = self.catalogs(path=node)<EOL>return next(x for x in catalogs)<EOL>", "docstring": "Get the available catalog for a given node.\n\n        :param node: (Required) The name of the PuppetDB node.\n        :type: :obj:`string`\n\n        :returns: An instance of Catalog\n        :rtype: :class:`pypuppetdb.types.Catalog`", "id": "f4028:c0:m16"}
{"signature": "def p_reviewer_2(self, p):", "body": "self.error = True<EOL>msg = ERROR_MESSAGES['<STR_LIT>'].format(p.lineno(<NUM_LIT:1>))<EOL>self.logger.log(msg)<EOL>", "docstring": "reviewer : REVIEWER error", "id": "f3753:c0:m112"}
{"signature": "def tf_next_step(self, x, iteration, conjugate, residual, squared_residual):", "body": "next_step = super(ConjugateGradient, self).tf_next_step(x, iteration, conjugate, residual, squared_residual)<EOL>return tf.logical_and(x=next_step, y=(squared_residual >= util.epsilon))<EOL>", "docstring": "Termination condition: max number of iterations, or residual sufficiently small.\n\nArgs:\n    x: Current solution estimate $x_t$.\n    iteration: Current iteration counter $t$.\n    conjugate: Current conjugate $c_t$.\n    residual: Current residual $r_t$.\n    squared_residual: Current squared residual $r_t^2$.\n\nReturns:\n    True if another iteration should be performed.", "id": "f14352:c0:m4"}
{"signature": "def update(self, aspect_ratio=None, fov=None, near=None, far=None):", "body": "self.aspect_ratio = aspect_ratio or self.aspect_ratio<EOL>self.fov = fov or self.fov<EOL>self.near = near or self.near<EOL>self.far = far or self.far<EOL>self.matrix = Matrix44.perspective_projection(self.fov, self.aspect_ratio, self.near, self.far)<EOL>", "docstring": "Update the internal projection matrix based on current values\nor values passed in if specified.\n\n:param aspect_ratio: New aspect ratio\n:param fov: New field of view\n:param near: New near value\n:param far: New far value", "id": "f14405:c0:m1"}
{"signature": "def group_val(group):", "body": "if group:<EOL><INDENT>return int(group)<EOL><DEDENT>else:<EOL><INDENT>return <NUM_LIT:0><EOL><DEDENT>", "docstring": "Returns value of regular expression group, if valid. 0 if not\n\n    Argument:\n        group - group to get value of", "id": "f7168:m0"}
{"signature": "def new_compiler(*args, **kwargs):", "body": "make_silent = kwargs.pop('<STR_LIT>', True)<EOL>cc = _new_compiler(*args, **kwargs)<EOL>if is_msvc(cc):<EOL><INDENT>from distutils.msvc9compiler import get_build_version<EOL>if get_build_version() == <NUM_LIT:10>:<EOL><INDENT>cc.initialize()<EOL>for ldflags in [cc.ldflags_shared, cc.ldflags_shared_debug]:<EOL><INDENT>unique_extend(ldflags, ['<STR_LIT>'])<EOL><DEDENT><DEDENT>elif get_build_version() == <NUM_LIT>:<EOL><INDENT>make_silent = False<EOL><DEDENT><DEDENT>if make_silent:<EOL><INDENT>cc.spawn = _CCompiler_spawn_silent<EOL><DEDENT>return cc<EOL>", "docstring": "Create a C compiler.\n\n    :param bool silent: Eat all stdio? Defaults to ``True``.\n\n    All other arguments passed to ``distutils.ccompiler.new_compiler``.", "id": "f8441:m6"}
{"signature": "def car(ol):", "body": "return(ol[<NUM_LIT:0>])<EOL>", "docstring": "from elist.elist import *\nol=[1,2,3,4]\ncar(ol)", "id": "f1599:m63"}
{"signature": "def readS8(self, register):", "body": "result = self.readU8(register)<EOL>if result > <NUM_LIT>:<EOL><INDENT>result -= <NUM_LIT><EOL><DEDENT>return result<EOL>", "docstring": "Read a signed byte from the specified register.", "id": "f8004:c0:m8"}
{"signature": "def encode(self, word, max_length=<NUM_LIT:4>):", "body": "word = unicode_normalize('<STR_LIT>', text_type(word.upper()))<EOL>word = word.replace('<STR_LIT>', '<STR_LIT>')<EOL>word = '<STR_LIT>'.join(c for c in word if c in self._uc_set)<EOL>if word[:<NUM_LIT:2>] in {'<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>'}:<EOL><INDENT>word = word[<NUM_LIT:1>:]<EOL><DEDENT>elif word[:<NUM_LIT:1>] == '<STR_LIT:X>':<EOL><INDENT>word = '<STR_LIT:S>' + word[<NUM_LIT:1>:]<EOL><DEDENT>elif word[:<NUM_LIT:2>] == '<STR_LIT>':<EOL><INDENT>word = '<STR_LIT>' + word[<NUM_LIT:2>:]<EOL><DEDENT>word = (<EOL>word.replace('<STR_LIT>', '<STR_LIT>').replace('<STR_LIT>', '<STR_LIT>').replace('<STR_LIT>', '<STR_LIT:0>')<EOL>)<EOL>word = word.translate(self._trans)<EOL>word = self._delete_consecutive_repeats(word)<EOL>word = word.replace('<STR_LIT:0>', '<STR_LIT>')<EOL>if max_length != -<NUM_LIT:1>:<EOL><INDENT>if len(word) < max_length:<EOL><INDENT>word += '<STR_LIT:0>' * (max_length - len(word))<EOL><DEDENT>else:<EOL><INDENT>word = word[:max_length]<EOL><DEDENT><DEDENT>return word<EOL>", "docstring": "Return the SoundD code.\n\n        Parameters\n        ----------\n        word : str\n            The word to transform\n        max_length : int\n            The length of the code returned (defaults to 4)\n\n        Returns\n        -------\n        str\n            The SoundD code\n\n        Examples\n        --------\n        >>> sound_d('Gough')\n        '2000'\n        >>> sound_d('pneuma')\n        '5500'\n        >>> sound_d('knight')\n        '5300'\n        >>> sound_d('trice')\n        '3620'\n        >>> sound_d('judge')\n        '2200'", "id": "f6584:c0:m0"}
{"signature": "def Crowl_Louvar_UFL(atoms):", "body": "nC, nH, nO = <NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT:0><EOL>if '<STR_LIT:C>' in atoms and atoms['<STR_LIT:C>']:<EOL><INDENT>nC = atoms['<STR_LIT:C>']<EOL><DEDENT>else:<EOL><INDENT>return None<EOL><DEDENT>if '<STR_LIT:H>' in atoms:<EOL><INDENT>nH = atoms['<STR_LIT:H>']<EOL><DEDENT>if '<STR_LIT:O>' in atoms:<EOL><INDENT>nO = atoms['<STR_LIT:O>']<EOL><DEDENT>return <NUM_LIT>/(<NUM_LIT>*nC + <NUM_LIT>*nH - <NUM_LIT>*nO + <NUM_LIT:1.>)<EOL>", "docstring": "r'''Calculates upper flammability limit, using the Crowl-Louvar [1]_\n    correlation. Uses molecular formula only.\n\n    The upper flammability limit of a gas is air is:\n\n    .. math::\n        C_mH_xO_y + zO_2 \\to mCO_2 + \\frac{x}{2}H_2O\n\n        \\text{UFL} = \\frac{3.5}{4.76m + 1.19x - 2.38y + 1}\n\n    Parameters\n    ----------\n    atoms : dict\n        Dictionary of atoms and atom counts\n\n    Returns\n    -------\n    UFL : float\n        Upper flammability limit, mole fraction\n\n    Notes\n    -----\n    Coefficient of 3.5 taken from [2]_\n\n    Examples\n    --------\n    Hexane, example from [1]_, lit. 7.5 %\n\n    >>> Crowl_Louvar_UFL({'H': 14, 'C': 6})\n    0.07572479446127219\n\n    References\n    ----------\n    .. [1] Crowl, Daniel A., and Joseph F. Louvar. Chemical Process Safety:\n       Fundamentals with Applications. 2E. Upper Saddle River, N.J:\n       Prentice Hall, 2001.\n    .. [2] Jones, G. W. \"Inflammation Limits and Their Practical Application\n       in Hazardous Industrial Operations.\" Chemical Reviews 22, no. 1\n       (February 1, 1938): 1-26. doi:10.1021/cr60071a001", "id": "f15791:m18"}
{"signature": "def raw_opener(ip_address, port, delay=<NUM_LIT:1>):", "body": "def target():<EOL><INDENT>time.sleep(delay)<EOL>url = '<STR_LIT>' % (ip_address, port)<EOL>webbrowser.open(url, new=<NUM_LIT:0>, autoraise=True)<EOL><DEDENT>threading.Thread(target=target, daemon=True).start()<EOL>", "docstring": "Wait a little and then open a web browser page for the control panel.", "id": "f1966:m1"}
{"signature": "def generateRecursive(self, component, all_components, builddir=None, modbuilddir=None, processed_components=None, application=None):", "body": "assert(self.configured)<EOL>if builddir is None:<EOL><INDENT>builddir = self.buildroot<EOL><DEDENT>if modbuilddir is None:<EOL><INDENT>modbuilddir = os.path.join(builddir, '<STR_LIT>')<EOL><DEDENT>if processed_components is None:<EOL><INDENT>processed_components = dict()<EOL><DEDENT>if not self.target:<EOL><INDENT>yield '<STR_LIT>' % self.target<EOL><DEDENT>toplevel = not len(processed_components)<EOL>logger.debug('<STR_LIT>' % (component, self.target))<EOL>recursive_deps = component.getDependenciesRecursive(<EOL>available_components = all_components,<EOL>target = self.target,<EOL>available_only = True,<EOL>test = True<EOL>)<EOL>dependencies = component.getDependencies(<EOL>all_components,<EOL>target = self.target,<EOL>available_only = True,<EOL>test = True<EOL>)<EOL>for name, dep in dependencies.items():<EOL><INDENT>if not dep:<EOL><INDENT>if dep.isTestDependency():<EOL><INDENT>logger.debug('<STR_LIT>' % (name, component))<EOL><DEDENT>else:<EOL><INDENT>yield '<STR_LIT>' % (name, component)<EOL><DEDENT><DEDENT><DEDENT>processed_components[component.getName()] = component<EOL>new_dependencies = OrderedDict([(name,c) for name,c in dependencies.items() if c and not name in processed_components])<EOL>self.generate(builddir, modbuilddir, component, new_dependencies, dependencies, recursive_deps, application, toplevel)<EOL>logger.debug('<STR_LIT>' % component)<EOL>for d in recursive_deps.values():<EOL><INDENT>logger.debug('<STR_LIT>' % d)<EOL><DEDENT>processed_components.update(new_dependencies)<EOL>for name, c in new_dependencies.items():<EOL><INDENT>for error in self.generateRecursive(<EOL>c, all_components, os.path.join(modbuilddir, name), modbuilddir, processed_components, application=application<EOL>):<EOL><INDENT>yield error<EOL><DEDENT><DEDENT>", "docstring": "generate top-level CMakeLists for this component and its\n            dependencies: the CMakeLists are all generated in self.buildroot,\n            which MUST be out-of-source\n\n            !!! NOTE: experimenting with a slightly different way of doing\n            things here, this function is a generator that yields any errors\n            produced, so the correct use is:\n\n            for error in gen.generateRecursive(...):\n                print(error)", "id": "f13557:c1:m3"}
{"signature": "@property<EOL><INDENT>def address(self):<DEDENT>", "body": "return self.server_host, self.server_port<EOL>", "docstring": "Server listen socket host and port as :py:class:`tuple`", "id": "f9227:c6:m1"}
{"signature": "def phase_magseries_with_errs(times, mags, errs, period, epoch,<EOL>wrap=True, sort=True):", "body": "<EOL>finiteind = np.isfinite(mags)<EOL>finite_times = times[finiteind]<EOL>finite_mags = mags[finiteind]<EOL>finite_errs = errs[finiteind]<EOL>magseries_phase = (<EOL>(finite_times - epoch)/period -<EOL>np.floor(((finite_times - epoch)/period))<EOL>)<EOL>outdict = {'<STR_LIT>':magseries_phase,<EOL>'<STR_LIT>':finite_mags,<EOL>'<STR_LIT>':finite_errs,<EOL>'<STR_LIT>':period,<EOL>'<STR_LIT>':epoch}<EOL>if sort:<EOL><INDENT>sortorder = np.argsort(outdict['<STR_LIT>'])<EOL>outdict['<STR_LIT>'] = outdict['<STR_LIT>'][sortorder]<EOL>outdict['<STR_LIT>'] = outdict['<STR_LIT>'][sortorder]<EOL>outdict['<STR_LIT>'] = outdict['<STR_LIT>'][sortorder]<EOL><DEDENT>if wrap:<EOL><INDENT>outdict['<STR_LIT>'] = np.concatenate((outdict['<STR_LIT>']-<NUM_LIT:1.0>,<EOL>outdict['<STR_LIT>']))<EOL>outdict['<STR_LIT>'] = np.concatenate((outdict['<STR_LIT>'],<EOL>outdict['<STR_LIT>']))<EOL>outdict['<STR_LIT>'] = np.concatenate((outdict['<STR_LIT>'],<EOL>outdict['<STR_LIT>']))<EOL><DEDENT>return outdict<EOL>", "docstring": "Phases a magnitude/flux time-series using a given period and epoch.\n\n    The equation used is::\n\n        phase = (times - epoch)/period - floor((times - epoch)/period)\n\n    This phases the given magnitude timeseries using the given period and\n    epoch. If wrap is True, wraps the result around 0.0 (and returns an array\n    that has twice the number of the original elements). If sort is True,\n    returns the magnitude timeseries in phase sorted order.\n\n    Parameters\n    ----------\n\n    times,mags,errs : np.array\n        The magnitude/flux time-series values and associated measurement errors\n        to phase using the provided `period` and `epoch`. Non-fiinite values\n        will be removed.\n\n    period : float\n        The period to use to phase the time-series.\n\n    epoch : float\n        The epoch to phase the time-series. This is usually the time-of-minimum\n        or time-of-maximum of some periodic light curve\n        phenomenon. Alternatively, one can use the minimum time value in\n        `times`.\n\n    wrap : bool\n        If this is True, the returned phased time-series will be wrapped around\n        phase 0.0, which is useful for plotting purposes. The arrays returned\n        will have twice the number of input elements because of this wrapping.\n\n    sort : bool\n        If this is True, the returned phased time-series will be sorted in\n        increasing phase order.\n\n    Returns\n    -------\n\n    dict\n        A dict of the following form is returned::\n\n            {'phase': the phase values,\n             'mags': the mags/flux values at each phase,\n             'errs': the err values at each phase,\n             'period': the input `period` used to phase the time-series,\n             'epoch': the input `epoch` used to phase the time-series}", "id": "f14742:m5"}
{"signature": "def stop(self) -> float:", "body": "raise NotImplementedError()<EOL>", "docstring": "Stop the timer. Should only be called once when stopping the timer.\n\nReturns:\n    The time the timer was stopped\n\nRaises:\n    NotImplementedError", "id": "f14424:c0:m4"}
{"signature": "def lint_file(in_file, out_file=None):", "body": "for line in in_file:<EOL><INDENT>print(line.strip(), file=out_file)<EOL><DEDENT>", "docstring": "Helps remove extraneous whitespace from the lines of a file\n\n    :param file in_file: A readable file or file-like\n    :param file out_file: A writable file or file-like", "id": "f9356:m0"}
{"signature": "def receive(self):", "body": "pkgidx_result_pairs = self.receive_all()<EOL>if pkgidx_result_pairs is None:<EOL><INDENT>return<EOL><DEDENT>results = [r for _, r in pkgidx_result_pairs]<EOL>return results<EOL>", "docstring": "return a list results of all tasks.\n\n        This method waits for all tasks to finish.\n\n        Returns\n        -------\n        list\n            A list of results of the tasks. The results are sorted in\n            the order in which the tasks are put.", "id": "f9057:c1:m8"}
{"signature": "def _create_archive(self):", "body": "self.status = '<STR_LIT>'<EOL>return self._create_encrypted_zip(source='<STR_LIT>', fs_target_dir=self.container.fs_archive_cleansed)<EOL>", "docstring": "creates an encrypted archive of the dropbox outside of the drop directory.", "id": "f10070:c1:m9"}
{"signature": "def dist_abs(self, src, tar, qval=<NUM_LIT:2>, normalized=False, alphabet=None):", "body": "return super(self.__class__, self).dist_abs(<EOL>src, tar, qval, <NUM_LIT:2>, normalized, alphabet<EOL>)<EOL>", "docstring": "Return the Euclidean distance between two strings.\n\n        Parameters\n        ----------\n        src : str\n            Source string (or QGrams/Counter objects) for comparison\n        tar : str\n            Target string (or QGrams/Counter objects) for comparison\n        qval : int\n            The length of each q-gram; 0 for non-q-gram version\n        normalized : bool\n            Normalizes to [0, 1] if True\n        alphabet : collection or int\n            The values or size of the alphabet\n\n        Returns\n        -------\n        float\n            The Euclidean distance\n\n        Examples\n        --------\n        >>> cmp = Euclidean()\n        >>> cmp.dist_abs('cat', 'hat')\n        2.0\n        >>> round(cmp.dist_abs('Niall', 'Neil'), 12)\n        2.645751311065\n        >>> cmp.dist_abs('Colin', 'Cuilen')\n        3.0\n        >>> round(cmp.dist_abs('ATCG', 'TAGC'), 12)\n        3.162277660168", "id": "f6622:c0:m0"}
{"signature": "def load_exposure_time_map(exposure_time_map_path, exposure_time_map_hdu, pixel_scale, shape, exposure_time,<EOL>exposure_time_map_from_inverse_noise_map, inverse_noise_map):", "body": "exposure_time_map_options = sum([exposure_time_map_from_inverse_noise_map])<EOL>if exposure_time is not None and exposure_time_map_path is not None:<EOL><INDENT>raise exc.DataException(<EOL>'<STR_LIT>'<EOL>'<STR_LIT>')<EOL><DEDENT>if exposure_time_map_options == <NUM_LIT:0>:<EOL><INDENT>if exposure_time is not None and exposure_time_map_path is None:<EOL><INDENT>return ExposureTimeMap.single_value(value=exposure_time, pixel_scale=pixel_scale, shape=shape)<EOL><DEDENT>elif exposure_time is None and exposure_time_map_path is not None:<EOL><INDENT>return ExposureTimeMap.from_fits_with_pixel_scale(file_path=exposure_time_map_path,<EOL>hdu=exposure_time_map_hdu, pixel_scale=pixel_scale)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if exposure_time_map_from_inverse_noise_map:<EOL><INDENT>return ExposureTimeMap.from_exposure_time_and_inverse_noise_map(pixel_scale=pixel_scale,<EOL>exposure_time=exposure_time,<EOL>inverse_noise_map=inverse_noise_map)<EOL><DEDENT><DEDENT>", "docstring": "Factory for loading the exposure time map from a .fits file.\n\n    This factory also includes a number of routines for computing the exposure-time map from other unblurred_image_1d \\\n    (e.g. the background noise-map).\n\n    Parameters\n    ----------\n    exposure_time_map_path : str\n        The path to the exposure_time_map .fits file containing the exposure time map \\\n        (e.g. '/path/to/exposure_time_map.fits')\n    exposure_time_map_hdu : int\n        The hdu the exposure_time_map is contained in the .fits file specified by *exposure_time_map_path*.\n    pixel_scale : float\n        The size of each pixel in arc seconds.\n    shape : (int, int)\n        The shape of the image, required if a single value is used to calculate the exposure time map.\n    exposure_time : float\n        The exposure-time used to compute the expsure-time map if only a single value is used.\n    exposure_time_map_from_inverse_noise_map : bool\n        If True, the exposure-time map is computed from the background noise_map map \\\n        (see *ExposureTimeMap.from_background_noise_map*)\n    inverse_noise_map : ndarray\n        The background noise-map, which the Poisson noise-map can be calculated using.", "id": "f5989:m8"}
{"signature": "def set_brightness(self, brightness):", "body": "command = \"<STR_LIT>\".format(self._zid, brightness)<EOL>response = self._hub.send_command(command)<EOL>_LOGGER.debug(\"<STR_LIT>\", repr(command), response)<EOL>return response<EOL>", "docstring": "Set brightness of bulb.", "id": "f831:c1:m9"}
{"signature": "def set_doc_data_lics(self, doc, lics):", "body": "if not self.doc_data_lics_set:<EOL><INDENT>self.doc_data_lics_set = True<EOL>if validations.validate_data_lics(lics):<EOL><INDENT>doc.data_license = document.License.from_identifier(lics)<EOL>return True<EOL><DEDENT>else:<EOL><INDENT>raise SPDXValueError('<STR_LIT>')<EOL><DEDENT><DEDENT>else:<EOL><INDENT>raise CardinalityError('<STR_LIT>')<EOL><DEDENT>", "docstring": "Sets the document data license.\n        Raises value error if malformed value, CardinalityError\n        if already defined.", "id": "f3754:c0:m2"}
{"signature": "def job_title():", "body": "result = random.choice(get_dictionary('<STR_LIT>')).strip()<EOL>result = result.replace('<STR_LIT>', job_title_suffix())<EOL>return result<EOL>", "docstring": "Return a random job title.", "id": "f14651:m6"}
{"signature": "def __init__(self, name=None, ips=None,  <EOL>dhcp=None, lan=None, firewall_active=None,<EOL>firewall_rules=None, nat=None, **kwargs):", "body": "if firewall_rules is None:<EOL><INDENT>firewall_rules = []<EOL><DEDENT>self.name = name<EOL>self.nat = nat<EOL>self.ips = ips<EOL>self.dhcp = dhcp<EOL>self.lan = lan<EOL>self.firewall_active = firewall_active<EOL>self.firewall_rules = firewall_rules<EOL>", "docstring": "NIC class initializer.\n\n:param      name: The name of the NIC.\n:type       name: ``str``\n\n:param      ips: A list of IPs.\n:type       ips: ``list``\n\n:param      dhcp: Enable or disable DHCP. Default is enabled.\n:type       dhcp: ``bool``\n\n:param      lan: ID of the LAN in which the NIC should reside.\n:type       lan: ``str``\n\n:param      nat: Enable or disable NAT. Default is disabled.\n:type       nat: ``bool``\n\n:param      firewall_active: Turns the firewall on or off;\n                             default is disabled.\n:type       firewall_active: ``bool``\n\n:param      firewall_rules: List of firewall rule dicts.\n:type       firewall_rules: ``list``", "id": "f811:c6:m0"}
{"signature": "def _generate_contents(self, tar):", "body": "text = self.render(files=False)<EOL>vpn_instances = vpn_pattern.split(text)<EOL>if '<STR_LIT>' in vpn_instances:<EOL><INDENT>vpn_instances.remove('<STR_LIT>')<EOL><DEDENT>for vpn in vpn_instances:<EOL><INDENT>lines = vpn.split('<STR_LIT:\\n>')<EOL>vpn_name = lines[<NUM_LIT:0>]<EOL>text_contents = '<STR_LIT:\\n>'.join(lines[<NUM_LIT:2>:])<EOL>if text_contents.endswith('<STR_LIT>'):<EOL><INDENT>text_contents = text_contents[<NUM_LIT:0>:-<NUM_LIT:1>]<EOL><DEDENT>self._add_file(tar=tar,<EOL>name='<STR_LIT>'.format(vpn_name, config_suffix),<EOL>contents=text_contents)<EOL><DEDENT>", "docstring": "Adds configuration files to tarfile instance.\n\n:param tar: tarfile instance\n:returns: None", "id": "f11891:c0:m0"}
{"signature": "@gcp_conn('<STR_LIT>')<EOL>def get_bucket(client=None, **kwargs):", "body": "bucket = client.lookup_bucket(kwargs['<STR_LIT>'])<EOL>return bucket<EOL>", "docstring": "Get bucket object.\n\n:param client: client object to use.\n:type client: Google Cloud Storage client\n\n:returns: Bucket object\n:rtype: ``object``", "id": "f10868:m1"}
{"signature": "def get_cmdclass():", "body": "if \"<STR_LIT>\" in sys.modules:<EOL><INDENT>del sys.modules[\"<STR_LIT>\"]<EOL><DEDENT>cmds = {}<EOL>from distutils.core import Command<EOL>class cmd_version(Command):<EOL><INDENT>description = \"<STR_LIT>\"<EOL>user_options = []<EOL>boolean_options = []<EOL>def initialize_options(self):<EOL><INDENT>pass<EOL><DEDENT>def finalize_options(self):<EOL><INDENT>pass<EOL><DEDENT>def run(self):<EOL><INDENT>vers = get_versions(verbose=True)<EOL>print(\"<STR_LIT>\" % vers[\"<STR_LIT:version>\"])<EOL>print(\"<STR_LIT>\" % vers.get(\"<STR_LIT>\"))<EOL>print(\"<STR_LIT>\" % vers.get(\"<STR_LIT>\"))<EOL>if vers[\"<STR_LIT:error>\"]:<EOL><INDENT>print(\"<STR_LIT>\" % vers[\"<STR_LIT:error>\"])<EOL><DEDENT><DEDENT><DEDENT>cmds[\"<STR_LIT:version>\"] = cmd_version<EOL>if \"<STR_LIT>\" in sys.modules:<EOL><INDENT>from setuptools.command.build_py import build_py as _build_py<EOL><DEDENT>else:<EOL><INDENT>from distutils.command.build_py import build_py as _build_py<EOL><DEDENT>class cmd_build_py(_build_py):<EOL><INDENT>def run(self):<EOL><INDENT>root = get_root()<EOL>cfg = get_config_from_root(root)<EOL>versions = get_versions()<EOL>_build_py.run(self)<EOL>if cfg.versionfile_build:<EOL><INDENT>target_versionfile = os.path.join(self.build_lib,<EOL>cfg.versionfile_build)<EOL>print(\"<STR_LIT>\" % target_versionfile)<EOL>write_to_version_file(target_versionfile, versions)<EOL><DEDENT><DEDENT><DEDENT>cmds[\"<STR_LIT>\"] = cmd_build_py<EOL>if \"<STR_LIT>\" in sys.modules:  <EOL><INDENT>from cx_Freeze.dist import build_exe as _build_exe<EOL>class cmd_build_exe(_build_exe):<EOL><INDENT>def run(self):<EOL><INDENT>root = get_root()<EOL>cfg = get_config_from_root(root)<EOL>versions = get_versions()<EOL>target_versionfile = cfg.versionfile_source<EOL>print(\"<STR_LIT>\" % target_versionfile)<EOL>write_to_version_file(target_versionfile, versions)<EOL>_build_exe.run(self)<EOL>os.unlink(target_versionfile)<EOL>with open(cfg.versionfile_source, \"<STR_LIT:w>\") as f:<EOL><INDENT>LONG = LONG_VERSION_PY[cfg.VCS]<EOL>f.write(LONG %<EOL>{\"<STR_LIT>\": \"<STR_LIT:$>\",<EOL>\"<STR_LIT>\": cfg.style,<EOL>\"<STR_LIT>\": cfg.tag_prefix,<EOL>\"<STR_LIT>\": cfg.parentdir_prefix,<EOL>\"<STR_LIT>\": cfg.versionfile_source,<EOL>})<EOL><DEDENT><DEDENT><DEDENT>cmds[\"<STR_LIT>\"] = cmd_build_exe<EOL>del cmds[\"<STR_LIT>\"]<EOL><DEDENT>if \"<STR_LIT>\" in sys.modules:<EOL><INDENT>from setuptools.command.sdist import sdist as _sdist<EOL><DEDENT>else:<EOL><INDENT>from distutils.command.sdist import sdist as _sdist<EOL><DEDENT>class cmd_sdist(_sdist):<EOL><INDENT>def run(self):<EOL><INDENT>versions = get_versions()<EOL>self._versioneer_generated_versions = versions<EOL>self.distribution.metadata.version = versions[\"<STR_LIT:version>\"]<EOL>return _sdist.run(self)<EOL><DEDENT>def make_release_tree(self, base_dir, files):<EOL><INDENT>root = get_root()<EOL>cfg = get_config_from_root(root)<EOL>_sdist.make_release_tree(self, base_dir, files)<EOL>target_versionfile = os.path.join(base_dir, cfg.versionfile_source)<EOL>print(\"<STR_LIT>\" % target_versionfile)<EOL>write_to_version_file(target_versionfile,<EOL>self._versioneer_generated_versions)<EOL><DEDENT><DEDENT>cmds[\"<STR_LIT>\"] = cmd_sdist<EOL>return cmds<EOL>", "docstring": "Get the custom setuptools/distutils subclasses used by Versioneer.", "id": "f10103:m21"}
{"signature": "def startlog(filename, overwrite=True):", "body": "if not filename:<EOL><INDENT>return <NUM_LIT:0><EOL><DEDENT>if overwrite:<EOL><INDENT>_mode = '<STR_LIT:w>'<EOL><DEDENT>else:<EOL><INDENT>_mode = '<STR_LIT:a>'<EOL><DEDENT>global _file_logger<EOL>_file_logger = logging.FileHandler(os.path.expanduser(filename), _mode)<EOL>_formatter = logging.Formatter('<STR_LIT>')<EOL>_file_logger.setFormatter(_formatter)<EOL>logger.addHandler(_file_logger)<EOL>if _ldtp_debug:<EOL><INDENT>_file_logger.setLevel(logging.DEBUG)<EOL><DEDENT>else:<EOL><INDENT>_file_logger.setLevel(logging.ERROR)<EOL><DEDENT>return <NUM_LIT:1><EOL>", "docstring": "@param filename: Start logging on the specified file\n@type filename: string\n@param overwrite: Overwrite or append\n    False - Append log to an existing file\n    True - Write log to a new file. If file already exist, \n    then erase existing file content and start log\n@type overwrite: boolean\n\n@return: 1 on success and 0 on error\n@rtype: integer", "id": "f10334:m5"}
{"signature": "def clubConsumables(self, fast=False):", "body": "method = '<STR_LIT:GET>'<EOL>url = '<STR_LIT>'<EOL>rc = self.__request__(method, url)<EOL>events = [self.pin.event('<STR_LIT>', '<STR_LIT>')]<EOL>self.pin.send(events, fast=fast)<EOL>events = [self.pin.event('<STR_LIT>', '<STR_LIT>')]<EOL>self.pin.send(events, fast=fast)<EOL>events = [self.pin.event('<STR_LIT>', '<STR_LIT>')]<EOL>self.pin.send(events, fast=fast)<EOL>return [itemParse(i) for i in rc.get('<STR_LIT>', ())]<EOL>", "docstring": "Return all consumables from club.", "id": "f3437:c0:m21"}
{"signature": "def topDownCompute(self, topDownIn=None):", "body": "output = numpy.zeros(self.numberOfColumns())<EOL>columns = [self.columnForCell(idx) for idx in self.getPredictiveCells()]<EOL>output[columns] = <NUM_LIT:1><EOL>return output<EOL>", "docstring": "(From `backtracking_tm.py`)\nTop-down compute - generate expected input given output of the TM\n\n@param topDownIn top down input from the level above us\n\n@returns best estimate of the TM input that would have generated bottomUpOut.", "id": "f17559:c4:m3"}
{"signature": "def setParameter(self, name, index, value):", "body": "if name == \"<STR_LIT>\":<EOL><INDENT>self.learningMode = bool(int(value))<EOL>self._epoch = <NUM_LIT:0><EOL><DEDENT>elif name == \"<STR_LIT>\":<EOL><INDENT>self._epoch = <NUM_LIT:0><EOL>if int(value) and not self.inferenceMode:<EOL><INDENT>self._finishLearning()<EOL><DEDENT>self.inferenceMode = bool(int(value))<EOL><DEDENT>elif name == \"<STR_LIT>\":<EOL><INDENT>self._knn.distanceNorm = value<EOL><DEDENT>elif name == \"<STR_LIT>\":<EOL><INDENT>self._knn.distanceMethod = value<EOL><DEDENT>elif name == \"<STR_LIT>\":<EOL><INDENT>self.keepAllDistances = bool(value)<EOL>if not self.keepAllDistances:<EOL><INDENT>if self._protoScores is not None and self._protoScores.shape[<NUM_LIT:0>] > <NUM_LIT:1>:<EOL><INDENT>self._protoScores = self._protoScores[-<NUM_LIT:1>,:]<EOL><DEDENT>if self._protoScores is not None:<EOL><INDENT>self._protoScoreCount = <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>self._protoScoreCount = <NUM_LIT:0><EOL><DEDENT><DEDENT><DEDENT>elif name == \"<STR_LIT>\":<EOL><INDENT>self.verbosity = value<EOL>self._knn.verbosity = value<EOL><DEDENT>else:<EOL><INDENT>return PyRegion.setParameter(self, name, index, value)<EOL><DEDENT>", "docstring": "Overrides :meth:`nupic.bindings.regions.PyRegion.PyRegion.setParameter`.", "id": "f17620:c0:m14"}
{"signature": "def explained_variance(ypred,y):", "body": "assert y.ndim == <NUM_LIT:1> and ypred.ndim == <NUM_LIT:1><EOL>vary = np.var(y)<EOL>return np.nan if vary==<NUM_LIT:0> else <NUM_LIT:1> - np.var(y-ypred)/vary<EOL>", "docstring": "Computes fraction of variance that ypred explains about y.\nReturns 1 - Var[y-ypred] / Var[y]\n\ninterpretation:\n    ev=0  =>  might as well have predicted zero\n    ev=1  =>  perfect prediction\n    ev<0  =>  worse than just predicting zero", "id": "f1341:m1"}
{"signature": "@staticmethod<EOL><INDENT>def align(val, align):<DEDENT>", "body": "return (val + align - <NUM_LIT:1>) & ~(align - <NUM_LIT:1>)<EOL>", "docstring": ":val: int\n:align: int\n:return: int", "id": "f1089:c1:m28"}
{"signature": "@staticmethod<EOL><INDENT>def prefetch_courses(persistent_course_grades):<DEDENT>", "body": "return CourseOverview.get_from_ids_if_exists(<EOL>[grade.course_id for grade in persistent_course_grades]<EOL>)<EOL>", "docstring": "Prefetch courses from the list of course_ids present in the persistent_course_grades.\n\nArguments:\n    persistent_course_grades (list): A list of PersistentCourseGrade.\n\nReturns:\n    (dict): A dictionary containing course_id to course_overview mapping.", "id": "f16208:c0:m6"}
{"signature": "def extr_lic(self, doc):", "body": "return doc.extracted_licenses[-<NUM_LIT:1>]<EOL>", "docstring": "Retrieves last license in extracted license list", "id": "f3754:c8:m1"}
{"signature": "def _SetAllFieldTypes(self, package, desc_proto, scope):", "body": "package = _PrefixWithDot(package)<EOL>main_desc = self._GetTypeFromScope(package, desc_proto.name, scope)<EOL>if package == '<STR_LIT:.>':<EOL><INDENT>nested_package = _PrefixWithDot(desc_proto.name)<EOL><DEDENT>else:<EOL><INDENT>nested_package = '<STR_LIT:.>'.join([package, desc_proto.name])<EOL><DEDENT>for field_proto, field_desc in zip(desc_proto.field, main_desc.fields):<EOL><INDENT>self._SetFieldType(field_proto, field_desc, nested_package, scope)<EOL><DEDENT>for extension_proto, extension_desc in (<EOL>zip(desc_proto.extension, main_desc.extensions)):<EOL><INDENT>extension_desc.containing_type = self._GetTypeFromScope(<EOL>nested_package, extension_proto.extendee, scope)<EOL>self._SetFieldType(extension_proto, extension_desc, nested_package, scope)<EOL><DEDENT>for nested_type in desc_proto.nested_type:<EOL><INDENT>self._SetAllFieldTypes(nested_package, nested_type, scope)<EOL><DEDENT>", "docstring": "Sets all the descriptor's fields's types.\n\n        This method also sets the containing types on any extensions.\n\n        Args:\n          package: The current package of desc_proto.\n          desc_proto: The message descriptor to update.\n          scope: Enclosing scope of available types.", "id": "f8666:c0:m16"}
{"signature": "def getMinPctOverlapDutyCycles(self):", "body": "return self._minPctOverlapDutyCycles<EOL>", "docstring": ":returns: (float) the minimum tolerated overlaps, given as percent of\n          neighbors overlap score", "id": "f17561:c4:m41"}
{"signature": "def GetParentControl(self) -> '<STR_LIT>':", "body": "ele = _AutomationClient.instance().ViewWalker.GetParentElement(self.Element)<EOL>return Control.CreateControlFromElement(ele)<EOL>", "docstring": "Return `Control` subclass or None.", "id": "f1782:c78:m50"}
{"signature": "def p_pkg_desc_2(self, p):", "body": "self.error = True<EOL>msg = ERROR_MESSAGES['<STR_LIT>'].format(p.lineno(<NUM_LIT:1>))<EOL>self.logger.log(msg)<EOL>", "docstring": "pkg_desc : PKG_DESC error", "id": "f3753:c0:m64"}
{"signature": "def set_connectionstyle(self, connectionstyle, **kw):", "body": "if connectionstyle==None:<EOL><INDENT>return ConnectionStyle.pprint_styles()<EOL><DEDENT>if isinstance(connectionstyle, ConnectionStyle._Base):<EOL><INDENT>self._connector = connectionstyle<EOL><DEDENT>elif callable(connectionstyle):<EOL><INDENT>self._connector = connectionstyle<EOL><DEDENT>else:<EOL><INDENT>self._connector = ConnectionStyle(connectionstyle, **kw)<EOL><DEDENT>", "docstring": "Set the connection style.\n\n*connectionstyle* can be a string with connectionstyle name with optional\n comma-separated attributes. Alternatively, the attrs can\n be probided as keywords.\n\n set_connectionstyle(\"arc,angleA=0,armA=30,rad=10\")\n set_connectionstyle(\"arc\", angleA=0,armA=30,rad=10)\n\nOld attrs simply are forgotten.\n\nWithout argument (or with connectionstyle=None), return\navailable styles as a list of strings.", "id": "f17197:c19:m5"}
{"signature": "def remove_event_detect(self, pin):", "body": "self.rpi_gpio.remove_event_detect(pin)<EOL>", "docstring": "Remove edge detection for a particular GPIO channel.  Pin should be\n        type IN.", "id": "f7998:c1:m6"}
{"signature": "@in_place_transformation<EOL>def remove_nodes_by_function_namespace(graph: BELGraph, func: str, namespace: Strings) -> None:", "body": "remove_filtered_nodes(graph, function_namespace_inclusion_builder(func, namespace))<EOL>", "docstring": "Remove nodes with the given function and namespace.\n\n    This might be useful to exclude information learned about distant species, such as excluding all information\n    from MGI and RGD in diseases where mice and rats don't give much insight to the human disease mechanism.", "id": "f9390:m4"}
{"signature": "def get(self, guild_id):", "body": "if guild_id not in self._players:<EOL><INDENT>p = self._player(lavalink=self.lavalink, guild_id=guild_id)<EOL>self._players[guild_id] = p<EOL><DEDENT>return self._players[guild_id]<EOL>", "docstring": "Returns a player from the cache, or creates one if it does not exist.", "id": "f6342:c3:m7"}
{"signature": "@property<EOL><INDENT>def type(self):<DEDENT>", "body": "return self.__class__.__name__<EOL>", "docstring": "The type (class name) of a derived JObject type", "id": "f11239:c0:m18"}
{"signature": "def __ne__(self, other):", "body": "return not self.__eq__(other)<EOL>", "docstring": "Not Equals", "id": "f9713:c2:m2"}
{"signature": "async def package_receiver(queue):", "body": "LOG.info(\"<STR_LIT>\")<EOL>while True:<EOL><INDENT>packet = await queue.get()<EOL>if packet is None:<EOL><INDENT>break<EOL><DEDENT>LOG.info(\"<STR_LIT>\", packet.framenumber)<EOL>header, cameras = packet.get_2d_markers()<EOL>LOG.info(\"<STR_LIT>\", header)<EOL>for i, camera in enumerate(cameras, <NUM_LIT:1>):<EOL><INDENT>LOG.info(\"<STR_LIT>\", i)<EOL>for marker in camera:<EOL><INDENT>LOG.info(\"<STR_LIT>\", marker)<EOL><DEDENT><DEDENT><DEDENT>LOG.info(\"<STR_LIT>\")<EOL>", "docstring": "Asynchronous function that processes queue until None is posted in queue", "id": "f8798:m0"}
{"signature": "def remove_user_from_acl(self, name, user):", "body": "if name not in self._acl:<EOL><INDENT>return False<EOL><DEDENT>if user in self._acl[name]['<STR_LIT>']:<EOL><INDENT>self._acl[name]['<STR_LIT>'].remove(user)<EOL><DEDENT>if user in self._acl[name]['<STR_LIT>']:<EOL><INDENT>self._acl[name]['<STR_LIT>'].remove(user)<EOL><DEDENT>return True<EOL>", "docstring": "Remove a user from the given acl (both allow and deny).", "id": "f13001:c0:m10"}
{"signature": "def _expand_str(path_cfg, alias_dict, overriding_kargs):", "body": "if path_cfg in alias_dict:<EOL><INDENT>return _expand_str_alias(path_cfg, alias_dict, overriding_kargs)<EOL><DEDENT>return _expand_for_lambda_str(path_cfg, alias_dict, overriding_kargs)<EOL>", "docstring": "expand a path config given as a string", "id": "f9023:m1"}
{"signature": "def convert_flatten(params, w_name, scope_name, inputs, layers, weights, names):", "body": "print('<STR_LIT>')<EOL>if names == '<STR_LIT>':<EOL><INDENT>tf_name = '<STR_LIT:R>' + random_string(<NUM_LIT:7>)<EOL><DEDENT>elif names == '<STR_LIT>':<EOL><INDENT>tf_name = w_name<EOL><DEDENT>else:<EOL><INDENT>tf_name = w_name + str(random.random())<EOL><DEDENT>reshape = keras.layers.Reshape([-<NUM_LIT:1>], name=tf_name)<EOL>layers[scope_name] = reshape(layers[inputs[<NUM_LIT:0>]])<EOL>", "docstring": "Convert reshape(view).\n\nArgs:\n    params: dictionary with layer parameters\n    w_name: name prefix in state_dict\n    scope_name: pytorch scope name\n    inputs: pytorch node inputs\n    layers: dictionary with keras tensors\n    weights: pytorch state_dict\n    names: use short names for keras layers", "id": "f5037:m0"}
{"signature": "@property<EOL><INDENT>def args(self):<DEDENT>", "body": "return self._impl[KEY]<EOL>", "docstring": "Return a tuple of the cells' arguments.", "id": "f13982:c3:m2"}
{"signature": "def sequence(self, line_data, child_type=None, reference=None):", "body": "<EOL>reference = reference or self.fasta_external or self.fasta_embedded<EOL>if not reference:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT>try:<EOL><INDENT>line_index = line_data['<STR_LIT>']<EOL><DEDENT>except TypeError:<EOL><INDENT>line_index = self.lines[line_data]['<STR_LIT>']<EOL><DEDENT>ld = self.lines[line_index]<EOL>if ld['<STR_LIT:type>'] != '<STR_LIT>':<EOL><INDENT>return None<EOL><DEDENT>seq = reference[ld['<STR_LIT>']][ld['<STR_LIT:start>']-<NUM_LIT:1>:ld['<STR_LIT:end>']]<EOL>if ld['<STR_LIT>'] == '<STR_LIT:->':<EOL><INDENT>seq = complement(seq[::-<NUM_LIT:1>])<EOL><DEDENT>return seq<EOL>", "docstring": "Get the sequence of line_data, according to the columns 'seqid', 'start', 'end', 'strand'.\nRequires fasta reference.\nWhen used on 'mRNA' type line_data, child_type can be used to specify which kind of sequence to return:\n* child_type=None:  pre-mRNA, returns the sequence of line_data from start to end, reverse complement according to strand. (default)\n* child_type='exon':  mature mRNA, concatenates the sequences of children type 'exon'.\n* child_type='CDS':  coding sequence, concatenates the sequences of children type 'CDS'. Use the helper\n                     function translate(seq) on the returned value to obtain the protein sequence.\n\n:param line_data: line_data(dict) with line_data['line_index'] or line_index(int)\n:param child_type: None or feature type(string)\n:param reference: If None, will use self.fasta_external or self.fasta_embedded(dict)\n:return: sequence(string)", "id": "f5359:c0:m16"}
{"signature": "def get_ip_number(self):", "body": "return self._ip_num<EOL>", "docstring": "Return the number of usable IP addresses.", "id": "f3601:c3:m11"}
{"signature": "def checkplot_infokey_worker(task):", "body": "cpf, keys = task<EOL>cpd = _read_checkplot_picklefile(cpf)<EOL>resultkeys = []<EOL>for k in keys:<EOL><INDENT>try:<EOL><INDENT>resultkeys.append(_dict_get(cpd, k))<EOL><DEDENT>except Exception as e:<EOL><INDENT>resultkeys.append(np.nan)<EOL><DEDENT><DEDENT>return resultkeys<EOL>", "docstring": "This gets the required keys from the requested file.\n\n    Parameters\n    ----------\n\n    task : tuple\n        Task is a two element tuple::\n\n        - task[0] is the dict to work on\n\n        - task[1] is a list of lists of str indicating all the key address to\n          extract items from the dict for\n\n    Returns\n    -------\n\n    list\n        This is a list of all of the items at the requested key addresses.", "id": "f14713:m1"}
{"signature": "def read(self, max_length):", "body": "if not isinstance(max_length, int_types):<EOL><INDENT>raise TypeError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>type_name(max_length)<EOL>))<EOL><DEDENT>buffered_length = len(self._decrypted_bytes)<EOL>if buffered_length >= max_length:<EOL><INDENT>output = self._decrypted_bytes[<NUM_LIT:0>:max_length]<EOL>self._decrypted_bytes = self._decrypted_bytes[max_length:]<EOL>return output<EOL><DEDENT>if self._ssl is None:<EOL><INDENT>self._raise_closed()<EOL><DEDENT>if buffered_length > <NUM_LIT:0> and not self.select_read(<NUM_LIT:0>):<EOL><INDENT>output = self._decrypted_bytes<EOL>self._decrypted_bytes = b'<STR_LIT>'<EOL>return output<EOL><DEDENT>to_read = min(self._buffer_size, max_length - buffered_length)<EOL>output = self._decrypted_bytes<EOL>again = True<EOL>while again:<EOL><INDENT>again = False<EOL>result = libssl.SSL_read(self._ssl, self._read_buffer, to_read)<EOL>self._raw_write()<EOL>if result <= <NUM_LIT:0>:<EOL><INDENT>error = libssl.SSL_get_error(self._ssl, result)<EOL>if error == LibsslConst.SSL_ERROR_WANT_READ:<EOL><INDENT>if self._raw_read() != b'<STR_LIT>':<EOL><INDENT>again = True<EOL>continue<EOL><DEDENT>raise_disconnection()<EOL><DEDENT>elif error == LibsslConst.SSL_ERROR_WANT_WRITE:<EOL><INDENT>self._raw_write()<EOL>again = True<EOL>continue<EOL><DEDENT>elif error == LibsslConst.SSL_ERROR_ZERO_RETURN:<EOL><INDENT>self._gracefully_closed = True<EOL>self._shutdown(False)<EOL>break<EOL><DEDENT>else:<EOL><INDENT>handle_openssl_error(<NUM_LIT:0>, TLSError)<EOL><DEDENT><DEDENT>output += bytes_from_buffer(self._read_buffer, result)<EOL><DEDENT>if self._gracefully_closed and len(output) == <NUM_LIT:0>:<EOL><INDENT>self._raise_closed()<EOL><DEDENT>self._decrypted_bytes = output[max_length:]<EOL>return output[<NUM_LIT:0>:max_length]<EOL>", "docstring": "Reads data from the TLS-wrapped socket\n\n:param max_length:\n    The number of bytes to read - output may be less than this\n\n:raises:\n    socket.socket - when a non-TLS socket error occurs\n    oscrypto.errors.TLSError - when a TLS-related error occurs\n    ValueError - when any of the parameters contain an invalid value\n    TypeError - when any of the parameters are of the wrong type\n    OSError - when an error is returned by the OS crypto library\n\n:return:\n    A byte string of the data read", "id": "f9532:c1:m5"}
{"signature": "def fit_lens_data_with_sensitivity_tracers(lens_data, tracer_normal, tracer_sensitive):", "body": "if (tracer_normal.has_light_profile and tracer_sensitive.has_light_profile) and(not tracer_normal.has_pixelization and not tracer_sensitive.has_pixelization):<EOL><INDENT>return SensitivityProfileFit(lens_data=lens_data, tracer_normal=tracer_normal,<EOL>tracer_sensitive=tracer_sensitive)<EOL><DEDENT>elif (not tracer_normal.has_light_profile and not tracer_sensitive.has_light_profile) and(tracer_normal.has_pixelization and tracer_sensitive.has_pixelization):<EOL><INDENT>return SensitivityInversionFit(lens_data=lens_data, tracer_normal=tracer_normal,<EOL>tracer_sensitive=tracer_sensitive)<EOL><DEDENT>else:<EOL><INDENT>raise exc.FittingException('<STR_LIT>'<EOL>'<STR_LIT>')<EOL><DEDENT>", "docstring": "Fit lens data with a normal tracer and sensitivity tracer, to determine our sensitivity to a selection of \\ \n    galaxy components. This factory automatically determines the type of fit based on the properties of the galaxies \\\n    in the tracers.\n\n    Parameters\n    -----------\n    lens_data : lens_data.LensData or lens_data.LensDataHyper\n        The lens-images that is fitted.\n    tracer_normal : ray_tracing.AbstractTracer\n        A tracer whose galaxies have the same model components (e.g. light profiles, mass profiles) as the \\\n        lens data that we are fitting.\n    tracer_sensitive : ray_tracing.AbstractTracerNonStack\n        A tracer whose galaxies have the same model components (e.g. light profiles, mass profiles) as the \\\n        lens data that we are fitting, but also addition components (e.g. mass clumps) which we measure \\\n        how sensitive we are too.", "id": "f5980:m0"}
{"signature": "def update_languages(self):", "body": "language_fields = record_get_field_instances(self.record, '<STR_LIT>')<EOL>language = \"<STR_LIT>\"<EOL>record_delete_fields(self.record, \"<STR_LIT>\")<EOL>for field in language_fields:<EOL><INDENT>subs = field_get_subfields(field)<EOL>if '<STR_LIT:a>' in subs:<EOL><INDENT>language = self.get_config_item(subs['<STR_LIT:a>'][<NUM_LIT:0>], \"<STR_LIT>\")<EOL>break<EOL><DEDENT><DEDENT>new_subs = [('<STR_LIT:a>', language)]<EOL>record_add_field(self.record, \"<STR_LIT>\", subfields=new_subs)<EOL>", "docstring": "041 Language.", "id": "f7926:c0:m27"}
{"signature": "def GetPropertyValue(self, propertyId: int) -> Any:", "body": "return self.Element.GetCurrentPropertyValue(propertyId)<EOL>", "docstring": "Call IUIAutomationElement::GetCurrentPropertyValue.\npropertyId: int, a value in class `PropertyId`.\nReturn Any, corresponding type according to propertyId.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationelement-getcurrentpropertyvalue", "id": "f1782:c78:m41"}
{"signature": "def on_panel_state_changed(self):", "body": "action = self.sender()<EOL>action.panel.enabled = action.isChecked()<EOL>action.panel.setVisible(action.isChecked())<EOL>", "docstring": "Enable disable the selected panel.", "id": "f666:c0:m23"}
{"signature": "def main():", "body": "parser = argparse.ArgumentParser(description = '<STR_LIT>',<EOL>parents = [XMPPSettings.get_arg_parser()])<EOL>parser.add_argument('<STR_LIT:source>', metavar = '<STR_LIT>', <EOL>help = '<STR_LIT>')<EOL>parser.add_argument('<STR_LIT:target>', metavar = '<STR_LIT>', nargs = '<STR_LIT:?>',<EOL>help = '<STR_LIT>')<EOL>parser.add_argument('<STR_LIT>',<EOL>action = '<STR_LIT>', dest = '<STR_LIT>',<EOL>const = logging.DEBUG, default = logging.INFO,<EOL>help = '<STR_LIT>')<EOL>parser.add_argument('<STR_LIT>', const = logging.ERROR,<EOL>action = '<STR_LIT>', dest = '<STR_LIT>',<EOL>help = '<STR_LIT>')<EOL>args = parser.parse_args()<EOL>settings = XMPPSettings()<EOL>settings.load_arguments(args)<EOL>if settings.get(\"<STR_LIT:password>\") is None:<EOL><INDENT>password = getpass(\"<STR_LIT>\".format(args.source))<EOL>if sys.version_info.major < <NUM_LIT:3>:<EOL><INDENT>password = password.decode(\"<STR_LIT:utf-8>\")<EOL><DEDENT>settings[\"<STR_LIT:password>\"] = password<EOL><DEDENT>if sys.version_info.major < <NUM_LIT:3>:<EOL><INDENT>args.source = args.source.decode(\"<STR_LIT:utf-8>\")<EOL><DEDENT>source = JID(args.source)<EOL>if args.target:<EOL><INDENT>if sys.version_info.major < <NUM_LIT:3>:<EOL><INDENT>args.target = args.target.decode(\"<STR_LIT:utf-8>\")<EOL><DEDENT>target = JID(args.target)<EOL><DEDENT>else:<EOL><INDENT>target = JID(source.domain)<EOL><DEDENT>logging.basicConfig(level = args.log_level)<EOL>checker = VersionChecker(source, target, settings)<EOL>try:<EOL><INDENT>checker.run()<EOL><DEDENT>except KeyboardInterrupt:<EOL><INDENT>checker.disconnect()<EOL><DEDENT>", "docstring": "Parse the command-line arguments and run the tool.", "id": "f15229:m0"}
{"signature": "def open(self, member, pwd=None):", "body": "if isinstance(member, RarInfo):<EOL><INDENT>member = member.filename<EOL><DEDENT>archive = unrarlib.RAROpenArchiveDataEx(<EOL>self.filename, mode=constants.RAR_OM_EXTRACT)<EOL>handle = self._open(archive)<EOL>password = pwd or self.pwd<EOL>if password is not None:<EOL><INDENT>unrarlib.RARSetPassword(handle, b(password))<EOL><DEDENT>data = _ReadIntoMemory()<EOL>c_callback = unrarlib.UNRARCALLBACK(data._callback)<EOL>unrarlib.RARSetCallback(handle, c_callback, <NUM_LIT:0>)<EOL>try:<EOL><INDENT>rarinfo = self._read_header(handle)<EOL>while rarinfo is not None:<EOL><INDENT>if rarinfo.filename == member:<EOL><INDENT>self._process_current(handle, constants.RAR_TEST)<EOL>break<EOL><DEDENT>else:<EOL><INDENT>self._process_current(handle, constants.RAR_SKIP)<EOL><DEDENT>rarinfo = self._read_header(handle)<EOL><DEDENT>if rarinfo is None:<EOL><INDENT>data = None<EOL><DEDENT><DEDENT>except unrarlib.MissingPassword:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\")<EOL><DEDENT>except unrarlib.BadPassword:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\")<EOL><DEDENT>except unrarlib.BadDataError:<EOL><INDENT>if password is not None:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\")<EOL><DEDENT>else:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\")<EOL><DEDENT><DEDENT>except unrarlib.UnrarException as e:<EOL><INDENT>raise BadRarFile(\"<STR_LIT>\" % str(e))<EOL><DEDENT>finally:<EOL><INDENT>self._close(handle)<EOL><DEDENT>if data is None:<EOL><INDENT>raise KeyError('<STR_LIT>' % member)<EOL><DEDENT>return data.get_bytes()<EOL>", "docstring": "Return file-like object for 'member'.\n\n           'member' may be a filename or a RarInfo object.", "id": "f12640:c3:m6"}
{"signature": "def logExceptions(logger=None):", "body": "logger = (logger if logger is not None else logging.getLogger(__name__))<EOL>def exceptionLoggingDecorator(func):<EOL><INDENT>@functools.wraps(func)<EOL>def exceptionLoggingWrap(*args, **kwargs):<EOL><INDENT>try:<EOL><INDENT>return func(*args, **kwargs)<EOL><DEDENT>except:<EOL><INDENT>logger.exception(<EOL>\"<STR_LIT>\",<EOL>sys.exc_info()[<NUM_LIT:1>], func, '<STR_LIT>'.join(traceback.format_stack()), )<EOL>raise<EOL><DEDENT><DEDENT>return exceptionLoggingWrap<EOL><DEDENT>return exceptionLoggingDecorator<EOL>", "docstring": "Returns a closure suitable for use as function/method decorator for\n    logging exceptions that leave the scope of the decorated function. Exceptions\n    are logged at ERROR level.\n\n    logger:    user-supplied logger instance. Defaults to logging.getLogger.\n\n    Usage Example:\n      NOTE: logging must be initialized *before* any loggers are created, else\n        there will be no output; see nupic.support.initLogging()\n\n      @logExceptions()\n      def myFunctionFoo():\n          ...\n          raise RuntimeError(\"something bad happened\")\n          ...", "id": "f17643:m0"}
{"signature": "def does_not_match(self, pattern):", "body": "if not isinstance(self.val, str_types):<EOL><INDENT>raise TypeError('<STR_LIT>')<EOL><DEDENT>if not isinstance(pattern, str_types):<EOL><INDENT>raise TypeError('<STR_LIT>')<EOL><DEDENT>if len(pattern) == <NUM_LIT:0>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if re.search(pattern, self.val) is not None:<EOL><INDENT>self._err('<STR_LIT>' % (self.val, pattern))<EOL><DEDENT>return self<EOL>", "docstring": "Asserts that val is string and does not match regex pattern.", "id": "f9307:c0:m47"}
{"signature": "@path_required<EOL><INDENT>def rename_directory(self, relativePath, newName, raiseError=True, ntrials=<NUM_LIT:3>):<DEDENT>", "body": "assert isinstance(raiseError, bool), \"<STR_LIT>\"<EOL>assert isinstance(ntrials, int), \"<STR_LIT>\"<EOL>assert ntrials><NUM_LIT:0>, \"<STR_LIT>\"<EOL>relativePath = self.to_repo_relative_path(path=relativePath, split=False)<EOL>parentPath, dirName = os.path.split(relativePath)<EOL>if relativePath == '<STR_LIT>':<EOL><INDENT>error = \"<STR_LIT>\"<EOL>assert not raiseError, error<EOL>return False, error<EOL><DEDENT>realPath = os.path.join(self.__path,relativePath)<EOL>newRealPath = os.path.join(os.path.dirname(realPath), newName)<EOL>if os.path.isdir(newRealPath):<EOL><INDENT>error = \"<STR_LIT>\"%(newRealPath,)<EOL>assert not raiseError, error<EOL>return False, error<EOL><DEDENT>LD =  Locker(filePath=None, lockPass=str(uuid.uuid1()), lockPath=os.path.join(self.__path,parentPath, self.__dirLock))<EOL>acquired, code = LD.acquire_lock()<EOL>if not acquired:<EOL><INDENT>error = \"<STR_LIT>\"%(code,dirPath)<EOL>assert not raiseError, error<EOL>return False, error<EOL><DEDENT>error = None<EOL>LR =  Locker(filePath=None, lockPass=str(uuid.uuid1()), lockPath=os.path.join(self.__path, self.__repoLock))<EOL>acquired, code = LR.acquire_lock()<EOL>if not acquired:<EOL><INDENT>LD.release_lock()<EOL>m = \"<STR_LIT>\"%(code,dirPath)<EOL>assert raiseError,  Exception(m)<EOL>return False,m<EOL><DEDENT>for _trial in range(ntrials):<EOL><INDENT>try:<EOL><INDENT>repo = self.__load_repository_pickle_file(os.path.join(self.__path, self.__repoFile))<EOL>self.__repo['<STR_LIT>'] = repo['<STR_LIT>']<EOL><DEDENT>except Exception as err:<EOL><INDENT>error = str(err)<EOL>if self.DEBUG_PRINT_FAILED_TRIALS: print(\"<STR_LIT>\"%(_trial, inspect.stack()[<NUM_LIT:1>][<NUM_LIT:3>], str(error)))<EOL><DEDENT>else:<EOL><INDENT>error = None<EOL>break<EOL><DEDENT><DEDENT>if error is not None:<EOL><INDENT>LD.release_lock()<EOL>LR.release_lock()<EOL>assert not raiseError, Exception(error)<EOL>return False, error<EOL><DEDENT>for _trial in range(ntrials):<EOL><INDENT>error = None<EOL>try:<EOL><INDENT>dirList = self.__get_repository_parent_directory(relativePath=relativePath)<EOL>assert dirList is not None, \"<STR_LIT>\"%(relativePath,)<EOL>_dirDict = [nd for nd in dirList  if isinstance(nd,dict)]<EOL>_dirDict = [nd for nd in _dirDict if dirName in nd]<EOL>assert len(_dirDict) == <NUM_LIT:1>, \"<STR_LIT>\"<EOL>os.rename(realPath, newRealPath)<EOL>_dirDict[<NUM_LIT:0>][newName] = _dirDict[<NUM_LIT:0>][dirName]<EOL>_dirDict[<NUM_LIT:0>].pop(dirName)<EOL>self.__save_dirinfo(description=None, dirInfoPath=parentPath, create=False)<EOL><DEDENT>except Exception as err:<EOL><INDENT>error = str(err)<EOL>if self.DEBUG_PRINT_FAILED_TRIALS: print(\"<STR_LIT>\"%(_trial, inspect.stack()[<NUM_LIT:1>][<NUM_LIT:3>], str(error)))<EOL><DEDENT>else:<EOL><INDENT>error = None<EOL>break<EOL><DEDENT><DEDENT>if error is None:<EOL><INDENT>_, error = self.__save_repository_pickle_file(lockFirst=False, raiseError=False)<EOL><DEDENT>LR.release_lock()<EOL>LD.release_lock()<EOL>assert error is None or not raiseError, \"<STR_LIT>\"%(relativePath, newName, ntrials, error,)<EOL>return error is None, error<EOL>", "docstring": "Rename a directory in the repository. It insures renaming the directory in the system.\n\n:Parameters:\n    #. relativePath (string): The relative to the repository path of\n       the directory to be renamed.\n    #. newName (string): The new directory name.\n    #. raiseError (boolean): Whether to raise encountered error instead\n       of returning failure.\n    #. ntrials (int): After aquiring all locks, ntrials is the maximum\n       number of trials allowed before failing.\n       In rare cases, when multiple processes\n       are accessing the same repository components, different processes\n       can alter repository components between successive lock releases\n       of some other process. Bigger number of trials lowers the\n       likelyhood of failure due to multiple processes same time\n       alteration.\n\n:Returns:\n    #. success (boolean): Whether renaming the directory was successful.\n    #. message (None, string): Some explanatory message or error reason\n       why directory was not renamed.", "id": "f13917:c1:m37"}
{"signature": "def paragraphs(quantity=<NUM_LIT:2>, separator='<STR_LIT>', wrap_start='<STR_LIT>', wrap_end='<STR_LIT>',<EOL>html=False, sentences_quantity=<NUM_LIT:3>, as_list=False):", "body": "if html:<EOL><INDENT>wrap_start = '<STR_LIT>'<EOL>wrap_end = '<STR_LIT>'<EOL>separator = '<STR_LIT>'<EOL><DEDENT>result = []<EOL>try:<EOL><INDENT>for _ in xrange(<NUM_LIT:0>, quantity):<EOL><INDENT>result.append(wrap_start +<EOL>sentences(sentences_quantity) +<EOL>wrap_end)<EOL><DEDENT><DEDENT>except NameError:<EOL><INDENT>for _ in range(<NUM_LIT:0>, quantity):<EOL><INDENT>result.append(wrap_start +<EOL>sentences(sentences_quantity) +<EOL>wrap_end)<EOL><DEDENT><DEDENT>if as_list:<EOL><INDENT>return result<EOL><DEDENT>else:<EOL><INDENT>return separator.join(result)<EOL><DEDENT>", "docstring": "Return random paragraphs.", "id": "f14650:m6"}
{"signature": "def to_int(data):", "body": "if not isinstance(data, basestring):<EOL><INDENT>raise TypeError('<STR_LIT>')<EOL><DEDENT>res = <NUM_LIT:0><EOL>for part in data.split('<STR_LIT:->'):<EOL><INDENT>if len(part) != <NUM_LIT:5>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>for j in range(<NUM_LIT:5>):<EOL><INDENT>try:<EOL><INDENT>if not j % <NUM_LIT:2>:<EOL><INDENT>res <<= <NUM_LIT:4><EOL>res |= CONSONANTS.index(part[j])<EOL><DEDENT>else:<EOL><INDENT>res <<= <NUM_LIT:2><EOL>res |= VOWELS.index(part[j])<EOL><DEDENT><DEDENT>except ValueError:<EOL><INDENT>raise ValueError('<STR_LIT>'.format(part[j]))<EOL><DEDENT><DEDENT><DEDENT>return res<EOL>", "docstring": ":params data: proquint\n:returns: proquint decoded into an integer\n:type data: string\n:rtype: int", "id": "f6299:m1"}
{"signature": "def _makeAdapter(self):", "body": "self._callHooks('<STR_LIT>')<EOL>context = pysyncml.Context(storage='<STR_LIT>' % (self.dataDir,),<EOL>owner=None, autoCommit=True)<EOL>self._callHooks('<STR_LIT>', context)<EOL>adapter = context.Adapter()<EOL>if hasattr(self, '<STR_LIT>') and self.serverConf.policy is not None:<EOL><INDENT>adapter.conflictPolicy = self.serverConf.policy<EOL><DEDENT>if self.options.name is not None or self.appDisplay is not None:<EOL><INDENT>adapter.name = self.options.name or self.appDisplay<EOL><DEDENT>if adapter.devinfo is None:<EOL><INDENT>log.info('<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>if self.options.devid is not None and self.options.devid != adapter.devinfo.devID:<EOL><INDENT>log.info('<STR_LIT>')<EOL>adapter.devinfo = None<EOL><DEDENT><DEDENT>if adapter.devinfo is None:<EOL><INDENT>devinfoParams = dict(<EOL>devID             = self.options.devid or self.defaultDevID,<EOL>devType           = pysyncml.DEVTYPE_SERVER if self.options.server elsepysyncml.DEVTYPE_WORKSTATION,<EOL>manufacturerName  = '<STR_LIT>',<EOL>modelName         = self.appLabel,<EOL>softwareVersion   = pysyncml.version,<EOL>hierarchicalSync  = self.agent.hierarchicalSync if self.agent is not None else False,<EOL>)<EOL>if self.devinfoParams is not None:<EOL><INDENT>devinfoParams.update(self.devinfoParams)<EOL><DEDENT>adapter.devinfo = context.DeviceInfo(**devinfoParams)<EOL><DEDENT>self._callHooks('<STR_LIT>', context, adapter)<EOL>if not self.options.server:<EOL><INDENT>if adapter.peer is None:<EOL><INDENT>if self.options.remote is None:<EOL><INDENT>self.options.remote = input('<STR_LIT>')<EOL>if self.options.username is None:<EOL><INDENT>self.options.username = input('<STR_LIT>')<EOL>if len(self.options.username) <= <NUM_LIT:0>:<EOL><INDENT>self.options.username = None<EOL><DEDENT><DEDENT><DEDENT>log.info('<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>if self.options.remote is not None:<EOL><INDENT>if self.options.remote != adapter.peer.urlor self.options.username != adapter.peer.usernameor self.options.password != adapter.peer.password:<EOL><INDENT>log.info('<STR_LIT>')<EOL>adapter.peer = None<EOL><DEDENT><DEDENT><DEDENT>if adapter.peer is None:<EOL><INDENT>auth = None<EOL>if self.options.username is not None:<EOL><INDENT>auth = pysyncml.NAMESPACE_AUTH_BASIC<EOL>if self.options.password is None:<EOL><INDENT>self.options.password = getpass.getpass('<STR_LIT>')<EOL><DEDENT><DEDENT>adapter.peer = context.RemoteAdapter(<EOL>url      = self.options.remote,<EOL>auth     = auth,<EOL>username = self.options.username,<EOL>password = self.options.password,<EOL>)<EOL><DEDENT>self._callHooks('<STR_LIT>', context, adapter, adapter.peer)<EOL><DEDENT>uri = self.storeParams.get('<STR_LIT>', self.appLabel)<EOL>if uri in adapter.stores:<EOL><INDENT>store = adapter.stores[uri]<EOL>store.agent = self.agent<EOL><DEDENT>else:<EOL><INDENT>storeParams = dict(<EOL>uri         = uri,<EOL>displayName = self.options.name or self.appDisplay,<EOL>agent       = self.agent,<EOL>maxObjSize  = None)<EOL>if self.storeParams is not None:<EOL><INDENT>storeParams.update(self.storeParams)<EOL><DEDENT>store = adapter.addStore(context.Store(**storeParams))<EOL><DEDENT>self._callHooks('<STR_LIT>', context, adapter, store)<EOL>if self.options.local:<EOL><INDENT>def locprint(msg):<EOL><INDENT>print(msg)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>locprint = log.info<EOL><DEDENT>def showChanges(changes, prefix):<EOL><INDENT>for c in changes:<EOL><INDENT>if c.state != pysyncml.ITEM_DELETED:<EOL><INDENT>item = self.agent.getItem(c.itemID)<EOL><DEDENT>else:<EOL><INDENT>item = '<STR_LIT>' % (c.itemID,)<EOL><DEDENT>locprint('<STR_LIT>' % (prefix, item, pysyncml.state2string(c.state)))<EOL><DEDENT><DEDENT>if self.options.server:<EOL><INDENT>peers = adapter.getKnownPeers()<EOL>if len(peers) > <NUM_LIT:0>:<EOL><INDENT>locprint('<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>locprint('<STR_LIT>')<EOL><DEDENT>for peer in peers:<EOL><INDENT>for puri, pstore in list(peer.stores.items()):<EOL><INDENT>if pstore.binding is None or pstore.binding.uri != store.uri:<EOL><INDENT>continue<EOL><DEDENT>changes = list(pstore.getRegisteredChanges())<EOL>if len(changes) <= <NUM_LIT:0>:<EOL><INDENT>locprint('<STR_LIT>' % (peer.devID, puri))<EOL><DEDENT>else:<EOL><INDENT>locprint('<STR_LIT>' % (peer.devID, puri))<EOL><DEDENT>showChanges(changes, '<STR_LIT:U+0020>')<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>if store.peer is None:<EOL><INDENT>locprint('<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>changes = list(store.peer.getRegisteredChanges())<EOL>if len(changes) <= <NUM_LIT:0>:<EOL><INDENT>locprint('<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>locprint('<STR_LIT>')<EOL><DEDENT>showChanges(changes, '<STR_LIT>')<EOL><DEDENT><DEDENT>self._callHooks('<STR_LIT>', context, adapter)<EOL>return (context, adapter)<EOL>", "docstring": "Creates a tuple of ( Context, Adapter ) based on the options\nspecified by `self.options`. The Context is the pysyncml.Context created for\nthe storage location specified in `self.options`, and the Adapter is a newly\ncreated Adapter if a previously created one was not found.", "id": "f12363:c1:m10"}
{"signature": "@app.route('<STR_LIT>', methods=['<STR_LIT:POST>'])<EOL>def __entry_point():", "body": "ip = request.remote_addr<EOL>ua = request.headers.get('<STR_LIT>', '<STR_LIT>')<EOL>method = request.json.get('<STR_LIT>')<EOL>if method is None:<EOL><INDENT>__query_logging(ip, ua, method, level='<STR_LIT>')<EOL>return json.dumps({'<STR_LIT:error>': '<STR_LIT>'})<EOL><DEDENT>if method not in authorized_methods:<EOL><INDENT>__query_logging(ip, ua, method, level='<STR_LIT>')<EOL>return json.dumps({'<STR_LIT:error>': '<STR_LIT>'})<EOL><DEDENT>fct = globals().get(method)<EOL>if fct is None:<EOL><INDENT>__query_logging(ip, ua, method, level='<STR_LIT>')<EOL>return json.dumps({'<STR_LIT:error>': '<STR_LIT>'})<EOL><DEDENT>if request.json.get('<STR_LIT>') is None:<EOL><INDENT>__query_logging(ip, ua, method, level='<STR_LIT>')<EOL>return json.dumps({'<STR_LIT:error>': '<STR_LIT>'})<EOL><DEDENT>try:<EOL><INDENT>result = fct(request.json)<EOL>__query_logging(ip, ua, method, request.json.get('<STR_LIT>'),<EOL>request.json.get('<STR_LIT>'), request.json.get('<STR_LIT>'))<EOL>return result<EOL><DEDENT>except Exception:<EOL><INDENT>__query_logging(ip, ua, method, request.json.get('<STR_LIT>'), level='<STR_LIT:error>')<EOL>return json.dumps({'<STR_LIT:error>': '<STR_LIT>'})<EOL><DEDENT>", "docstring": "Function called when an query is made on /json. Expects a JSON\nobject with at least a 'method' entry.", "id": "f3650:m2"}
{"signature": "def set_width(self, w):", "body": "self._width = w<EOL>", "docstring": "Set the width rectangle\n\nACCEPTS: float", "id": "f17197:c2:m14"}
{"signature": "def strip_fit(self, **kwargs):", "body": "kwargs.setdefault('<STR_LIT>', '<STR_LIT>')<EOL>kw_fit = {}<EOL>for k in ('<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT:input>'):<EOL><INDENT>if k in kwargs:<EOL><INDENT>kw_fit[k] = kwargs.pop(k)<EOL><DEDENT><DEDENT>kwargs['<STR_LIT:input>'] = kwargs.pop('<STR_LIT>', ['<STR_LIT>'])<EOL>kwargs['<STR_LIT>'] = kw_fit['<STR_LIT>'] = kwargs.pop('<STR_LIT>', self.force)<EOL>paths = self.strip_water(**kwargs)    <EOL>transformer_nowater = self.nowater[paths['<STR_LIT>']]  <EOL>return transformer_nowater.fit(**kw_fit)          <EOL>", "docstring": "Strip water and fit to the remaining system.\n\n        First runs :meth:`strip_water` and then :meth:`fit`; see there\n        for arguments.\n\n        - *strip_input* is used for :meth:`strip_water` (but is only useful in\n          special cases, e.g. when there is no Protein group defined. Then set\n          *strip_input* = ``['Other']``.\n\n        - *input* is passed on to :meth:`fit` and can contain the\n          ``[center_group, fit_group, output_group]``\n\n        - *fitgroup* is only passed to :meth:`fit` and just contains\n          the group to fit to (\"backbone\" by default)\n\n          .. warning:: *fitgroup* can only be a Gromacs default group and not\n                       a custom group (because the indices change after stripping)\n\n        - By default *fit* = \"rot+trans\" (and *fit* is passed to :meth:`fit`,\n          together with the *xy* = ``False`` keyword)\n\n        .. Note:: The call signature of :meth:`strip_water` is somewhat different from this one.", "id": "f6863:c2:m8"}
{"signature": "@staticmethod<EOL><INDENT>def get_queries(parameters):<DEDENT>", "body": "parsed_params = urlparse.parse_qs(parameters)<EOL>if '<STR_LIT:q>' not in parsed_params:<EOL><INDENT>return []<EOL><DEDENT>queries = parsed_params['<STR_LIT:q>']<EOL>if not isinstance(queries, list):<EOL><INDENT>queries = [queries]<EOL><DEDENT>return queries<EOL>", "docstring": "Get a list of all queries (q=... parameters) from an URL parameter string\n:param parameters: The url parameter list", "id": "f1397:c0:m6"}
{"signature": "def elemDisappearedCallback(retelem, obj, **kwargs):", "body": "return not obj.findFirstR(**kwargs)<EOL>", "docstring": "Callback for checking if a UI element is no longer onscreen.\n\n    kwargs should contains some unique set of identifier (e.g. title/value, role)\n    Returns:  Boolean", "id": "f10336:m0"}
{"signature": "def save_reduce(self, func, args, state=None, <EOL>listitems=None, dictitems=None, obj=None):", "body": "<EOL>if not isinstance(args, tuple):<EOL><INDENT>raise pickle.PicklingError(\"<STR_LIT>\")<EOL><DEDENT>if not hasattr(func, '<STR_LIT>'):<EOL><INDENT>raise pickle.PicklingError(\"<STR_LIT>\")<EOL><DEDENT>save = self.save<EOL>write = self.write<EOL>if self.proto >= <NUM_LIT:2> and getattr(func, \"<STR_LIT>\", \"<STR_LIT>\") == \"<STR_LIT>\":<EOL><INDENT>cls = args[<NUM_LIT:0>]<EOL>if not hasattr(cls, \"<STR_LIT>\"):<EOL><INDENT>raise pickle.PicklingError(<EOL>\"<STR_LIT>\")<EOL><DEDENT>if obj is not None and cls is not obj.__class__:<EOL><INDENT>raise pickle.PicklingError(<EOL>\"<STR_LIT>\")<EOL><DEDENT>args = args[<NUM_LIT:1>:]<EOL>save(cls)<EOL>if hasattr(obj, '<STR_LIT>'):<EOL><INDENT>transient = obj.__transient__<EOL>state = state.copy()<EOL>for k in list(state.keys()):<EOL><INDENT>if k in transient:<EOL><INDENT>del state[k]<EOL><DEDENT><DEDENT><DEDENT>save(args)<EOL>write(pickle.NEWOBJ)<EOL><DEDENT>else:<EOL><INDENT>save(func)<EOL>save(args)<EOL>write(pickle.REDUCE)<EOL><DEDENT>if obj is not None:<EOL><INDENT>self.memoize(obj)<EOL><DEDENT>if listitems is not None:<EOL><INDENT>self._batch_appends(listitems)<EOL><DEDENT>if dictitems is not None:<EOL><INDENT>self._batch_setitems(dictitems)<EOL><DEDENT>if state is not None:<EOL><INDENT>save(state)<EOL>write(pickle.BUILD)<EOL><DEDENT>", "docstring": "Modified to support __transient__ on new objects\n        Change only affects protocol level 2 (which is always used by PiCloud", "id": "f7244:c0:m18"}
{"signature": "def write_then_readinto(self, out_buffer, in_buffer, *,<EOL>out_start=<NUM_LIT:0>, out_end=None, in_start=<NUM_LIT:0>, in_end=None, stop=True):", "body": "if out_end is None:<EOL><INDENT>out_end = len(out_buffer)<EOL><DEDENT>if in_end is None:<EOL><INDENT>in_end = len(in_buffer)<EOL><DEDENT>if hasattr(self.i2c, '<STR_LIT>'):<EOL><INDENT>if self._debug:<EOL><INDENT>print(\"<STR_LIT>\",<EOL>[hex(i) for i in out_buffer[out_start:out_end]])<EOL><DEDENT>self.i2c.writeto_then_readfrom(self.device_address, out_buffer, in_buffer,<EOL>out_start=out_start, out_end=out_end,<EOL>in_start=in_start, in_end=in_end, stop=stop)<EOL>if self._debug:<EOL><INDENT>print(\"<STR_LIT>\",<EOL>[hex(i) for i in in_buffer[in_start:in_end]])<EOL><DEDENT><DEDENT>else:<EOL><INDENT>self.write(out_buffer, start=out_start, end=out_end, stop=stop)<EOL>if self._debug:<EOL><INDENT>print(\"<STR_LIT>\",<EOL>[hex(i) for i in out_buffer[out_start:out_end]])<EOL><DEDENT>self.readinto(in_buffer, start=in_start, end=in_end)<EOL>if self._debug:<EOL><INDENT>print(\"<STR_LIT>\",<EOL>[hex(i) for i in in_buffer[in_start:in_end]])<EOL><DEDENT><DEDENT>", "docstring": "Write the bytes from ``out_buffer`` to the device, then immediately\nreads into ``in_buffer`` from the device. The number of bytes read\nwill be the length of ``in_buffer``.\nTransmits a stop bit after the write, if ``stop`` is set.\n\nIf ``out_start`` or ``out_end`` is provided, then the output buffer\nwill be sliced as if ``out_buffer[out_start:out_end]``. This will\nnot cause an allocation like ``buffer[out_start:out_end]`` will so\nit saves memory.\n\nIf ``in_start`` or ``in_end`` is provided, then the input buffer\nwill be sliced as if ``in_buffer[in_start:in_end]``. This will not\ncause an allocation like ``in_buffer[in_start:in_end]`` will so\nit saves memory.\n\n:param bytearray out_buffer: buffer containing the bytes to write\n:param bytearray in_buffer: buffer containing the bytes to read into\n:param int out_start: Index to start writing from\n:param int out_end: Index to read up to but not include\n:param int in_start: Index to start writing at\n:param int in_end: Index to write up to but not include\n:param bool stop: If true, output an I2C stop condition after the buffer is written", "id": "f501:c0:m3"}
{"signature": "def plot_TP_dependent_property(self, Tmin=None, Tmax=None, Pmin=None,<EOL>Pmax=None,  methods_P=[], pts=<NUM_LIT:15>, <EOL>only_valid=True):  ", "body": "if not has_matplotlib:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT>from mpl_toolkits.mplot3d import axes3d<EOL>from matplotlib.ticker import FormatStrFormatter<EOL>import numpy.ma as ma<EOL>if Pmin is None:<EOL><INDENT>if self.Pmin is not None:<EOL><INDENT>Pmin = self.Pmin<EOL><DEDENT>else:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT><DEDENT>if Pmax is None:<EOL><INDENT>if self.Pmax is not None:<EOL><INDENT>Pmax = self.Pmax<EOL><DEDENT>else:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT><DEDENT>if Tmin is None:<EOL><INDENT>if self.Tmin is not None:<EOL><INDENT>Tmin = self.Tmin<EOL><DEDENT>else:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT><DEDENT>if Tmax is None:<EOL><INDENT>if self.Tmax is not None:<EOL><INDENT>Tmax = self.Tmax<EOL><DEDENT>else:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT><DEDENT>if not methods_P:<EOL><INDENT>methods_P = self.user_methods_P if self.user_methods_P else self.all_methods_P<EOL><DEDENT>Ps = np.linspace(Pmin, Pmax, pts)<EOL>Ts = np.linspace(Tmin, Tmax, pts)<EOL>Ts_mesh, Ps_mesh = np.meshgrid(Ts, Ps)<EOL>fig = plt.figure()<EOL>ax = fig.gca(projection='<STR_LIT>')<EOL>handles = []<EOL>for method_P in methods_P:<EOL><INDENT>if only_valid:<EOL><INDENT>properties = []<EOL>for T in Ts:<EOL><INDENT>T_props = []<EOL>for P in Ps:<EOL><INDENT>if self.test_method_validity_P(T, P, method_P):<EOL><INDENT>try:<EOL><INDENT>p = self.calculate_P(T, P, method_P)<EOL>if self.test_property_validity(p):<EOL><INDENT>T_props.append(p)<EOL><DEDENT>else:<EOL><INDENT>T_props.append(None)<EOL><DEDENT><DEDENT>except:<EOL><INDENT>T_props.append(None)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>T_props.append(None)<EOL><DEDENT><DEDENT>properties.append(T_props)<EOL><DEDENT>properties = ma.masked_invalid(np.array(properties, dtype=np.float).T)<EOL>handles.append(ax.plot_surface(Ts_mesh, Ps_mesh, properties, cstride=<NUM_LIT:1>, rstride=<NUM_LIT:1>, alpha=<NUM_LIT:0.5>))<EOL><DEDENT>else:<EOL><INDENT>properties = [[self.calculate_P(T, P, method_P) for P in Ps] for T in Ts]<EOL>handles.append(ax.plot_surface(Ts_mesh, Ps_mesh, properties, cstride=<NUM_LIT:1>, rstride=<NUM_LIT:1>, alpha=<NUM_LIT:0.5>))<EOL><DEDENT><DEDENT>ax.yaxis.set_major_formatter(FormatStrFormatter('<STR_LIT>'))<EOL>ax.zaxis.set_major_formatter(FormatStrFormatter('<STR_LIT>'))<EOL>ax.xaxis.set_major_formatter(FormatStrFormatter('<STR_LIT>'))<EOL>ax.set_xlabel('<STR_LIT>')<EOL>ax.set_ylabel('<STR_LIT>')<EOL>ax.set_zlabel(self.name + '<STR_LIT:U+002CU+0020>' + self.units)<EOL>plt.title(self.name + '<STR_LIT>' + self.CASRN)<EOL>plt.show(block=False)<EOL>ax.legend(handles, methods_P)<EOL>plt.show(block=False)<EOL>", "docstring": "r'''Method to create a plot of the property vs temperature and pressure \n        according to either a specified list of methods, or user methods (if \n        set), or all methods. User-selectable number of points for each \n        variable. If only_valid is set,`test_method_validity_P` will be used to\n        check if each condition in the specified range is valid, and\n        `test_property_validity` will be used to test the answer, and the\n        method is allowed to fail; only the valid points will be plotted.\n        Otherwise, the result will be calculated and displayed as-is. This will\n        not suceed if the any method fails for any point.\n\n        Parameters\n        ----------\n        Tmin : float\n            Minimum temperature, to begin calculating the property, [K]\n        Tmax : float\n            Maximum temperature, to stop calculating the property, [K]\n        Pmin : float\n            Minimum pressure, to begin calculating the property, [Pa]\n        Pmax : float\n            Maximum pressure, to stop calculating the property, [Pa]\n        methods_P : list, optional\n            List of methods to consider\n        pts : int, optional\n            A list of points to calculate the property at for both temperature \n            and pressure; pts^2 points will be calculated.\n        only_valid : bool\n            If True, only plot successful methods and calculated properties,\n            and handle errors; if False, attempt calculation without any\n            checking and use methods outside their bounds", "id": "f15806:c1:m9"}
{"signature": "def adopted(self, old_child, new_child):", "body": "pass<EOL>", "docstring": "Transfer parents from old_child to new_child\n\n:param old_child: line_data(dict) with line_data['line_index'] or line_index(int)\n:param new_child: line_data(dict) with line_data['line_index'] or line_index(int)\n:return: List of parents transferred", "id": "f5359:c0:m11"}
{"signature": "def pid(kp=<NUM_LIT:0.>, ki=<NUM_LIT:0.>, kd=<NUM_LIT:0.>, smooth=<NUM_LIT:0.1>):", "body": "state = dict(p=<NUM_LIT:0>, i=<NUM_LIT:0>, d=<NUM_LIT:0>)<EOL>def control(error, dt=<NUM_LIT:1>):<EOL><INDENT>state['<STR_LIT:d>'] = smooth * state['<STR_LIT:d>'] + (<NUM_LIT:1> - smooth) * (error - state['<STR_LIT:p>']) / dt<EOL>state['<STR_LIT:i>'] += error * dt<EOL>state['<STR_LIT:p>'] = error<EOL>return kp * state['<STR_LIT:p>'] + ki * state['<STR_LIT:i>'] + kd * state['<STR_LIT:d>']<EOL><DEDENT>return control<EOL>", "docstring": "r'''Create a callable that implements a PID controller.\n\n    A PID controller returns a control signal :math:`u(t)` given a history of\n    error measurements :math:`e(0) \\dots e(t)`, using proportional (P), integral\n    (I), and derivative (D) terms, according to:\n\n    .. math::\n\n       u(t) = kp * e(t) + ki * \\int_{s=0}^t e(s) ds + kd * \\frac{de(s)}{ds}(t)\n\n    The proportional term is just the current error, the integral term is the\n    sum of all error measurements, and the derivative term is the instantaneous\n    derivative of the error measurement.\n\n    Parameters\n    ----------\n    kp : float\n        The weight associated with the proportional term of the PID controller.\n    ki : float\n        The weight associated with the integral term of the PID controller.\n    kd : float\n        The weight associated with the derivative term of the PID controller.\n    smooth : float in [0, 1]\n        Derivative values will be smoothed with this exponential average. A\n        value of 1 never incorporates new derivative information, a value of 0.5\n        uses the mean of the historic and new information, and a value of 0\n        discards historic information (i.e., the derivative in this case will be\n        unsmoothed). The default is 0.1.\n\n    Returns\n    -------\n    controller : callable (float, float) -> float\n        Returns a function that accepts an error measurement and a delta-time\n        value since the previous measurement, and returns a control signal.", "id": "f14890:m0"}
{"signature": "def _get_grammar_errors(self,pos,text,tokens):", "body": "word_counts = [max(len(t),<NUM_LIT:1>) for t in tokens]<EOL>good_pos_tags = []<EOL>min_pos_seq=<NUM_LIT:2><EOL>max_pos_seq=<NUM_LIT:4><EOL>bad_pos_positions=[]<EOL>for i in xrange(<NUM_LIT:0>, len(text)):<EOL><INDENT>pos_seq = [tag[<NUM_LIT:1>] for tag in pos[i]]<EOL>pos_ngrams = util_functions.ngrams(pos_seq, min_pos_seq, max_pos_seq)<EOL>long_pos_ngrams=[z for z in pos_ngrams if z.count('<STR_LIT:U+0020>')==(max_pos_seq-<NUM_LIT:1>)]<EOL>bad_pos_tuples=[[z,z+max_pos_seq] for z in xrange(<NUM_LIT:0>,len(long_pos_ngrams)) if long_pos_ngrams[z] not in self._good_pos_ngrams]<EOL>bad_pos_tuples.sort(key=operator.itemgetter(<NUM_LIT:1>))<EOL>to_delete=[]<EOL>for m in reversed(xrange(len(bad_pos_tuples)-<NUM_LIT:1>)):<EOL><INDENT>start, end = bad_pos_tuples[m]<EOL>for j in xrange(m+<NUM_LIT:1>, len(bad_pos_tuples)):<EOL><INDENT>lstart, lend = bad_pos_tuples[j]<EOL>if lstart >= start and lstart <= end:<EOL><INDENT>bad_pos_tuples[m][<NUM_LIT:1>]=bad_pos_tuples[j][<NUM_LIT:1>]<EOL>to_delete.append(j)<EOL><DEDENT><DEDENT><DEDENT>fixed_bad_pos_tuples=[bad_pos_tuples[z] for z in xrange(<NUM_LIT:0>,len(bad_pos_tuples)) if z not in to_delete]<EOL>bad_pos_positions.append(fixed_bad_pos_tuples)<EOL>overlap_ngrams = [z for z in pos_ngrams if z in self._good_pos_ngrams]<EOL>if (len(pos_ngrams)-len(overlap_ngrams))><NUM_LIT:0>:<EOL><INDENT>divisor=len(pos_ngrams)/len(pos_seq)<EOL><DEDENT>else:<EOL><INDENT>divisor=<NUM_LIT:1><EOL><DEDENT>if divisor == <NUM_LIT:0>:<EOL><INDENT>divisor=<NUM_LIT:1><EOL><DEDENT>good_grammar_ratio = (len(pos_ngrams)-len(overlap_ngrams))/divisor<EOL>good_pos_tags.append(good_grammar_ratio)<EOL><DEDENT>return good_pos_tags,bad_pos_positions<EOL>", "docstring": "Internal function to get the number of grammar errors in given text\npos - part of speech tagged text (list)\ntext - normal text (list)\ntokens - list of lists of tokenized text", "id": "f7965:c0:m3"}
{"signature": "def get_last_created_file(input_dir, glob_pattern='<STR_LIT:*>'):", "body": "return get_last_file(input_dir, glob_pattern, key=op.getctime)<EOL>", "docstring": "Return the path to the last created file in `input_dir`.\n    See `get_last_file` docstring for description of the parameters.", "id": "f4057:m16"}
{"signature": "def simplified_edges(self):", "body": "for group, edgelist in self.edges.items():<EOL><INDENT>for u, v, d in edgelist:<EOL><INDENT>yield (u, v)<EOL><DEDENT><DEDENT>", "docstring": "A generator for getting all of the edges without consuming extra\nmemory.", "id": "f4539:c0:m1"}
{"signature": "def range_decompress(cl):", "body": "def cond_func(ele):<EOL><INDENT>length = ele.__len__()<EOL>cond = (length == <NUM_LIT:1>)<EOL>if(cond):<EOL><INDENT>return(ord(ele))<EOL><DEDENT>else:<EOL><INDENT>x = ord(ele[<NUM_LIT:0>])<EOL>y = ord(ele[<NUM_LIT:1>])<EOL>return((x,y))<EOL><DEDENT><DEDENT>if(type(cl[<NUM_LIT:0>])==type(<NUM_LIT:0>)):<EOL><INDENT>T = True<EOL><DEDENT>elif(cl[<NUM_LIT:0>].__len__() == <NUM_LIT:1>):<EOL><INDENT>T = (type(cl[<NUM_LIT:0>]) == type(<NUM_LIT:0>))<EOL><DEDENT>else:<EOL><INDENT>T = (type(cl[<NUM_LIT:0>][<NUM_LIT:0>]) == type(<NUM_LIT:0>))<EOL><DEDENT>if(T):<EOL><INDENT>l = cl <EOL><DEDENT>else:<EOL><INDENT>l = array_map(cl,cond_func)<EOL><DEDENT>rslt = []<EOL>for i in range(<NUM_LIT:0>,l.__len__()):<EOL><INDENT>ele = l[i]<EOL>if(type(ele) == type(<NUM_LIT:0>)):<EOL><INDENT>arr = [ele]<EOL><DEDENT>elif(ele.__len__() == <NUM_LIT:1>):<EOL><INDENT>arr = [ele]<EOL><DEDENT>else:<EOL><INDENT>sv = ele[<NUM_LIT:0>]<EOL>ev = ele[<NUM_LIT:1>]<EOL>arr = init_range(sv,ev+<NUM_LIT:1>,<NUM_LIT:1>)<EOL><DEDENT>if(T):<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>arr = array_map(arr,chr)<EOL><DEDENT>rslt.extend(arr)<EOL><DEDENT>return(rslt)<EOL>", "docstring": "#only support sorted-ints or sorted-ascii\ncl = [1, (5, 8), (13, 14), 18, (30, 34)]\nrange_decompress(cl)\ncl = [1, (5, 8), (13, 14), 18, (30, 34), 40]\nrange_decompress(cl)\ncl = [('a', 'd'), ('j', 'n'), 'u', ('y', 'z')]\nrange_decompress(cl)", "id": "f1599:m204"}
{"signature": "def strip_msdu(self, idx):", "body": "<EOL>padding = <NUM_LIT:0><EOL>len_payload = <NUM_LIT:0><EOL>msdu = {<EOL>'<STR_LIT>': {},<EOL>'<STR_LIT>': None,<EOL>'<STR_LIT>': None,<EOL>'<STR_LIT>': None,<EOL>'<STR_LIT>': <NUM_LIT:0><EOL>}<EOL>(da_mac, sa_mac) = struct.unpack('<STR_LIT>', self._packet[idx:idx + <NUM_LIT:12>])<EOL>msdu['<STR_LIT>'] = Wifi.get_mac_addr(da_mac)<EOL>msdu['<STR_LIT>'] = Wifi.get_mac_addr(sa_mac)<EOL>idx += <NUM_LIT:12><EOL>msdu['<STR_LIT>'] = struct.unpack('<STR_LIT>', self._packet[idx:idx + <NUM_LIT:2>])[<NUM_LIT:0>]<EOL>idx += <NUM_LIT:2><EOL>offset, msdu['<STR_LIT>'] = self.strip_llc(idx)<EOL>idx += offset<EOL>len_payload = msdu['<STR_LIT>'] - offset<EOL>msdu['<STR_LIT>'] = self._packet[idx:idx + len_payload]<EOL>padding = <NUM_LIT:4> - (len_payload % <NUM_LIT:4>)<EOL>return msdu, msdu['<STR_LIT>'] + padding + <NUM_LIT:12><EOL>", "docstring": "strip single mac servis data unit(msdu)\n        see -> https://mrncciew.com/2014/11/01/cwap-802-11-data-frame-aggregation/\n        :idx: int\n        :return: dict\n            msdu\n        :return: int\n            number of processed bytes", "id": "f1089:c4:m3"}
{"signature": "def tf_step(self, time, variables, **kwargs):", "body": "global_variables = kwargs[\"<STR_LIT>\"]<EOL>assert all(<EOL>util.shape(global_variable) == util.shape(local_variable)<EOL>for global_variable, local_variable in zip(global_variables, variables)<EOL>)<EOL>local_deltas = self.optimizer.step(time=time, variables=variables, **kwargs)<EOL>with tf.control_dependencies(control_inputs=local_deltas):<EOL><INDENT>applied = self.optimizer.apply_step(variables=global_variables, deltas=local_deltas)<EOL><DEDENT>with tf.control_dependencies(control_inputs=(applied,)):<EOL><INDENT>update_deltas = list()<EOL>for global_variable, local_variable in zip(global_variables, variables):<EOL><INDENT>delta = global_variable - local_variable<EOL>update_deltas.append(delta)<EOL><DEDENT>applied = self.apply_step(variables=variables, deltas=update_deltas)<EOL><DEDENT>with tf.control_dependencies(control_inputs=(applied,)):<EOL><INDENT>return [local_delta + update_delta for local_delta, update_delta in zip(local_deltas, update_deltas)]<EOL><DEDENT>", "docstring": "Keyword Args:\n    global_variables: List of global variables to apply the proposed optimization step to.\n\nReturns:\n    List of delta tensors corresponding to the updates for each optimized variable.", "id": "f14342:c0:m1"}
{"signature": "def do_fullscreen(self, widget):", "body": "self.fullscreen()<EOL>self.is_fullscreen = True<EOL>while Gtk.events_pending():<EOL><INDENT>Gtk.main_iteration()<EOL><DEDENT>self.bot._screen_width = Gdk.Screen.width()<EOL>self.bot._screen_height = Gdk.Screen.height()<EOL>self.bot._screen_ratio = self.bot._screen_width / self.bot._screen_height<EOL>", "docstring": "Widget Action to Make the window fullscreen and update the bot.", "id": "f11530:c0:m14"}
{"signature": "def get_invalid_ISSNs(self):", "body": "return [<EOL>self._clean_isbn(issn)<EOL>for issn in self[\"<STR_LIT>\"] + self[\"<STR_LIT>\"]<EOL>]<EOL>", "docstring": "Get list of invalid ISSNs (``022z`` + ``022y``).\n\nReturns:\n    list: List with INVALID ISSN strings.", "id": "f1163:c0:m19"}
{"signature": "def delete(self, store_id, cart_id, line_id):", "body": "self.store_id = store_id<EOL>self.cart_id = cart_id<EOL>self.line_id = line_id<EOL>return self._mc_client._delete(url=self._build_path(store_id, '<STR_LIT>', cart_id, '<STR_LIT>', line_id))<EOL>", "docstring": "Delete a cart.\n\n:param store_id: The store id.\n:type store_id: :py:class:`str`\n:param cart_id: The id for the cart.\n:type cart_id: :py:class:`str`\n:param line_id: The id for the line item of a cart.\n:type line_id: :py:class:`str`", "id": "f292:c0:m5"}
{"signature": "def _dot_to_dec(ip, check=True):", "body": "if check and not is_dot(ip):<EOL><INDENT>raise ValueError('<STR_LIT>' % ip)<EOL><DEDENT>octets = str(ip).split('<STR_LIT:.>')<EOL>dec = <NUM_LIT:0><EOL>dec |= int(octets[<NUM_LIT:0>]) << <NUM_LIT><EOL>dec |= int(octets[<NUM_LIT:1>]) << <NUM_LIT:16><EOL>dec |= int(octets[<NUM_LIT:2>]) << <NUM_LIT:8><EOL>dec |= int(octets[<NUM_LIT:3>])<EOL>return dec<EOL>", "docstring": "Dotted decimal notation to decimal conversion.", "id": "f3601:m15"}
{"signature": "def authenticate_token( self, token ):", "body": "token_data = self.data_store.fetch( '<STR_LIT>', token=token )<EOL>if not token_data:<EOL><INDENT>raise Proauth2Error( '<STR_LIT>',<EOL>'<STR_LIT>' )<EOL><DEDENT>return token_data['<STR_LIT>']<EOL>", "docstring": "authenticate_token checks the passed token and returns the user_id it is\nassociated with. it is assumed that this method won't be directly exposed to\nthe oauth client, but some kind of framework or wrapper. this allows the\nframework to have the user_id without doing additional DB calls.", "id": "f835:c1:m4"}
{"signature": "def optimize_media(file_, max_size, formats):", "body": "if not PIL:<EOL><INDENT>msg = (\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL>raise RuntimeError(msg)<EOL><DEDENT>img = PIL.Image.open(file_)<EOL>ratio = max(hw / max_hw for hw, max_hw in zip(img.size, max_size))<EOL>if ratio > <NUM_LIT:1>:<EOL><INDENT>size = tuple(int(hw // ratio) for hw in img.size)<EOL>img = img.resize(size, PIL.Image.ANTIALIAS)<EOL><DEDENT>media = convert(img, formats)<EOL>if not hasattr(file_, '<STR_LIT>'):<EOL><INDENT>img.close()<EOL><DEDENT>return media<EOL>", "docstring": "Optimize an image\nResize the picture to the ``max_size``, defaulting to the large\nphoto size of Twitter in :meth:`PeonyClient.upload_media` when\nused with the ``optimize_media`` argument.\nParameters\n----------\nfile_ : file object\n    the file object of an image\nmax_size : :obj:`tuple` or :obj:`list` of :obj:`int`\n    a tuple in the format (width, height) which is maximum size of\n    the picture returned by this function\nformats : :obj`list` or :obj:`tuple` of :obj:`dict`\n    a list of all the formats to convert the picture to\nReturns\n-------\nfile\n    The smallest file created in this function", "id": "f4749:m1"}
{"signature": "def setStimulusThreshold(self, stimulusThreshold):", "body": "self._stimulusThreshold = stimulusThreshold<EOL>", "docstring": ":param stimulusThreshold: (float) value to set.", "id": "f17561:c4:m16"}
{"signature": "def support_recursive_delete(self):", "body": "return False<EOL>", "docstring": "Return True, if delete() may be called on non-empty collections\n        (see comments there).\n\n        This default implementation returns False.", "id": "f8596:c2:m7"}
{"signature": "def image_psf_shape_tag_from_image_psf_shape(image_psf_shape):", "body": "if image_psf_shape is None:<EOL><INDENT>return '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>y = str(image_psf_shape[<NUM_LIT:0>])<EOL>x = str(image_psf_shape[<NUM_LIT:1>])<EOL>return ('<STR_LIT>' + y + '<STR_LIT:x>' + x)<EOL><DEDENT>", "docstring": "Generate an image psf shape tag, to customize phase names based on size of the image PSF that the original PSF \\\n    is trimmed to for faster run times.\n\n    This changes the phase name 'phase_name' as follows:\n\n    image_psf_shape = 1 -> phase_name\n    image_psf_shape = 2 -> phase_name_image_psf_shape_2\n    image_psf_shape = 2 -> phase_name_image_psf_shape_2", "id": "f5966:m4"}
{"signature": "def getMaxSynapsesPerSegment(self):", "body": "return self.maxSynapsesPerSegment<EOL>", "docstring": "Get the maximum number of synapses per segment.\n\n:returns: (int) max number of synapses per segment", "id": "f17571:c0:m47"}
{"signature": "def get_object(name: str):", "body": "<EOL>elms = name.split(\"<STR_LIT:.>\")<EOL>parent = get_models()[elms.pop(<NUM_LIT:0>)]<EOL>while len(elms) > <NUM_LIT:0>:<EOL><INDENT>obj = elms.pop(<NUM_LIT:0>)<EOL>parent = getattr(parent, obj)<EOL><DEDENT>return parent<EOL>", "docstring": "Get a modelx object from its full name.", "id": "f13986:m9"}
{"signature": "def rmDirPatterns(*patterns, **kwargs):", "body": "kwargs['<STR_LIT:action>'] = '<STR_LIT>'<EOL>kwargs['<STR_LIT>'] = '<STR_LIT>'<EOL>return _walkWithAction(*patterns, **kwargs)<EOL>", "docstring": "Remove all directories under the current path with the given patterns.", "id": "f3724:m3"}
{"signature": "def _redirect_with_params(url_name, *args, **kwargs):", "body": "url = urlresolvers.reverse(url_name, args=args)<EOL>params = parse.urlencode(kwargs, True)<EOL>return \"<STR_LIT>\".format(url, params)<EOL>", "docstring": "Helper method to create a redirect response with URL params.\n\n    This builds a redirect string that converts kwargs into a\n    query string.\n\n    Args:\n        url_name: The name of the url to redirect to.\n        kwargs: the query string param and their values to build.\n\n    Returns:\n        A properly formatted redirect string.", "id": "f2452:m4"}
{"signature": "def _lexists(self, path):", "body": "try:<EOL><INDENT>return bool(self._lstat(path))<EOL><DEDENT>except os.error:<EOL><INDENT>return False<EOL><DEDENT>", "docstring": "IMPORTANT: expects `path` to already be deref()'erenced.", "id": "f3252:c4:m20"}
{"signature": "def get_repository(self, path, info=None, verbose=True):", "body": "<EOL>if path.strip() in ('<STR_LIT>','<STR_LIT:.>'):<EOL><INDENT>path = os.getcwd()<EOL><DEDENT>realPath = os.path.realpath( os.path.expanduser(path) )<EOL>if not os.path.isdir(realPath):<EOL><INDENT>os.makedirs(realPath)<EOL><DEDENT>if not self.is_repository(realPath):<EOL><INDENT>self.create_repository(realPath, info=info, verbose=verbose)<EOL><DEDENT>else:<EOL><INDENT>self.load_repository(realPath)<EOL><DEDENT>", "docstring": "Create a repository at given real path or load any existing one.\nThis method insures the creation of the directory in the system if it is missing.\\n\nUnlike create_repository, this method doesn't erase any existing repository\nin the path but loads it instead.\n\n**N.B. On some systems and some paths, creating a directory may requires root permissions.**\n\n:Parameters:\n    #. path (string): The real absolute path where to create the Repository.\n       If '.' or an empty string is passed, the current working directory will be used.\n    #. info (None, object): Any information that can identify the repository.\n    #. verbose (boolean): Whether to be warn and informed about any abnormalities.", "id": "f13923:c0:m30"}
{"signature": "def __init__(self):", "body": "if self.__class__ is MucStanzaExt:<EOL><INDENT>raise RuntimeError(\"<STR_LIT>\")<EOL><DEDENT>self.xmlnode=None<EOL>self.muc_child=None<EOL>", "docstring": "Initialize a `MucStanzaExt` derived object.", "id": "f15255:c9:m0"}
{"signature": "def empty_like(array, dtype=None):", "body": "array = numpy.asarray(array)<EOL>if dtype is None: <EOL><INDENT>dtype = array.dtype<EOL><DEDENT>return anonymousmemmap(array.shape, dtype)<EOL>", "docstring": "Create a shared memory array from the shape of array.", "id": "f2546:m5"}
{"signature": "def set_fontweight(self, weight):", "body": "return self.set_weight(weight)<EOL>", "docstring": "alias for set_weight", "id": "f17190:c0:m55"}
{"signature": "def __init__(self, bbox, transform):", "body": "assert bbox.is_bbox<EOL>assert isinstance(transform, Transform)<EOL>assert transform.input_dims == <NUM_LIT:2><EOL>assert transform.output_dims == <NUM_LIT:2><EOL>BboxBase.__init__(self)<EOL>self._bbox = bbox<EOL>self._transform = transform<EOL>self.set_children(bbox, transform)<EOL>self._points = None<EOL>", "docstring": "*bbox*: a child :class:`Bbox`\n\n*transform*: a 2D :class:`Transform`", "id": "f17186:c3:m0"}
{"signature": "def find_descriptor(self, uuid):", "body": "for desc in self.list_descriptors():<EOL><INDENT>if desc.uuid == uuid:<EOL><INDENT>return desc<EOL><DEDENT><DEDENT>return None<EOL>", "docstring": "Return the first child descriptor found that has the specified\n        UUID.  Will return None if no descriptor that matches is found.", "id": "f9589:c1:m6"}
{"signature": "def _check_official_error(self, json_data):", "body": "if \"<STR_LIT>\" in json_data and json_data[\"<STR_LIT>\"] != <NUM_LIT:0>:<EOL><INDENT>raise OfficialAPIError(errcode=json_data.get('<STR_LIT>'), errmsg=json_data.get('<STR_LIT>', '<STR_LIT>'))<EOL><DEDENT>", "docstring": "\u68c0\u6d4b\u5fae\u4fe1\u516c\u4f17\u5e73\u53f0\u8fd4\u56de\u503c\u4e2d\u662f\u5426\u5305\u542b\u9519\u8bef\u7684\u8fd4\u56de\u7801\n:raises OfficialAPIError: \u5982\u679c\u8fd4\u56de\u7801\u63d0\u793a\u6709\u9519\u8bef\uff0c\u629b\u51fa\u5f02\u5e38\uff1b\u5426\u5219\u8fd4\u56de True", "id": "f588:c0:m50"}
{"signature": "def __init__(self,<EOL>*args, **kw):", "body": "super(LocalUserSyncEngine, self).__init__(*args, **kw)<EOL>raise NotImplementedError()<EOL>", "docstring": "TODO: document & implement...", "id": "f12363:c3:m0"}
{"signature": "def win_exists(title, **kwargs):", "body": "text = kwargs.get(\"<STR_LIT:text>\", \"<STR_LIT>\")<EOL>ret = AUTO_IT.AU3_WinExists(LPCWSTR(title), LPCWSTR(text))<EOL>return ret<EOL>", "docstring": "Checks to see if a specified window exists.\n:param title: The title of the window to check.\n:param text: The text of the window to check.\n:return: Returns 1 if the window exists, otherwise returns 0.", "id": "f5587:m6"}
{"signature": "async def login(self, user=DEFAULT_USER, password=DEFAULT_PASSWORD,<EOL>account=DEFAULT_ACCOUNT):", "body": "code, info = await self.command(\"<STR_LIT>\" + user, (\"<STR_LIT>\", \"<STR_LIT>\"))<EOL>while code.matches(\"<STR_LIT>\"):<EOL><INDENT>if code == \"<STR_LIT>\":<EOL><INDENT>cmd = \"<STR_LIT>\" + password<EOL><DEDENT>elif code == \"<STR_LIT>\":<EOL><INDENT>cmd = \"<STR_LIT>\" + account<EOL><DEDENT>else:<EOL><INDENT>raise errors.StatusCodeError(\"<STR_LIT>\", code, info)<EOL><DEDENT>code, info = await self.command(cmd, (\"<STR_LIT>\", \"<STR_LIT>\"))<EOL><DEDENT>", "docstring": ":py:func:`asyncio.coroutine`\n\nServer authentication.\n\n:param user: username\n:type user: :py:class:`str`\n\n:param password: password\n:type password: :py:class:`str`\n\n:param account: account (almost always blank)\n:type account: :py:class:`str`\n\n:raises aioftp.StatusCodeError: if unknown code received", "id": "f9225:c3:m1"}
{"signature": "def graph_data_on_the_same_graph(list_of_plots, output_directory, resource_path, output_filename):", "body": "maximum_yvalue = -float('<STR_LIT>')<EOL>minimum_yvalue = float('<STR_LIT>')<EOL>plots = curate_plot_list(list_of_plots)<EOL>plot_count = len(plots)<EOL>if plot_count == <NUM_LIT:0>:<EOL><INDENT>return False, None<EOL><DEDENT>graph_height, graph_width, graph_title = get_graph_metadata(plots)<EOL>current_plot_count = <NUM_LIT:0><EOL>fig, axis = plt.subplots()<EOL>fig.set_size_inches(graph_width, graph_height)<EOL>if plot_count < <NUM_LIT:2>:<EOL><INDENT>fig.subplots_adjust(left=CONSTANTS.SUBPLOT_LEFT_OFFSET, bottom=CONSTANTS.SUBPLOT_BOTTOM_OFFSET, right=CONSTANTS.SUBPLOT_RIGHT_OFFSET)<EOL><DEDENT>else:<EOL><INDENT>fig.subplots_adjust(left=CONSTANTS.SUBPLOT_LEFT_OFFSET, bottom=CONSTANTS.SUBPLOT_BOTTOM_OFFSET,<EOL>right=CONSTANTS.SUBPLOT_RIGHT_OFFSET - CONSTANTS.Y_AXIS_OFFSET * (plot_count - <NUM_LIT:2>))<EOL><DEDENT>for plot in plots:<EOL><INDENT>current_plot_count += <NUM_LIT:1><EOL>logger.info('<STR_LIT>' + plot.input_csv + '<STR_LIT>' + output_filename + '<STR_LIT>')<EOL>xval, yval = numpy.loadtxt(plot.input_csv, unpack=True, delimiter='<STR_LIT:U+002C>')<EOL>axis.plot(xval, yval, linestyle='<STR_LIT:->', marker=None, color=get_current_color(current_plot_count), label=plot.plot_label)<EOL>axis.legend()<EOL>maximum_yvalue = max(maximum_yvalue, numpy.amax(yval) * (<NUM_LIT:1.0> + CONSTANTS.ZOOM_FACTOR * current_plot_count))<EOL>minimum_yvalue = min(minimum_yvalue, numpy.amin(yval) * (<NUM_LIT:1.0> - CONSTANTS.ZOOM_FACTOR * current_plot_count))<EOL><DEDENT>axis.yaxis.set_ticks_position('<STR_LIT:left>')<EOL>axis.set_xlabel(plots[<NUM_LIT:0>].x_label)<EOL>axis.set_ylabel(plots[<NUM_LIT:0>].y_label, fontsize=CONSTANTS.Y_LABEL_FONTSIZE)<EOL>axis.set_ylim([minimum_yvalue, maximum_yvalue])<EOL>axis.yaxis.grid(True)<EOL>axis.xaxis.grid(True)<EOL>axis.set_title(graph_title)<EOL>plot_file_name = os.path.join(output_directory, output_filename + \"<STR_LIT>\")<EOL>fig.savefig(plot_file_name)<EOL>plt.close()<EOL>with open(os.path.join(output_directory, output_filename + '<STR_LIT>'), '<STR_LIT:w>') as div_file:<EOL><INDENT>div_file.write('<STR_LIT>' + os.path.basename(plot_file_name).replace(\"<STR_LIT>\", \"<STR_LIT>\").replace(\"<STR_LIT>\", \"<STR_LIT>\") + '<STR_LIT>' +<EOL>resource_path + '<STR_LIT:/>' + os.path.basename(plot_file_name) + '<STR_LIT>' + os.path.basename(plot_file_name) +<EOL>'<STR_LIT>' + os.path.basename(plot_file_name) + '<STR_LIT>')<EOL><DEDENT>return True, os.path.join(output_directory, output_filename + '<STR_LIT>')<EOL>", "docstring": "graph_data_on_the_same_graph: put a list of plots on the same graph: currently it supports CDF", "id": "f7860:m6"}
{"signature": "def convert(img, formats):", "body": "media = None<EOL>min_size = <NUM_LIT:0><EOL>for kwargs in formats:<EOL><INDENT>f = io.BytesIO()<EOL>if img.mode == \"<STR_LIT>\" and kwargs['<STR_LIT>'] != \"<STR_LIT>\":<EOL><INDENT>if min_size < <NUM_LIT:5> * <NUM_LIT>**<NUM_LIT:2>:<EOL><INDENT>continue<EOL><DEDENT>else:<EOL><INDENT>img.convert('<STR_LIT>')<EOL><DEDENT><DEDENT>img.save(f, **kwargs)<EOL>size = f.tell()<EOL>if media is None or size < min_size:<EOL><INDENT>if media is not None:<EOL><INDENT>media.close()<EOL><DEDENT>media = f<EOL>min_size = size<EOL><DEDENT>else:<EOL><INDENT>f.close()<EOL><DEDENT><DEDENT>return media<EOL>", "docstring": "Convert the image to all the formats specified\nParameters\n----------\nimg : PIL.Image.Image\n    The image to convert\nformats : list\n    List of all the formats to use\nReturns\n-------\nio.BytesIO\n    A file object containing the converted image", "id": "f4749:m0"}
{"signature": "def __init__(self, sort=None, logic=None, weight=None, simple=None, simple_weight=None, extract_as=None,<EOL>extract_all=None, extract_when_missing=None, length=None, offset=None, **kwargs):", "body": "self._sort = None<EOL>self.sort = sort<EOL>self._logic = None<EOL>self.logic = logic<EOL>self._weight = None<EOL>self.weight = weight<EOL>self._simple = None<EOL>self.simple = simple<EOL>self._simple_weight = None<EOL>self.simple_weight = simple_weight<EOL>self._extract_as = None<EOL>self.extract_as = extract_as<EOL>self._extract_all = None<EOL>self.extract_all = extract_all<EOL>self._extract_when_missing = None<EOL>self.extract_when_missing = extract_when_missing<EOL>self._length = None<EOL>self.length = length<EOL>self._offset = None<EOL>self.offset = offset<EOL>", "docstring": "Constructor.\n\n:param sort: ASCENDING or DESCENDING to set the sort order on this field.\n:param logic: Logic for this query. Must be equal to one of \"MUST\", \"MUST_NOT\", \"SHOULD\", or \"OPTIONAL\".\n:param weight: Weight for the query.\n:param simple: String with the simple search to run against all fields.\n:param simple_weight: Dictionary of relative paths to their weights for simple queries.\n:param extract_as: String with the alias to save this field under.\n:param extract_all: Boolean setting whether all values in an array should be extracted.\n:param extract_when_missing: Any valid JSON-supported object or PIF object. This value is returned when a value is missing that should be extracted (and the overall query is still satisfied).\n:param length: One or more :class:`FieldQuery` operations against the length field.\n:param offset: One or more :class:`FieldQuery` operations against the offset field.", "id": "f3551:c0:m0"}
{"signature": "def __init__(self, datafile, path='<STR_LIT>', nparams=dict(), attr_params=dict(),<EOL>mode='<STR_LIT:r>'):", "body": "super().__init__(datafile, path=path, nparams=nparams,<EOL>attr_params=attr_params, mode=mode)<EOL>if mode != '<STR_LIT:r>':<EOL><INDENT>if '<STR_LIT>' not in self.h5file.root:<EOL><INDENT>self.h5file.create_group('<STR_LIT:/>', '<STR_LIT>',<EOL>'<STR_LIT>')<EOL><DEDENT><DEDENT>", "docstring": "Return a new HDF5 file to store simulation results.\n\n        The HDF5 file has two groups:\n        '/parameters'\n            containing all the simulation numeric-parameters\n\n        '/timestamps'\n            containing simulated timestamps\n\n        If `overwrite=True` (default) `datafile` is overwritten (if exists).", "id": "f15425:c3:m0"}
{"signature": "def set_output_format(self, file_type=None, rate=None, bits=None,<EOL>channels=None, encoding=None, comments=None,<EOL>append_comments=True):", "body": "if file_type not in VALID_FORMATS + [None]:<EOL><INDENT>raise ValueError(<EOL>'<STR_LIT>'.format(VALID_FORMATS)<EOL>)<EOL><DEDENT>if not is_number(rate) and rate is not None:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if rate is not None and rate <= <NUM_LIT:0>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if not isinstance(bits, int) and bits is not None:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if bits is not None and bits <= <NUM_LIT:0>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if not isinstance(channels, int) and channels is not None:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if channels is not None and channels <= <NUM_LIT:0>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if encoding not in ENCODING_VALS + [None]:<EOL><INDENT>raise ValueError(<EOL>'<STR_LIT>'.format(ENCODING_VALS)<EOL>)<EOL><DEDENT>if comments is not None and not isinstance(comments, str):<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>if not isinstance(append_comments, bool):<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>output_format = []<EOL>if file_type is not None:<EOL><INDENT>output_format.extend(['<STR_LIT>', '<STR_LIT:{}>'.format(file_type)])<EOL><DEDENT>if rate is not None:<EOL><INDENT>output_format.extend(['<STR_LIT>', '<STR_LIT>'.format(rate)])<EOL><DEDENT>if bits is not None:<EOL><INDENT>output_format.extend(['<STR_LIT>', '<STR_LIT:{}>'.format(bits)])<EOL><DEDENT>if channels is not None:<EOL><INDENT>output_format.extend(['<STR_LIT:-c>', '<STR_LIT:{}>'.format(channels)])<EOL><DEDENT>if encoding is not None:<EOL><INDENT>output_format.extend(['<STR_LIT>', '<STR_LIT:{}>'.format(encoding)])<EOL><DEDENT>if comments is not None:<EOL><INDENT>if append_comments:<EOL><INDENT>output_format.extend(['<STR_LIT>', comments])<EOL><DEDENT>else:<EOL><INDENT>output_format.extend(['<STR_LIT>', comments])<EOL><DEDENT><DEDENT>self.output_format = output_format<EOL>return self<EOL>", "docstring": "Sets output file format arguments. These arguments will overwrite\n        any format related arguments supplied by other effects (e.g. rate).\n\n        If this function is not explicity called the output format is inferred\n        from the file extension or the file's header.\n\n        Parameters\n        ----------\n        file_type : str or None, default=None\n            The file type of the output audio file. Should be the same as what\n            the file extension would be, for ex. 'mp3' or 'wav'.\n        rate : float or None, default=None\n            The sample rate of the output audio file. If None the sample rate\n            is inferred.\n        bits : int or None, default=None\n            The number of bits per sample. If None, the number of bits per\n            sample is inferred.\n        channels : int or None, default=None\n            The number of channels in the audio file. If None the number of\n            channels is inferred.\n        encoding : str or None, default=None\n            The audio encoding type. Sometimes needed with file-types that\n            support more than one encoding type. One of:\n                * signed-integer : PCM data stored as signed (\u2018two\u2019s\n                    complement\u2019) integers. Commonly used with a 16 or 24\u2212bit\n                    encoding size. A value of 0 represents minimum signal\n                    power.\n                * unsigned-integer : PCM data stored as unsigned integers.\n                    Commonly used with an 8-bit encoding size. A value of 0\n                    represents maximum signal power.\n                * floating-point : PCM data stored as IEEE 753 single precision\n                    (32-bit) or double precision (64-bit) floating-point\n                    (\u2018real\u2019) numbers. A value of 0 represents minimum signal\n                    power.\n                * a-law : International telephony standard for logarithmic\n                    encoding to 8 bits per sample. It has a precision\n                    equivalent to roughly 13-bit PCM and is sometimes encoded\n                    with reversed bit-ordering.\n                * u-law : North American telephony standard for logarithmic\n                    encoding to 8 bits per sample. A.k.a. \u03bc-law. It has a\n                    precision equivalent to roughly 14-bit PCM and is sometimes\n                    encoded with reversed bit-ordering.\n                * oki-adpcm : OKI (a.k.a. VOX, Dialogic, or Intel) 4-bit ADPCM;\n                    it has a precision equivalent to roughly 12-bit PCM. ADPCM\n                    is a form of audio compression that has a good compromise\n                    between audio quality and encoding/decoding speed.\n                * ima-adpcm : IMA (a.k.a. DVI) 4-bit ADPCM; it has a precision\n                    equivalent to roughly 13-bit PCM.\n                * ms-adpcm : Microsoft 4-bit ADPCM; it has a precision\n                    equivalent to roughly 14-bit PCM.\n                * gsm-full-rate : GSM is currently used for the vast majority\n                    of the world\u2019s digital wireless telephone calls. It\n                    utilises several audio formats with different bit-rates and\n                    associated speech quality. SoX has support for GSM\u2019s\n                    original 13kbps \u2018Full Rate\u2019 audio format. It is usually\n                    CPU-intensive to work with GSM audio.\n        comments : str or None, default=None\n            If not None, the string is added as a comment in the header of the\n            output audio file. If None, no comments are added.\n        append_comments : bool, default=True\n            If True, comment strings are appended to SoX's default comments. If\n            False, the supplied comment replaces the existing comment.", "id": "f3809:c0:m3"}
{"signature": "def __repr__(self):", "body": "return self.__str__()<EOL>", "docstring": "Return uniquely identifying string representation.", "id": "f16211:c0:m1"}
{"signature": "def _delete_deployment(self, deployment_name):", "body": "api_response = self.kube_client.delete_namespaced_deployment(<EOL>name=deployment_name,<EOL>namespace=self.namespace,<EOL>body=client.V1DeleteOptions(<EOL>propagation_policy='<STR_LIT>',<EOL>grace_period_seconds=<NUM_LIT:5>))<EOL>logger.debug(\"<STR_LIT>\".format(<EOL>str(api_response.status)))<EOL>", "docstring": "Delete deployment", "id": "f2788:c0:m7"}
{"signature": "def is_numlike(x):", "body": "if iterable(x):<EOL><INDENT>for thisx in x:<EOL><INDENT>return is_numlike(thisx)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>return is_numlike(x)<EOL><DEDENT>", "docstring": "The matplotlib datalim, autoscaling, locators etc work with\nscalars which are the units converted to floats given the\ncurrent unit.  The converter may be passed these floats, or\narrays of them, even when units are set.  Derived conversion\ninterfaces may opt to pass plain-ol unitless numbers through\nthe conversion interface and this is a helper function for\nthem.", "id": "f17205:c1:m3"}
{"signature": "def __init__(self, marathon_client, group, cert_store, mlb_client,<EOL>txacme_client_creator, reactor, email=None,<EOL>allow_multiple_certs=False):", "body": "self.marathon_client = marathon_client<EOL>self.group = group<EOL>self.reactor = reactor<EOL>responder = HTTP01Responder()<EOL>self.server = MarathonAcmeServer(responder.resource)<EOL>mlb_cert_store = MlbCertificateStore(cert_store, mlb_client)<EOL>self.txacme_service = AcmeIssuingService(<EOL>mlb_cert_store, txacme_client_creator, reactor, [responder], email)<EOL>self._allow_multiple_certs = allow_multiple_certs<EOL>self._server_listening = None<EOL>", "docstring": "Create the marathon-acme service.\n\n:param marathon_client: The Marathon API client.\n:param group: The name of the marathon-lb group.\n:param cert_store: The ``ICertificateStore`` instance to use.\n:param mlb_clinet: The marathon-lb API client.\n:param txacme_client_creator: Callable to create the txacme client.\n:param reactor: The reactor to use.\n:param email: The ACME registration email.\n:param allow_multiple_certs:\n    Whether to allow multiple certificates per app port.", "id": "f13715:c0:m0"}
{"signature": "def read_value(self, timeout_sec=TIMEOUT_SEC):", "body": "<EOL>self._value_read.clear()<EOL>self._device._peripheral.readValueForCharacteristic_(self._characteristic)<EOL>if not self._value_read.wait(timeout_sec):<EOL><INDENT>raise RuntimeError('<STR_LIT>')<EOL><DEDENT>return self._characteristic.value()<EOL>", "docstring": "Read the value of this characteristic.", "id": "f9584:c1:m3"}
{"signature": "def delete_volume(self, datacenter_id, volume_id):", "body": "response = self._perform_request(<EOL>url='<STR_LIT>' % (<EOL>datacenter_id, volume_id), method='<STR_LIT>')<EOL>return response<EOL>", "docstring": "Removes a volume from the data center.\n\n:param      datacenter_id: The unique ID of the data center.\n:type       datacenter_id: ``str``\n\n:param      volume_id: The unique ID of the volume.\n:type       volume_id: ``str``", "id": "f811:c0:m94"}
{"signature": "@classmethod<EOL><INDENT>def isspace(cls, nextchar):<DEDENT>", "body": "return nextchar.isspace()<EOL>", "docstring": "Whether the next character is whitespace", "id": "f15158:c0:m8"}
{"signature": "def p_mixin(self, p):", "body": "self.scope.add_mixin(Mixin(list(p)[<NUM_LIT:1>:], p.lineno(<NUM_LIT:3>)).parse(self.scope))<EOL>self.scope.pop()<EOL>p[<NUM_LIT:0>] = None<EOL>", "docstring": "mixin_decl                : open_mixin declaration_list brace_close", "id": "f12427:c2:m16"}
{"signature": "def multi(method):", "body": "@functools.wraps(method)<EOL>def multi(self, address='<STR_LIT>'):<EOL><INDENT>values = flask.request.values<EOL>address = urllib.parse.unquote_plus(address)<EOL>if address and values and not address.endswith('<STR_LIT:.>'):<EOL><INDENT>address += '<STR_LIT:.>'<EOL><DEDENT>result = {}<EOL>for a in values or '<STR_LIT>':<EOL><INDENT>try:<EOL><INDENT>if not self.project:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>ed = editor.Editor(address + a, self.project)<EOL>result[address + a] = {'<STR_LIT:value>': method(self, ed, a)}<EOL><DEDENT>except:<EOL><INDENT>if self.project:<EOL><INDENT>traceback.print_exc()<EOL><DEDENT>result[address + a] = {'<STR_LIT:error>': '<STR_LIT>' % a}<EOL><DEDENT><DEDENT>return flask.jsonify(result)<EOL><DEDENT>return multi<EOL>", "docstring": "Decorator for RestServer methods that take multiple addresses", "id": "f1908:m1"}
{"signature": "def compound_interest(principal, annual_rate, years):", "body": "return principal * (<NUM_LIT:1> + <NUM_LIT> * annual_rate) ** years<EOL>", "docstring": "Returns the future value of money invested at an annual\ninterest rate, compounded annually for a given number of years.\n\nArgs:\n    principal: The beginning ammount of money invested\n\n    annual_rate: The interest rate paid out\n\n    years: The number of years invested\n\nReturns:\n    A basic calculation of compound interest.", "id": "f1541:m6"}
{"signature": "def batch(self, batch_size, batch_num, fluxes=True):", "body": "for i in range(batch_num):<EOL><INDENT>yield self.sample(batch_size, fluxes=fluxes)<EOL><DEDENT>", "docstring": "Create a batch generator.\n\n        This is useful to generate n batches of m samples each.\n\n        Parameters\n        ----------\n        batch_size : int\n            The number of samples contained in each batch (m).\n        batch_num : int\n            The number of batches in the generator (n).\n        fluxes : boolean\n            Whether to return fluxes or the internal solver variables. If set\n            to False will return a variable for each forward and backward flux\n            as well as all additional variables you might have defined in the\n            model.\n\n        Yields\n        ------\n        pandas.DataFrame\n            A DataFrame with dimensions (batch_size x n_r) containing\n            a valid flux sample for a total of n_r reactions (or variables if\n            fluxes=False) in each row.", "id": "f15977:c0:m8"}
{"signature": "def covers(self, other):", "body": "return bool(self.poly.covers(other.poly))<EOL>", "docstring": "Check if the shape completely covers another shape.\n\n        Parameters\n        ----------\n        other : |Shape|\n\n        Returns\n        -------\n        bool", "id": "f3037:c0:m27"}
{"signature": "def delete_project(project_id):", "body": "try:<EOL><INDENT>res = _pybossa_req('<STR_LIT>', '<STR_LIT>', project_id)<EOL>if type(res).__name__ == '<STR_LIT:bool>':<EOL><INDENT>return True<EOL><DEDENT>else:<EOL><INDENT>return res<EOL><DEDENT><DEDENT>except:  <EOL><INDENT>raise<EOL><DEDENT>", "docstring": "Delete a Project with id = project_id.\n\n    :param project_id: PYBOSSA Project ID\n    :type project_id: integer\n    :returns: True -- the response status code", "id": "f10088:m7"}
{"signature": "def _add_solids(self, X, Y, C):", "body": "<EOL>if self.orientation == '<STR_LIT>':<EOL><INDENT>args = (X, Y, C)<EOL><DEDENT>else:<EOL><INDENT>args = (np.transpose(Y), np.transpose(X), np.transpose(C))<EOL><DEDENT>kw = {'<STR_LIT>':self.cmap, '<STR_LIT>':self.norm,<EOL>'<STR_LIT>':'<STR_LIT>', '<STR_LIT>':self.alpha}<EOL>_hold = self.ax.ishold()<EOL>self.ax.hold(True)<EOL>col = self.ax.pcolor(*args, **kw)<EOL>self.ax.hold(_hold)<EOL>self.solids = col<EOL>if self.drawedges:<EOL><INDENT>self.dividers = collections.LineCollection(self._edges(X,Y),<EOL>colors=(mpl.rcParams['<STR_LIT>'],),<EOL>linewidths=(<NUM_LIT:0.5>*mpl.rcParams['<STR_LIT>'],)<EOL>)<EOL>self.ax.add_collection(self.dividers)<EOL><DEDENT>", "docstring": "Draw the colors using :meth:`~matplotlib.axes.Axes.pcolor`;\noptionally add separators.", "id": "f17170:c0:m7"}
{"signature": "def _pop(self):", "body": "self._send('<STR_LIT>')<EOL>", "docstring": "Recall the last pushed constraint store and state.", "id": "f16986:c1:m15"}
{"signature": "def pop(self, key, *args):", "body": "if len(args) > <NUM_LIT:1>:<EOL><INDENT>raise TypeError(\"<STR_LIT>\" % (<NUM_LIT:1> + len(args)))<EOL><DEDENT>try:<EOL><INDENT>value = self.get_value(key)<EOL>self.remove(key)<EOL>return value<EOL><DEDENT>except KeyError:<EOL><INDENT>if len(args) == <NUM_LIT:0>:<EOL><INDENT>raise<EOL><DEDENT>else:<EOL><INDENT>return args[<NUM_LIT:0>]<EOL><DEDENT><DEDENT>", "docstring": "T.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n        If key is not found, d is returned if given, otherwise KeyError is raised", "id": "f16266:c3:m15"}
{"signature": "def _get_filter_field(field_name, field_value):", "body": "filter_field = None<EOL>if isinstance(field_value, ValueRange):<EOL><INDENT>range_values = {}<EOL>if field_value.lower:<EOL><INDENT>range_values.update({\"<STR_LIT>\": field_value.lower_string})<EOL><DEDENT>if field_value.upper:<EOL><INDENT>range_values.update({\"<STR_LIT>\": field_value.upper_string})<EOL><DEDENT>filter_field = {<EOL>\"<STR_LIT>\": {<EOL>field_name: range_values<EOL>}<EOL>}<EOL><DEDENT>elif _is_iterable(field_value):<EOL><INDENT>filter_field = {<EOL>\"<STR_LIT>\": {<EOL>field_name: field_value<EOL>}<EOL>}<EOL><DEDENT>else:<EOL><INDENT>filter_field = {<EOL>\"<STR_LIT>\": {<EOL>field_name: field_value<EOL>}<EOL>}<EOL><DEDENT>return filter_field<EOL>", "docstring": "Return field to apply into filter, if an array then use a range, otherwise look for a term match", "id": "f9819:m1"}
{"signature": "def parse_date(ims):", "body": "try:<EOL><INDENT>ts = email.utils.parsedate_tz(ims)<EOL>return time.mktime(ts[:<NUM_LIT:8>] + (<NUM_LIT:0>, )) - (ts[<NUM_LIT:9>] or <NUM_LIT:0>) - time.timezone<EOL><DEDENT>except (TypeError, ValueError, IndexError, OverflowError):<EOL><INDENT>return None<EOL><DEDENT>", "docstring": "Parse rfc1123, rfc850 and asctime timestamps and return UTC epoch.", "id": "f12971:m15"}
{"signature": "def __init__(self, canvas, window):", "body": "gtk.Toolbar.__init__(self)<EOL>self.canvas = canvas<EOL>self.win    = window<EOL>self.set_style(gtk.TOOLBAR_ICONS)<EOL>if gtk.pygtk_version >= (<NUM_LIT:2>,<NUM_LIT:4>,<NUM_LIT:0>):<EOL><INDENT>self._create_toolitems_2_4()<EOL>self.update = self._update_2_4<EOL>self.fileselect = FileChooserDialog(<EOL>title='<STR_LIT>',<EOL>parent=self.win,<EOL>filetypes=self.canvas.get_supported_filetypes(),<EOL>default_filetype=self.canvas.get_default_filetype())<EOL><DEDENT>else:<EOL><INDENT>self._create_toolitems_2_2()<EOL>self.update = self._update_2_2<EOL>self.fileselect = FileSelection(title='<STR_LIT>',<EOL>parent=self.win)<EOL><DEDENT>self.show_all()<EOL>self.update()<EOL>", "docstring": "figManager is the FigureManagerGTK instance that contains the\ntoolbar, with attributes figure, window and drawingArea", "id": "f17212:c3:m0"}
{"signature": "def status(self, order_id):", "body": "self.logger.debug('<STR_LIT>' + order_id)<EOL>url = '<STR_LIT>' % {<EOL>'<STR_LIT>': self.base_url, '<STR_LIT>': order_id<EOL>}<EOL>r = self.gbdx_connection.get(url)<EOL>r.raise_for_status()<EOL>return r.json().get(\"<STR_LIT>\", {})<EOL>", "docstring": "Checks imagery order status. There can be more than one image per\n           order and this function returns the status of all images\n           within the order.\n\n           Args:\n               order_id (str): The id of the order placed.\n\n           Returns:\n               List of dictionaries, one per image. Each dictionary consists\n               of the keys 'acquisition_id', 'location' and 'state'.", "id": "f7053:c0:m2"}
{"signature": "def scalar_write(self, address, block_count, data_file, meta_file):", "body": "cmd = [\"<STR_LIT>\", \"<STR_LIT>\", self.envs[\"<STR_LIT>\"], \"<STR_LIT>\".format(address),<EOL>\"<STR_LIT>\".format(block_count-<NUM_LIT:1>), \"<STR_LIT>\".format(data_file), \"<STR_LIT>\".format(meta_file),<EOL>\"<STR_LIT>\".format(block_count * self.envs[\"<STR_LIT>\"]),<EOL>\"<STR_LIT>\".format(block_count * self.envs[\"<STR_LIT>\"])]<EOL>status, _, _ = cij.ssh.command(cmd, shell=True)<EOL>return status<EOL>", "docstring": "nvme write", "id": "f13334:c0:m16"}
{"signature": "def start(self):", "body": "<EOL>logger.info(\"<STR_LIT>\")<EOL>logger.info(\"<STR_LIT>\")<EOL>logger.info(\"<STR_LIT>\")<EOL>logger.info(\"<STR_LIT>\")<EOL>if not self.check_es_access():<EOL><INDENT>print('<STR_LIT>')<EOL>sys.exit(<NUM_LIT:1>)<EOL><DEDENT>if self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>if not self.check_redis_access():<EOL><INDENT>print('<STR_LIT>')<EOL>sys.exit(<NUM_LIT:1>)<EOL><DEDENT>if not self.check_arthur_access():<EOL><INDENT>print('<STR_LIT>')<EOL>sys.exit(<NUM_LIT:1>)<EOL><DEDENT><DEDENT>if self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>if not self.check_bestiary_access():<EOL><INDENT>print('<STR_LIT>')<EOL>sys.exit(<NUM_LIT:1>)<EOL><DEDENT><DEDENT>self.__execute_initial_load()<EOL>all_tasks_cls = []<EOL>all_tasks_cls.append(TaskProjects)  <EOL>if self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>if not self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>all_tasks_cls.append(TaskRawDataCollection)<EOL><DEDENT>else:<EOL><INDENT>all_tasks_cls.append(TaskRawDataArthurCollection)<EOL><DEDENT><DEDENT>if self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>all_tasks_cls.append(TaskIdentitiesLoad)<EOL>all_tasks_cls.append(TaskIdentitiesMerge)<EOL>all_tasks_cls.append(TaskIdentitiesExport)<EOL><DEDENT>if self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>all_tasks_cls.append(TaskEnrich)<EOL><DEDENT>if self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>all_tasks_cls.append(TaskTrackItems)<EOL><DEDENT>if self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>all_tasks_cls.append(TaskReport)<EOL><DEDENT>while True:<EOL><INDENT>if not all_tasks_cls:<EOL><INDENT>logger.warning(\"<STR_LIT>\")<EOL>break<EOL><DEDENT>try:<EOL><INDENT>if not self.conf['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>self.execute_batch_tasks(all_tasks_cls,<EOL>self.conf['<STR_LIT>']['<STR_LIT>'],<EOL>self.conf['<STR_LIT>']['<STR_LIT>'])<EOL>self.execute_batch_tasks(all_tasks_cls,<EOL>self.conf['<STR_LIT>']['<STR_LIT>'],<EOL>self.conf['<STR_LIT>']['<STR_LIT>'])<EOL>break<EOL><DEDENT>else:<EOL><INDENT>self.execute_nonstop_tasks(all_tasks_cls)<EOL><DEDENT><DEDENT>except DataCollectionError as e:<EOL><INDENT>logger.error(str(e))<EOL>var = traceback.format_exc()<EOL>logger.error(var)<EOL><DEDENT>except DataEnrichmentError as e:<EOL><INDENT>logger.error(str(e))<EOL>var = traceback.format_exc()<EOL>logger.error(var)<EOL><DEDENT><DEDENT>logger.info(\"<STR_LIT>\")<EOL>", "docstring": "This method defines the workflow of SirMordred. So it calls to:\n- initialize the databases\n- execute the different phases for the first iteration\n  (collection, identities, enrichment)\n- start the collection and enrichment in parallel by data source\n- start also the Sorting Hat merge", "id": "f9690:c0:m11"}
{"signature": "def __init__(self, descriptor_db=None):", "body": "self._internal_db = descriptor_database.DescriptorDatabase()<EOL>self._descriptor_db = descriptor_db<EOL>self._descriptors = {}<EOL>self._enum_descriptors = {}<EOL>self._file_descriptors = {}<EOL>", "docstring": "Initializes a Pool of proto buffs.\n\n        The descriptor_db argument to the constructor is provided to allow\n        specialized file descriptor proto lookup code to be triggered on demand. An\n        example would be an implementation which will read and compile a file\n        specified in a call to FindFileByName() and not require the call to Add()\n        at all. Results from this database will be cached internally here as well.\n\n        Args:\n          descriptor_db: A secondary source of file descriptors.", "id": "f8666:c0:m0"}
{"signature": "def get_normalization_scale(norm, norm_min, norm_max, linthresh, linscale):", "body": "if norm is '<STR_LIT>':<EOL><INDENT>return colors.Normalize(vmin=norm_min, vmax=norm_max)<EOL><DEDENT>elif norm is '<STR_LIT>':<EOL><INDENT>if norm_min == <NUM_LIT:0.0>:<EOL><INDENT>norm_min = <NUM_LIT><EOL><DEDENT>return colors.LogNorm(vmin=norm_min, vmax=norm_max)<EOL><DEDENT>elif norm is '<STR_LIT>':<EOL><INDENT>return colors.SymLogNorm(linthresh=linthresh, linscale=linscale, vmin=norm_min, vmax=norm_max)<EOL><DEDENT>else:<EOL><INDENT>raise exc.PlottingException('<STR_LIT>'<EOL>'<STR_LIT>')<EOL><DEDENT>", "docstring": "Get the normalization scale of the colormap. This will be scaled based on the input min / max normalization \\\n    values.\n\n    For a 'symmetric_log' colormap, linthesh and linscale also change the colormap.\n\n    If norm_min / norm_max are not supplied, the minimum / maximum values of the array of data are used.\n\n    Parameters\n    -----------\n    array : data.array.scaled_array.ScaledArray\n        The 2D array of data which is plotted.\n    norm_min : float or None\n        The minimum array value the colormap map spans (all values below this value are plotted the same color).\n    norm_max : float or None\n        The maximum array value the colormap map spans (all values above this value are plotted the same color).\n    linthresh : float\n        For the 'symmetric_log' colormap normalization ,this specifies the range of values within which the colormap \\\n        is linear.\n    linscale : float\n        For the 'symmetric_log' colormap normalization, this allowws the linear range set by linthresh to be stretched \\\n        relative to the logarithmic range.", "id": "f5985:m4"}
{"signature": "def relax(self, steps=<NUM_LIT:1000>):", "body": "for step in range(steps):<EOL><INDENT>self.forces = self.force_hertzian() + self.force_damp()<EOL>self.integrate(self.forces)<EOL>self.boundary_condition()<EOL><DEDENT>", "docstring": "Relax the current configuration using just pair wise forces (no noise)", "id": "f5763:c0:m7"}
{"signature": "def linear(Ks, dim, num_q, rhos, nus):", "body": "return _get_linear(Ks, dim)(num_q, rhos, nus)<EOL>", "docstring": "r'''\n    Estimates the linear inner product \\int p q between two distributions,\n    based on kNN distances.", "id": "f14617:m5"}
{"signature": "def remove(self):", "body": "if self.disco is None:<EOL><INDENT>return<EOL><DEDENT>self.xmlnode.unlinkNode()<EOL>oldns=self.xmlnode.ns()<EOL>ns=self.xmlnode.newNs(oldns.getContent(),None)<EOL>self.xmlnode.replaceNs(oldns,ns)<EOL>common_root.addChild(self.xmlnode())<EOL>self.disco=None<EOL>", "docstring": "Remove `self` from the containing `DiscoInfo` object.", "id": "f15250:c1:m3"}
{"signature": "def convert_nm(nm, notation=IP_DOT, inotation=IP_UNKNOWN, check=True):", "body": "return _convert(nm, notation, inotation, _check=check, _isnm=True)<EOL>", "docstring": "Convert a netmask to another notation.", "id": "f3601:m40"}
{"signature": "def __init__(self, targetfig, toolfig):", "body": "self.targetfig = targetfig<EOL>toolfig.subplots_adjust(left=<NUM_LIT>, right=<NUM_LIT>)<EOL>class toolbarfmt:<EOL><INDENT>def __init__(self, slider):<EOL><INDENT>self.slider = slider<EOL><DEDENT>def __call__(self, x, y):<EOL><INDENT>fmt = '<STR_LIT>'%(self.slider.label.get_text(), self.slider.valfmt)<EOL>return fmt%x<EOL><DEDENT><DEDENT>self.axleft = toolfig.add_subplot(<NUM_LIT>)<EOL>self.axleft.set_title('<STR_LIT>')<EOL>self.axleft.set_navigate(False)<EOL>self.sliderleft = Slider(self.axleft, '<STR_LIT:left>', <NUM_LIT:0>, <NUM_LIT:1>, targetfig.subplotpars.left, closedmax=False)<EOL>self.sliderleft.on_changed(self.funcleft)<EOL>self.axbottom = toolfig.add_subplot(<NUM_LIT>)<EOL>self.axbottom.set_navigate(False)<EOL>self.sliderbottom = Slider(self.axbottom, '<STR_LIT>', <NUM_LIT:0>, <NUM_LIT:1>, targetfig.subplotpars.bottom, closedmax=False)<EOL>self.sliderbottom.on_changed(self.funcbottom)<EOL>self.axright = toolfig.add_subplot(<NUM_LIT>)<EOL>self.axright.set_navigate(False)<EOL>self.sliderright = Slider(self.axright, '<STR_LIT:right>', <NUM_LIT:0>, <NUM_LIT:1>, targetfig.subplotpars.right, closedmin=False)<EOL>self.sliderright.on_changed(self.funcright)<EOL>self.axtop = toolfig.add_subplot(<NUM_LIT>)<EOL>self.axtop.set_navigate(False)<EOL>self.slidertop = Slider(self.axtop, '<STR_LIT>', <NUM_LIT:0>, <NUM_LIT:1>, targetfig.subplotpars.top, closedmin=False)<EOL>self.slidertop.on_changed(self.functop)<EOL>self.axwspace = toolfig.add_subplot(<NUM_LIT>)<EOL>self.axwspace.set_navigate(False)<EOL>self.sliderwspace = Slider(self.axwspace, '<STR_LIT>', <NUM_LIT:0>, <NUM_LIT:1>, targetfig.subplotpars.wspace, closedmax=False)<EOL>self.sliderwspace.on_changed(self.funcwspace)<EOL>self.axhspace = toolfig.add_subplot(<NUM_LIT>)<EOL>self.axhspace.set_navigate(False)<EOL>self.sliderhspace = Slider(self.axhspace, '<STR_LIT>', <NUM_LIT:0>, <NUM_LIT:1>, targetfig.subplotpars.hspace, closedmax=False)<EOL>self.sliderhspace.on_changed(self.funchspace)<EOL>self.sliderleft.slidermax = self.sliderright<EOL>self.sliderright.slidermin = self.sliderleft<EOL>self.sliderbottom.slidermax = self.slidertop<EOL>self.slidertop.slidermin = self.sliderbottom<EOL>bax = toolfig.add_axes([<NUM_LIT>, <NUM_LIT>, <NUM_LIT>, <NUM_LIT>])<EOL>self.buttonreset = Button(bax, '<STR_LIT>')<EOL>sliders = (self.sliderleft, self.sliderbottom, self.sliderright,<EOL>self.slidertop, self.sliderwspace, self.sliderhspace, )<EOL>def func(event):<EOL><INDENT>thisdrawon = self.drawon<EOL>self.drawon = False<EOL>bs = []<EOL>for slider in sliders:<EOL><INDENT>bs.append(slider.drawon)<EOL>slider.drawon = False<EOL><DEDENT>for slider in sliders:<EOL><INDENT>slider.reset()<EOL><DEDENT>for slider, b in zip(sliders, bs):<EOL><INDENT>slider.drawon = b<EOL><DEDENT>self.drawon = thisdrawon<EOL>if self.drawon:<EOL><INDENT>toolfig.canvas.draw()<EOL>self.targetfig.canvas.draw()<EOL><DEDENT><DEDENT>validate = toolfig.subplotpars.validate<EOL>toolfig.subplotpars.validate = False<EOL>self.buttonreset.on_clicked(func)<EOL>toolfig.subplotpars.validate = validate<EOL>", "docstring": "targetfig is the figure to adjust\n\ntoolfig is the figure to embed the the subplot tool into.  If\nNone, a default pylab figure will be created.  If you are\nusing this from the GUI", "id": "f17235:c6:m0"}
{"signature": "def needleman_wunsch(src, tar, gap_cost=<NUM_LIT:1>, sim_func=sim_ident):", "body": "return NeedlemanWunsch().dist_abs(src, tar, gap_cost, sim_func)<EOL>", "docstring": "Return the Needleman-Wunsch score of two strings.\n\n    This is a wrapper for :py:meth:`NeedlemanWunsch.dist_abs`.\n\n    Parameters\n    ----------\n    src : str\n        Source string for comparison\n    tar : str\n        Target string for comparison\n    gap_cost : float\n        The cost of an alignment gap (1 by default)\n    sim_func : function\n        A function that returns the similarity of two characters (identity\n        similarity by default)\n\n    Returns\n    -------\n    float\n        Needleman-Wunsch score\n\n    Examples\n    --------\n    >>> needleman_wunsch('cat', 'hat')\n    2.0\n    >>> needleman_wunsch('Niall', 'Neil')\n    1.0\n    >>> needleman_wunsch('aluminum', 'Catalan')\n    -1.0\n    >>> needleman_wunsch('ATCG', 'TAGC')\n    0.0", "id": "f6623:m0"}
{"signature": "@property<EOL><INDENT>def is_open(self):<EOL><DEDENT>", "body": "return self._open<EOL>", "docstring": "Show the current connection state.\n\n        :return: True if connection is open", "id": "f13258:c0:m16"}
{"signature": "def _transliterate(self, text, outFormat):", "body": "def getResult(): <EOL><INDENT>if curMatch.isspace():<EOL><INDENT>result.append(curMatch)<EOL>return<EOL><DEDENT>if prevMatch in self:<EOL><INDENT>prev = self[prevMatch]<EOL><DEDENT>else:<EOL><INDENT>prev = None<EOL><DEDENT>if nextMatch in self:<EOL><INDENT>next = self[nextMatch]<EOL><DEDENT>else:<EOL><INDENT>next = None<EOL><DEDENT>try:<EOL><INDENT>equiv = outFormat._equivalent(self[curMatch], <EOL>prev, <EOL>next, <EOL>self._implicitA)<EOL><DEDENT>except KeyError:<EOL><INDENT>equiv = _unrecognised(curMatch)<EOL><DEDENT>for e in equiv:<EOL><INDENT>result.append(e)<EOL><DEDENT><DEDENT>def incr(c):<EOL><INDENT>if self._longestEntry == <NUM_LIT:1>:<EOL><INDENT>return <NUM_LIT:1><EOL><DEDENT>return len(c)<EOL><DEDENT>result = []<EOL>text = self._preprocess(text)<EOL>i = <NUM_LIT:0><EOL>prevMatch = None<EOL>nextMatch = None<EOL>curMatch = self._getNextChar(text, i)<EOL>i = i + len(curMatch)<EOL>while i < len(text):<EOL><INDENT>nextMatch = self._getNextChar(text, i)<EOL>getResult()<EOL>i = i + len(nextMatch)<EOL>prevMatch = curMatch<EOL>curMatch = nextMatch<EOL>nextMatch = None<EOL><DEDENT>getResult() <EOL>return result<EOL>", "docstring": "Transliterate a devanagari text into the target format.\n\n        Transliterating a character to or from Devanagari is not a simple \n        lookup: it depends on the preceding and following characters.", "id": "f8768:c4:m0"}
{"signature": "def updateSynapsePermanence(self, synapse, permanence):", "body": "synapse.permanence = permanence<EOL>", "docstring": "Updates the permanence for a synapse.\n\n:param synapse: (class:`Synapse`) to be updated.\n:param permanence: (float) New permanence.", "id": "f17567:c3:m14"}
{"signature": "@cli.command()<EOL>@click.argument('<STR_LIT>', nargs=-<NUM_LIT:1>)<EOL>@click.option('<STR_LIT>', is_flag=True, help='<STR_LIT>')<EOL>@click.option('<STR_LIT>', is_flag=True, help='<STR_LIT>')<EOL>def activate(paths, skip_local, skip_shared):", "body": "if not paths:<EOL><INDENT>ctx = click.get_current_context()<EOL>if cpenv.get_active_env():<EOL><INDENT>ctx.invoke(info)<EOL>return<EOL><DEDENT>click.echo(ctx.get_help())<EOL>examples = (<EOL>'<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>'<EOL>)<EOL>click.echo(examples)<EOL>return<EOL><DEDENT>if skip_local:<EOL><INDENT>cpenv.module_resolvers.remove(cpenv.resolver.module_resolver)<EOL>cpenv.module_resolvers.remove(cpenv.resolver.active_env_module_resolver)<EOL><DEDENT>if skip_shared:<EOL><INDENT>cpenv.module_resolvers.remove(cpenv.resolver.modules_path_resolver)<EOL><DEDENT>try:<EOL><INDENT>r = cpenv.resolve(*paths)<EOL><DEDENT>except cpenv.ResolveError as e:<EOL><INDENT>click.echo('<STR_LIT:\\n>' + str(e))<EOL>return<EOL><DEDENT>resolved = set(r.resolved)<EOL>active_modules = set()<EOL>env = cpenv.get_active_env()<EOL>if env:<EOL><INDENT>active_modules.add(env)<EOL><DEDENT>active_modules.update(cpenv.get_active_modules())<EOL>new_modules = resolved - active_modules<EOL>old_modules = active_modules & resolved<EOL>if old_modules and not new_modules:<EOL><INDENT>click.echo(<EOL>'<STR_LIT>'<EOL>+ bold('<STR_LIT:U+0020>'.join([obj.name for obj in old_modules]))<EOL>)<EOL>return<EOL><DEDENT>if env and contains_env(new_modules):<EOL><INDENT>click.echo('<STR_LIT>')<EOL>return<EOL><DEDENT>click.echo('<STR_LIT>')<EOL>click.echo(format_objects(r.resolved))<EOL>r.activate()<EOL>click.echo(blue('<STR_LIT>'))<EOL>modules = sorted(resolved | active_modules, key=_type_and_name)<EOL>prompt = '<STR_LIT::>'.join([obj.name for obj in modules])<EOL>shell.launch(prompt)<EOL>", "docstring": "Activate an environment", "id": "f8303:m8"}
{"signature": "@property<EOL><INDENT>def cookies(self):<DEDENT>", "body": "http_cookie = self.environ.get('<STR_LIT>', '<STR_LIT>')<EOL>_cookies = {<EOL>k: v.value<EOL>for (k, v) in SimpleCookie(http_cookie).items()<EOL>}<EOL>return _cookies<EOL>", "docstring": "Request cookies\n\n        :rtype: dict", "id": "f13791:c0:m11"}
{"signature": "def urlunsplit(data):", "body": "scheme, netloc, url, query, fragment = data<EOL>if netloc or (scheme and scheme in uses_netloc and url[:<NUM_LIT:2>] != '<STR_LIT>'):<EOL><INDENT>if url and url[:<NUM_LIT:1>] != '<STR_LIT:/>': url = '<STR_LIT:/>' + url<EOL>url = '<STR_LIT>' + (netloc or '<STR_LIT>') + url<EOL><DEDENT>if scheme:<EOL><INDENT>url = scheme + '<STR_LIT::>' + url<EOL><DEDENT>if query:<EOL><INDENT>url = url + '<STR_LIT:?>' + query<EOL><DEDENT>if fragment:<EOL><INDENT>url = url + '<STR_LIT:#>' + fragment<EOL><DEDENT>return url<EOL>", "docstring": "Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).", "id": "f16401:m6"}
{"signature": "def transfer(self, data, assert_ss=True, deassert_ss=True):", "body": "if self._mosi is None:<EOL><INDENT>raise RuntimeError('<STR_LIT>')<EOL><DEDENT>if self._miso is None:<EOL><INDENT>raise RuntimeError('<STR_LIT>')<EOL><DEDENT>if assert_ss and self._ss is not None:<EOL><INDENT>self._gpio.set_low(self._ss)<EOL><DEDENT>result = bytearray(len(data))<EOL>for i in range(len(data)):<EOL><INDENT>for j in range(<NUM_LIT:8>):<EOL><INDENT>if self._write_shift(data[i], j) & self._mask:<EOL><INDENT>self._gpio.set_high(self._mosi)<EOL><DEDENT>else:<EOL><INDENT>self._gpio.set_low(self._mosi)<EOL><DEDENT>self._gpio.output(self._sclk, not self._clock_base)<EOL>if self._read_leading:<EOL><INDENT>if self._gpio.is_high(self._miso):<EOL><INDENT>result[i] |= self._read_shift(self._mask, j)<EOL><DEDENT>else:<EOL><INDENT>result[i] &= ~self._read_shift(self._mask, j)<EOL><DEDENT><DEDENT>self._gpio.output(self._sclk, self._clock_base)<EOL>if not self._read_leading:<EOL><INDENT>if self._gpio.is_high(self._miso):<EOL><INDENT>result[i] |= self._read_shift(self._mask, j)<EOL><DEDENT>else:<EOL><INDENT>result[i] &= ~self._read_shift(self._mask, j)<EOL><DEDENT><DEDENT><DEDENT><DEDENT>if deassert_ss and self._ss is not None:<EOL><INDENT>self._gpio.set_high(self._ss)<EOL><DEDENT>return result<EOL>", "docstring": "Full-duplex SPI read and write.  If assert_ss is true, the SS line\n        will be asserted low, the specified bytes will be clocked out the MOSI\n        line while bytes will also be read from the MISO line, and if\n        deassert_ss is true the SS line will be put back high.  Bytes which are\n        read will be returned as a bytearray object.", "id": "f8003:c2:m7"}
{"signature": "def sim_crb_diff(std0, std1, N=<NUM_LIT>):", "body": "a = std0*np.random.randn(N, len(std0))<EOL>b = std1*np.random.randn(N, len(std1))<EOL>return a - b<EOL>", "docstring": "each element of std0 should correspond with the element of std1", "id": "f5746:m11"}
{"signature": "def output(self, to=None, formatted=False, indent=<NUM_LIT:0>, indentation='<STR_LIT:U+0020>', *args, **kwargs):", "body": "if formatted:<EOL><INDENT>to.write(self.start_tag)<EOL>to.write('<STR_LIT:\\n>')<EOL>if not self.tag_self_closes:<EOL><INDENT>for blok in self.blox:<EOL><INDENT>to.write(indentation * (indent + <NUM_LIT:1>))<EOL>blok.output(to=to, indent=indent + <NUM_LIT:1>, formatted=True, indentation=indentation, *args, **kwargs)<EOL>to.write('<STR_LIT:\\n>')<EOL><DEDENT><DEDENT>to.write(indentation * indent)<EOL>to.write(self.end_tag)<EOL>if not indentation:<EOL><INDENT>to.write('<STR_LIT:\\n>')<EOL><DEDENT><DEDENT>else:<EOL><INDENT>to.write(self.start_tag)<EOL>if not self.tag_self_closes:<EOL><INDENT>for blok in self.blox:<EOL><INDENT>blok.output(to=to, *args, **kwargs)<EOL><DEDENT><DEDENT>to.write(self.end_tag)<EOL><DEDENT>", "docstring": "Outputs to a stream (like a file or request)", "id": "f3317:c8:m1"}
{"signature": "def login(self, username=None, password=None, android_id=None):", "body": "cls_name = type(self).__name__<EOL>if username is None:<EOL><INDENT>username = input(\"<STR_LIT>\")<EOL><DEDENT>if password is None:<EOL><INDENT>password = getpass.getpass(\"<STR_LIT>\")<EOL><DEDENT>if android_id is None:<EOL><INDENT>android_id = Mobileclient.FROM_MAC_ADDRESS<EOL><DEDENT>try:<EOL><INDENT>self.api.login(username, password, android_id)<EOL><DEDENT>except OSError:<EOL><INDENT>logger.exception(\"<STR_LIT>\".format(cls_name))<EOL><DEDENT>if not self.is_authenticated:<EOL><INDENT>logger.warning(\"<STR_LIT>\".format(cls_name))<EOL>return False<EOL><DEDENT>logger.info(\"<STR_LIT>\".format(cls_name))<EOL>return True<EOL>", "docstring": "Authenticate the gmusicapi Mobileclient instance.\n\n        Parameters:\n                username (Optional[str]): Your Google Music username. Will be prompted if not given.\n\n                password (Optional[str]): Your Google Music password. Will be prompted if not given.\n\n                android_id (Optional[str]): The 16 hex digits from an Android device ID.\n                        Default: Use gmusicapi.Mobileclient.FROM_MAC_ADDRESS to create ID from computer's MAC address.\n\n        Returns:\n                ``True`` on successful login or ``False`` on unsuccessful login.", "id": "f35:c0:m1"}
{"signature": "@classmethod<EOL><INDENT>def pprint_styles(klass):<DEDENT>", "body": "return _pprint_styles(klass._style_list)<EOL>", "docstring": "A class method which returns a string of the available styles.", "id": "f17197:c14:m2"}
{"signature": "def setVerbosity(verbosity, tm, tmPy):", "body": "tm.cells4.setVerbosity(verbosity)<EOL>tm.verbosity = verbosity<EOL>tmPy.verbosity = verbosity<EOL>", "docstring": "Set verbosity levels of the TM's", "id": "f17295:m1"}
{"signature": "def p_VarPart(self, p):", "body": "p[<NUM_LIT:0>] = p[<NUM_LIT:1>] and p[<NUM_LIT:3>] or []<EOL>", "docstring": "VarPart : VARIABLES '{' VarTypes '}'\n                   | empty", "id": "f5672:c0:m45"}
{"signature": "@classmethod<EOL><INDENT>def describe(cls, policy):<DEDENT>", "body": "if cls.validate(policy):<EOL><INDENT>return cls.descriptions[policy]<EOL><DEDENT>", "docstring": "Policy description.", "id": "f7814:c0:m0"}
{"signature": "def population(self):", "body": "return self._tp + self._tn + self._fp + self._fn<EOL>", "docstring": "Return population, N.\n\n        Returns\n        -------\n        int\n            The population (N) of the confusion table\n\n        Example\n        -------\n        >>> ct = ConfusionTable(120, 60, 20, 30)\n        >>> ct.population()\n        230", "id": "f6662:c0:m15"}
{"signature": "@classmethod<EOL><INDENT>def createFromSource(cls, vs, name=None):<DEDENT>", "body": "return GithubComponent(vs.location, vs.spec, vs.semantic_spec, name)<EOL>", "docstring": "returns a github component for any github url (including\n            git+ssh:// git+http:// etc. or None if this is not a Github URL.\n            For all of these we use the github api to grab a tarball, because\n            that's faster.\n\n            Normally version will be empty, unless the original url was of the\n            form: 'owner/repo @version' or 'url://...#version', which can be used\n            to grab a particular tagged version.\n\n            (Note that for github components we ignore the component name - it\n             doesn't have to match the github module name)", "id": "f13545:c1:m1"}
{"signature": "def _getvalue(self, expression):", "body": "if not issymbolic(expression):<EOL><INDENT>return expression<EOL><DEDENT>assert isinstance(expression, Variable)<EOL>if isinstance(expression, Array):<EOL><INDENT>result = bytearray()<EOL>for c in expression:<EOL><INDENT>expression_str = translate_to_smtlib(c)<EOL>self._send('<STR_LIT>' % expression_str)<EOL>response = self._recv()<EOL>result.append(int('<STR_LIT>'.format(response.split(expression_str)[<NUM_LIT:1>][<NUM_LIT:3>:-<NUM_LIT:2>]), <NUM_LIT:16>))<EOL><DEDENT>return bytes(result)<EOL><DEDENT>else:<EOL><INDENT>self._send('<STR_LIT>' % expression.name)<EOL>ret = self._recv()<EOL>assert ret.startswith('<STR_LIT>') and ret.endswith('<STR_LIT>'), ret<EOL>if isinstance(expression, Bool):<EOL><INDENT>return {'<STR_LIT:true>': True, '<STR_LIT:false>': False}[ret[<NUM_LIT:2>:-<NUM_LIT:2>].split('<STR_LIT:U+0020>')[<NUM_LIT:1>]]<EOL><DEDENT>elif isinstance(expression, BitVec):<EOL><INDENT>pattern, base = self._get_value_fmt<EOL>m = pattern.match(ret)<EOL>expr, value = m.group('<STR_LIT>'), m.group('<STR_LIT:value>')<EOL>return int(value, base)<EOL><DEDENT><DEDENT>raise NotImplementedError(\"<STR_LIT>\")<EOL>", "docstring": "Ask the solver for one possible assignment for given expression using current set of constraints.\nThe current set of expressions must be sat.\n\nNOTE: This is an internal method: it uses the current solver state (set of constraints!).", "id": "f16986:c1:m13"}
{"signature": "def calculate_stats(self):", "body": "metric_type = self.metric_type.split('<STR_LIT:->')[<NUM_LIT:0>]<EOL>if metric_type in naarad.naarad_imports.metric_classes or metric_type in naarad.naarad_imports.aggregate_metric_classes:<EOL><INDENT>self.calculate_other_metric_stats()<EOL><DEDENT>else:<EOL><INDENT>self.calculate_base_metric_stats()<EOL><DEDENT>", "docstring": "Calculate stats with different function depending on the metric type:\nData is recorded in memory for base metric type, and use calculate_base_metric_stats()\nData is recorded in CSV file for other metric types, and use calculate_other_metric_stats()", "id": "f7872:c0:m18"}
{"signature": "def balls(timeout=timeout):", "body": "rc = requests.get(messages_url, timeout=timeout)<EOL>rc.encoding = '<STR_LIT:utf-8>'  <EOL>rc = rc.text<EOL>data = re.findall('<STR_LIT>', rc)<EOL>balls = {}<EOL>for i in data:<EOL><INDENT>balls[int(i[<NUM_LIT:0>])] = i[<NUM_LIT:1>]<EOL><DEDENT>return balls<EOL>", "docstring": "Return all balls in dict {id0: ball0, id1: ball1}.", "id": "f3437:m6"}
{"signature": "def _data_rate_default(self):", "body": "raise NotImplementedError('<STR_LIT>')<EOL>", "docstring": "Retrieve the default data rate for this ADC (in samples per second).\n        Should be implemented by subclasses.", "id": "f5460:c0:m1"}
{"signature": "def noise3d(self, x, y, z):", "body": "<EOL>stretch_offset = (x + y + z) * STRETCH_CONSTANT_3D<EOL>xs = x + stretch_offset<EOL>ys = y + stretch_offset<EOL>zs = z + stretch_offset<EOL>xsb = floor(xs)<EOL>ysb = floor(ys)<EOL>zsb = floor(zs)<EOL>squish_offset = (xsb + ysb + zsb) * SQUISH_CONSTANT_3D<EOL>xb = xsb + squish_offset<EOL>yb = ysb + squish_offset<EOL>zb = zsb + squish_offset<EOL>xins = xs - xsb<EOL>yins = ys - ysb<EOL>zins = zs - zsb<EOL>in_sum = xins + yins + zins<EOL>dx0 = x - xb<EOL>dy0 = y - yb<EOL>dz0 = z - zb<EOL>value = <NUM_LIT:0><EOL>extrapolate = self._extrapolate3d<EOL>if in_sum <= <NUM_LIT:1>: <EOL><INDENT>a_point = <NUM_LIT><EOL>a_score = xins<EOL>b_point = <NUM_LIT><EOL>b_score = yins<EOL>if a_score >= b_score and zins > b_score:<EOL><INDENT>b_score = zins<EOL>b_point = <NUM_LIT><EOL><DEDENT>elif a_score < b_score and zins > a_score:<EOL><INDENT>a_score = zins<EOL>a_point = <NUM_LIT><EOL><DEDENT>wins = <NUM_LIT:1> - in_sum<EOL>if wins > a_score or wins > b_score: <EOL><INDENT>c = b_point if (b_score > a_score) else a_point <EOL>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>xsv_ext0 = xsb - <NUM_LIT:1><EOL>xsv_ext1 = xsb<EOL>dx_ext0 = dx0 + <NUM_LIT:1><EOL>dx_ext1 = dx0<EOL><DEDENT>else:<EOL><INDENT>xsv_ext0 = xsv_ext1 = xsb + <NUM_LIT:1><EOL>dx_ext0 = dx_ext1 = dx0 - <NUM_LIT:1><EOL><DEDENT>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>ysv_ext0 = ysv_ext1 = ysb<EOL>dy_ext0 = dy_ext1 = dy0<EOL>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>ysv_ext1 -= <NUM_LIT:1><EOL>dy_ext1 += <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>ysv_ext0 -= <NUM_LIT:1><EOL>dy_ext0 += <NUM_LIT:1><EOL><DEDENT><DEDENT>else:<EOL><INDENT>ysv_ext0 = ysv_ext1 = ysb + <NUM_LIT:1><EOL>dy_ext0 = dy_ext1 = dy0 - <NUM_LIT:1><EOL><DEDENT>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>zsv_ext0 = zsb<EOL>zsv_ext1 = zsb - <NUM_LIT:1><EOL>dz_ext0 = dz0<EOL>dz_ext1 = dz0 + <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>zsv_ext0 = zsv_ext1 = zsb + <NUM_LIT:1><EOL>dz_ext0 = dz_ext1 = dz0 - <NUM_LIT:1><EOL><DEDENT><DEDENT>else: <EOL><INDENT>c = (a_point | b_point) <EOL>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>xsv_ext0 = xsb<EOL>xsv_ext1 = xsb - <NUM_LIT:1><EOL>dx_ext0 = dx0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dx_ext1 = dx0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>xsv_ext0 = xsv_ext1 = xsb + <NUM_LIT:1><EOL>dx_ext0 = dx0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dx_ext1 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL><DEDENT>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>ysv_ext0 = ysb<EOL>ysv_ext1 = ysb - <NUM_LIT:1><EOL>dy_ext0 = dy0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>ysv_ext0 = ysv_ext1 = ysb + <NUM_LIT:1><EOL>dy_ext0 = dy0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL><DEDENT>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>zsv_ext0 = zsb<EOL>zsv_ext1 = zsb - <NUM_LIT:1><EOL>dz_ext0 = dz0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>zsv_ext0 = zsv_ext1 = zsb + <NUM_LIT:1><EOL>dz_ext0 = dz0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL><DEDENT><DEDENT>attn0 = <NUM_LIT:2> - dx0 * dx0 - dy0 * dy0 - dz0 * dz0<EOL>if attn0 > <NUM_LIT:0>:<EOL><INDENT>attn0 *= attn0<EOL>value += attn0 * attn0 * extrapolate(xsb + <NUM_LIT:0>, ysb + <NUM_LIT:0>, zsb + <NUM_LIT:0>, dx0, dy0, dz0)<EOL><DEDENT>dx1 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy1 = dy0 - <NUM_LIT:0> - SQUISH_CONSTANT_3D<EOL>dz1 = dz0 - <NUM_LIT:0> - SQUISH_CONSTANT_3D<EOL>attn1 = <NUM_LIT:2> - dx1 * dx1 - dy1 * dy1 - dz1 * dz1<EOL>if attn1 > <NUM_LIT:0>:<EOL><INDENT>attn1 *= attn1<EOL>value += attn1 * attn1 * extrapolate(xsb + <NUM_LIT:1>, ysb + <NUM_LIT:0>, zsb + <NUM_LIT:0>, dx1, dy1, dz1)<EOL><DEDENT>dx2 = dx0 - <NUM_LIT:0> - SQUISH_CONSTANT_3D<EOL>dy2 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz2 = dz1<EOL>attn2 = <NUM_LIT:2> - dx2 * dx2 - dy2 * dy2 - dz2 * dz2<EOL>if attn2 > <NUM_LIT:0>:<EOL><INDENT>attn2 *= attn2<EOL>value += attn2 * attn2 * extrapolate(xsb + <NUM_LIT:0>, ysb + <NUM_LIT:1>, zsb + <NUM_LIT:0>, dx2, dy2, dz2)<EOL><DEDENT>dx3 = dx2<EOL>dy3 = dy1<EOL>dz3 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>attn3 = <NUM_LIT:2> - dx3 * dx3 - dy3 * dy3 - dz3 * dz3<EOL>if attn3 > <NUM_LIT:0>:<EOL><INDENT>attn3 *= attn3<EOL>value += attn3 * attn3 * extrapolate(xsb + <NUM_LIT:0>, ysb + <NUM_LIT:0>, zsb + <NUM_LIT:1>, dx3, dy3, dz3)<EOL><DEDENT><DEDENT>elif in_sum >= <NUM_LIT:2>: <EOL><INDENT>a_point = <NUM_LIT><EOL>a_score = xins<EOL>b_point = <NUM_LIT><EOL>b_score = yins<EOL>if a_score <= b_score and zins < b_score:<EOL><INDENT>b_score = zins<EOL>b_point = <NUM_LIT><EOL><DEDENT>elif a_score > b_score and zins < a_score:<EOL><INDENT>a_score = zins<EOL>a_point = <NUM_LIT><EOL><DEDENT>wins = <NUM_LIT:3> - in_sum<EOL>if wins < a_score or wins < b_score: <EOL><INDENT>c = b_point if (b_score < a_score) else a_point <EOL>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>xsv_ext0 = xsb + <NUM_LIT:2><EOL>xsv_ext1 = xsb + <NUM_LIT:1><EOL>dx_ext0 = dx0 - <NUM_LIT:2> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>dx_ext1 = dx0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>xsv_ext0 = xsv_ext1 = xsb<EOL>dx_ext0 = dx_ext1 = dx0 - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL><DEDENT>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>ysv_ext0 = ysv_ext1 = ysb + <NUM_LIT:1><EOL>dy_ext0 = dy_ext1 = dy0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>ysv_ext1 += <NUM_LIT:1><EOL>dy_ext1 -= <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>ysv_ext0 += <NUM_LIT:1><EOL>dy_ext0 -= <NUM_LIT:1><EOL><DEDENT><DEDENT>else:<EOL><INDENT>ysv_ext0 = ysv_ext1 = ysb<EOL>dy_ext0 = dy_ext1 = dy0 - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL><DEDENT>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>zsv_ext0 = zsb + <NUM_LIT:1><EOL>zsv_ext1 = zsb + <NUM_LIT:2><EOL>dz_ext0 = dz0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:2> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>zsv_ext0 = zsv_ext1 = zsb<EOL>dz_ext0 = dz_ext1 = dz0 - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL><DEDENT><DEDENT>else: <EOL><INDENT>c = (a_point & b_point) <EOL>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>xsv_ext0 = xsb + <NUM_LIT:1><EOL>xsv_ext1 = xsb + <NUM_LIT:2><EOL>dx_ext0 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dx_ext1 = dx0 - <NUM_LIT:2> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>xsv_ext0 = xsv_ext1 = xsb<EOL>dx_ext0 = dx0 - SQUISH_CONSTANT_3D<EOL>dx_ext1 = dx0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL><DEDENT>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>ysv_ext0 = ysb + <NUM_LIT:1><EOL>ysv_ext1 = ysb + <NUM_LIT:2><EOL>dy_ext0 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:2> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>ysv_ext0 = ysv_ext1 = ysb<EOL>dy_ext0 = dy0 - SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL><DEDENT>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>zsv_ext0 = zsb + <NUM_LIT:1><EOL>zsv_ext1 = zsb + <NUM_LIT:2><EOL>dz_ext0 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:2> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL><DEDENT>else:<EOL><INDENT>zsv_ext0 = zsv_ext1 = zsb<EOL>dz_ext0 = dz0 - SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL><DEDENT><DEDENT>dx3 = dx0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy3 = dy0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz3 = dz0 - <NUM_LIT:0> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>attn3 = <NUM_LIT:2> - dx3 * dx3 - dy3 * dy3 - dz3 * dz3<EOL>if attn3 > <NUM_LIT:0>:<EOL><INDENT>attn3 *= attn3<EOL>value += attn3 * attn3 * extrapolate(xsb + <NUM_LIT:1>, ysb + <NUM_LIT:1>, zsb + <NUM_LIT:0>, dx3, dy3, dz3)<EOL><DEDENT>dx2 = dx3<EOL>dy2 = dy0 - <NUM_LIT:0> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz2 = dz0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>attn2 = <NUM_LIT:2> - dx2 * dx2 - dy2 * dy2 - dz2 * dz2<EOL>if attn2 > <NUM_LIT:0>:<EOL><INDENT>attn2 *= attn2<EOL>value += attn2 * attn2 * extrapolate(xsb + <NUM_LIT:1>, ysb + <NUM_LIT:0>, zsb + <NUM_LIT:1>, dx2, dy2, dz2)<EOL><DEDENT>dx1 = dx0 - <NUM_LIT:0> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy1 = dy3<EOL>dz1 = dz2<EOL>attn1 = <NUM_LIT:2> - dx1 * dx1 - dy1 * dy1 - dz1 * dz1<EOL>if attn1 > <NUM_LIT:0>:<EOL><INDENT>attn1 *= attn1<EOL>value += attn1 * attn1 * extrapolate(xsb + <NUM_LIT:0>, ysb + <NUM_LIT:1>, zsb + <NUM_LIT:1>, dx1, dy1, dz1)<EOL><DEDENT>dx0 = dx0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>dy0 = dy0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>dz0 = dz0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>attn0 = <NUM_LIT:2> - dx0 * dx0 - dy0 * dy0 - dz0 * dz0<EOL>if attn0 > <NUM_LIT:0>:<EOL><INDENT>attn0 *= attn0<EOL>value += attn0 * attn0 * extrapolate(xsb + <NUM_LIT:1>, ysb + <NUM_LIT:1>, zsb + <NUM_LIT:1>, dx0, dy0, dz0)<EOL><DEDENT><DEDENT>else: <EOL><INDENT>p1 = xins + yins<EOL>if p1 > <NUM_LIT:1>:<EOL><INDENT>a_score = p1 - <NUM_LIT:1><EOL>a_point = <NUM_LIT><EOL>a_is_further_side = True<EOL><DEDENT>else:<EOL><INDENT>a_score = <NUM_LIT:1> - p1<EOL>a_point = <NUM_LIT><EOL>a_is_further_side = False<EOL><DEDENT>p2 = xins + zins<EOL>if p2 > <NUM_LIT:1>:<EOL><INDENT>b_score = p2 - <NUM_LIT:1><EOL>b_point = <NUM_LIT><EOL>b_is_further_side = True<EOL><DEDENT>else:<EOL><INDENT>b_score = <NUM_LIT:1> - p2<EOL>b_point = <NUM_LIT><EOL>b_is_further_side = False<EOL><DEDENT>p3 = yins + zins<EOL>if p3 > <NUM_LIT:1>:<EOL><INDENT>score = p3 - <NUM_LIT:1><EOL>if a_score <= b_score and a_score < score:<EOL><INDENT>a_point = <NUM_LIT><EOL>a_is_further_side = True<EOL><DEDENT>elif a_score > b_score and b_score < score:<EOL><INDENT>b_point = <NUM_LIT><EOL>b_is_further_side = True<EOL><DEDENT><DEDENT>else:<EOL><INDENT>score = <NUM_LIT:1> - p3<EOL>if a_score <= b_score and a_score < score:<EOL><INDENT>a_point = <NUM_LIT><EOL>a_is_further_side = False<EOL><DEDENT>elif a_score > b_score and b_score < score:<EOL><INDENT>b_point = <NUM_LIT><EOL>b_is_further_side = False<EOL><DEDENT><DEDENT>if a_is_further_side == b_is_further_side:<EOL><INDENT>if a_is_further_side: <EOL><INDENT>dx_ext0 = dx0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>dy_ext0 = dy0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>dz_ext0 = dz0 - <NUM_LIT:1> - <NUM_LIT:3> * SQUISH_CONSTANT_3D<EOL>xsv_ext0 = xsb + <NUM_LIT:1><EOL>ysv_ext0 = ysb + <NUM_LIT:1><EOL>zsv_ext0 = zsb + <NUM_LIT:1><EOL>c = (a_point & b_point)<EOL>if (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>dx_ext1 = dx0 - <NUM_LIT:2> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>xsv_ext1 = xsb + <NUM_LIT:2><EOL>ysv_ext1 = ysb<EOL>zsv_ext1 = zsb<EOL><DEDENT>elif (c & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>dx_ext1 = dx0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:2> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>xsv_ext1 = xsb<EOL>ysv_ext1 = ysb + <NUM_LIT:2><EOL>zsv_ext1 = zsb<EOL><DEDENT>else:<EOL><INDENT>dx_ext1 = dx0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:2> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>xsv_ext1 = xsb<EOL>ysv_ext1 = ysb<EOL>zsv_ext1 = zsb + <NUM_LIT:2><EOL><DEDENT><DEDENT>else:<EOL><INDENT>dx_ext0 = dx0<EOL>dy_ext0 = dy0<EOL>dz_ext0 = dz0<EOL>xsv_ext0 = xsb<EOL>ysv_ext0 = ysb<EOL>zsv_ext0 = zsb<EOL>c = (a_point | b_point)<EOL>if (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>dx_ext1 = dx0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>xsv_ext1 = xsb - <NUM_LIT:1><EOL>ysv_ext1 = ysb + <NUM_LIT:1><EOL>zsv_ext1 = zsb + <NUM_LIT:1><EOL><DEDENT>elif (c & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>dx_ext1 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>xsv_ext1 = xsb + <NUM_LIT:1><EOL>ysv_ext1 = ysb - <NUM_LIT:1><EOL>zsv_ext1 = zsb + <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>dx_ext1 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>xsv_ext1 = xsb + <NUM_LIT:1><EOL>ysv_ext1 = ysb + <NUM_LIT:1><EOL>zsv_ext1 = zsb - <NUM_LIT:1><EOL><DEDENT><DEDENT><DEDENT>else: <EOL><INDENT>if a_is_further_side:<EOL><INDENT>c1 = a_point<EOL>c2 = b_point<EOL><DEDENT>else:<EOL><INDENT>c1 = b_point<EOL>c2 = a_point<EOL><DEDENT>if (c1 & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>dx_ext0 = dx0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy_ext0 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz_ext0 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>xsv_ext0 = xsb - <NUM_LIT:1><EOL>ysv_ext0 = ysb + <NUM_LIT:1><EOL>zsv_ext0 = zsb + <NUM_LIT:1><EOL><DEDENT>elif (c1 & <NUM_LIT>) == <NUM_LIT:0>:<EOL><INDENT>dx_ext0 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy_ext0 = dy0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz_ext0 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>xsv_ext0 = xsb + <NUM_LIT:1><EOL>ysv_ext0 = ysb - <NUM_LIT:1><EOL>zsv_ext0 = zsb + <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>dx_ext0 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy_ext0 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz_ext0 = dz0 + <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>xsv_ext0 = xsb + <NUM_LIT:1><EOL>ysv_ext0 = ysb + <NUM_LIT:1><EOL>zsv_ext0 = zsb - <NUM_LIT:1><EOL><DEDENT>dx_ext1 = dx0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy_ext1 = dy0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz_ext1 = dz0 - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>xsv_ext1 = xsb<EOL>ysv_ext1 = ysb<EOL>zsv_ext1 = zsb<EOL>if (c2 & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>dx_ext1 -= <NUM_LIT:2><EOL>xsv_ext1 += <NUM_LIT:2><EOL><DEDENT>elif (c2 & <NUM_LIT>) != <NUM_LIT:0>:<EOL><INDENT>dy_ext1 -= <NUM_LIT:2><EOL>ysv_ext1 += <NUM_LIT:2><EOL><DEDENT>else:<EOL><INDENT>dz_ext1 -= <NUM_LIT:2><EOL>zsv_ext1 += <NUM_LIT:2><EOL><DEDENT><DEDENT>dx1 = dx0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dy1 = dy0 - <NUM_LIT:0> - SQUISH_CONSTANT_3D<EOL>dz1 = dz0 - <NUM_LIT:0> - SQUISH_CONSTANT_3D<EOL>attn1 = <NUM_LIT:2> - dx1 * dx1 - dy1 * dy1 - dz1 * dz1<EOL>if attn1 > <NUM_LIT:0>:<EOL><INDENT>attn1 *= attn1<EOL>value += attn1 * attn1 * extrapolate(xsb + <NUM_LIT:1>, ysb + <NUM_LIT:0>, zsb + <NUM_LIT:0>, dx1, dy1, dz1)<EOL><DEDENT>dx2 = dx0 - <NUM_LIT:0> - SQUISH_CONSTANT_3D<EOL>dy2 = dy0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>dz2 = dz1<EOL>attn2 = <NUM_LIT:2> - dx2 * dx2 - dy2 * dy2 - dz2 * dz2<EOL>if attn2 > <NUM_LIT:0>:<EOL><INDENT>attn2 *= attn2<EOL>value += attn2 * attn2 * extrapolate(xsb + <NUM_LIT:0>, ysb + <NUM_LIT:1>, zsb + <NUM_LIT:0>, dx2, dy2, dz2)<EOL><DEDENT>dx3 = dx2<EOL>dy3 = dy1<EOL>dz3 = dz0 - <NUM_LIT:1> - SQUISH_CONSTANT_3D<EOL>attn3 = <NUM_LIT:2> - dx3 * dx3 - dy3 * dy3 - dz3 * dz3<EOL>if attn3 > <NUM_LIT:0>:<EOL><INDENT>attn3 *= attn3<EOL>value += attn3 * attn3 * extrapolate(xsb + <NUM_LIT:0>, ysb + <NUM_LIT:0>, zsb + <NUM_LIT:1>, dx3, dy3, dz3)<EOL><DEDENT>dx4 = dx0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy4 = dy0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz4 = dz0 - <NUM_LIT:0> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>attn4 = <NUM_LIT:2> - dx4 * dx4 - dy4 * dy4 - dz4 * dz4<EOL>if attn4 > <NUM_LIT:0>:<EOL><INDENT>attn4 *= attn4<EOL>value += attn4 * attn4 * extrapolate(xsb + <NUM_LIT:1>, ysb + <NUM_LIT:1>, zsb + <NUM_LIT:0>, dx4, dy4, dz4)<EOL><DEDENT>dx5 = dx4<EOL>dy5 = dy0 - <NUM_LIT:0> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dz5 = dz0 - <NUM_LIT:1> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>attn5 = <NUM_LIT:2> - dx5 * dx5 - dy5 * dy5 - dz5 * dz5<EOL>if attn5 > <NUM_LIT:0>:<EOL><INDENT>attn5 *= attn5<EOL>value += attn5 * attn5 * extrapolate(xsb + <NUM_LIT:1>, ysb + <NUM_LIT:0>, zsb + <NUM_LIT:1>, dx5, dy5, dz5)<EOL><DEDENT>dx6 = dx0 - <NUM_LIT:0> - <NUM_LIT:2> * SQUISH_CONSTANT_3D<EOL>dy6 = dy4<EOL>dz6 = dz5<EOL>attn6 = <NUM_LIT:2> - dx6 * dx6 - dy6 * dy6 - dz6 * dz6<EOL>if attn6 > <NUM_LIT:0>:<EOL><INDENT>attn6 *= attn6<EOL>value += attn6 * attn6 * extrapolate(xsb + <NUM_LIT:0>, ysb + <NUM_LIT:1>, zsb + <NUM_LIT:1>, dx6, dy6, dz6)<EOL><DEDENT><DEDENT>attn_ext0 = <NUM_LIT:2> - dx_ext0 * dx_ext0 - dy_ext0 * dy_ext0 - dz_ext0 * dz_ext0<EOL>if attn_ext0 > <NUM_LIT:0>:<EOL><INDENT>attn_ext0 *= attn_ext0<EOL>value += attn_ext0 * attn_ext0 * extrapolate(xsv_ext0, ysv_ext0, zsv_ext0, dx_ext0, dy_ext0, dz_ext0)<EOL><DEDENT>attn_ext1 = <NUM_LIT:2> - dx_ext1 * dx_ext1 - dy_ext1 * dy_ext1 - dz_ext1 * dz_ext1<EOL>if attn_ext1 > <NUM_LIT:0>:<EOL><INDENT>attn_ext1 *= attn_ext1<EOL>value += attn_ext1 * attn_ext1 * extrapolate(xsv_ext1, ysv_ext1, zsv_ext1, dx_ext1, dy_ext1, dz_ext1)<EOL><DEDENT>return value / NORM_CONSTANT_3D<EOL>", "docstring": "Generate 3D OpenSimplex noise from X,Y,Z coordinates.", "id": "f16267:c0:m5"}
{"signature": "@uni_in_place_transformation<EOL>def expand_internal_causal(universe: BELGraph, graph: BELGraph) -> None:", "body": "expand_internal(universe, graph, edge_predicates=is_causal_relation)<EOL>", "docstring": "Add causal edges between entities in the sub-graph.\n\n    Is an extremely thin wrapper around :func:`expand_internal`.\n\n    :param universe: A BEL graph representing the universe of all knowledge\n    :param graph: The target BEL graph to enrich with causal relations between contained nodes\n\n    Equivalent to:\n\n    >>> from pybel_tools.mutation import expand_internal\n    >>> from pybel.struct.filters.edge_predicates import is_causal_relation\n    >>> expand_internal(universe, graph, edge_predicates=is_causal_relation)", "id": "f9376:m15"}
{"signature": "def pr_heronian_mean(self):", "body": "return heronian_mean((self.precision(), self.recall()))<EOL>", "docstring": "r\"\"\"Return Heronian mean of precision & recall.\n\n        The Heronian mean of precision and recall is defined as:\n        :math:`\\frac{precision + \\sqrt{precision \\cdot recall} + recall}{3}`\n\n        Cf. https://en.wikipedia.org/wiki/Heronian_mean\n\n        Returns\n        -------\n        float\n            The Heronian mean of the confusion table's precision & recall\n\n        Example\n        -------\n        >>> ct = ConfusionTable(120, 60, 20, 30)\n        >>> ct.pr_heronian_mean()\n        0.8284071761178939", "id": "f6662:c0:m37"}
{"signature": "def _get_user_class(self, name):", "body": "self._user_classes.setdefault(name, _make_user_class(self, name))<EOL>return self._user_classes[name]<EOL>", "docstring": "Get or create a user class of the given type.", "id": "f423:c0:m20"}
{"signature": "def _generate_circle(self):", "body": "total_weight = <NUM_LIT:0><EOL>for node in self.nodes:<EOL><INDENT>total_weight += self.weights.get(node, <NUM_LIT:1>)<EOL><DEDENT>for node in self.nodes:<EOL><INDENT>weight = <NUM_LIT:1><EOL>if node in self.weights:<EOL><INDENT>weight = self.weights.get(node)<EOL><DEDENT>factor = math.floor((<NUM_LIT> * len(self.nodes) * weight) / total_weight)<EOL>for j in range(<NUM_LIT:0>, int(factor)):<EOL><INDENT>b_key = bytearray(self._hash_digest('<STR_LIT>' % (node, j)))<EOL>for i in range(<NUM_LIT:0>, <NUM_LIT:3>):<EOL><INDENT>key = self._hash_val(b_key, lambda x: x + i * <NUM_LIT:4>)<EOL>self.ring[key] = node<EOL>self._sorted_keys.append(key)<EOL><DEDENT><DEDENT><DEDENT>self._sorted_keys.sort()<EOL>", "docstring": "Generates the circle.", "id": "f8573:c0:m1"}
{"signature": "@DictProperty('<STR_LIT>', '<STR_LIT>', read_only=True)<EOL><INDENT>def params(self):<DEDENT>", "body": "params = FormsDict()<EOL>for key, value in self.query.allitems():<EOL><INDENT>params[key] = value<EOL><DEDENT>for key, value in self.forms.allitems():<EOL><INDENT>params[key] = value<EOL><DEDENT>return params<EOL>", "docstring": "A :class:`FormsDict` with the combined values of :attr:`query` and\n            :attr:`forms`. File uploads are stored in :attr:`files`.", "id": "f12971:c12:m12"}
{"signature": "def relative_folder(self, module, folder):", "body": "folder = self._relative_to_absolute(module, folder)<EOL>return self.folder(folder)<EOL>", "docstring": "Load a folder located relative to a module and return the processed\n        result.\n\n        :param str module: can be\n\n          - a path to a folder\n          - a path to a file\n          - a module name\n\n        :param str folder: the path of a folder relative to :paramref:`module`\n        :return: a list of the results of the processing\n        :rtype: list\n\n        Depending on :meth:`chooses_path` some paths may not be loaded.\n        Every loaded path is processed and returned part of the returned list.\n        You can use :meth:`choose_paths` to find out which paths are chosen to\n        load.", "id": "f507:c0:m5"}
{"signature": "def _new_report(self,<EOL>type,<EOL>start_date,<EOL>end_date,<EOL>product_id='<STR_LIT>',<EOL>account_id=None,<EOL>format=None,<EOL>email=None):", "body": "data = {<EOL>'<STR_LIT:type>':type,<EOL>'<STR_LIT>':self._format_iso_time(start_date),<EOL>'<STR_LIT>':self._format_iso_time(end_date),<EOL>'<STR_LIT>':product_id,<EOL>'<STR_LIT>':account_id,<EOL>'<STR_LIT>':format,<EOL>'<STR_LIT:email>':email<EOL>}<EOL>return self._post('<STR_LIT>', data=data)<EOL>", "docstring": "`<https://docs.exchange.coinbase.com/#create-a-new-report>`_", "id": "f11268:c1:m17"}
{"signature": "def makePics(self):", "body": "rescanNeeded=False<EOL>for fname in smartSort(self.fnames):<EOL><INDENT>if fname in self.fnames2:<EOL><INDENT>continue<EOL><DEDENT>ext=os.path.splitext(fname)[<NUM_LIT:1>].lower()<EOL>if ext in [\"<STR_LIT>\",\"<STR_LIT>\"]:<EOL><INDENT>if not fname in self.abfFolder2:<EOL><INDENT>self.log.debug(\"<STR_LIT>\",fname)<EOL>shutil.copy(os.path.join(self.abfFolder,fname),os.path.join(self.abfFolder2,fname))<EOL>rescanNeeded=True<EOL><DEDENT><DEDENT>if ext in [\"<STR_LIT>\",\"<STR_LIT>\"]:<EOL><INDENT>if not fname+\"<STR_LIT>\" in self.fnames2:<EOL><INDENT>self.log.debug(\"<STR_LIT>\",fname)<EOL>swhlab.swh_image.TIF_to_jpg(os.path.join(self.abfFolder,fname),saveAs=os.path.join(self.abfFolder2,fname+\"<STR_LIT>\"))<EOL>rescanNeeded=True<EOL><DEDENT><DEDENT><DEDENT>if rescanNeeded:<EOL><INDENT>self.log.debug(\"<STR_LIT>\")<EOL>self.log.debug(\"<STR_LIT>\")<EOL>self.folderScan()<EOL><DEDENT>", "docstring": "convert every .image we find to a ./swhlab/ image", "id": "f11344:c0:m7"}
{"signature": "@app.route(\"<STR_LIT:/>\")<EOL>def helloWorld():", "body": "return", "docstring": "Since the path '/' does not match the regular expression r'/api/*',\nthis route does not have CORS headers set.", "id": "f13107:m0"}
{"signature": "def T_dependent_property_integral_over_T(self, T1, T2):", "body": "Tavg = <NUM_LIT:0.5>*(T1+T2)<EOL>if self.method:<EOL><INDENT>if self.test_method_validity(Tavg, self.method):<EOL><INDENT>try:<EOL><INDENT>return self.calculate_integral_over_T(T1, T2, self.method)<EOL><DEDENT>except:  <EOL><INDENT>pass<EOL><DEDENT><DEDENT><DEDENT>sorted_valid_methods = self.select_valid_methods(Tavg)<EOL>for method in sorted_valid_methods:<EOL><INDENT>try:<EOL><INDENT>return self.calculate_integral_over_T(T1, T2, method)<EOL><DEDENT>except:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>return None<EOL>", "docstring": "r'''Method to calculate the integral of a property over temperature \n        with respect to temperature, using a specified method. Methods found\n        valid by `select_valid_methods` are attempted until a method succeeds. \n        If no methods are valid and succeed, None is returned.\n\n        Calls `calculate_integral_over_T` internally to perform the actual\n        calculation.\n\n        .. math::\n            \\text{integral} = \\int_{T_1}^{T_2} \\frac{\\text{property}}{T} \\; dT\n\n        Parameters\n        ----------\n        T1 : float\n            Lower limit of integration, [K]\n        T2 : float\n            Upper limit of integration, [K]\n        method : str\n            Method for which to find the integral\n\n        Returns\n        -------\n        integral : float\n            Calculated integral of the property over the given range, \n            [`units`]", "id": "f15806:c0:m17"}
{"signature": "def get_position(self):", "body": "x = float(self.convert_xunits(self._dashx))<EOL>y = float(self.convert_yunits(self._dashy))<EOL>return x, y<EOL>", "docstring": "Return the position of the text as a tuple (*x*, *y*)", "id": "f17190:c1:m2"}
{"signature": "def t_CLIENT_KWD(t):", "body": "return t<EOL>", "docstring": "r'client", "id": "f12143:m1"}
{"signature": "def p_conc_license_1(self, p):", "body": "p[<NUM_LIT:0>] = utils.NoAssert()<EOL>", "docstring": "conc_license : NO_ASSERT", "id": "f3753:c0:m48"}
{"signature": "def __init__(self, response=None, error=None, data=None, url=None,<EOL>message=None):", "body": "self.response = response<EOL>self.data = data<EOL>self.error = error<EOL>self.url = url<EOL>if not message:<EOL><INDENT>message = self.get_message()<EOL><DEDENT>if url:<EOL><INDENT>message += \"<STR_LIT>\" + url<EOL><DEDENT>super().__init__(message)<EOL>", "docstring": "Add the response and data attributes\n\nExtract message from the error if not explicitly given", "id": "f4747:c0:m0"}
{"signature": "def iter_by_block(self, count=DEFAULT_BLOCK_SIZE):", "body": "return AsyncStreamIterator(lambda: self.read(count))<EOL>", "docstring": "Read/iterate stream by block.\n\n:rtype: :py:class:`aioftp.AsyncStreamIterator`\n\n::\n\n    >>> async for block in stream.iter_by_block(block_size):\n    ...     ...", "id": "f9228:c6:m9"}
{"signature": "def __enter__(self):<EOL>", "body": "self.open()<EOL>return self<EOL>", "docstring": "Open on entering with-block.", "id": "f13258:c0:m1"}
{"signature": "@main.command(name='<STR_LIT>', short_help='<STR_LIT>')<EOL>@click.argument('<STR_LIT>')<EOL>@click.argument('<STR_LIT>', nargs=-<NUM_LIT:1>, required=True)<EOL>@click.option('<STR_LIT>',<EOL>'<STR_LIT>',<EOL>help=\"<STR_LIT>\")<EOL>@click.option('<STR_LIT>',<EOL>metavar='<STR_LIT>',<EOL>multiple=True,<EOL>help='<STR_LIT>'<EOL>'<STR_LIT>')<EOL>@click.option('<STR_LIT>',<EOL>'<STR_LIT>',<EOL>is_flag=True,<EOL>help='<STR_LIT>')<EOL>@click.option('<STR_LIT>',<EOL>'<STR_LIT>',<EOL>is_flag=True,<EOL>help='<STR_LIT>')<EOL>@click.option('<STR_LIT>',<EOL>is_flag=True,<EOL>help='<STR_LIT>'<EOL>'<STR_LIT>')<EOL>@click.option('<STR_LIT>',<EOL>'<STR_LIT>',<EOL>type=click.Choice(['<STR_LIT>', '<STR_LIT>']),<EOL>default='<STR_LIT>',<EOL>help='<STR_LIT>')<EOL>@stash_option<EOL>@passphrase_option<EOL>@backend_option<EOL>def put_key(key_name,<EOL>value,<EOL>description,<EOL>meta,<EOL>modify,<EOL>add,<EOL>lock,<EOL>key_type,<EOL>stash,<EOL>passphrase,<EOL>backend):", "body": "stash = _get_stash(backend, stash, passphrase)<EOL>try:<EOL><INDENT>click.echo('<STR_LIT>'.format(key_type))<EOL>stash.put(<EOL>name=key_name,<EOL>value=_build_dict_from_key_value(value),<EOL>modify=modify,<EOL>metadata=_build_dict_from_key_value(meta),<EOL>description=description,<EOL>lock=lock,<EOL>key_type=key_type,<EOL>add=add)<EOL>click.echo('<STR_LIT>')<EOL><DEDENT>except GhostError as ex:<EOL><INDENT>sys.exit(ex)<EOL><DEDENT>", "docstring": "Insert a key to the stash\n\n    `KEY_NAME` is the name of the key to insert\n\n    `VALUE` is a key=value argument which can be provided multiple times.\n    it is the encrypted value of your key", "id": "f575:m14"}
{"signature": "def split(s, sep=None, maxsplit=-<NUM_LIT:1>):", "body": "return s.split(sep, maxsplit)<EOL>", "docstring": "split(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string.  If maxsplit is given, splits at no more than\n    maxsplit places (resulting in at most maxsplit+1 words).  If sep\n    is not specified or is None, any whitespace string is a separator.\n\n    (split and splitfields are synonymous)", "id": "f16453:m8"}
{"signature": "def move_not_inwards(s):", "body": "if s.op == '<STR_LIT>':<EOL><INDENT>NOT = lambda b: move_not_inwards(~b)<EOL>a = s.args[<NUM_LIT:0>]<EOL>if a.op == '<STR_LIT>': return move_not_inwards(a.args[<NUM_LIT:0>]) <EOL>if a.op =='<STR_LIT:&>': return associate('<STR_LIT:|>', list(map(NOT, a.args)))<EOL>if a.op =='<STR_LIT:|>': return associate('<STR_LIT:&>', list(map(NOT, a.args)))<EOL>return s<EOL><DEDENT>elif is_symbol(s.op) or not s.args:<EOL><INDENT>return s<EOL><DEDENT>else:<EOL><INDENT>return Expr(s.op, *list(map(move_not_inwards, s.args)))<EOL><DEDENT>", "docstring": "Rewrite sentence s by moving negation sign inward.\n    >>> move_not_inwards(~(A | B))\n    (~A & ~B)\n    >>> move_not_inwards(~(A & B))\n    (~A | ~B)\n    >>> move_not_inwards(~(~(A | ~B) | ~~C))\n    ((A | ~B) & ~C)", "id": "f1683:m15"}
{"signature": "def is_valid_with_config(self, config):", "body": "raise NotImplementedError<EOL>", "docstring": "Check if output format is valid with other process parameters.\n\nParameters\n----------\nconfig : dictionary\n    output configuration parameters\n\nReturns\n-------\nis_valid : bool", "id": "f12806:c2:m4"}
{"signature": "def get_iso_string(input):", "body": "return input.replace(tzinfo=None, microsecond=<NUM_LIT:0>).isoformat() + '<STR_LIT>'<EOL>", "docstring": "Strips out the microseconds from datetime objects, and returns a proper ISO-format UTC string.\n\n    :param input: Datetime object.\n    :returns string: A datetime ISO format string with", "id": "f10862:m0"}
{"signature": "def load_all_methods(self):", "body": "methods = [SIMPLE]     <EOL>self.all_methods = set(methods)<EOL>", "docstring": "r'''Method to initialize the object by precomputing any values which\n        may be used repeatedly and by retrieving mixture-specific variables.\n        All data are stored as attributes. This method also sets :obj:`Tmin`, \n        :obj:`Tmax`, and :obj:`all_methods` as a set of methods which should \n        work to calculate the property.\n\n        Called on initialization only. See the source code for the variables at\n        which the coefficients are stored. The coefficients can safely be\n        altered once the class is initialized. This method can be called again\n        to reset the parameters.", "id": "f15779:c5:m1"}
{"signature": "def hump_to_underscore(name):", "body": "new_name = '<STR_LIT>'<EOL>pos = <NUM_LIT:0><EOL>for c in name:<EOL><INDENT>if pos == <NUM_LIT:0>:<EOL><INDENT>new_name = c.lower()<EOL><DEDENT>elif <NUM_LIT> <= ord(c) <= <NUM_LIT>:<EOL><INDENT>new_name += '<STR_LIT:_>' + c.lower()<EOL>pass<EOL><DEDENT>else:<EOL><INDENT>new_name += c<EOL><DEDENT>pos += <NUM_LIT:1><EOL>pass<EOL><DEDENT>return new_name<EOL>", "docstring": "Convert Hump style to underscore\n\n:param name: Hump Character\n:return: str", "id": "f1110:m1"}
{"signature": "def _validate_zooms(zooms):", "body": "if isinstance(zooms, dict):<EOL><INDENT>if any([a not in zooms for a in [\"<STR_LIT>\", \"<STR_LIT>\"]]):<EOL><INDENT>raise MapcheteConfigError(\"<STR_LIT>\")<EOL><DEDENT>zmin = _validate_zoom(zooms[\"<STR_LIT>\"])<EOL>zmax = _validate_zoom(zooms[\"<STR_LIT>\"])<EOL>if zmin > zmax:<EOL><INDENT>raise MapcheteConfigError(<EOL>\"<STR_LIT>\")<EOL><DEDENT>return list(range(zmin, zmax + <NUM_LIT:1>))<EOL><DEDENT>elif isinstance(zooms, list):<EOL><INDENT>if len(zooms) == <NUM_LIT:1>:<EOL><INDENT>return zooms<EOL><DEDENT>elif len(zooms) == <NUM_LIT:2>:<EOL><INDENT>zmin, zmax = sorted([_validate_zoom(z) for z in zooms])<EOL>return list(range(zmin, zmax + <NUM_LIT:1>))<EOL><DEDENT>else:<EOL><INDENT>return zooms<EOL><DEDENT><DEDENT>else:<EOL><INDENT>return [_validate_zoom(zooms)]<EOL><DEDENT>", "docstring": "Return a list of zoom levels.\n\nFollowing inputs are converted:\n- int --> [int]\n- dict{min, max} --> range(min, max + 1)\n- [int] --> [int]\n- [int, int] --> range(smaller int, bigger int + 1)", "id": "f12805:m10"}
{"signature": "def token_from_fragment(self, authorization_response):", "body": "self._client.parse_request_uri_response(<EOL>authorization_response, state=self._state<EOL>)<EOL>self.token = self._client.token<EOL>return self.token<EOL>", "docstring": "Parse token from the URI fragment, used by MobileApplicationClients.\n\n        :param authorization_response: The full URL of the redirect back to you\n        :return: A token dict", "id": "f6413:c1:m13"}
{"signature": "def _cond_cc(self, word, suffix_len):", "body": "return word[-suffix_len - <NUM_LIT:1>] == '<STR_LIT:l>'<EOL>", "docstring": "Return Lovins' condition CC.\n\n        Parameters\n        ----------\n        word : str\n            Word to check\n        suffix_len : int\n            Suffix length\n\n        Returns\n        -------\n        bool\n            True if condition is met", "id": "f6562:c0:m27"}
{"signature": "def __init__(self, system, supervisor):", "body": "self._system = system<EOL>self._supervisor = supervisor<EOL>", "docstring": ":param system:\n:type system: :class:`ActorSystem`\n\n:param supervisor:\n:type supervisor: :class:`InternalRef`", "id": "f5555:c0:m0"}
{"signature": "def set_dicom_file2(self, dcm_file):", "body": "self.dcmf2 = self._read_dcmfile(dcm_file)<EOL>", "docstring": "Parameters\n----------\ndcm_file: str (path to file) or DicomFile or namedtuple", "id": "f4068:c1:m3"}
{"signature": "@logExceptions(_LOGGER)<EOL><INDENT>@g_retrySQL<EOL>def modelSetCompleted(self, modelID, completionReason, completionMsg,<EOL>cpuTime=<NUM_LIT:0>, useConnectionID=True):<DEDENT>", "body": "if completionMsg is None:<EOL><INDENT>completionMsg = '<STR_LIT>'<EOL><DEDENT>query = '<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>''<STR_LIT>'% (self.modelsTableName,)<EOL>sqlParams = [self.STATUS_COMPLETED, completionReason, completionMsg,<EOL>cpuTime, modelID]<EOL>if useConnectionID:<EOL><INDENT>query += \"<STR_LIT>\"<EOL>sqlParams.append(self._connectionID)<EOL><DEDENT>with ConnectionFactory.get() as conn:<EOL><INDENT>numRowsAffected = conn.cursor.execute(query, sqlParams)<EOL><DEDENT>if numRowsAffected != <NUM_LIT:1>:<EOL><INDENT>raise InvalidConnectionException(<EOL>(\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\") % (modelID, self._connectionID, numRowsAffected))<EOL><DEDENT>", "docstring": "Mark a model as completed, with the given completionReason and\n        completionMsg. This will fail if the model does not currently belong to this\n        client (connection_id doesn't match).\n\n        Parameters:\n        ----------------------------------------------------------------\n        modelID:             model ID of model to modify\n        completionReason:    completionReason string\n        completionMsg:       completionMsg string\n        cpuTime:             amount of CPU time spent on this model\n        useConnectionID:     True if the connection id of the calling function\n                              must be the same as the connection that created the\n                              job. Set to True for hypersearch workers, which use\n                              this mechanism for orphaned model detection.", "id": "f17555:c1:m62"}
{"signature": "def p_unit_list(self, p):", "body": "if isinstance(p[<NUM_LIT:1>], list):<EOL><INDENT>if len(p) >= <NUM_LIT:3>:<EOL><INDENT>if isinstance(p[<NUM_LIT:2>], list):<EOL><INDENT>p[<NUM_LIT:1>].extend(p[<NUM_LIT:2>])<EOL><DEDENT>else:<EOL><INDENT>p[<NUM_LIT:1>].append(p[<NUM_LIT:2>])<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>p[<NUM_LIT:1>] = [p[<NUM_LIT:1>]]<EOL><DEDENT>p[<NUM_LIT:0>] = p[<NUM_LIT:1>]<EOL>", "docstring": "unit_list                : unit_list unit\n                                     | unit", "id": "f12427:c2:m5"}
{"signature": "def __len__(self):", "body": "return self._size<EOL>", "docstring": "!\n        @brief Returns size of the network that defines by amount of neuron in it.\n\n        @return (uint) Size of self-organized map (amount of neurons).", "id": "f15666:c3:m6"}
{"signature": "def load_resource_module(self):", "body": "<EOL>try:<EOL><INDENT>name = '<STR_LIT>'.format(self.name, '<STR_LIT>')<EOL>self.dependencies_module = importlib.import_module(name)<EOL><DEDENT>except ModuleNotFoundError as err:<EOL><INDENT>raise EffectError(<EOL>(<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>).format(self.name, err))<EOL><DEDENT>try:<EOL><INDENT>self.resources = getattr(self.dependencies_module, '<STR_LIT>')<EOL><DEDENT>except AttributeError:<EOL><INDENT>raise EffectError(\"<STR_LIT>\".format(name))<EOL><DEDENT>if not isinstance(self.resources, list):<EOL><INDENT>raise EffectError(<EOL>\"<STR_LIT>\".format(<EOL>name, type(self.resources)))<EOL><DEDENT>try:<EOL><INDENT>self.effect_packages = getattr(self.dependencies_module, '<STR_LIT>')<EOL><DEDENT>except AttributeError:<EOL><INDENT>raise EffectError(\"<STR_LIT>\".format(name))<EOL><DEDENT>if not isinstance(self.effect_packages, list):<EOL><INDENT>raise EffectError(<EOL>\"<STR_LIT>\".format(<EOL>name, type(self.effects)))<EOL><DEDENT>", "docstring": "Fetch the resource list", "id": "f14432:c1:m9"}
{"signature": "def create_topology(self, topologyName, topology):", "body": "if not topology or not topology.IsInitialized():<EOL><INDENT>raise_(StateException(\"<STR_LIT>\",<EOL>StateException.EX_TYPE_PROTOBUF_ERROR), sys.exc_info()[<NUM_LIT:2>])<EOL><DEDENT>path = self.get_topology_path(topologyName)<EOL>LOG.info(\"<STR_LIT>\".format(<EOL>topologyName, path))<EOL>topologyString = topology.SerializeToString()<EOL>try:<EOL><INDENT>self.client.create(path, value=topologyString, makepath=True)<EOL>return True<EOL><DEDENT>except NoNodeError:<EOL><INDENT>raise_(StateException(\"<STR_LIT>\",<EOL>StateException.EX_TYPE_NO_NODE_ERROR), sys.exc_info()[<NUM_LIT:2>])<EOL><DEDENT>except NodeExistsError:<EOL><INDENT>raise_(StateException(\"<STR_LIT>\",<EOL>StateException.EX_TYPE_NODE_EXISTS_ERROR), sys.exc_info()[<NUM_LIT:2>])<EOL><DEDENT>except ZookeeperError:<EOL><INDENT>raise_(StateException(\"<STR_LIT>\",<EOL>StateException.EX_TYPE_ZOOKEEPER_ERROR), sys.exc_info()[<NUM_LIT:2>])<EOL><DEDENT>except Exception:<EOL><INDENT>raise<EOL><DEDENT>", "docstring": "crate topology", "id": "f7426:c0:m8"}
{"signature": "def add_cmd_to_checkplot(<EOL>cpx,<EOL>cmdpkl,<EOL>require_cmd_magcolor=True,<EOL>save_cmd_pngs=False<EOL>):", "body": "<EOL>if isinstance(cpx, str) and os.path.exists(cpx):<EOL><INDENT>cpdict = _read_checkplot_picklefile(cpx)<EOL><DEDENT>elif isinstance(cpx, dict):<EOL><INDENT>cpdict = cpx<EOL><DEDENT>else:<EOL><INDENT>LOGERROR('<STR_LIT>')<EOL>return None<EOL><DEDENT>if isinstance(cmdpkl, str) and os.path.exists(cmdpkl):<EOL><INDENT>with open(cmdpkl, '<STR_LIT:rb>') as infd:<EOL><INDENT>cmd = pickle.load(infd)<EOL><DEDENT><DEDENT>elif isinstance(cmdpkl, dict):<EOL><INDENT>cmd = cmdpkl<EOL><DEDENT>cpdict['<STR_LIT>'] = {}<EOL>cplist_mags = cmd['<STR_LIT>']<EOL>cplist_colors = cmd['<STR_LIT>']<EOL>for c1, c2, ym, ind in zip(cmd['<STR_LIT>'],<EOL>cmd['<STR_LIT>'],<EOL>cmd['<STR_LIT>'],<EOL>range(len(cmd['<STR_LIT>']))):<EOL><INDENT>if (c1 in cpdict['<STR_LIT>'] and<EOL>cpdict['<STR_LIT>'][c1] is not None):<EOL><INDENT>c1mag = cpdict['<STR_LIT>'][c1]<EOL><DEDENT>else:<EOL><INDENT>c1mag = np.nan<EOL><DEDENT>if (c2 in cpdict['<STR_LIT>'] and<EOL>cpdict['<STR_LIT>'][c2] is not None):<EOL><INDENT>c2mag = cpdict['<STR_LIT>'][c2]<EOL><DEDENT>else:<EOL><INDENT>c2mag = np.nan<EOL><DEDENT>if (ym in cpdict['<STR_LIT>'] and<EOL>cpdict['<STR_LIT>'][ym] is not None):<EOL><INDENT>ymmag = cpdict['<STR_LIT>'][ym]<EOL><DEDENT>else:<EOL><INDENT>ymmag = np.nan<EOL><DEDENT>if (require_cmd_magcolor and<EOL>not (np.isfinite(c1mag) and<EOL>np.isfinite(c2mag) and<EOL>np.isfinite(ymmag))):<EOL><INDENT>LOGWARNING(\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\" %<EOL>(c1, c2, ym, cpdict['<STR_LIT>']))<EOL>continue<EOL><DEDENT>try:<EOL><INDENT>thiscmd_title = r'<STR_LIT>' % (CMD_LABELS[c1],<EOL>CMD_LABELS[c2],<EOL>CMD_LABELS[ym])<EOL>fig = plt.figure(figsize=(<NUM_LIT:10>,<NUM_LIT:8>))<EOL>plt.plot(cplist_colors[:,ind],<EOL>cplist_mags[:,ind],<EOL>rasterized=True,<EOL>marker='<STR_LIT:o>',<EOL>linestyle='<STR_LIT:none>',<EOL>mew=<NUM_LIT:0>,<EOL>ms=<NUM_LIT:3>)<EOL>plt.plot([c1mag - c2mag], [ymmag],<EOL>ms=<NUM_LIT:20>,<EOL>color='<STR_LIT>',<EOL>marker='<STR_LIT:*>',<EOL>mew=<NUM_LIT:0>)<EOL>plt.xlabel(r'<STR_LIT>' % (CMD_LABELS[c1], CMD_LABELS[c2]))<EOL>plt.ylabel(r'<STR_LIT>' % CMD_LABELS[ym])<EOL>plt.title('<STR_LIT>' % (cpdict['<STR_LIT>'], thiscmd_title))<EOL>plt.gca().invert_yaxis()<EOL>cmdpng = StrIO()<EOL>plt.savefig(cmdpng, bbox_inches='<STR_LIT>',<EOL>pad_inches=<NUM_LIT:0.0>, format='<STR_LIT>')<EOL>cmdpng.seek(<NUM_LIT:0>)<EOL>cmdb64 = base64.b64encode(cmdpng.read())<EOL>cmdpng.close()<EOL>plt.close('<STR_LIT:all>')<EOL>plt.gcf().clear()<EOL>cpdict['<STR_LIT>']['<STR_LIT>' % (c1,c2,ym)] = cmdb64<EOL>if save_cmd_pngs:<EOL><INDENT>if isinstance(cpx, str):<EOL><INDENT>outpng = os.path.join(os.path.dirname(cpx),<EOL>'<STR_LIT>' %<EOL>(cpdict['<STR_LIT>'],<EOL>c1,c2,ym))<EOL><DEDENT>else:<EOL><INDENT>outpng = '<STR_LIT>' % (cpdict['<STR_LIT>'],<EOL>c1,c2,ym)<EOL><DEDENT>_base64_to_file(cmdb64, outpng)<EOL><DEDENT><DEDENT>except Exception as e:<EOL><INDENT>LOGEXCEPTION('<STR_LIT>' %<EOL>(c1, c2, ym, cmdpkl))<EOL>continue<EOL><DEDENT><DEDENT>if isinstance(cpx, str):<EOL><INDENT>cpf = _write_checkplot_picklefile(cpdict, outfile=cpx, protocol=<NUM_LIT:4>)<EOL>return cpf<EOL><DEDENT>elif isinstance(cpx, dict):<EOL><INDENT>return cpdict<EOL><DEDENT>", "docstring": "This adds CMD figures to a checkplot dict or pickle.\n\n    Looks up the CMDs in `cmdpkl`, adds the object from `cpx` as a gold(-ish)\n    star in the plot, and then saves the figure to a base64 encoded PNG, which\n    can then be read and used by the `checkplotserver`.\n\n    Parameters\n    ----------\n\n    cpx : str or dict\n        This is the input checkplot pickle or dict to add the CMD to.\n\n    cmdpkl : str or dict\n        The CMD pickle generated by the `colormagdiagram_cplist` or\n        `colormagdiagram_cpdir` functions above, or the dict produced by reading\n        this pickle in.\n\n    require_cmd_magcolor : bool\n        If this is True, a CMD plot will not be made if the color and mag keys\n        required by the CMD are not present or are nan in this checkplot's\n        objectinfo dict.\n\n    save_cmd_png : bool\n        If this is True, then will save the CMD plots that were generated and\n        added back to the checkplotdict as PNGs to the same directory as\n        `cpx`. If `cpx` is a dict, will save them to the current working\n        directory.\n\n    Returns\n    -------\n\n    str or dict\n        If `cpx` was a str filename of checkplot pickle, this will return that\n        filename to indicate that the CMD was added to the file. If `cpx` was a\n        checkplotdict, this will return the checkplotdict with a new key called\n        'colormagdiagram' containing the base64 encoded PNG binary streams of\n        all CMDs generated.", "id": "f14703:m5"}
{"signature": "def send_discuss_msg(self, *, discuss_id, message, auto_escape=False):", "body": "return super().__getattr__('<STR_LIT>')(discuss_id=discuss_id, message=message, auto_escape=auto_escape)<EOL>", "docstring": "\u53d1\u9001\u8ba8\u8bba\u7ec4\u6d88\u606f\n\n------------\n\n:param int discuss_id: \u8ba8\u8bba\u7ec4 ID\uff08\u6b63\u5e38\u60c5\u51b5\u4e0b\u770b\u4e0d\u5230\uff0c\u9700\u8981\u4ece\u8ba8\u8bba\u7ec4\u6d88\u606f\u4e0a\u62a5\u7684\u6570\u636e\u4e2d\u83b7\u5f97\uff09\n:param str | list[ dict[ str, unknown ] ] message: \u8981\u53d1\u9001\u7684\u5185\u5bb9\n:param bool auto_escape: \u6d88\u606f\u5185\u5bb9\u662f\u5426\u4f5c\u4e3a\u7eaf\u6587\u672c\u53d1\u9001\uff08\u5373\u4e0d\u89e3\u6790 CQ \u7801\uff09\uff0c`message` \u6570\u636e\u7c7b\u578b\u4e3a `list` \u65f6\u65e0\u6548\n:return: {\"message_id\": int \u6d88\u606fID}\n:rtype: dict[string, int]", "id": "f847:c0:m5"}
{"signature": "@map_types<EOL><INDENT>def __init__(self,<EOL>centre: dim.Position = (<NUM_LIT:0.0>, <NUM_LIT:0.0>),<EOL>intensity: dim.Luminosity = <NUM_LIT:0.1>,<EOL>sigma: dim.Length = <NUM_LIT>):<DEDENT>", "body": "super(SphericalGaussian, self).__init__(centre=centre, axis_ratio=<NUM_LIT:1.0>, phi=<NUM_LIT:0.0>, intensity=intensity,<EOL>sigma=sigma)<EOL>", "docstring": "The spherical Gaussian light profile.\n\n        Parameters\n        ----------\n        centre : (float, float)\n            The (y,x) arc-second coordinates of the profile centre.\n        intensity : float\n            Overall intensity normalisation of the light profiles (electrons per second).\n        sigma : float\n            The full-width half-maximum of the Gaussian.", "id": "f5955:c3:m0"}
{"signature": "def locked_put(self, credentials):", "body": "return self._backend.locked_put(self._key, credentials)<EOL>", "docstring": "Writes the given credentials to the store.\n\n        Args:\n            credentials: an instance of\n                :class:`oauth2client.client.Credentials`.", "id": "f2461:c1:m4"}
{"signature": "@property<EOL><INDENT>def tzinfo(self):<DEDENT>", "body": "return self._tzinfo<EOL>", "docstring": "timezone info object", "id": "f16496:c3:m5"}
{"signature": "@property<EOL><INDENT>def address(self):<DEDENT>", "body": "return GrapheneAddress.from_pubkey(repr(self), prefix=self.prefix)<EOL>", "docstring": "Obtain a GrapheneAddress from a public key", "id": "f8268:c4:m16"}
{"signature": "def _normalize_resource_type(self, type_):", "body": "return '<STR_LIT>'.join([s.capitalize() for s in type_.split('<STR_LIT>')])<EOL>", "docstring": "Normalizes the type passed to the api by capitalizing each part\n        of the type. For example:\n\n        sysctl::value -> Sysctl::Value\n        user -> User", "id": "f4028:c0:m4"}
{"signature": "def neurommsig_gene_ora(graph: BELGraph, genes: List[Gene]) -> float:", "body": "graph_genes = set(get_nodes_by_function(graph, GENE))<EOL>return len(graph_genes.intersection(genes)) / len(graph_genes)<EOL>", "docstring": "Calculate the percentage of target genes mappable to the graph.\n\n    Assume: graph central dogma inferred, collapsed to genes, collapsed variants", "id": "f9408:m3"}
{"signature": "def build_default_map(self):", "body": "default_map = {<EOL>\"<STR_LIT>\": {<EOL>\"<STR_LIT>\": self.check_following,<EOL>\"<STR_LIT>\": self.timeout,<EOL>\"<STR_LIT>\": self.porcelain,<EOL>},<EOL>\"<STR_LIT>\": {<EOL>\"<STR_LIT>\": self.twtfile,<EOL>},<EOL>\"<STR_LIT>\": {<EOL>\"<STR_LIT>\": self.use_pager,<EOL>\"<STR_LIT>\": self.use_cache,<EOL>\"<STR_LIT>\": self.limit_timeline,<EOL>\"<STR_LIT>\": self.timeout,<EOL>\"<STR_LIT>\": self.sorting,<EOL>\"<STR_LIT>\": self.porcelain,<EOL>\"<STR_LIT>\": self.twtfile,<EOL>\"<STR_LIT>\": self.timeline_update_interval,<EOL>},<EOL>\"<STR_LIT>\": {<EOL>\"<STR_LIT>\": self.use_pager,<EOL>\"<STR_LIT>\": self.use_cache,<EOL>\"<STR_LIT>\": self.limit_timeline,<EOL>\"<STR_LIT>\": self.timeout,<EOL>\"<STR_LIT>\": self.sorting,<EOL>\"<STR_LIT>\": self.porcelain,<EOL>\"<STR_LIT>\": self.timeline_update_interval,<EOL>}<EOL>}<EOL>return default_map<EOL>", "docstring": "Maps config options to the default values used by click, returns :class:`dict`.", "id": "f14593:c0:m28"}
{"signature": "def __init__(self, xy, width, height, angle=<NUM_LIT:0.0>, **kwargs):", "body": "Patch.__init__(self, **kwargs)<EOL>self.center = xy<EOL>self.width, self.height = width, height<EOL>self.angle = angle<EOL>self._path = Path.unit_circle()<EOL>self._patch_transform = transforms.IdentityTransform()<EOL>", "docstring": "*xy*\n  center of ellipse\n\n*width*\n  length of horizontal axis\n\n*height*\n  length of vertical axis\n\n*angle*\n  rotation in degrees (anti-clockwise)\n\nValid kwargs are:\n%(Patch)s", "id": "f17197:c11:m1"}
{"signature": "def _apply_template(template, target, *, checkout, extra_context):", "body": "with tempfile.TemporaryDirectory() as tempdir:<EOL><INDENT>repo_dir = cc_main.cookiecutter(<EOL>template,<EOL>checkout=checkout,<EOL>no_input=True,<EOL>output_dir=tempdir,<EOL>extra_context=extra_context)<EOL>for item in os.listdir(repo_dir):<EOL><INDENT>src = os.path.join(repo_dir, item)<EOL>dst = os.path.join(target, item)<EOL>if os.path.isdir(src):<EOL><INDENT>if os.path.exists(dst):<EOL><INDENT>shutil.rmtree(dst)<EOL><DEDENT>shutil.copytree(src, dst)<EOL><DEDENT>else:<EOL><INDENT>if os.path.exists(dst):<EOL><INDENT>os.remove(dst)<EOL><DEDENT>shutil.copy2(src, dst)<EOL><DEDENT><DEDENT><DEDENT>", "docstring": "Apply a template to a temporary directory and then copy results to target.", "id": "f13748:m4"}
{"signature": "@property<EOL><INDENT>def provider_name(self):<DEDENT>", "body": "identity_provider = utils.get_identity_provider(self.provider_id)<EOL>return identity_provider and identity_provider.name<EOL>", "docstring": "Readable name for the identity provider.", "id": "f16090:c8:m2"}
{"signature": "def _stop_docker_vm():", "body": "check_call_demoted(['<STR_LIT>', '<STR_LIT>', constants.VM_MACHINE_NAME], redirect_stderr=True)<EOL>", "docstring": "Stop the Dusty VM if it is not already stopped.", "id": "f3150:m16"}
{"signature": "def compand(self, attack_time=<NUM_LIT>, decay_time=<NUM_LIT>, soft_knee_db=<NUM_LIT>,<EOL>tf_points=[(-<NUM_LIT>, -<NUM_LIT>), (-<NUM_LIT>, -<NUM_LIT:20>), (<NUM_LIT:0>, <NUM_LIT:0>)],<EOL>):", "body": "if not is_number(attack_time) or attack_time <= <NUM_LIT:0>:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if not is_number(decay_time) or decay_time <= <NUM_LIT:0>:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if attack_time > decay_time:<EOL><INDENT>logger.warning(<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>)<EOL><DEDENT>if not (is_number(soft_knee_db) or soft_knee_db is None):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if not isinstance(tf_points, list):<EOL><INDENT>raise TypeError(\"<STR_LIT>\")<EOL><DEDENT>if len(tf_points) == <NUM_LIT:0>:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if any(not isinstance(pair, tuple) for pair in tf_points):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if any(len(pair) != <NUM_LIT:2> for pair in tf_points):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if any(not (is_number(p[<NUM_LIT:0>]) and is_number(p[<NUM_LIT:1>])) for p in tf_points):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if any((p[<NUM_LIT:0>] > <NUM_LIT:0> or p[<NUM_LIT:1>] > <NUM_LIT:0>) for p in tf_points):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if len(tf_points) > len(set([p[<NUM_LIT:0>] for p in tf_points])):<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>tf_points = sorted(<EOL>tf_points,<EOL>key=lambda tf_points: tf_points[<NUM_LIT:0>]<EOL>)<EOL>transfer_list = []<EOL>for point in tf_points:<EOL><INDENT>transfer_list.extend([<EOL>\"<STR_LIT>\".format(point[<NUM_LIT:0>]), \"<STR_LIT>\".format(point[<NUM_LIT:1>])<EOL>])<EOL><DEDENT>effect_args = [<EOL>'<STR_LIT>',<EOL>\"<STR_LIT>\".format(attack_time, decay_time)<EOL>]<EOL>if soft_knee_db is not None:<EOL><INDENT>effect_args.append(<EOL>\"<STR_LIT>\".format(soft_knee_db, \"<STR_LIT:U+002C>\".join(transfer_list))<EOL>)<EOL><DEDENT>else:<EOL><INDENT>effect_args.append(\"<STR_LIT:U+002C>\".join(transfer_list))<EOL><DEDENT>self.effects.extend(effect_args)<EOL>self.effects_log.append('<STR_LIT>')<EOL>return self<EOL>", "docstring": "Compand (compress or expand) the dynamic range of the audio.\n\n        Parameters\n        ----------\n        attack_time : float, default=0.3\n            The time in seconds over which the instantaneous level of the input\n            signal is averaged to determine increases in volume.\n        decay_time : float, default=0.8\n            The time in seconds over which the instantaneous level of the input\n            signal is averaged to determine decreases in volume.\n        soft_knee_db : float or None, default=6.0\n            The ammount (in dB) for which the points at where adjacent line\n            segments on the transfer function meet will be rounded.\n            If None, no soft_knee is applied.\n        tf_points : list of tuples\n            Transfer function points as a list of tuples corresponding to\n            points in (dB, dB) defining the compander's transfer function.\n\n        See Also\n        --------\n        mcompand, contrast", "id": "f3809:c0:m15"}
{"signature": "@property<EOL><INDENT>def y0(self):<DEDENT>", "body": "return self._sorted_y[<NUM_LIT:0>]<EOL>", "docstring": "The smaller x coordinate.", "id": "f12127:c0:m7"}
{"signature": "def compare(self, buf, offset=<NUM_LIT:0>, length=<NUM_LIT:1>, ignore=\"<STR_LIT>\"):", "body": "for i in range(offset, offset + length):<EOL><INDENT>if isinstance(self.m_types, (type(Union), type(Structure))):<EOL><INDENT>if compare(self.m_buf[i], buf[i], ignore=ignore):<EOL><INDENT>return <NUM_LIT:1><EOL><DEDENT><DEDENT>elif self.m_buf[i] != buf[i]:<EOL><INDENT>return <NUM_LIT:1><EOL><DEDENT><DEDENT>return <NUM_LIT:0><EOL>", "docstring": "Compare buffer", "id": "f13344:c0:m10"}
{"signature": "def deepcopy(self):", "body": "segmap = SegmentationMapOnImage(self.arr, shape=self.shape, nb_classes=self.nb_classes)<EOL>segmap.input_was = self.input_was<EOL>return segmap<EOL>", "docstring": "Create a deep copy of the segmentation map object.\n\nReturns\n-------\nimgaug.SegmentationMapOnImage\n    Deep copy.", "id": "f16283:c0:m11"}
{"signature": "@_helpers.positional(<NUM_LIT:3>)<EOL>def run_flow(flow, storage, flags=None, http=None):", "body": "if flags is None:<EOL><INDENT>flags = argparser.parse_args()<EOL><DEDENT>logging.getLogger().setLevel(getattr(logging, flags.logging_level))<EOL>if not flags.noauth_local_webserver:<EOL><INDENT>success = False<EOL>port_number = <NUM_LIT:0><EOL>for port in flags.auth_host_port:<EOL><INDENT>port_number = port<EOL>try:<EOL><INDENT>httpd = ClientRedirectServer((flags.auth_host_name, port),<EOL>ClientRedirectHandler)<EOL><DEDENT>except socket.error:<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>success = True<EOL>break<EOL><DEDENT><DEDENT>flags.noauth_local_webserver = not success<EOL>if not success:<EOL><INDENT>print(_FAILED_START_MESSAGE)<EOL><DEDENT><DEDENT>if not flags.noauth_local_webserver:<EOL><INDENT>oauth_callback = '<STR_LIT>'.format(<EOL>host=flags.auth_host_name, port=port_number)<EOL><DEDENT>else:<EOL><INDENT>oauth_callback = client.OOB_CALLBACK_URN<EOL><DEDENT>flow.redirect_uri = oauth_callback<EOL>authorize_url = flow.step1_get_authorize_url()<EOL>if not flags.noauth_local_webserver:<EOL><INDENT>import webbrowser<EOL>webbrowser.open(authorize_url, new=<NUM_LIT:1>, autoraise=True)<EOL>print(_BROWSER_OPENED_MESSAGE.format(address=authorize_url))<EOL><DEDENT>else:<EOL><INDENT>print(_GO_TO_LINK_MESSAGE.format(address=authorize_url))<EOL><DEDENT>code = None<EOL>if not flags.noauth_local_webserver:<EOL><INDENT>httpd.handle_request()<EOL>if '<STR_LIT:error>' in httpd.query_params:<EOL><INDENT>sys.exit('<STR_LIT>')<EOL><DEDENT>if '<STR_LIT:code>' in httpd.query_params:<EOL><INDENT>code = httpd.query_params['<STR_LIT:code>']<EOL><DEDENT>else:<EOL><INDENT>print('<STR_LIT>'<EOL>'<STR_LIT>')<EOL>sys.exit('<STR_LIT>')<EOL><DEDENT><DEDENT>else:<EOL><INDENT>code = input('<STR_LIT>').strip()<EOL><DEDENT>try:<EOL><INDENT>credential = flow.step2_exchange(code, http=http)<EOL><DEDENT>except client.FlowExchangeError as e:<EOL><INDENT>sys.exit('<STR_LIT>'.format(e))<EOL><DEDENT>storage.put(credential)<EOL>credential.set_store(storage)<EOL>print('<STR_LIT>')<EOL>return credential<EOL>", "docstring": "Core code for a command-line application.\n\n    The ``run()`` function is called from your application and runs\n    through all the steps to obtain credentials. It takes a ``Flow``\n    argument and attempts to open an authorization server page in the\n    user's default web browser. The server asks the user to grant your\n    application access to the user's data. If the user grants access,\n    the ``run()`` function returns new credentials. The new credentials\n    are also stored in the ``storage`` argument, which updates the file\n    associated with the ``Storage`` object.\n\n    It presumes it is run from a command-line application and supports the\n    following flags:\n\n        ``--auth_host_name`` (string, default: ``localhost``)\n           Host name to use when running a local web server to handle\n           redirects during OAuth authorization.\n\n        ``--auth_host_port`` (integer, default: ``[8080, 8090]``)\n           Port to use when running a local web server to handle redirects\n           during OAuth authorization. Repeat this option to specify a list\n           of values.\n\n        ``--[no]auth_local_webserver`` (boolean, default: ``True``)\n           Run a local web server to handle redirects during OAuth\n           authorization.\n\n    The tools module defines an ``ArgumentParser`` the already contains the\n    flag definitions that ``run()`` requires. You can pass that\n    ``ArgumentParser`` to your ``ArgumentParser`` constructor::\n\n        parser = argparse.ArgumentParser(\n            description=__doc__,\n            formatter_class=argparse.RawDescriptionHelpFormatter,\n            parents=[tools.argparser])\n        flags = parser.parse_args(argv)\n\n    Args:\n        flow: Flow, an OAuth 2.0 Flow to step through.\n        storage: Storage, a ``Storage`` to store the credential in.\n        flags: ``argparse.Namespace``, (Optional) The command-line flags. This\n               is the object returned from calling ``parse_args()`` on\n               ``argparse.ArgumentParser`` as described above. Defaults\n               to ``argparser.parse_args()``.\n        http: An instance of ``httplib2.Http.request`` or something that\n              acts like it.\n\n    Returns:\n        Credentials, the obtained credential.", "id": "f2476:m1"}
{"signature": "@register.tag<EOL>def bootstrap_paginate(parser, token):", "body": "bits = token.split_contents()<EOL>if len(bits) < <NUM_LIT:2>:<EOL><INDENT>raise TemplateSyntaxError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\" % bits[<NUM_LIT:0>])<EOL><DEDENT>page = parser.compile_filter(bits[<NUM_LIT:1>])<EOL>kwargs = {}<EOL>bits = bits[<NUM_LIT:2>:]<EOL>kwarg_re = re.compile(r'<STR_LIT>')<EOL>if len(bits):<EOL><INDENT>for bit in bits:<EOL><INDENT>match = kwarg_re.match(bit)<EOL>if not match:<EOL><INDENT>raise TemplateSyntaxError(\"<STR_LIT>\")<EOL><DEDENT>name, value = match.groups()<EOL>kwargs[name] = parser.compile_filter(value)<EOL><DEDENT><DEDENT>return BootstrapPaginationNode(page, kwargs)<EOL>", "docstring": "Renders a Page object as a Twitter Bootstrap styled pagination bar.\nCompatible with Bootstrap 3.x and 4.x only.\n\nExample::\n\n    {% bootstrap_paginate page_obj range=10 %}\n\n\nNamed Parameters::\n\n    range - The size of the pagination bar (ie, if set to 10 then, at most,\n            10 page numbers will display at any given time) Defaults to\n            None, which shows all pages.\n\n\n    size - Accepts \"small\", and \"large\". Defaults to\n                None which is the standard size.\n\n    show_prev_next - Accepts \"true\" or \"false\". Determines whether or not\n                    to show the previous and next page links. Defaults to\n                    \"true\"\n\n\n    show_first_last - Accepts \"true\" or \"false\". Determines whether or not\n                      to show the first and last page links. Defaults to\n                      \"false\"\n\n    previous_label - The text to display for the previous page link.\n                     Defaults to \"&larr;\"\n\n    next_label - The text to display for the next page link. Defaults to\n                 \"&rarr;\"\n\n    first_label - The text to display for the first page link. Defaults to\n                  \"&laquo;\"\n\n    last_label - The text to display for the last page link. Defaults to\n                 \"&raquo;\"\n\n    url_view_name - The named URL to use. Defaults to None. If None, then the\n                    default template simply appends the url parameter as a\n                    relative URL link, eg: <a href=\"?page=1\">1</a>\n\n    url_param_name - The name of the parameter to use in the URL. If\n                     url_view_name is set to None, this string is used as the\n                     parameter name in the relative URL path. If a URL\n                     name is specified, this string is used as the\n                     parameter name passed into the reverse() method for\n                     the URL.\n\n    url_extra_args - This is used only in conjunction with url_view_name.\n                     When referencing a URL, additional arguments may be\n                     passed in as a list.\n\n    url_extra_kwargs - This is used only in conjunction with url_view_name.\n                       When referencing a URL, additional named arguments\n                       may be passed in as a dictionary.\n\n    url_get_params - The other get parameters to pass, only the page\n                     number will be overwritten. Use this to preserve\n                     filters.\n\n    url_anchor - The anchor to use in URLs. Defaults to None.\n\n    extra_pagination_classes - A space separated list of CSS class names\n                               that will be added to the top level <ul>\n                               HTML element. In particular, this can be\n                               utilized in Bootstrap 4 installatinos  to\n                               add the appropriate alignment classes from\n                               Flexbox utilites, eg:  justify-content-center", "id": "f13724:m3"}
{"signature": "def request_authorization( self, client_id, user_id, response_type,<EOL>redirect_uri=None, scope=None, state=None,<EOL>expires=<NUM_LIT> ):", "body": "if response_type != '<STR_LIT:code>':<EOL><INDENT>raise Proauth2Error( '<STR_LIT>',<EOL>'<STR_LIT>', state=state )<EOL><DEDENT>client = self.data_store.fetch( '<STR_LIT>', client_id=client_id )<EOL>if not client: raise Proauth2Error( '<STR_LIT>' )<EOL>if redirect_uri and client['<STR_LIT>'] != redirect_uri:<EOL><INDENT>raise Proauth2Error( '<STR_LIT>', \"<STR_LIT>\" )<EOL><DEDENT>nonce_code = self._generate_token()<EOL>expires = time() + expires<EOL>try:<EOL><INDENT>self.data_store.store( '<STR_LIT>', code=nonce_code,<EOL>client_id=client_id, expires=expires,<EOL>user_id=user_id, scope=scope )<EOL><DEDENT>except Proauth2Error as e:<EOL><INDENT>e.state = state<EOL>raise e<EOL><DEDENT>return { '<STR_LIT:code>':nonce_code, '<STR_LIT:state>':state }<EOL>", "docstring": "request_authorization generates a nonce, and stores it in the data_store along with the\nclient_id, user_id, and expiration timestamp.\nIt then returns a dictionary containing the nonce as \"code,\" and the passed\nstate.\n---\nresponse_type MUST be \"code.\" this is directly from the OAuth2 spec.\nthis probably doesn't need to be checked here, but if it's in the spec I\nguess it should be verified somewhere.\nscope has not been implemented here. it will be stored, but there is no\nscope-checking built in here at this time.\nif a redirect_uri is passed, it must match the registered redirect_uri.\nagain, this is per spec.", "id": "f835:c1:m2"}
{"signature": "def sse_content(response, handler, **sse_kwargs):", "body": "<EOL>raise_for_not_ok_status(response)<EOL>raise_for_header(response, '<STR_LIT:Content-Type>', '<STR_LIT>')<EOL>finished, _ = _sse_content_with_protocol(response, handler, **sse_kwargs)<EOL>return finished<EOL>", "docstring": "Callback to collect the Server-Sent Events content of a response. Callbacks\npassed will receive event data.\n\n:param response:\n    The response from the SSE request.\n:param handler:\n    The handler for the SSE protocol.", "id": "f13710:m2"}
{"signature": "def set_y(self, y):", "body": "self._y = y<EOL>", "docstring": "Set the bottom coord of the rectangle\n\nACCEPTS: float", "id": "f17197:c16:m14"}
{"signature": "@property<EOL><INDENT>def http_session(self):<DEDENT>", "body": "return self.endpoint.http_session<EOL>", "docstring": "HTTP session object", "id": "f13282:c0:m3"}
{"signature": "def remove_trailing_whitespace(tex_source):", "body": "<EOL>return re.sub(r'<STR_LIT>', '<STR_LIT>', tex_source, flags=re.M)<EOL>", "docstring": "Delete trailing whitespace from TeX source.\n\n    Parameters\n    ----------\n    tex_source : str\n        TeX source content.\n\n    Returns\n    -------\n    tex_source : str\n        TeX source without trailing whitespace.", "id": "f4207:m1"}
{"signature": "def n_pareto_optimal_trips(self):", "body": "return float(len(self._labels_within_time_frame))<EOL>", "docstring": "Get number of pareto-optimal trips\n\nReturns\n-------\nn_trips: float", "id": "f12871:c0:m36"}
{"signature": "def launch_task(self, task_id, executable, *args, **kwargs):", "body": "self.tasks[task_id]['<STR_LIT>'] = datetime.datetime.now()<EOL>hit, memo_fu = self.memoizer.check_memo(task_id, self.tasks[task_id])<EOL>if hit:<EOL><INDENT>logger.info(\"<STR_LIT>\".format(task_id))<EOL>return memo_fu<EOL><DEDENT>executor_label = self.tasks[task_id][\"<STR_LIT>\"]<EOL>try:<EOL><INDENT>executor = self.executors[executor_label]<EOL><DEDENT>except Exception:<EOL><INDENT>logger.exception(\"<STR_LIT>\".format(task_id, executor_label, self._config))<EOL><DEDENT>if self.monitoring is not None and self.monitoring.resource_monitoring_enabled:<EOL><INDENT>executable = self.monitoring.monitor_wrapper(executable, task_id,<EOL>self.monitoring.monitoring_hub_url,<EOL>self.run_id,<EOL>self.monitoring.resource_monitoring_interval)<EOL><DEDENT>with self.submitter_lock:<EOL><INDENT>exec_fu = executor.submit(executable, *args, **kwargs)<EOL><DEDENT>self.tasks[task_id]['<STR_LIT:status>'] = States.launched<EOL>if self.monitoring is not None:<EOL><INDENT>task_log_info = self._create_task_log_info(task_id, '<STR_LIT>')<EOL>self.monitoring.send(MessageType.TASK_INFO, task_log_info)<EOL><DEDENT>exec_fu.retries_left = self._config.retries -self.tasks[task_id]['<STR_LIT>']<EOL>logger.info(\"<STR_LIT>\".format(task_id, executor.label))<EOL>return exec_fu<EOL>", "docstring": "Handle the actual submission of the task to the executor layer.\n\n        If the app task has the executors attributes not set (default=='all')\n        the task is launched on a randomly selected executor from the\n        list of executors. This behavior could later be updated to support\n        binding to executors based on user specified criteria.\n\n        If the app task specifies a particular set of executors, it will be\n        targeted at those specific executors.\n\n        Args:\n            task_id (uuid string) : A uuid string that uniquely identifies the task\n            executable (callable) : A callable object\n            args (list of positional args)\n            kwargs (arbitrary keyword arguments)\n\n\n        Returns:\n            Future that tracks the execution of the submitted executable", "id": "f2769:c0:m7"}
{"signature": "@property<EOL><INDENT>def selenium(self):<DEDENT>", "body": "warn(\"<STR_LIT>\", DeprecationWarning, stacklevel=<NUM_LIT:2>)<EOL>return self.driver<EOL>", "docstring": "Backwards compatibility attribute", "id": "f11286:c0:m1"}
{"signature": "def write(self, process_tile, data):", "body": "if isinstance(process_tile, tuple):<EOL><INDENT>process_tile = self.config.process_pyramid.tile(*process_tile)<EOL><DEDENT>elif not isinstance(process_tile, BufferedTile):<EOL><INDENT>raise ValueError(\"<STR_LIT>\" % type(process_tile))<EOL><DEDENT>if self.config.mode not in [\"<STR_LIT>\", \"<STR_LIT>\"]:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if self.config.mode == \"<STR_LIT>\" and (<EOL>self.config.output.tiles_exist(process_tile)<EOL>):<EOL><INDENT>message = \"<STR_LIT>\"<EOL>logger.debug((process_tile.id, message))<EOL>return ProcessInfo(<EOL>tile=process_tile,<EOL>processed=False,<EOL>process_msg=None,<EOL>written=False,<EOL>write_msg=message<EOL>)<EOL><DEDENT>elif data is None:<EOL><INDENT>message = \"<STR_LIT>\"<EOL>logger.debug((process_tile.id, message))<EOL>return ProcessInfo(<EOL>tile=process_tile,<EOL>processed=False,<EOL>process_msg=None,<EOL>written=False,<EOL>write_msg=message<EOL>)<EOL><DEDENT>else:<EOL><INDENT>with Timer() as t:<EOL><INDENT>self.config.output.write(process_tile=process_tile, data=data)<EOL><DEDENT>message = \"<STR_LIT>\" % t<EOL>logger.debug((process_tile.id, message))<EOL>return ProcessInfo(<EOL>tile=process_tile,<EOL>processed=False,<EOL>process_msg=None,<EOL>written=True,<EOL>write_msg=message<EOL>)<EOL><DEDENT>", "docstring": "Write data into output format.\n\nParameters\n----------\nprocess_tile : BufferedTile or tile index tuple\n    process tile\ndata : NumPy array or features\n    data to be written", "id": "f12819:c0:m7"}
{"signature": "def deprecate_module_attr(mod, deprecated):", "body": "deprecated = set(deprecated)<EOL>class Wrapper(object):<EOL><INDENT>def __getattr__(self, attr):<EOL><INDENT>if attr in deprecated:<EOL><INDENT>warnings.warn(\"<STR_LIT>\".format(attr), GBDXDeprecation)<EOL><DEDENT>return getattr(mod, attr)<EOL><DEDENT>def __setattr__(self, attr, value):<EOL><INDENT>if attr in deprecated:<EOL><INDENT>warnings.warn(\"<STR_LIT>\".format(attr), GBDXDeprecation)<EOL><DEDENT>return setattr(mod, attr, value)<EOL><DEDENT><DEDENT>return Wrapper()<EOL>", "docstring": "Return a wrapped object that warns about deprecated accesses", "id": "f7105:m1"}
{"signature": "def backends():", "body": "return Loader().all_names<EOL>", "docstring": "Back-end names as a list.\n\n    :return: back-ends as string list", "id": "f14951:m5"}
{"signature": "def _wrap_chunks(self, chunks):", "body": "lines = []<EOL>if self.width <= <NUM_LIT:0>:<EOL><INDENT>raise ValueError(\"<STR_LIT>\" % self.width)<EOL><DEDENT>chunks.reverse()<EOL>while chunks:<EOL><INDENT>cur_line = []<EOL>cur_len = <NUM_LIT:0><EOL>if lines:<EOL><INDENT>indent = self.subsequent_indent<EOL><DEDENT>else:<EOL><INDENT>indent = self.initial_indent<EOL><DEDENT>width = self.width - len(indent)<EOL>if self.drop_whitespace and chunks[-<NUM_LIT:1>].strip() == '<STR_LIT>' and lines:<EOL><INDENT>chunks.pop()<EOL><DEDENT>while chunks:<EOL><INDENT>l = len(chunks[-<NUM_LIT:1>])<EOL>if cur_len + l <= width:<EOL><INDENT>cur_line.append(chunks.pop())<EOL>cur_len += l<EOL><DEDENT>else:<EOL><INDENT>break<EOL><DEDENT><DEDENT>if chunks and len(chunks[-<NUM_LIT:1>]) > width:<EOL><INDENT>self._handle_long_word(chunks, cur_line, cur_len, width)<EOL><DEDENT>if self.drop_whitespace and cur_line and cur_line[-<NUM_LIT:1>].strip() == '<STR_LIT>':<EOL><INDENT>cur_line.pop()<EOL><DEDENT>if cur_line:<EOL><INDENT>lines.append(indent + '<STR_LIT>'.join(cur_line))<EOL><DEDENT><DEDENT>return lines<EOL>", "docstring": "_wrap_chunks(chunks : [string]) -> [string]\n\n        Wrap a sequence of text chunks and return a list of lines of\n        length 'self.width' or less.  (If 'break_long_words' is false,\n        some lines may be longer than this.)  Chunks correspond roughly\n        to words and the whitespace between them: each chunk is\n        indivisible (modulo 'break_long_words'), but a line break can\n        come between any two chunks.  Chunks should not have internal\n        whitespace; ie. a chunk is either all whitespace or a \"word\".\n        Whitespace chunks will be removed from the beginning and end of\n        lines, but apart from that whitespace is preserved.", "id": "f16450:c0:m5"}
{"signature": "def index(request):", "body": "return {}<EOL>", "docstring": "Base view to load our template", "id": "f11763:m0"}
{"signature": "def __str__(self):", "body": "return self.__grouping.__str__()<EOL>", "docstring": "ToString method for GroupedDataFrame.\n\n:return: returns the string representation\n:rtype: str", "id": "f15342:c0:m2"}
{"signature": "@rules.predicate<EOL>def rbac_permissions_disabled(user, obj):  ", "body": "return not waffle.switch_is_active(ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH)<EOL>", "docstring": "Temporary check for rbac based permissions being enabled.", "id": "f16069:m6"}
{"signature": "def submit(self, command, blocksize, tasks_per_node, job_name=\"<STR_LIT>\"):", "body": "if self.provisioned_blocks >= self.max_blocks:<EOL><INDENT>logger.warn(\"<STR_LIT>\", self.label)<EOL>return None<EOL><DEDENT>if blocksize < self.nodes_per_block:<EOL><INDENT>blocksize = self.nodes_per_block<EOL><DEDENT>job_name = \"<STR_LIT>\".format(job_name, time.time())<EOL>script_path = \"<STR_LIT>\".format(self.script_dir, job_name)<EOL>script_path = os.path.abspath(script_path)<EOL>logger.debug(\"<STR_LIT>\", blocksize, self.nodes_per_block,<EOL>tasks_per_node)<EOL>job_config = {}<EOL>job_config[\"<STR_LIT>\"] = self.channel.script_dir<EOL>job_config[\"<STR_LIT>\"] = self.nodes_per_block<EOL>job_config[\"<STR_LIT>\"] = self.nodes_per_block * tasks_per_node<EOL>job_config[\"<STR_LIT>\"] = self.nodes_per_block<EOL>job_config[\"<STR_LIT>\"] = tasks_per_node<EOL>job_config[\"<STR_LIT>\"] = self.walltime<EOL>job_config[\"<STR_LIT>\"] = self.scheduler_options<EOL>job_config[\"<STR_LIT>\"] = self.worker_init<EOL>job_config[\"<STR_LIT>\"] = command<EOL>job_config[\"<STR_LIT>\"] = self.launcher(command,<EOL>tasks_per_node,<EOL>self.nodes_per_block)<EOL>logger.debug(\"<STR_LIT>\")<EOL>self._write_submit_script(template_string, script_path, job_name, job_config)<EOL>channel_script_path = self.channel.push_file(script_path, self.channel.script_dir)<EOL>submit_options = '<STR_LIT>'<EOL>if self.queue is not None:<EOL><INDENT>submit_options = '<STR_LIT>'.format(submit_options, self.queue)<EOL><DEDENT>if self.account is not None:<EOL><INDENT>submit_options = '<STR_LIT>'.format(submit_options, self.account)<EOL><DEDENT>launch_cmd = \"<STR_LIT>\".format(submit_options, channel_script_path)<EOL>retcode, stdout, stderr = super().execute_wait(launch_cmd)<EOL>job_id = None<EOL>if retcode == <NUM_LIT:0>:<EOL><INDENT>for line in stdout.split('<STR_LIT:\\n>'):<EOL><INDENT>if line.strip():<EOL><INDENT>job_id = line.strip()<EOL>self.resources[job_id] = {'<STR_LIT>': job_id, '<STR_LIT:status>': '<STR_LIT>', '<STR_LIT>': blocksize}<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>message = \"<STR_LIT>\".format(launch_cmd, retcode)<EOL>if (stdout is not None) and (stderr is not None):<EOL><INDENT>message += \"<STR_LIT>\".format(stderr.strip(), stdout.strip())<EOL><DEDENT>logger.error(message)<EOL><DEDENT>return job_id<EOL>", "docstring": "Submits the command onto an Local Resource Manager job of blocksize parallel elements.\n        Submit returns an ID that corresponds to the task that was just submitted.\n\n        If tasks_per_node <  1 : ! This is illegal. tasks_per_node should be integer\n\n        If tasks_per_node == 1:\n             A single node is provisioned\n\n        If tasks_per_node >  1 :\n             tasks_per_node * blocksize number of nodes are provisioned.\n\n        Args:\n             - command  :(String) Commandline invocation to be made on the remote side.\n             - blocksize   :(float)\n             - tasks_per_node (int) : command invocations to be launched per node\n\n        Kwargs:\n             - job_name (String): Name for job, must be unique\n\n        Returns:\n             - None: At capacity, cannot provision more\n             - job_id: (string) Identifier for the job", "id": "f2795:c0:m2"}
{"signature": "def add_peer(self, peer):", "body": "if type(peer) == list:<EOL><INDENT>for i in peer:<EOL><INDENT>check_url(i)<EOL><DEDENT>self.PEERS.extend(peer)<EOL><DEDENT>elif type(peer) == str:<EOL><INDENT>check_url(peer)<EOL>self.PEERS.append(peer)<EOL><DEDENT>", "docstring": "Add a peer or multiple peers to the PEERS variable, takes a single string or a list.\n\n:param peer(list or string)", "id": "f7948:c0:m1"}
{"signature": "def load_roster(self, source):", "body": "try:<EOL><INDENT>tree = ElementTree.parse(source)<EOL><DEDENT>except ElementTree.ParseError as err:<EOL><INDENT>raise ValueError(\"<STR_LIT>\".format(err))<EOL><DEDENT>roster = Roster.from_xml(tree.getroot())<EOL>for item in roster:<EOL><INDENT>item.verify_roster_result(True)<EOL><DEDENT>self.roster = roster<EOL>", "docstring": "Load roster from an XML file.\n\n        Can be used before the connection is started to load saved\n        roster copy, for efficient retrieval of versioned roster.\n\n        :Parameters:\n            - `source`: file name or a file object\n        :Types:\n            - `source`: `str` or file-like object", "id": "f15245:c6:m1"}
{"signature": "def parse_timedelta(str):", "body": "deltas = (_parse_timedelta_part(part.strip()) for part in str.split('<STR_LIT:U+002C>'))<EOL>return sum(deltas, datetime.timedelta())<EOL>", "docstring": "Take a string representing a span of time and parse it to a time delta.\nAccepts any string of comma-separated numbers each with a unit indicator.\n\n>>> parse_timedelta('1 day')\ndatetime.timedelta(days=1)\n\n>>> parse_timedelta('1 day, 30 seconds')\ndatetime.timedelta(days=1, seconds=30)\n\n>>> parse_timedelta('47.32 days, 20 minutes, 15.4 milliseconds')\ndatetime.timedelta(days=47, seconds=28848, microseconds=15400)\n\nSupports weeks, months, years\n\n>>> parse_timedelta('1 week')\ndatetime.timedelta(days=7)\n\n>>> parse_timedelta('1 year, 1 month')\ndatetime.timedelta(days=395, seconds=58685)\n\nNote that months and years strict intervals, not aligned\nto a calendar:\n\n>>> now = datetime.datetime.now()\n>>> later = now + parse_timedelta('1 year')\n>>> diff = later.replace(year=now.year) - now\n>>> diff.seconds\n20940", "id": "f14200:m10"}
{"signature": "@api.check(<NUM_LIT:1>, \"<STR_LIT>\")<EOL>def control_get_focus_by_handle(hwnd, buf_size=<NUM_LIT>):", "body": "ctrl_with_focus = ctypes.create_unicode_buffer(buf_size)<EOL>AUTO_IT.AU3_ControlGetFocusByHandle(HWND(hwnd), ctrl_with_focus,<EOL>INT(buf_size))<EOL>return ctrl_with_focus.value.rstrip()<EOL>", "docstring": ":param hwnd:\n:param buf_size:\n:return:", "id": "f5585:m13"}
{"signature": "def process(self, tokens, scope):", "body": "while True:<EOL><INDENT>tokens = list(utility.flatten(tokens))<EOL>done = True<EOL>if any(t for t in tokens if hasattr(t, '<STR_LIT>')):<EOL><INDENT>tokens = [<EOL>t.parse(scope) if hasattr(t, '<STR_LIT>') else t<EOL>for t in tokens<EOL>]<EOL>done = False<EOL><DEDENT>if any(<EOL>t for t in tokens<EOL>if (utility.is_variable(t)) or str(type(t)) ==<EOL>\"<STR_LIT>\"):<EOL><INDENT>tokens = self.replace_variables(tokens, scope)<EOL>done = False<EOL><DEDENT>if done:<EOL><INDENT>break<EOL><DEDENT><DEDENT>return tokens<EOL>", "docstring": "Process tokenslist, flattening and parsing it\n        args:\n            tokens (list): tokenlist\n            scope (Scope): Current scope\n        returns:\n            list", "id": "f12452:c0:m2"}
{"signature": "def setUp(self):", "body": "super(BaseTestEnterpriseCustomerManageLearnersView, self).setUp()<EOL>self.user = UserFactory.create(is_staff=True, is_active=True, id=<NUM_LIT:1>)<EOL>self.user.set_password(\"<STR_LIT>\")<EOL>self.user.save()<EOL>self.enterprise_customer = EnterpriseCustomerFactory()<EOL>self.default_context = {<EOL>\"<STR_LIT>\": True,<EOL>\"<STR_LIT>\": self.enterprise_customer._meta,<EOL>\"<STR_LIT:user>\": self.user<EOL>}<EOL>self.view_url = reverse(<EOL>\"<STR_LIT>\" + enterprise_admin.utils.UrlNames.MANAGE_LEARNERS,<EOL>args=(self.enterprise_customer.uuid,)<EOL>)<EOL>self.client = Client()<EOL>self.context_parameters = EnterpriseCustomerManageLearnersView.ContextParameters<EOL>", "docstring": "Test set up - installs common dependencies.", "id": "f16142:c1:m0"}
{"signature": "def merge_after_pysam(data, clust):", "body": "try:<EOL><INDENT>r1file = tempfile.NamedTemporaryFile(mode='<STR_LIT:wb>', delete=False,<EOL>dir=data.dirs.edits,<EOL>suffix=\"<STR_LIT>\")<EOL>r2file = tempfile.NamedTemporaryFile(mode='<STR_LIT:wb>', delete=False,<EOL>dir=data.dirs.edits,<EOL>suffix=\"<STR_LIT>\")<EOL>r1dat = []<EOL>r2dat = []<EOL>for locus in clust:<EOL><INDENT>sname, seq = locus.split(\"<STR_LIT:\\n>\")<EOL>sname = \"<STR_LIT:@>\" + sname[<NUM_LIT:1>:]<EOL>r1, r2 = seq.split(\"<STR_LIT>\")<EOL>r1dat.append(\"<STR_LIT>\".format(sname, r1, \"<STR_LIT:+>\", \"<STR_LIT:B>\"*(len(r1))))<EOL>r2dat.append(\"<STR_LIT>\".format(sname, r2, \"<STR_LIT:+>\", \"<STR_LIT:B>\"*(len(r2))))<EOL><DEDENT>r1file.write(\"<STR_LIT:\\n>\".join(r1dat))<EOL>r2file.write(\"<STR_LIT:\\n>\".join(r2dat))<EOL>r1file.close()<EOL>r2file.close()<EOL>merged_file = tempfile.NamedTemporaryFile(mode='<STR_LIT:wb>',<EOL>dir=data.dirs.edits,<EOL>suffix=\"<STR_LIT>\").name<EOL>clust = []<EOL>merge_pairs(data, [(r1file.name, r2file.name)], merged_file, <NUM_LIT:0>, <NUM_LIT:1>)<EOL>with open(merged_file) as infile:<EOL><INDENT>quarts = itertools.izip(*[iter(infile)]*<NUM_LIT:4>)<EOL>while <NUM_LIT:1>:<EOL><INDENT>try:<EOL><INDENT>sname, seq, _, _ = quarts.next()<EOL>if not \"<STR_LIT>\" in sname.rsplit(\"<STR_LIT:;>\", <NUM_LIT:1>)[<NUM_LIT:1>]:<EOL><INDENT>try:<EOL><INDENT>R1, R2 = seq.split(\"<STR_LIT>\")<EOL>seq = R1 + \"<STR_LIT>\" + revcomp(R2)<EOL><DEDENT>except ValueError as inst:<EOL><INDENT>LOGGER.error(\"<STR_LIT>\".format(sname, seq))<EOL>raise<EOL><DEDENT><DEDENT><DEDENT>except StopIteration:<EOL><INDENT>break<EOL><DEDENT>sname = \"<STR_LIT:>>\" + sname[<NUM_LIT:1>:]<EOL>clust.extend([sname.strip(), seq.strip()])<EOL><DEDENT><DEDENT><DEDENT>except:<EOL><INDENT>LOGGER.info(\"<STR_LIT>\")<EOL>raise<EOL><DEDENT>finally:<EOL><INDENT>for i in [r1file.name, r2file.name, merged_file]:<EOL><INDENT>if os.path.exists(i):<EOL><INDENT>log_level = logging.getLevelName(LOGGER.getEffectiveLevel())<EOL>os.remove(i)<EOL><DEDENT><DEDENT><DEDENT>return clust<EOL>", "docstring": "This is for pysam post-flight merging. The input is a cluster\nfor an individual locus. We have to split the clusters, write\nR1 and R2 to files then call merge_pairs(). This is not ideal,\nit's slow, but it works. This is the absolute worst way to do this,\nit bounces all the files for each locus off the disk. I/O _hog_.", "id": "f5312:m7"}
{"signature": "def GetSelectionItemPattern(self) -> SelectionItemPattern:", "body": "return self.GetPattern(PatternId.SelectionItemPattern)<EOL>", "docstring": "Return `SelectionItemPattern` if it supports the pattern else None(Must support according to MSDN).", "id": "f1782:c102:m1"}
{"signature": "def __init__(self, fig, page_step=<NUM_LIT:1>):", "body": "<EOL>self.fig = fig<EOL>self.page_step = page_step <EOL>self.xmin, self.xmax = fig.axes[<NUM_LIT:0>].get_xlim()<EOL>self.width = min(<NUM_LIT:1>, self.xmax-self.xmin) <EOL>self.pos = <NUM_LIT:0>   <EOL>self.scale = <NUM_LIT> <EOL>self.ax = self.fig.axes[<NUM_LIT:0>]<EOL>self.draw = self.fig.canvas.draw<EOL>QMainWin = fig.canvas.parent()<EOL>toolbar = QtGui.QToolBar(QMainWin)<EOL>QMainWin.addToolBar(QtCore.Qt.BottomToolBarArea, toolbar)<EOL>self.set_slider(toolbar)<EOL>self.set_spinbox(toolbar)<EOL>self.ax.set_xlim(self.pos,self.pos+self.width)<EOL>self.draw()<EOL>", "docstring": "Make a scrolling x axis on figure `fig`.\n        `page_step` is the multiplier for page-step scrolling.", "id": "f15435:c2:m0"}
{"signature": "def plot_results(<EOL>allresults, *,<EOL>xy_fn=default_xy_fn,<EOL>split_fn=default_split_fn,<EOL>group_fn=default_split_fn,<EOL>average_group=False,<EOL>shaded_std=True,<EOL>shaded_err=True,<EOL>figsize=None,<EOL>legend_outside=False,<EOL>resample=<NUM_LIT:0>,<EOL>smooth_step=<NUM_LIT:1.0><EOL>):", "body": "if split_fn is None: split_fn = lambda _ : '<STR_LIT>'<EOL>if group_fn is None: group_fn = lambda _ : '<STR_LIT>'<EOL>sk2r = defaultdict(list) <EOL>for result in allresults:<EOL><INDENT>splitkey = split_fn(result)<EOL>sk2r[splitkey].append(result)<EOL><DEDENT>assert len(sk2r) > <NUM_LIT:0><EOL>assert isinstance(resample, int), \"<STR_LIT>\"<EOL>nrows = len(sk2r)<EOL>ncols = <NUM_LIT:1><EOL>figsize = figsize or (<NUM_LIT:6>, <NUM_LIT:6> * nrows)<EOL>f, axarr = plt.subplots(nrows, ncols, sharex=False, squeeze=False, figsize=figsize)<EOL>groups = list(set(group_fn(result) for result in allresults))<EOL>default_samples = <NUM_LIT><EOL>if average_group:<EOL><INDENT>resample = resample or default_samples<EOL><DEDENT>for (isplit, sk) in enumerate(sorted(sk2r.keys())):<EOL><INDENT>g2l = {}<EOL>g2c = defaultdict(int)<EOL>sresults = sk2r[sk]<EOL>gresults = defaultdict(list)<EOL>ax = axarr[isplit][<NUM_LIT:0>]<EOL>for result in sresults:<EOL><INDENT>group = group_fn(result)<EOL>g2c[group] += <NUM_LIT:1><EOL>x, y = xy_fn(result)<EOL>if x is None: x = np.arange(len(y))<EOL>x, y = map(np.asarray, (x, y))<EOL>if average_group:<EOL><INDENT>gresults[group].append((x,y))<EOL><DEDENT>else:<EOL><INDENT>if resample:<EOL><INDENT>x, y, counts = symmetric_ema(x, y, x[<NUM_LIT:0>], x[-<NUM_LIT:1>], resample, decay_steps=smooth_step)<EOL><DEDENT>l, = ax.plot(x, y, color=COLORS[groups.index(group) % len(COLORS)])<EOL>g2l[group] = l<EOL><DEDENT><DEDENT>if average_group:<EOL><INDENT>for group in sorted(groups):<EOL><INDENT>xys = gresults[group]<EOL>if not any(xys):<EOL><INDENT>continue<EOL><DEDENT>color = COLORS[groups.index(group) % len(COLORS)]<EOL>origxs = [xy[<NUM_LIT:0>] for xy in xys]<EOL>minxlen = min(map(len, origxs))<EOL>def allequal(qs):<EOL><INDENT>return all((q==qs[<NUM_LIT:0>]).all() for q in qs[<NUM_LIT:1>:])<EOL><DEDENT>if resample:<EOL><INDENT>low  = max(x[<NUM_LIT:0>] for x in origxs)<EOL>high = min(x[-<NUM_LIT:1>] for x in origxs)<EOL>usex = np.linspace(low, high, resample)<EOL>ys = []<EOL>for (x, y) in xys:<EOL><INDENT>ys.append(symmetric_ema(x, y, low, high, resample, decay_steps=smooth_step)[<NUM_LIT:1>])<EOL><DEDENT><DEDENT>else:<EOL><INDENT>assert allequal([x[:minxlen] for x in origxs]),'<STR_LIT>'<EOL>usex = origxs[<NUM_LIT:0>]<EOL>ys = [xy[<NUM_LIT:1>][:minxlen] for xy in xys]<EOL><DEDENT>ymean = np.mean(ys, axis=<NUM_LIT:0>)<EOL>ystd = np.std(ys, axis=<NUM_LIT:0>)<EOL>ystderr = ystd / np.sqrt(len(ys))<EOL>l, = axarr[isplit][<NUM_LIT:0>].plot(usex, ymean, color=color)<EOL>g2l[group] = l<EOL>if shaded_err:<EOL><INDENT>ax.fill_between(usex, ymean - ystderr, ymean + ystderr, color=color, alpha=<NUM_LIT>)<EOL><DEDENT>if shaded_std:<EOL><INDENT>ax.fill_between(usex, ymean - ystd,    ymean + ystd,    color=color, alpha=<NUM_LIT>)<EOL><DEDENT><DEDENT><DEDENT>plt.tight_layout()<EOL>if any(g2l.keys()):<EOL><INDENT>ax.legend(<EOL>g2l.values(),<EOL>['<STR_LIT>'%(g, g2c[g]) for g in g2l] if average_group else g2l.keys(),<EOL>loc=<NUM_LIT:2> if legend_outside else None,<EOL>bbox_to_anchor=(<NUM_LIT:1>,<NUM_LIT:1>) if legend_outside else None)<EOL><DEDENT>ax.set_title(sk)<EOL><DEDENT>return f, axarr<EOL>", "docstring": "Plot multiple Results objects\n\nxy_fn: function Result -> x,y           - function that converts results objects into tuple of x and y values.\n                                          By default, x is cumsum of episode lengths, and y is episode rewards\n\nsplit_fn: function Result -> hashable   - function that converts results objects into keys to split curves into sub-panels by.\n                                          That is, the results r for which split_fn(r) is different will be put on different sub-panels.\n                                          By default, the portion of r.dirname between last / and -<digits> is returned. The sub-panels are\n                                          stacked vertically in the figure.\n\ngroup_fn: function Result -> hashable   - function that converts results objects into keys to group curves by.\n                                          That is, the results r for which group_fn(r) is the same will be put into the same group.\n                                          Curves in the same group have the same color (if average_group is False), or averaged over\n                                          (if average_group is True). The default value is the same as default value for split_fn\n\naverage_group: bool                     - if True, will average the curves in the same group and plot the mean. Enables resampling\n                                          (if resample = 0, will use 512 steps)\n\nshaded_std: bool                        - if True (default), the shaded region corresponding to standard deviation of the group of curves will be\n                                          shown (only applicable if average_group = True)\n\nshaded_err: bool                        - if True (default), the shaded region corresponding to error in mean estimate of the group of curves\n                                          (that is, standard deviation divided by square root of number of curves) will be\n                                          shown (only applicable if average_group = True)\n\nfigsize: tuple or None                  - size of the resulting figure (including sub-panels). By default, width is 6 and height is 6 times number of\n                                          sub-panels.\n\n\nlegend_outside: bool                    - if True, will place the legend outside of the sub-panels.\n\nresample: int                           - if not zero, size of the uniform grid in x direction to resample onto. Resampling is performed via symmetric\n                                          EMA smoothing (see the docstring for symmetric_ema).\n                                          Default is zero (no resampling). Note that if average_group is True, resampling is necessary; in that case, default\n                                          value is 512.\n\nsmooth_step: float                      - when resampling (i.e. when resample > 0 or average_group is True), use this EMA decay parameter (in units of the new grid step).\n                                          See docstrings for decay_steps in symmetric_ema or one_sided_ema functions.", "id": "f1364:m6"}
{"signature": "def __delitem__(self, name):", "body": "del self.headers<EOL>", "docstring": "Removes a header with the passed name.", "id": "f10144:c1:m21"}
{"signature": "def get(self, key, delete_if_expired=True):", "body": "self._update_cache_stats(key, None)<EOL>if key in self._CACHE:<EOL><INDENT>(expiration, obj) = self._CACHE[key]<EOL>if expiration > self._now():<EOL><INDENT>self._update_cache_stats(key, '<STR_LIT>')<EOL>return obj<EOL><DEDENT>else:<EOL><INDENT>if delete_if_expired:<EOL><INDENT>self.delete(key)<EOL>self._update_cache_stats(key, '<STR_LIT>')<EOL>return None<EOL><DEDENT><DEDENT><DEDENT>self._update_cache_stats(key, '<STR_LIT>')<EOL>return None<EOL>", "docstring": "Retrieve key from Cache.\n\n:param key: key to look up in cache.\n:type key: ``object``\n\n:param delete_if_expired: remove value from cache if it is expired.\n                          Default is True.\n:type delete_if_expired: ``bool``\n\n:returns: value from cache or None\n:rtype: varies or None", "id": "f10864:c0:m1"}
{"signature": "def internals_spec(self):", "body": "return dict()<EOL>", "docstring": "Returns the internal states specification.\n\nReturns:\n    Internal states specification", "id": "f14309:c0:m3"}
{"signature": "def reset(self):", "body": "fetches = [self.global_episode, self.global_timestep]<EOL>for name in sorted(self.states_preprocessing):<EOL><INDENT>fetch = self.states_preprocessing[name].reset()<EOL>if fetch is not None:<EOL><INDENT>fetches.extend(fetch)<EOL><DEDENT><DEDENT>if self.flush_summarizer is not None:<EOL><INDENT>fetches.append(self.flush_summarizer)<EOL><DEDENT>fetch_list = self.monitored_session.run(fetches=fetches)<EOL>episode, timestep = fetch_list[:<NUM_LIT:2>]<EOL>return episode, timestep, self.internals_init<EOL>", "docstring": "Resets the model to its initial state on episode start. This should also reset all preprocessor(s).\n\nReturns:\n    tuple:\n        Current episode, timestep counter and the shallow-copied list of internal state initialization Tensors.", "id": "f14275:c0:m22"}
{"signature": "def resource_isdir(resource_name):", "body": "", "docstring": "Is the named resource a directory?  (like ``os.path.isdir()``)", "id": "f17162:c5:m4"}
{"signature": "def setUp(self):     ", "body": "self.tibber = tibber.Tibber()<EOL>self.tibber.sync_update_info()<EOL>", "docstring": "things to be run when tests are started.", "id": "f12606:c0:m0"}
{"signature": "def alignment(self, d=<NUM_LIT:5>):", "body": "vx = vy = vz = <NUM_LIT:0><EOL>for b in self.boids:<EOL><INDENT>if b != self:<EOL><INDENT>vx, vy, vz = vx+b.vx, vy+b.vy, vz+b.vz<EOL><DEDENT><DEDENT>n = len(self.boids)-<NUM_LIT:1><EOL>vx, vy, vz = vx/n, vy/n, vz/n<EOL>return (vx-self.vx)/d, (vy-self.vy)/d, (vz-self.vz)/d<EOL>", "docstring": "Boids match velocity with other boids.", "id": "f11568:c0:m4"}
{"signature": "def calculate(self, **state):", "body": "T = state['<STR_LIT:T>']<EOL>x = state['<STR_LIT:x>']<EOL>x_total = sum([<EOL>x for compound, x in x.items()<EOL>if compound in materials])<EOL>x = {<EOL>compound: x[compound]/x_total<EOL>for compound in x.keys()<EOL>if compound in materials}<EOL>mu = {i: materials[i].mu(T=T) for i in x.keys()}<EOL>result = sum([mu[i] * x[i] * sqrt(M(i)) for i in x.keys()])<EOL>result /= sum([x[i] * sqrt(M(i)) for i in x.keys()])<EOL>return result<EOL>", "docstring": "Calculate dynamic viscosity at the specified temperature and\ncomposition:\n\n:param T: [K] temperature\n:param x: [mole fraction] composition dictionary , e.g.\n  {'CO': 0.25, 'CO2': 0.25, 'N2': 0.25, 'O2': 0.25}\n\n:returns: [Pa.s] dynamic viscosity\n\nThe **state parameter contains the keyword argument(s) specified above\nthat are used to describe the state of the material.", "id": "f15854:c1:m1"}
{"signature": "def map(self, features=None, query=None, styles=None,<EOL>bbox=[-<NUM_LIT>,-<NUM_LIT>,<NUM_LIT>,<NUM_LIT>], zoom=<NUM_LIT:10>, center=None, <EOL>image=None, image_bounds=None, cmap='<STR_LIT>',<EOL>api_key=os.environ.get('<STR_LIT>', None), **kwargs):", "body": "try:<EOL><INDENT>from IPython.display import display<EOL><DEDENT>except:<EOL><INDENT>print(\"<STR_LIT>\")<EOL>return<EOL><DEDENT>assert api_key is not None, \"<STR_LIT>\"<EOL>if features is None and query is not None:<EOL><INDENT>wkt = box(*bbox).wkt<EOL>features = self.query(wkt, query, index=None)<EOL><DEDENT>elif features is None and query is None and image is None:<EOL><INDENT>print('<STR_LIT>')<EOL>return<EOL><DEDENT>if styles is not None and not isinstance(styles, list):<EOL><INDENT>styles = [styles]<EOL><DEDENT>geojson = {\"<STR_LIT:type>\":\"<STR_LIT>\", \"<STR_LIT>\": features}<EOL>if center is None and features is not None:<EOL><INDENT>union = cascaded_union([shape(f['<STR_LIT>']) for f in features])<EOL>lon, lat = union.centroid.coords[<NUM_LIT:0>]<EOL><DEDENT>elif center is None and image is not None:<EOL><INDENT>try:<EOL><INDENT>lon, lat = shape(image).centroid.coords[<NUM_LIT:0>]<EOL><DEDENT>except:<EOL><INDENT>lon, lat = box(*image_bounds).centroid.coords[<NUM_LIT:0>]<EOL><DEDENT><DEDENT>else:<EOL><INDENT>lat, lon = center<EOL><DEDENT>map_id = \"<STR_LIT>\".format(str(int(time.time())))<EOL>map_data = VectorGeojsonLayer(geojson, styles=styles, **kwargs)<EOL>image_layer = self._build_image_layer(image, image_bounds, cmap)<EOL>template = BaseTemplate(map_id, **{<EOL>\"<STR_LIT>\": lat, <EOL>\"<STR_LIT>\": lon, <EOL>\"<STR_LIT>\": zoom,<EOL>\"<STR_LIT>\": json.dumps(map_data.datasource),<EOL>\"<STR_LIT>\": json.dumps(map_data.layers),<EOL>\"<STR_LIT>\": image_layer,<EOL>\"<STR_LIT>\": api_key,<EOL>\"<STR_LIT>\": '<STR_LIT>'<EOL>})<EOL>template.inject()<EOL>", "docstring": "Renders a mapbox gl map from a vector service query or a list of geojson features\n\nArgs:\n  features (list): a list of geojson features\n  query (str): a VectorServices query \n  styles (list): a list of VectorStyles to apply to the features  \n  bbox (list): a bounding box to query for features ([minx, miny, maxx, maxy])\n  zoom (int): the initial zoom level of the map\n  center (list): a list of [lat, lon] used to center the map\n  api_key (str): a valid Mapbox API key\n  image (dict): a CatalogImage or a ndarray\n  image_bounds (list): a list of bounds for image positioning \n  Use outside of GBDX Notebooks requires a MapBox API key, sign up for free at https://www.mapbox.com/pricing/\n  Pass the key using the `api_key` keyword or set an environmental variable called `MAPBOX API KEY`\n  cmap (str): MatPlotLib colormap to use for rendering single band images (default: viridis)", "id": "f7095:c0:m8"}
{"signature": "def _parse_persons(self, datafield, subfield, roles=[\"<STR_LIT>\"]):", "body": "<EOL>parsed_persons = []<EOL>raw_persons = self.get_subfields(datafield, subfield)<EOL>for person in raw_persons:<EOL><INDENT>other_subfields = person.other_subfields<EOL>if \"<STR_LIT:4>\" in other_subfields and roles != [\"<STR_LIT>\"]:<EOL><INDENT>person_roles = other_subfields[\"<STR_LIT:4>\"]  <EOL>relevant = any(map(lambda role: role in roles, person_roles))<EOL>if not relevant:<EOL><INDENT>continue<EOL><DEDENT><DEDENT>ind1 = person.i1<EOL>ind2 = person.i2<EOL>person = person.strip()<EOL>name = \"<STR_LIT>\"<EOL>second_name = \"<STR_LIT>\"<EOL>surname = \"<STR_LIT>\"<EOL>title = \"<STR_LIT>\"<EOL>if ind1 == \"<STR_LIT:1>\" and ind2 == \"<STR_LIT:U+0020>\":<EOL><INDENT>if \"<STR_LIT:U+002C>\" in person:<EOL><INDENT>surname, name = person.split(\"<STR_LIT:U+002C>\", <NUM_LIT:1>)<EOL><DEDENT>elif \"<STR_LIT:U+0020>\" in person:<EOL><INDENT>surname, name = person.split(\"<STR_LIT:U+0020>\", <NUM_LIT:1>)<EOL><DEDENT>else:<EOL><INDENT>surname = person<EOL><DEDENT>if \"<STR_LIT:c>\" in other_subfields:<EOL><INDENT>title = \"<STR_LIT:U+002C>\".join(other_subfields[\"<STR_LIT:c>\"])<EOL><DEDENT><DEDENT>elif ind1 == \"<STR_LIT:0>\" and ind2 == \"<STR_LIT:U+0020>\":<EOL><INDENT>name = person.strip()<EOL>if \"<STR_LIT:b>\" in other_subfields:<EOL><INDENT>second_name = \"<STR_LIT:U+002C>\".join(other_subfields[\"<STR_LIT:b>\"])<EOL><DEDENT>if \"<STR_LIT:c>\" in other_subfields:<EOL><INDENT>surname = \"<STR_LIT:U+002C>\".join(other_subfields[\"<STR_LIT:c>\"])<EOL><DEDENT><DEDENT>elif ind1 == \"<STR_LIT:1>\" and ind2 == \"<STR_LIT:0>\" or ind1 == \"<STR_LIT:0>\" and ind2 == \"<STR_LIT:0>\":<EOL><INDENT>name = person.strip()<EOL>if \"<STR_LIT:c>\" in other_subfields:<EOL><INDENT>title = \"<STR_LIT:U+002C>\".join(other_subfields[\"<STR_LIT:c>\"])<EOL><DEDENT><DEDENT>parsed_persons.append(<EOL>Person(<EOL>name.strip(),<EOL>second_name.strip(),<EOL>surname.strip(),<EOL>title.strip()<EOL>)<EOL>)<EOL><DEDENT>return parsed_persons<EOL>", "docstring": "Parse persons from given datafield.\n\nArgs:\n    datafield (str): code of datafield (\"010\", \"730\", etc..)\n    subfield (char):  code of subfield (\"a\", \"z\", \"4\", etc..)\n    role (list of str): set to [\"any\"] for any role, [\"aut\"] for\n         authors, etc.. For details see\n         http://www.loc.gov/marc/relators/relaterm.html\n\nMain records for persons are: \"100\", \"600\" and \"700\", subrecords \"c\".\n\nReturns:\n    list: Person objects.", "id": "f1163:c0:m2"}
{"signature": "def get_project(project_id):", "body": "try:<EOL><INDENT>res = _pybossa_req('<STR_LIT>', '<STR_LIT>', project_id)<EOL>if res.get('<STR_LIT:id>'):<EOL><INDENT>return Project(res)<EOL><DEDENT>else:<EOL><INDENT>return res<EOL><DEDENT><DEDENT>except:  <EOL><INDENT>raise<EOL><DEDENT>", "docstring": "Return a PYBOSSA Project for the project_id.\n\n    :param project_id: PYBOSSA Project ID\n    :type project_id: integer\n    :rtype: PYBOSSA Project\n    :returns: A PYBOSSA Project object", "id": "f10088:m3"}
{"signature": "@tornado.gen.coroutine<EOL>def get_topologies_states():", "body": "request_url = create_url(TOPOLOGIES_STATS_URL_FMT)<EOL>raise tornado.gen.Return((yield fetch_url_as_json(request_url)))<EOL>", "docstring": "Get the list of topologies and their states\n:return:", "id": "f7414:m4"}
{"signature": "def point_distance(point1, point2):", "body": "return ((point1[<NUM_LIT:0>] - point2[<NUM_LIT:0>]) ** <NUM_LIT:2> + (point1[<NUM_LIT:1>] - point2[<NUM_LIT:1>]) ** <NUM_LIT:2>) ** <NUM_LIT:0.5><EOL>", "docstring": "Computes the distance beteen two points on a plane.\n\nArgs:\n    point1: Tuple or list, the x and y coordinate of the first point.\n\n    point2: Tuple or list, the x and y coordinate of the second point.\n\nReturns:\n    The distance between the two points as a floating point number.", "id": "f1541:m8"}
{"signature": "def __getattr__(self, a):", "body": "if a in self:<EOL><INDENT>return self[a]<EOL><DEDENT>raise AttributeError(\"<STR_LIT>\"+a+\"<STR_LIT:'>\")<EOL>", "docstring": "Keys in the dictionaries are accessible as attributes.", "id": "f11607:c0:m4"}
{"signature": "def sendMessage(self, data):", "body": "opcode = BINARY<EOL>if _check_unicode(data):<EOL><INDENT>opcode = TEXT<EOL><DEDENT>self._sendMessage(False, opcode, data)<EOL>", "docstring": "Send websocket data frame to the client.\n\nIf data is a unicode object then the frame is sent as Text.\nIf the data is a bytearray object then the frame is sent as Binary.", "id": "f9797:c2:m11"}
{"signature": "def _from_base_type(self, value):", "body": "if not value:<EOL><INDENT>return None<EOL><DEDENT>try:<EOL><INDENT>credentials = client.Credentials.new_from_json(value)<EOL><DEDENT>except ValueError:<EOL><INDENT>credentials = None<EOL><DEDENT>return credentials<EOL>", "docstring": "Converts our stored JSON string back to the desired type.\n\n        Args:\n            value: A value from the datastore to be converted to the\n                   desired type.\n\n        Returns:\n            A deserialized Credentials (or subclass) object, else None if\n            the value can't be parsed.", "id": "f2459:c2:m2"}
{"signature": "def __getstate__(self):", "body": "state = self.__dict__.copy()<EOL>state['<STR_LIT>'] = self._knn.__getstate__()<EOL>del state['<STR_LIT>']<EOL>for field in self._getEphemeralAttributes():<EOL><INDENT>del state[field]<EOL><DEDENT>return state<EOL>", "docstring": "Get serializable state.", "id": "f17620:c0:m5"}
{"signature": "def caseless_literal(s):", "body": "return make_caseless_literal(s)()<EOL>", "docstring": "A literal string, case independant.", "id": "f1511:m7"}
{"signature": "def getBinaries(self):", "body": "<EOL>if '<STR_LIT>' in self.description:<EOL><INDENT>return {os.path.normpath(self.description['<STR_LIT>']): self.getName()}<EOL><DEDENT>else:<EOL><INDENT>return {}<EOL><DEDENT>", "docstring": "Return a dictionary of binaries to compile: {\"dirname\":\"exename\"},\n            this is used when automatically generating CMakeLists\n\n            Note that currently modules may define only a single executable\n            binary or library to be built by the automatic build system, by\n            specifying `\"bin\": \"dir-to-be-built-into-binary\"`, or `\"lib\":\n            \"dir-to-be-built-into-library\"`, and the bin/lib will always have\n            the same name as the module. The default behaviour if nothing is\n            specified is for the 'source' directory to be built into a library.\n\n            The module.json syntax may allow for other combinations in the\n            future (and callers of this function should not rely on it\n            returning only a single item). For example, a \"bin\": {\"dirname\":\n            \"exename\"} syntax might be supported, however currently more\n            complex builds must be controlled by custom CMakeLists.", "id": "f13549:c0:m16"}
{"signature": "def set_pad(self, val):", "body": "self._pad = val<EOL>", "docstring": "Set the tick label pad in points\n\nACCEPTS: float", "id": "f17198:c0:m5"}
{"signature": "def unregister_service(self, name):", "body": "return self.srvreg.unregister_service(name)<EOL>", "docstring": "Implementation of :meth:`twitcher.api.IRegistry.unregister_service`.", "id": "f13833:c0:m5"}
{"signature": "def getCallbacks(self, *args, **kwargs):", "body": "engine_internal.Network.getCallbacks(self, *args, **kwargs)<EOL>", "docstring": "@doc:place_holder(Network.getCallbacks)", "id": "f17536:c6:m8"}
{"signature": "def cleanup(self, pin=None):", "body": "if pin is None:<EOL><INDENT>self.bbio_gpio.cleanup()<EOL><DEDENT>else:<EOL><INDENT>self.bbio_gpio.cleanup(pin)<EOL><DEDENT>", "docstring": "Clean up GPIO event detection for specific pin, or all pins if none \n        is specified.", "id": "f7998:c2:m10"}
{"signature": "@pipe.func<EOL>def grep(prev, pattern, *args, **kw):", "body": "inv = False if '<STR_LIT>' not in kw else kw.pop('<STR_LIT>')<EOL>pattern_obj = re.compile(pattern, *args, **kw)<EOL>for data in prev:<EOL><INDENT>if bool(inv) ^ bool(pattern_obj.match(data)):<EOL><INDENT>yield data<EOL><DEDENT><DEDENT>", "docstring": "The pipe greps the data passed from previous generator according to\n    given regular expression.\n\n    :param prev: The previous iterator of pipe.\n    :type prev: Pipe\n    :param pattern: The pattern which used to filter out data.\n    :type pattern: str|unicode|re pattern object\n    :param inv: If true, invert the match condition.\n    :type inv: boolean\n    :param kw:\n    :type kw: dict\n    :returns: generator", "id": "f2259:m13"}
{"signature": "def __init__(self, time_series, baseline_time_series=None, precision=None, lag_window_size=None,<EOL>future_window_size=None, chunk_size=None):", "body": "super(BitmapDetector, self).__init__(self.__class__.__name__, time_series, baseline_time_series)<EOL>self.precision = precision if precision and precision > <NUM_LIT:0> else DEFAULT_BITMAP_PRECISION<EOL>self.chunk_size = chunk_size if chunk_size and chunk_size > <NUM_LIT:0> else DEFAULT_BITMAP_CHUNK_SIZE<EOL>if lag_window_size:<EOL><INDENT>self.lag_window_size = lag_window_size<EOL><DEDENT>else:<EOL><INDENT>self.lag_window_size = int(self.time_series_length * DEFAULT_BITMAP_LAGGING_WINDOW_SIZE_PCT)<EOL><DEDENT>if future_window_size:<EOL><INDENT>self.future_window_size = future_window_size<EOL><DEDENT>else:<EOL><INDENT>self.future_window_size = int(self.time_series_length * DEFAULT_BITMAP_LEADING_WINDOW_SIZE_PCT)<EOL><DEDENT>self._sanity_check()<EOL>", "docstring": "Initializer\n:param TimeSeries time_series: a TimeSeries object.\n:param TimeSeries baseline_time_series: baseline TimeSeries.\n:param int precision: how many sections to categorize values.\n:param int lag_window_size: lagging window size.\n:param int future_window_size: future window size.\n:param int chunk_size: chunk size.", "id": "f7845:c0:m0"}
{"signature": "def get_child_by_name(parent, name):", "body": "<EOL>def iterate_children(widget, name):<EOL><INDENT>if widget.get_name() == name:<EOL><INDENT>return widget<EOL><DEDENT>try:<EOL><INDENT>for w in widget.get_children():<EOL><INDENT>result = iterate_children(w, name)<EOL>if result is not None:<EOL><INDENT>return result<EOL><DEDENT>else:<EOL><INDENT>continue<EOL><DEDENT><DEDENT><DEDENT>except AttributeError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>return iterate_children(parent, name)<EOL>", "docstring": "Iterate through a gtk container, `parent`,\nand return the widget with the name `name`.", "id": "f11494:m4"}
{"signature": "def SendKey(self, key: int, waitTime: float = OPERATION_WAIT_TIME) -> None:", "body": "self.SetFocus()<EOL>SendKey(key, waitTime)<EOL>", "docstring": "Make control have focus first and type a key.\n`self.SetFocus` may not work for some controls, you may need to click it to make it have focus.\nkey: int, a key code value in class Keys.\nwaitTime: float.", "id": "f1782:c78:m75"}
{"signature": "def help_option(*param_decls, **attrs):", "body": "def decorator(f):<EOL><INDENT>def callback(ctx, param, value):<EOL><INDENT>if value and not ctx.resilient_parsing:<EOL><INDENT>echo(ctx.get_help(), color=ctx.color)<EOL>ctx.exit()<EOL><DEDENT><DEDENT>attrs.setdefault('<STR_LIT>', True)<EOL>attrs.setdefault('<STR_LIT>', False)<EOL>attrs.setdefault('<STR_LIT>', '<STR_LIT>')<EOL>attrs.setdefault('<STR_LIT>', True)<EOL>attrs['<STR_LIT>'] = callback<EOL>return option(*(param_decls or ('<STR_LIT>',)), **attrs)(f)<EOL><DEDENT>return decorator<EOL>", "docstring": "Adds a ``--help`` option which immediately ends the program\n    printing out the help page.  This is usually unnecessary to add as\n    this is added by default to all commands unless suppressed.\n\n    Like :func:`version_option`, this is implemented as eager option that\n    prints in the callback and exits.\n\n    All arguments are forwarded to :func:`option`.", "id": "f8342:m12"}
{"signature": "def _validate_num_channels(input_filepath_list, combine_type):", "body": "channels = [<EOL>file_info.channels(f) for f in input_filepath_list<EOL>]<EOL>if not core.all_equal(channels):<EOL><INDENT>raise IOError(<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>.format(combine_type)<EOL>)<EOL><DEDENT>", "docstring": "Check if files in input file list have the same number of channels", "id": "f3808:m2"}
{"signature": "@classmethod<EOL><INDENT>def from_json_list(cls, api_client, data):<DEDENT>", "body": "return [cls.from_json(api_client, item) for item in data]<EOL>", "docstring": "Convert a list of JSON values to a list of models", "id": "f2402:c4:m0"}
{"signature": "@classmethod<EOL><INDENT>def fromvalue(cls, v):<DEDENT>", "body": "if isinstance(v, text_type):<EOL><INDENT>return cls(base=v)<EOL><DEDENT>if isinstance(v, dict):<EOL><INDENT>return cls(**DescriptionBase.partition_properties(v))<EOL><DEDENT>if isinstance(v, cls):<EOL><INDENT>return v<EOL><DEDENT>raise ValueError(v)<EOL>", "docstring": ":param v: Initialization data for `cls`; either a single string that is the main datatype \\\nof the values of the cell or a datatype description object, i.e. a `dict` or a `cls`\ninstance.\n:return: An instance of `cls`", "id": "f15003:c4:m0"}
{"signature": "def setPotentialPct(self, potentialPct):", "body": "self._potentialPct = potentialPct<EOL>", "docstring": ":param potentialPct: (float) value to set", "id": "f17561:c4:m8"}
{"signature": "def _set_request(self, request):", "body": "self.request = request<EOL>", "docstring": "Saves the request object for future use by the different Namespaces.\n\n        This is called by socketio_manage().", "id": "f11809:c0:m2"}
{"signature": "@property<EOL><INDENT>def teams(self, year=<NUM_LIT>):<DEDENT>", "body": "if year not in self._teams:<EOL><INDENT>self._teams[year] = teams(year)<EOL><DEDENT>return self._teams[year]<EOL>", "docstring": "Return all teams in dict {id0: team0, id1: team1}.\n\n        :params year: Year.", "id": "f3437:c0:m10"}
{"signature": "def __abs__(self):", "body": "raise NotImplementedError<EOL>", "docstring": "Returns the Real distance from 0. Called for abs(self).", "id": "f13982:c1:m23"}
{"signature": "def make_game():", "body": "return ascii_art.ascii_art_to_game(<EOL>GAME_ART, what_lies_beneath='<STR_LIT:U+0020>',<EOL>sprites=dict(<EOL>[('<STR_LIT:P>', PlayerSprite)] +<EOL>[(c, UpwardLaserBoltSprite) for c in UPWARD_BOLT_CHARS] +<EOL>[(c, DownwardLaserBoltSprite) for c in DOWNWARD_BOLT_CHARS]),<EOL>drapes=dict(X=MarauderDrape,<EOL>B=BunkerDrape),<EOL>update_schedule=['<STR_LIT:P>', '<STR_LIT:B>', '<STR_LIT:X>'] + list(_ALL_BOLT_CHARS))<EOL>", "docstring": "Builds and returns an Extraterrestrial Marauders game.", "id": "f14375:m0"}
{"signature": "def weighted_replicate(seq, weights, n):", "body": "assert len(seq) == len(weights)<EOL>weights = normalize(weights)<EOL>wholes = [int(w*n) for w in weights]<EOL>fractions = [(w*n) % <NUM_LIT:1> for w in weights]<EOL>return (flatten([x] * nx for x, nx in zip(seq, wholes))<EOL>+ weighted_sample_with_replacement(seq, fractions, n - sum(wholes)))<EOL>", "docstring": "Return n selections from seq, with the count of each element of\n    seq proportional to the corresponding weight (filling in fractions\n    randomly).\n    >>> weighted_replicate('ABC', [1,2,1], 4)\n    ['A', 'B', 'B', 'C']", "id": "f1680:m20"}
{"signature": "def show(self, axis=None, display=True):", "body": "if (not axis):<EOL><INDENT>(_, axis) = plt.subplots(self.__size, <NUM_LIT:1>);<EOL><DEDENT>self.__format_canvases(axis);<EOL>for dynamic in self.__dynamic_storage:<EOL><INDENT>self.__display_dynamic(axis, dynamic);<EOL><DEDENT>if (display):<EOL><INDENT>plt.show();<EOL><DEDENT>", "docstring": "!\n        @brief Draw and show output dynamics.\n\n        @param[in] axis (axis): If is not 'None' then user specified axis is used to display output dynamic.\n        @param[in] display (bool): Whether output dynamic should be displayed or not, if not, then user\n                    should call 'plt.show()' by himself.", "id": "f15629:c2:m4"}
{"signature": "def __init__(self, name, N):", "body": "<EOL>self.__name = None<EOL>self.name = name<EOL>self.__N = N<EOL>self.__rho = <NUM_LIT:0><EOL>self.__k = None<EOL>self.__old_data = None<EOL>self.__data = None<EOL>self.__norm = True<EOL>", "docstring": "Create a criteria object\n\n        :param name: a string or list of strings containing valid criteria\n            method's name\n        :param int N: size of the data sample.", "id": "f10926:c0:m0"}
{"signature": "def convert_maxpool3(params, w_name, scope_name, inputs, layers, weights, names):", "body": "print('<STR_LIT>')<EOL>if names == '<STR_LIT>':<EOL><INDENT>tf_name = '<STR_LIT:P>' + random_string(<NUM_LIT:7>)<EOL><DEDENT>elif names == '<STR_LIT>':<EOL><INDENT>tf_name = w_name<EOL><DEDENT>else:<EOL><INDENT>tf_name = w_name + str(random.random())<EOL><DEDENT>if '<STR_LIT>' in params:<EOL><INDENT>height, width, depth = params['<STR_LIT>']<EOL><DEDENT>else:<EOL><INDENT>height, width, depth = params['<STR_LIT>']<EOL><DEDENT>if '<STR_LIT>' in params:<EOL><INDENT>stride_height, stride_width, stride_depth = params['<STR_LIT>']<EOL><DEDENT>else:<EOL><INDENT>stride_height, stride_width, stride_depth = params['<STR_LIT>']<EOL><DEDENT>if '<STR_LIT>' in params:<EOL><INDENT>padding_h, padding_w, padding_d, _, _ = params['<STR_LIT>']<EOL><DEDENT>else:<EOL><INDENT>padding_h, padding_w, padding_d = params['<STR_LIT>']<EOL><DEDENT>input_name = inputs[<NUM_LIT:0>]<EOL>if padding_h > <NUM_LIT:0> and padding_w > <NUM_LIT:0> and padding_d > <NUM_LIT:0>:<EOL><INDENT>padding_name = tf_name + '<STR_LIT>'<EOL>padding_layer = keras.layers.ZeroPadding3D(<EOL>padding=(padding_h, padding_w, padding_d),<EOL>name=padding_name<EOL>)<EOL>layers[padding_name] = padding_layer(layers[inputs[<NUM_LIT:0>]])<EOL>input_name = padding_name<EOL><DEDENT>pooling = keras.layers.MaxPooling3D(<EOL>pool_size=(height, width, depth),<EOL>strides=(stride_height, stride_width, stride_depth),<EOL>padding='<STR_LIT>',<EOL>name=tf_name<EOL>)<EOL>layers[scope_name] = pooling(layers[input_name])<EOL>", "docstring": "Convert 3d Max pooling.\n\nArgs:\n    params: dictionary with layer parameters\n    w_name: name prefix in state_dict\n    scope_name: pytorch scope name\n    inputs: pytorch node inputs\n    layers: dictionary with keras tensors\n    weights: pytorch state_dict\n    names: use short names for keras layers", "id": "f5036:m2"}
{"signature": "def _collect_certificate_data(self, enterprise_enrollment):", "body": "if self.certificates_api is None:<EOL><INDENT>self.certificates_api = CertificatesApiClient(self.user)<EOL><DEDENT>course_id = enterprise_enrollment.course_id<EOL>username = enterprise_enrollment.enterprise_customer_user.user.username<EOL>try:<EOL><INDENT>certificate = self.certificates_api.get_course_certificate(course_id, username)<EOL>completed_date = certificate.get('<STR_LIT>')<EOL>if completed_date:<EOL><INDENT>completed_date = parse_datetime(completed_date)<EOL><DEDENT>else:<EOL><INDENT>completed_date = timezone.now()<EOL><DEDENT>is_passing = certificate.get('<STR_LIT>')<EOL>grade = self.grade_passing if is_passing else self.grade_failing<EOL><DEDENT>except HttpNotFoundError:<EOL><INDENT>completed_date = None<EOL>grade = self.grade_incomplete<EOL>is_passing = False<EOL><DEDENT>return completed_date, grade, is_passing<EOL>", "docstring": "Collect the learner completion data from the course certificate.\n\nUsed for Instructor-paced courses.\n\nIf no certificate is found, then returns the completed_date = None, grade = In Progress, on the idea that a\ncertificate will eventually be generated.\n\nArgs:\n    enterprise_enrollment (EnterpriseCourseEnrollment): the enterprise enrollment record for which we need to\n    collect completion/grade data\n\nReturns:\n    completed_date: Date the course was completed, this is None if course has not been completed.\n    grade: Current grade in the course.\n    is_passing: Boolean indicating if the grade is a passing grade or not.", "id": "f16246:c0:m6"}
{"signature": "def sigma_Stiel_Thodos(Vc, Zc):", "body": "Vc = Vc*<NUM_LIT><EOL>sigma = <NUM_LIT>*Vc**(<NUM_LIT:1>/<NUM_LIT>)*Zc**(-<NUM_LIT>)<EOL>return sigma<EOL>", "docstring": "r'''Calculates Lennard-Jones molecular diameter.\n    Uses critical volume and compressibility. CSP method by [1]_.\n\n    .. math::\n        \\sigma = 0.1866 V_c^{1/3} Z_c^{-6/5}\n\n    Parameters\n    ----------\n    Vc : float\n        Critical volume of fluid [m^3/mol]\n    Zc : float\n        Critical compressibility of fluid, [-]\n\n    Returns\n    -------\n    sigma : float\n        Lennard-Jones molecular diameter, [Angstrom]\n\n    Notes\n    -----\n    Vc is originally in units of mL/mol.\n\n    Examples\n    --------\n    Monofluorobenzene\n\n    >>> sigma_Stiel_Thodos(0.000271, 0.265)\n    5.94300853971033\n\n    References\n    ----------\n    .. [1] Stiel, L. I., and George Thodos. \"Lennard-Jones Force Constants\n       Predicted from Critical Properties.\" Journal of Chemical & Engineering\n       Data 7, no. 2 (April 1, 1962): 234-36. doi:10.1021/je60013a023", "id": "f15793:m7"}
{"signature": "def get_course_duration(self, obj):", "body": "duration = obj.end - obj.start if obj.start and obj.end else None<EOL>if duration:<EOL><INDENT>return strfdelta(duration, '<STR_LIT>')<EOL><DEDENT>return '<STR_LIT>'<EOL>", "docstring": "Get course's duration as a timedelta.\n\nArguments:\n    obj (CourseOverview): CourseOverview object\n\nReturns:\n    (timedelta): Duration of a course.", "id": "f16199:c1:m0"}
{"signature": "def type(self, array):", "body": "return self.get_stats()['<STR_LIT>'][array]['<STR_LIT:type>']<EOL>", "docstring": "Return the array's type.", "id": "f5087:c0:m7"}
{"signature": "def default_headers(self):", "body": "_headers = {<EOL>\"<STR_LIT>\": \"<STR_LIT>\" % __version__,<EOL>\"<STR_LIT>\": \"<STR_LIT:%s>\" % __api_version__,<EOL>}<EOL>if self.api_key:<EOL><INDENT>_headers[\"<STR_LIT>\"] = \"<STR_LIT>\" % self.api_key<EOL><DEDENT>return _headers<EOL>", "docstring": "It's always OK to include these headers", "id": "f5629:c0:m1"}
{"signature": "def __call__(self, x, pos=None):", "body": "xmin, xmax = self.axis.get_view_interval()<EOL>d = abs(xmax - xmin)<EOL>return self.pprint_val(x,d)<EOL>", "docstring": "Return the format for tick val *x* at position *pos*", "id": "f17239:c6:m0"}
{"signature": "def visit_BitVecOr(self, expression, *operands):", "body": "left = expression.operands[<NUM_LIT:0>]<EOL>right = expression.operands[<NUM_LIT:1>]<EOL>if isinstance(right, BitVecConstant):<EOL><INDENT>if right.value == <NUM_LIT:0>:<EOL><INDENT>return left<EOL><DEDENT>elif right.value == left.mask:<EOL><INDENT>return right<EOL><DEDENT>elif isinstance(left, BitVecOr):<EOL><INDENT>left_left = left.operands[<NUM_LIT:0>]<EOL>left_right = left.operands[<NUM_LIT:1>]<EOL>if isinstance(right, Constant):<EOL><INDENT>return BitVecOr(left_left, (left_right | right), taint=expression.taint)<EOL><DEDENT><DEDENT><DEDENT>elif isinstance(left, BitVecConstant):<EOL><INDENT>return BitVecOr(right, left, taint=expression.taint)<EOL><DEDENT>", "docstring": "a | 0 => a\n            0 | a => a\n            0xffffffff & a => 0xffffffff\n            a & 0xffffffff => 0xffffffff", "id": "f16987:c6:m10"}
{"signature": "def unpackFromCache(cache_key, to_directory):", "body": "if cache_key is None:<EOL><INDENT>raise NotInCache('<STR_LIT>')<EOL><DEDENT>cache_key = _encodeCacheKey(cache_key)<EOL>cache_dir = folders.cacheDirectory()<EOL>fsutils.mkDirP(cache_dir)<EOL>path = os.path.join(cache_dir, cache_key)<EOL>logger.debug('<STR_LIT>', path, to_directory)<EOL>try:<EOL><INDENT>unpackFrom(path, to_directory)<EOL>try:<EOL><INDENT>shutil.copy(path + '<STR_LIT>', os.path.join(to_directory, '<STR_LIT>'))<EOL><DEDENT>except IOError as e:<EOL><INDENT>if e.errno == errno.ENOENT:<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>raise<EOL><DEDENT><DEDENT>cache_logger.debug('<STR_LIT>', cache_key, to_directory)<EOL>return<EOL><DEDENT>except IOError as e:<EOL><INDENT>if e.errno == errno.ENOENT:<EOL><INDENT>cache_logger.debug('<STR_LIT>', cache_key)<EOL>raise NotInCache('<STR_LIT>')<EOL><DEDENT><DEDENT>except OSError as e:<EOL><INDENT>if e.errno == errno.ENOTEMPTY:<EOL><INDENT>logger.error('<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>raise<EOL><DEDENT><DEDENT>", "docstring": "If the specified cache key exists, unpack the tarball into the\n        specified directory, otherwise raise NotInCache (a KeyError subclass).", "id": "f13560:m6"}
{"signature": "def __init__(self, access_token, client_id, client_secret, refresh_token,<EOL>token_expiry, token_uri, user_agent,<EOL>revoke_uri=oauth2client.GOOGLE_REVOKE_URI):", "body": "super(GoogleCredentials, self).__init__(<EOL>access_token, client_id, client_secret, refresh_token,<EOL>token_expiry, token_uri, user_agent, revoke_uri=revoke_uri)<EOL>", "docstring": "Create an instance of GoogleCredentials.\n\n        This constructor is not usually called by the user, instead\n        GoogleCredentials objects are instantiated by\n        GoogleCredentials.from_stream() or\n        GoogleCredentials.get_application_default().\n\n        Args:\n            access_token: string, access token.\n            client_id: string, client identifier.\n            client_secret: string, client secret.\n            refresh_token: string, refresh token.\n            token_expiry: datetime, when the access_token expires.\n            token_uri: string, URI of token endpoint.\n            user_agent: string, The HTTP User-Agent to provide for this\n                        application.\n            revoke_uri: string, URI for revoke endpoint. Defaults to\n                        oauth2client.GOOGLE_REVOKE_URI; a token can't be\n                        revoked if this is None.", "id": "f2472:c18:m0"}
{"signature": "def match_list(lst, pattern, group_names=[]):", "body": "filtfn = re.compile(pattern).match<EOL>filtlst = filter_list(lst, filtfn)<EOL>if not group_names:<EOL><INDENT>return [m.string for m in filtlst]<EOL><DEDENT>else:<EOL><INDENT>return [m.group(group_names) for m in filtlst]<EOL><DEDENT>", "docstring": "Parameters\n----------\nlst: list of str\n\nregex: string\n\ngroup_names: list of strings\n    See re.MatchObject group docstring\n\nReturns\n-------\nlist of strings\n    Filtered list, with the strings that match the pattern", "id": "f4077:m2"}
{"signature": "@classmethod<EOL><INDENT>def from_yamlfile(cls, fp, selector_handler=None, strict=False, debug=False):<DEDENT>", "body": "return cls.from_yamlstring(fp.read(), selector_handler=selector_handler, strict=strict, debug=debug)<EOL>", "docstring": "Create a Parselet instance from a file containing\nthe Parsley script as a YAML object\n\n>>> import parslepy\n>>> with open('parselet.yml') as fp:\n...     parslepy.Parselet.from_yamlfile(fp)\n...\n<parslepy.base.Parselet object at 0x2014e50>\n\n:param file fp: an open file-like pointer containing the Parsley script\n:rtype: :class:`.Parselet`\n\nOther arguments: same as for :class:`.Parselet` contructor", "id": "f9180:c4:m2"}
{"signature": "def obs_to_dict(obs):", "body": "if isinstance(obs, dict):<EOL><INDENT>return obs<EOL><DEDENT>return {None: obs}<EOL>", "docstring": "Convert an observation into a dict.", "id": "f1343:m3"}
{"signature": "def run(self, shell=True, cmdline=False, echo=True):", "body": "if env():<EOL><INDENT>return <NUM_LIT:1><EOL><DEDENT>cmd = [\"<STR_LIT>\"] + self.__parse_parms()<EOL>if cmdline:<EOL><INDENT>cij.emph(\"<STR_LIT>\" % (shell, cmd))<EOL><DEDENT>return cij.ssh.command(cmd, shell, echo)<EOL>", "docstring": "Run FIO job", "id": "f13347:c1:m8"}
{"signature": "def seiffert_mean(nums):", "body": "if len(nums) == <NUM_LIT:1>:<EOL><INDENT>return nums[<NUM_LIT:0>]<EOL><DEDENT>if len(nums) > <NUM_LIT:2>:<EOL><INDENT>raise AttributeError('<STR_LIT>')<EOL><DEDENT>if nums[<NUM_LIT:0>] + nums[<NUM_LIT:1>] == <NUM_LIT:0> or nums[<NUM_LIT:0>] - nums[<NUM_LIT:1>] == <NUM_LIT:0>:<EOL><INDENT>return float('<STR_LIT>')<EOL><DEDENT>return (nums[<NUM_LIT:0>] - nums[<NUM_LIT:1>]) / (<EOL><NUM_LIT:2> * math.asin((nums[<NUM_LIT:0>] - nums[<NUM_LIT:1>]) / (nums[<NUM_LIT:0>] + nums[<NUM_LIT:1>]))<EOL>)<EOL>", "docstring": "r\"\"\"Return Seiffert's mean.\n\n    Seiffert's mean of two numbers x and y is:\n    :math:`\\frac{x - y}{4 \\cdot arctan \\sqrt{\\frac{x}{y}} - \\pi}`\n\n    It is defined in :cite:`Seiffert:1993`.\n\n    Parameters\n    ----------\n    nums : list\n        A series of numbers\n\n    Returns\n    -------\n    float\n        Sieffert's mean of nums\n\n    Raises\n    ------\n    AttributeError\n        seiffert_mean supports no more than two values\n\n    Examples\n    --------\n    >>> seiffert_mean([1, 2])\n    1.4712939827611637\n    >>> seiffert_mean([1, 0])\n    0.3183098861837907\n    >>> seiffert_mean([2, 4])\n    2.9425879655223275\n    >>> seiffert_mean([2, 1000])\n    336.84053300118825", "id": "f6663:m7"}
{"signature": "def convert_onnx_to_model(onnx_input_path):", "body": "model = onnx.load(onnx_input_path)<EOL>tf_rep = prepare(model)<EOL>img = np.load(\"<STR_LIT>\")<EOL>output = tf_rep.run(img.reshape([<NUM_LIT:1>, <NUM_LIT>]))<EOL>print(\"<STR_LIT>\", np.argmax(output))<EOL>", "docstring": "Reimplementation of the TensorFlow-onnx official tutorial convert the onnx file to specific: model\n\n    Parameters\n    -----------\n    onnx_input_path : string\n    the path where you save the onnx file.\n\n    References\n    -----------\n    - `onnx-tf exporting tutorial <https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb>`__", "id": "f11112:m3"}
{"signature": "def insert(self, index, object):", "body": "self._check(object.id)<EOL>list.insert(self, index, object)<EOL>_dict = self._dict<EOL>for i, j in iteritems(_dict):<EOL><INDENT>if j >= index:<EOL><INDENT>_dict[i] = j + <NUM_LIT:1><EOL><DEDENT><DEDENT>_dict[object.id] = index<EOL>", "docstring": "insert object before index", "id": "f15894:c0:m23"}
{"signature": "def get_attribute(element, attribute, default=<NUM_LIT:0>):", "body": "a = element.getAttribute(attribute)<EOL>if a == \"<STR_LIT>\": <EOL><INDENT>return default<EOL><DEDENT>return a<EOL>", "docstring": "Returns XML element's attribute, or default if none.", "id": "f11566:m2"}
{"signature": "@contextmanager<EOL><INDENT>def _named_stream(self, name, binary=False):<DEDENT>", "body": "with self._store.save_stream(self._named_key(name), binary=binary) as s:<EOL><INDENT>yield s<EOL><DEDENT>", "docstring": "Create an indexed output stream i.e. 'test_00000001.name'\n\n:param name: Identifier for the stream\n:return: A context-managed stream-like object", "id": "f16991:c6:m7"}
{"signature": "def docker_vm_is_running():", "body": "running_vms = check_output_demoted(['<STR_LIT>', '<STR_LIT:list>', '<STR_LIT>'])<EOL>for line in running_vms.splitlines():<EOL><INDENT>if '<STR_LIT>'.format(constants.VM_MACHINE_NAME) in line:<EOL><INDENT>return True<EOL><DEDENT><DEDENT>return False<EOL>", "docstring": "Using VBoxManage is 0.5 seconds or so faster than Machine.", "id": "f3150:m18"}
{"signature": "def versions_from_parentdir(parentdir_prefix, root, verbose):", "body": "rootdirs = []<EOL>for i in range(<NUM_LIT:3>):<EOL><INDENT>dirname = os.path.basename(root)<EOL>if dirname.startswith(parentdir_prefix):<EOL><INDENT>return {\"<STR_LIT:version>\": dirname[len(parentdir_prefix):],<EOL>\"<STR_LIT>\": None,<EOL>\"<STR_LIT>\": False, \"<STR_LIT:error>\": None, \"<STR_LIT:date>\": None}<EOL><DEDENT>else:<EOL><INDENT>rootdirs.append(root)<EOL>root = os.path.dirname(root)  <EOL><DEDENT><DEDENT>if verbose:<EOL><INDENT>print(\"<STR_LIT>\" %<EOL>(str(rootdirs), parentdir_prefix))<EOL><DEDENT>raise NotThisMethod(\"<STR_LIT>\")<EOL>", "docstring": "Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory", "id": "f12746:m4"}
{"signature": "def show(mainloop=True):", "body": "for manager in Gcf.get_all_fig_managers():<EOL><INDENT>manager.window.show()<EOL><DEDENT>if mainloop and gtk.main_level() == <NUM_LIT:0> andlen(Gcf.get_all_fig_managers())><NUM_LIT:0>:<EOL><INDENT>gtk.main()<EOL><DEDENT>", "docstring": "Show all the figures and enter the gtk main loop\nThis should be the last line of your script", "id": "f17212:m3"}
{"signature": "def __float__(self):", "body": "return self._value<EOL>", "docstring": "Get the current estimate", "id": "f1375:c1:m2"}
{"signature": "def _translate_residue(self, selection, default_atomname='<STR_LIT>'):", "body": "m = self.RESIDUE.match(selection)<EOL>if not m:<EOL><INDENT>errmsg = \"<STR_LIT>\".format(**vars())<EOL>logger.error(errmsg)<EOL>raise ValueError(errmsg)<EOL><DEDENT>gmx_resid = self.gmx_resid(int(m.group('<STR_LIT>')))    <EOL>residue = m.group('<STR_LIT>')<EOL>if len(residue) == <NUM_LIT:1>:<EOL><INDENT>gmx_resname = utilities.convert_aa_code(residue) <EOL><DEDENT>else:<EOL><INDENT>gmx_resname = residue                            <EOL><DEDENT>gmx_atomname = m.group('<STR_LIT>')<EOL>if gmx_atomname is None:<EOL><INDENT>gmx_atomname = default_atomname<EOL><DEDENT>return {'<STR_LIT>':gmx_resname, '<STR_LIT>':gmx_resid, '<STR_LIT>':gmx_atomname}<EOL>", "docstring": "Translate selection for a single res to make_ndx syntax.", "id": "f6863:c1:m10"}
{"signature": "def execute(helper, config, args):", "body": "env_config = parse_env_config(config, args.environment)<EOL>environments_to_wait_for_term = []<EOL>environments = helper.get_environments()<EOL>for env in environments:<EOL><INDENT>if env['<STR_LIT>'] == args.environment:<EOL><INDENT>if env['<STR_LIT>'] != '<STR_LIT>':<EOL><INDENT>out(\"<STR_LIT>\" + env['<STR_LIT>']<EOL>+ \"<STR_LIT>\"<EOL>+ env['<STR_LIT>'] + \"<STR_LIT:)>\")<EOL><DEDENT>else:<EOL><INDENT>out(\"<STR_LIT>\"+env['<STR_LIT>'])<EOL>helper.delete_environment(env['<STR_LIT>'])<EOL>environments_to_wait_for_term.append(env['<STR_LIT>'])<EOL><DEDENT><DEDENT><DEDENT>if not args.dont_wait:<EOL><INDENT>helper.wait_for_environments(environments_to_wait_for_term,<EOL>status='<STR_LIT>',<EOL>include_deleted=True)<EOL><DEDENT>out(\"<STR_LIT>\")<EOL>return <NUM_LIT:0><EOL>", "docstring": "Deletes an environment", "id": "f15097:m1"}
{"signature": "def query_yes_no(question, default='<STR_LIT:yes>'):", "body": "valid = {'<STR_LIT:yes>': True, '<STR_LIT:y>': True, '<STR_LIT>': True,<EOL>'<STR_LIT>': False, '<STR_LIT:n>': False}<EOL>if default is None:<EOL><INDENT>prompt = '<STR_LIT>'<EOL><DEDENT>elif default == '<STR_LIT:yes>':<EOL><INDENT>prompt = '<STR_LIT>'<EOL><DEDENT>elif default == '<STR_LIT>':<EOL><INDENT>prompt = '<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>raise ValueError('<STR_LIT>' % default)<EOL><DEDENT>while True:<EOL><INDENT>sys.stderr.write(question + prompt)<EOL>choice = raw_input().strip().lower()<EOL>if default is not None and choice == '<STR_LIT>':<EOL><INDENT>return valid[default]<EOL><DEDENT>elif choice in valid:<EOL><INDENT>return valid[choice]<EOL><DEDENT>else:<EOL><INDENT>sys.stderr.write('<STR_LIT>')<EOL><DEDENT><DEDENT>", "docstring": "Ask a yes/no question via raw_input() and return their answer.\n\n    'question' is a string that is presented to the user.\n    'default' is the presumed answer if the user just hits <Enter>.\n        It must be 'yes' (the default), 'no' or None (meaning\n        an answer is required of the user).\n\n    The 'answer' return value is one of 'yes' or 'no'.", "id": "f5356:m0"}
{"signature": "@cache.command()<EOL>@click.option('<STR_LIT>', '<STR_LIT>', multiple=True, help='<STR_LIT>')<EOL>def clear(skip):", "body": "for name in sorted(MODULES):<EOL><INDENT>if name in skip:<EOL><INDENT>continue<EOL><DEDENT>click.secho(f'<STR_LIT>', fg='<STR_LIT>', bold=True)<EOL>clear_cache(name)<EOL><DEDENT>", "docstring": "Clear all caches.", "id": "f1584:m5"}
{"signature": "def get_path_collection_extents(*args):", "body": "from transforms import Bbox<EOL>if len(args[<NUM_LIT:1>]) == <NUM_LIT:0>:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>return Bbox.from_extents(*_get_path_collection_extents(*args))<EOL>", "docstring": "Given a sequence of :class:`Path` objects, returns the bounding\nbox that encapsulates all of them.", "id": "f17169:m0"}
{"signature": "def path(self, path):", "body": "self._path(path)<EOL>", "docstring": "Saves the dump in a file named :paramref:`path`.\n\n        :param str path: a valid path to a file location. The file can exist.", "id": "f564:c0:m11"}
{"signature": "def render_git_describe_long(pieces):", "body": "if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered = pieces[\"<STR_LIT>\"]<EOL>rendered += \"<STR_LIT>\" % (pieces[\"<STR_LIT>\"], pieces[\"<STR_LIT>\"])<EOL><DEDENT>else:<EOL><INDENT>rendered = pieces[\"<STR_LIT>\"]<EOL><DEDENT>if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered += \"<STR_LIT>\"<EOL><DEDENT>return rendered<EOL>", "docstring": "TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)", "id": "f10103:m17"}
{"signature": "def __str__(self):", "body": "return \"<STR_LIT>\".format(<EOL>customer=self.enterprise_customer,<EOL>id=self.entitlement_id<EOL>)<EOL>", "docstring": "Return human-readable string representation.", "id": "f16090:c9:m0"}
{"signature": "def add_titles(parser):", "body": "parser._positionals.title = \"<STR_LIT>\"<EOL>parser._optionals.title = \"<STR_LIT>\"<EOL>return parser<EOL>", "docstring": "add titles", "id": "f7394:m1"}
{"signature": "@cli.command()<EOL>def list():", "body": "click.echo('<STR_LIT>')<EOL>", "docstring": "List all posts", "id": "f12547:m58"}
{"signature": "@blueprint.route('<STR_LIT>',<EOL>methods=['<STR_LIT:POST>'])<EOL>@login_required<EOL>def approve(group_id, user_id):", "body": "membership = Membership.query.get_or_404((user_id, group_id))<EOL>group = membership.group<EOL>if group.can_edit(current_user):<EOL><INDENT>try:<EOL><INDENT>membership.accept()<EOL><DEDENT>except Exception as e:<EOL><INDENT>flash(str(e), '<STR_LIT:error>')<EOL>return redirect(url_for('<STR_LIT>', group_id=membership.group.id))<EOL><DEDENT>flash(_('<STR_LIT>',<EOL>user=membership.user.email,<EOL>name=membership.group.name), '<STR_LIT:success>')<EOL>return redirect(url_for('<STR_LIT>', group_id=membership.group.id))<EOL><DEDENT>flash(<EOL>_(<EOL>'<STR_LIT>',<EOL>group_name=group.name<EOL>),<EOL>'<STR_LIT:error>'<EOL>)<EOL>return redirect(url_for('<STR_LIT>'))<EOL>", "docstring": "Approve a user.", "id": "f7809:m9"}
{"signature": "def mostLikely(self, pred):", "body": "if len(pred) == <NUM_LIT:1>:<EOL><INDENT>return list(pred.keys())[<NUM_LIT:0>]<EOL><DEDENT>mostLikelyOutcome = None<EOL>maxProbability = <NUM_LIT:0><EOL>for prediction, probability in list(pred.items()):<EOL><INDENT>if probability > maxProbability:<EOL><INDENT>mostLikelyOutcome = prediction<EOL>maxProbability = probability<EOL><DEDENT><DEDENT>return mostLikelyOutcome<EOL>", "docstring": "Helper function to return a scalar value representing the most\n            likely outcome given a probability distribution", "id": "f17676:c12:m7"}
{"signature": "async def build_user_conversation_list(client):", "body": "conv_states, sync_timestamp = await _sync_all_conversations(client)<EOL>required_user_ids = set()<EOL>for conv_state in conv_states:<EOL><INDENT>required_user_ids |= {<EOL>user.UserID(chat_id=part.id.chat_id, gaia_id=part.id.gaia_id)<EOL>for part in conv_state.conversation.participant_data<EOL>}<EOL><DEDENT>required_entities = []<EOL>if required_user_ids:<EOL><INDENT>logger.debug('<STR_LIT>'<EOL>.format(required_user_ids))<EOL>try:<EOL><INDENT>response = await client.get_entity_by_id(<EOL>hangouts_pb2.GetEntityByIdRequest(<EOL>request_header=client.get_request_header(),<EOL>batch_lookup_spec=[<EOL>hangouts_pb2.EntityLookupSpec(<EOL>gaia_id=user_id.gaia_id,<EOL>create_offnetwork_gaia=True,<EOL>)<EOL>for user_id in required_user_ids<EOL>],<EOL>)<EOL>)<EOL>for entity_result in response.entity_result:<EOL><INDENT>required_entities.extend(entity_result.entity)<EOL><DEDENT><DEDENT>except exceptions.NetworkError as e:<EOL><INDENT>logger.warning('<STR_LIT>'.format(e))<EOL><DEDENT><DEDENT>conv_part_list = []<EOL>for conv_state in conv_states:<EOL><INDENT>conv_part_list.extend(conv_state.conversation.participant_data)<EOL><DEDENT>get_self_info_response = await client.get_self_info(<EOL>hangouts_pb2.GetSelfInfoRequest(<EOL>request_header=client.get_request_header(),<EOL>)<EOL>)<EOL>self_entity = get_self_info_response.self_entity<EOL>user_list = user.UserList(client, self_entity, required_entities,<EOL>conv_part_list)<EOL>conversation_list = ConversationList(client, conv_states,<EOL>user_list, sync_timestamp)<EOL>return (user_list, conversation_list)<EOL>", "docstring": "Build :class:`.UserList` and :class:`.ConversationList`.\n\n    This method requests data necessary to build the list of conversations and\n    users. Users that are not in the contact list but are participating in a\n    conversation will also be retrieved.\n\n    Args:\n        client (Client): Connected client.\n\n    Returns:\n        (:class:`.UserList`, :class:`.ConversationList`):\n            Tuple of built objects.", "id": "f10020:m0"}
{"signature": "def flatten_fft(scale=<NUM_LIT:1.0>):", "body": "_len = len(audio.spectrogram)<EOL>for i, v in enumerate(audio.spectrogram):<EOL><INDENT>yield scale * (i * v) / _len<EOL><DEDENT>", "docstring": "Produces a nicer graph, I'm not sure if this is correct", "id": "f11546:m1"}
{"signature": "@compiles(utcnow)<EOL>def _default_utcnow(element, compiler, **kw):", "body": "return \"<STR_LIT>\"<EOL>", "docstring": "default compilation handler.\n\n    Note that there is no SQL \"utcnow()\" function; this is a\n    \"fake\" string so that we can produce SQL strings that are dialect-agnostic,\n    such as within tests.", "id": "f3608:m0"}
{"signature": "def _restore_resources(resources):", "body": "resources = deepcopy(resources)<EOL>for resource in resources:<EOL><INDENT>schema = resource['<STR_LIT>']<EOL>for fk in schema.get('<STR_LIT>', []):<EOL><INDENT>_, name = _restore_path(fk['<STR_LIT>']['<STR_LIT>'])<EOL>fk['<STR_LIT>']['<STR_LIT>'] = name<EOL><DEDENT><DEDENT>return resources<EOL>", "docstring": "Restore schemas from being compatible with storage schemas.\n\n    Foreign keys related operations.\n\n    Args:\n        list: resources from storage\n\n    Returns:\n        list: restored resources", "id": "f7553:m5"}
{"signature": "def send_message(self,body):", "body": "m=Message(to_jid=self.room_jid.bare(),stanza_type=\"<STR_LIT>\",body=body)<EOL>self.manager.stream.send(m)<EOL>", "docstring": "Send a message to the room.\n\n:Parameters:\n    - `body`: the message body.\n:Types:\n    - `body`: `unicode`", "id": "f15256:c2:m5"}
{"signature": "def try_read_file(s):", "body": "try:<EOL><INDENT>with open(s, '<STR_LIT:r>') as f:<EOL><INDENT>data = f.read()<EOL><DEDENT><DEDENT>except FileNotFoundError:<EOL><INDENT>return s<EOL><DEDENT>except EnvironmentError as ex:<EOL><INDENT>print_err('<STR_LIT>'.format(s, ex))<EOL>return None<EOL><DEDENT>return data<EOL>", "docstring": "If `s` is a file name, read the file and return it's content.\n        Otherwise, return the original string.\n        Returns None if the file was opened, but errored during reading.", "id": "f9614:m5"}
{"signature": "def elements(self):", "body": "<EOL>return _chain.from_iterable(_starmap(_repeat, self.iteritems()))<EOL>", "docstring": "Iterator over elements repeating each as many times as its count.\n\n        >>> c = Counter('ABCABC')\n        >>> sorted(c.elements())\n        ['A', 'A', 'B', 'B', 'C', 'C']\n\n        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n        >>> product = 1\n        >>> for factor in prime_factors.elements():     # loop over factors\n        ...     product *= factor                       # and multiply them\n        >>> product\n        1836\n\n        Note, if an element's count has been set to zero or is a negative\n        number, elements() will ignore it.", "id": "f16398:c1:m3"}
{"signature": "def __enter__(self):", "body": "default_app.push(self)<EOL>return self<EOL>", "docstring": "Use this application as default for all module-level shortcuts.", "id": "f12971:c11:m28"}
{"signature": "def bdib(self, ticker, start_datetime, end_datetime, event_type, interval,<EOL>elms=None):", "body": "elms = [] if not elms else elms<EOL>logger = _get_logger(self.debug)<EOL>while(self._session.tryNextEvent()):<EOL><INDENT>pass<EOL><DEDENT>request = self.refDataService.createRequest('<STR_LIT>')<EOL>request.set('<STR_LIT>', ticker)<EOL>request.set('<STR_LIT>', event_type)<EOL>request.set('<STR_LIT>', interval)  <EOL>request.set('<STR_LIT>', start_datetime)<EOL>request.set('<STR_LIT>', end_datetime)<EOL>for name, val in elms:<EOL><INDENT>request.set(name, val)<EOL><DEDENT>logger.info('<STR_LIT>'.format(request))<EOL>self._session.sendRequest(request, identity=self._identity)<EOL>data = []<EOL>flds = ['<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>']<EOL>for msg in self._receive_events():<EOL><INDENT>d = msg['<STR_LIT>']['<STR_LIT>']<EOL>for bar in d['<STR_LIT>']['<STR_LIT>']:<EOL><INDENT>data.append(bar['<STR_LIT>'])<EOL><DEDENT><DEDENT>data = pd.DataFrame(data).set_index('<STR_LIT:time>').sort_index().loc[:, flds]<EOL>return data<EOL>", "docstring": "Get Open, High, Low, Close, Volume, and numEvents for a ticker.\nReturn pandas DataFrame\n\nParameters\n----------\nticker: string\n    String corresponding to ticker\nstart_datetime: string\n    UTC datetime in format YYYY-mm-ddTHH:MM:SS\nend_datetime: string\n    UTC datetime in format YYYY-mm-ddTHH:MM:SS\nevent_type: string {TRADE, BID, ASK, BID_BEST, ASK_BEST, BEST_BID,\n                   BEST_ASK}\n    Requested data event type\ninterval: int {1... 1440}\n    Length of time bars\nelms: list of tuples\n    List of tuples where each tuple corresponds to the other elements\n    to be set. Refer to the IntradayBarRequest section in the\n    'Services & schemas reference guide' for more info on these values", "id": "f5580:c0:m17"}
{"signature": "def trigger(self, *args):", "body": "self.task_spec._on_trigger(self, *args)<EOL>", "docstring": "If recursive is True, the state is applied to the tree recursively.", "id": "f7734:c0:m38"}
{"signature": "def do_rewind(self, line):", "body": "self.print_response(\"<STR_LIT>\" % self.bot._frame)<EOL>self.bot._frame = <NUM_LIT:0><EOL>", "docstring": "rewind", "id": "f11497:c0:m11"}
{"signature": "@property<EOL><INDENT>def LDOF(self):<DEDENT>", "body": "return self.ode_obj.getNumAxes()<EOL>", "docstring": "Number of linear degrees of freedom for this motor.", "id": "f14887:c9:m0"}
{"signature": "@click.group()<EOL>def main():", "body": "", "docstring": "Run EpiCom Reloaded.", "id": "f9419:m0"}
{"signature": "def find_elements(strategy, locator, root=None):", "body": "", "docstring": "Finds elements on the page.\n\n        :param strategy: Location strategy to use (type depends on the driver implementation)\n        :param locator: Location of target elements.\n        :param root: (optional) root node.\n        :type strategy: str\n        :type locator: str\n        :type root: web element object or None.\n        :return: iterable of web element objects\n        :rtype: iterable (if depends on the driver implementation)", "id": "f11290:c1:m3"}
{"signature": "def get_room_jid(self,nick=None):", "body": "if nick is None:<EOL><INDENT>return self.room_jid<EOL><DEDENT>return JID(self.room_jid.node,self.room_jid.domain,nick)<EOL>", "docstring": "Get own room JID or a room JID for given `nick`.\n\n:Parameters:\n    - `nick`: a nick for which the room JID is requested.\n:Types:\n    - `nick`: `unicode`\n\n:return: the room JID.\n:returntype: `JID`", "id": "f15256:c2:m8"}
{"signature": "def get(self, **queryparams):", "body": "return self._mc_client._get(url=self._build_path(), **queryparams)<EOL>", "docstring": "Search all campaigns for the specified query terms.\n\n:param queryparams: The query string parameters\nqueryparams['fields'] = array\nqueryparams['exclude_fields'] = array\nqueryparams['query'] = string\nqueryparams['snip_start'] = string\nqueryparams['snip_end'] = string\nqueryparams['offset'] = integer", "id": "f252:c0:m1"}
{"signature": "def WalkControl(control: Control, includeTop: bool = False, maxDepth: int = <NUM_LIT>):", "body": "if includeTop:<EOL><INDENT>yield control, <NUM_LIT:0><EOL><DEDENT>if maxDepth <= <NUM_LIT:0>:<EOL><INDENT>return<EOL><DEDENT>depth = <NUM_LIT:0><EOL>child = control.GetFirstChildControl()<EOL>controlList = [child]<EOL>while depth >= <NUM_LIT:0>:<EOL><INDENT>lastControl = controlList[-<NUM_LIT:1>]<EOL>if lastControl:<EOL><INDENT>yield lastControl, depth + <NUM_LIT:1><EOL>child = lastControl.GetNextSiblingControl()<EOL>controlList[depth] = child<EOL>if depth + <NUM_LIT:1> < maxDepth:<EOL><INDENT>child = lastControl.GetFirstChildControl()<EOL>if child:<EOL><INDENT>depth += <NUM_LIT:1><EOL>controlList.append(child)<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>del controlList[depth]<EOL>depth -= <NUM_LIT:1><EOL><DEDENT><DEDENT>", "docstring": "control: `Control` or its subclass.\nincludeTop: bool, if True, yield (control, 0) first.\nmaxDepth: int, enum depth.\nYield 2 items tuple(control: Control, depth: int).", "id": "f1782:m81"}
{"signature": "def cross_entropy_seq(logits, target_seqs, batch_size=None):  ", "body": "sequence_loss_by_example_fn = tf.contrib.legacy_seq2seq.sequence_loss_by_example<EOL>loss = sequence_loss_by_example_fn(<EOL>[logits], [tf.reshape(target_seqs, [-<NUM_LIT:1>])], [tf.ones_like(tf.reshape(target_seqs, [-<NUM_LIT:1>]), dtype=tf.float32)]<EOL>)<EOL>cost = tf.reduce_sum(loss)  <EOL>if batch_size is not None:<EOL><INDENT>cost = cost / batch_size<EOL><DEDENT>return cost<EOL>", "docstring": "Returns the expression of cross-entropy of two sequences, implement\n    softmax internally. Normally be used for fixed length RNN outputs, see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.\n\n    Parameters\n    ----------\n    logits : Tensor\n        2D tensor with shape of `[batch_size * n_steps, n_classes]`.\n    target_seqs : Tensor\n        The target sequence, 2D tensor `[batch_size, n_steps]`, if the number of step is dynamic, please use ``tl.cost.cross_entropy_seq_with_mask`` instead.\n    batch_size : None or int.\n        Whether to divide the cost by batch size.\n            - If integer, the return cost will be divided by `batch_size`.\n            - If None (default), the return cost will not be divided by anything.\n\n    Examples\n    --------\n    >>> see `PTB example <https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py>`__.for more details\n    >>> input_data = tf.placeholder(tf.int32, [batch_size, n_steps])\n    >>> targets = tf.placeholder(tf.int32, [batch_size, n_steps])\n    >>> # build the network\n    >>> print(net.outputs)\n    (batch_size * n_steps, n_classes)\n    >>> cost = tl.cost.cross_entropy_seq(network.outputs, targets)", "id": "f11218:m9"}
{"signature": "def get_metadata(self):", "body": "return self.metadata<EOL>", "docstring": "Get some metadata on the provider or the vocab it represents.\n\n        :rtype: Dict.", "id": "f13799:c0:m6"}
{"signature": "def tearDown(self):", "body": "HTTPretty.disable()<EOL>", "docstring": "Tear stuff down", "id": "f5624:c0:m38"}
{"signature": "def perm(lst, func):", "body": "for i in range(<NUM_LIT:1>, <NUM_LIT:2>**len(lst)):<EOL><INDENT>yield [func(item) if (<NUM_LIT:1><<j)&i else item for (j, item) in enumerate(lst)]<EOL><DEDENT>", "docstring": "Produce permutations of `lst`, where permutations are mutated by `func`. Used for flipping constraints. highly\n    possible that returned constraints can be unsat this does it blindly, without any attention to the constraints\n    themselves\n\n    Considering lst as a list of constraints, e.g.\n\n      [ C1, C2, C3 ]\n\n    we'd like to consider scenarios of all possible permutations of flipped constraints, excluding the original list.\n    So we'd like to generate:\n\n      [ func(C1),      C2 ,       C3 ],\n      [      C1 , func(C2),       C3 ],\n      [ func(C1), func(C2),       C3 ],\n      [      C1 ,      C2 ,  func(C3)],\n      .. etc\n\n    This is effectively treating the list of constraints as a bitmask of width len(lst) and counting up, skipping the\n    0th element (unmodified array).\n\n    The code below yields lists of constraints permuted as above by treating list indeces as bitmasks from 1 to\n     2**len(lst) and applying func to all the set bit offsets.", "id": "f17135:m4"}
{"signature": "def _coeff4(N, a0, a1, a2, a3):", "body": "if N == <NUM_LIT:1>:<EOL><INDENT>return ones(<NUM_LIT:1>)<EOL><DEDENT>n = arange(<NUM_LIT:0>, N)<EOL>N1 = N - <NUM_LIT:1.><EOL>w = a0 -a1*cos(<NUM_LIT>*pi*n / N1) + a2*cos(<NUM_LIT>*pi*n / N1) - a3*cos(<NUM_LIT>*pi*n / N1)<EOL>return w<EOL>", "docstring": "a common internal function to some window functions with 4 coeffs\n\n\n    For the blackmna harris for instance, the results are identical to octave if N is odd\n    but not for even values...if n =0 whatever N is, the w(0) must be equal to a0-a1+a2-a3, which\n    is the case here, but not in octave...", "id": "f10924:m15"}
{"signature": "def __init__(self):", "body": "<EOL>self.done = False<EOL>self.flow_level = <NUM_LIT:0><EOL>self.tokens = []<EOL>self.fetch_stream_start()<EOL>self.tokens_taken = <NUM_LIT:0><EOL>self.indent = -<NUM_LIT:1><EOL>self.indents = []<EOL>self.allow_simple_key = True<EOL>self.possible_simple_keys = {}<EOL>", "docstring": "Initialize the scanner.", "id": "f8327:c2:m0"}
{"signature": "def set_label_props(self, label, text, color):", "body": "label.set_text(text)<EOL>label.set_color(color)<EOL>label.set_fontproperties(self.labelFontProps)<EOL>label.set_clip_box(self.ax.bbox)<EOL>", "docstring": "set the label properties - color, fontsize, text", "id": "f17192:c0:m6"}
{"signature": "def is_dec(ip):", "body": "try:<EOL><INDENT>dec = int(str(ip))<EOL><DEDENT>except ValueError:<EOL><INDENT>return False<EOL><DEDENT>if dec > <NUM_LIT> or dec < <NUM_LIT:0>:<EOL><INDENT>return False<EOL><DEDENT>return True<EOL>", "docstring": "Return true if the IP address is in decimal notation.", "id": "f3601:m6"}
{"signature": "def nn_interpolator(self, z, default_value=np.nan):", "body": "z = np.asarray(z, dtype=np.float64)<EOL>if z.shape != self.old_shape:<EOL><INDENT>raise ValueError(\"<STR_LIT>\")<EOL><DEDENT>if self.j_unique is not None:<EOL><INDENT>z = z[self.j_unique]<EOL><DEDENT>return NNInterpolator(self, z, default_value)<EOL>", "docstring": "Get an object which can interpolate within the convex hull by\n        the natural neighbors method.\n\n        z -- an array of floats giving the known function values at each point\n          in the triangulation.", "id": "f17202:c1:m4"}
{"signature": "def get(self):", "body": "pass<EOL>", "docstring": "Return the `natural` part of the model. In the case of most elements it\nis the calculated field, for others it is the object itself.", "id": "f5758:c2:m7"}
{"signature": "def __init__(self, pos, shape=None, param_prefix='<STR_LIT>', category='<STR_LIT>',<EOL>support_pad=<NUM_LIT:4>, float_precision=np.float64):", "body": "if pos.ndim != <NUM_LIT:2>:<EOL><INDENT>raise ValueError('<STR_LIT>')<EOL><DEDENT>self.category = category<EOL>self.support_pad = support_pad<EOL>self.pos = pos.astype('<STR_LIT:float>')<EOL>self.param_prefix = param_prefix<EOL>if float_precision not in (np.float64, np.float32, np.float16):<EOL><INDENT>raise ValueError('<STR_LIT>' +<EOL>'<STR_LIT>')<EOL><DEDENT>self.float_precision = float_precision<EOL>self.shape = shape<EOL>self.setup_variables()<EOL>if self.shape:<EOL><INDENT>self.inner = self.shape.copy()<EOL>self.tile = self.inner.copy()<EOL>self.initialize()<EOL><DEDENT>", "docstring": "Parent class for a large collection of particles, such as spheres or\npoints or ellipsoids or rods.\n\nThis class is good for a collection of objects which each have a\nposition as well as (possibly) some other parameters, like particle\nradius, aspect ratio, or brightness. Its .get() method returns a\nfield of the drawn particles, selected on the current tile. Any\ndaughter classes need the following methods:\n\n* _draw_particle\n* _update_type\n* setup_variables\n* get_values\n* set_values\n* add_particle\n* remove_particle\n\nIn addition, the following methods should be modified for particles\nwith more parameters than just positions:\n\n* _drawargs\n* _tile\n* param_particle\n* exports\n* _p2i\n\nIf you have a few objects to group, like 2 or 3 slabs, group them\nwith a `peri.comp.ComponentCollection` instead.\n\nParameters\n----------\npos : ndarray [N,d]\n    Initial positions of the particles. Re-cast as float internally\n\nshape : ``peri.util.Tile``, optional\n    Shape of the field over which to draw the platonic spheres.\n    Default is None.\n\nparam_prefix : string, optional\n    Prefix for the particle parameter names. Default is `'sph'`\n\ncategory : string, optional\n    Category, as in comp.Component. Default is `'obj'`.\n\nsupport_pad : Int, optional\n    How much to pad the boundary of particles when calculating the\n    support so that particles do not leak out the edges. Default is 4\n\nfloat_precision : numpy float datatype, optional\n    One of numpy.float16, numpy.float32, numpy.float64; precision\n    for precomputed arrays. Default is np.float64; make it 16 or 32\n    to save memory.", "id": "f5759:c0:m0"}
{"signature": "def write(args):", "body": "logging.info(\"<STR_LIT>\"%args.config_file)<EOL>if args.config_file is None:<EOL><INDENT>return<EOL><DEDENT>config = cparser.ConfigParser()<EOL>config.add_section(\"<STR_LIT>\")<EOL>for p in [x for x in dir(args) if not x.startswith(\"<STR_LIT:_>\")]:<EOL><INDENT>if p in IGNORE_ARGS:<EOL><INDENT>continue<EOL><DEDENT>value = getattr(args, p)<EOL>if value is not None:<EOL><INDENT>config.set('<STR_LIT>', p, str(value))<EOL><DEDENT><DEDENT>with open(args.config_file, '<STR_LIT:w>') as f:<EOL><INDENT>config.write(f)<EOL><DEDENT>", "docstring": "Writing the configure file with the attributes in 'args", "id": "f10619:m1"}
{"signature": "def transform(self, X):", "body": "X = check_array(X, copy=self.copy)<EOL>X *= self.scale_<EOL>X += self.min_<EOL>if self.truncate:<EOL><INDENT>np.maximum(self.feature_range[<NUM_LIT:0>], X, out=X)<EOL>np.minimum(self.feature_range[<NUM_LIT:1>], X, out=X)<EOL><DEDENT>return X<EOL>", "docstring": "Scaling features of X according to feature_range.\n\n        Parameters\n        ----------\n        X : array-like with shape [n_samples, n_features]\n            Input data that will be transformed.", "id": "f14614:c2:m2"}
{"signature": "def get(self, store_id, promo_rule_id, **queryparams):", "body": "self.store_id = store_id<EOL>self.promo_rule_id = promo_rule_id<EOL>return self._mc_client._get(url=self._build_path(store_id, '<STR_LIT>', promo_rule_id), **queryparams)<EOL>", "docstring": "Get information about a specific promo rule.\n\n:param store_id: The store's id\n:type store_id: `string`\n:param queryparams: The query string parameters\nqueryparams['fields'] = []\nqueryparams['exclude_fields'] = []\nqueryparams['count'] = integer\nqueryparams['offset'] = integer", "id": "f307:c0:m3"}
{"signature": "def build_spia_matrices(nodes: Set[str]) -> Dict[str, pd.DataFrame]:", "body": "nodes = list(sorted(nodes))<EOL>matrices = OrderedDict()<EOL>for relation in KEGG_RELATIONS:<EOL><INDENT>matrices[relation] = pd.DataFrame(<NUM_LIT:0>, index=nodes, columns=nodes)<EOL><DEDENT>return matrices<EOL>", "docstring": "Build an adjacency matrix for each KEGG relationship and return in a dictionary.\n\n    :param nodes: A set of HGNC gene symbols\n    :return: Dictionary of adjacency matrix for each relationship", "id": "f9417:m2"}
{"signature": "@classmethod<EOL><INDENT>def from_mask(cls, mask):<DEDENT>", "body": "array = grid_util.regular_grid_1d_masked_from_mask_pixel_scales_and_origin(mask=mask,<EOL>pixel_scales=mask.pixel_scales)<EOL>return cls(array, mask)<EOL>", "docstring": "Setup a regular-grid from a mask, wehere the center of every unmasked pixel gives the grid's (y,x) \\\n        arc-second coordinates.\n\n        Parameters\n        -----------\n        mask : Mask\n            The mask whose unmasked pixels are used to setup the regular-pixel grid.", "id": "f5990:c1:m6"}
{"signature": "def print_rec(rec, format=<NUM_LIT:1>, tags=None):", "body": "if tags is None:<EOL><INDENT>tags = []<EOL><DEDENT>if format == <NUM_LIT:1>:<EOL><INDENT>text = record_xml_output(rec, tags)<EOL><DEDENT>else:<EOL><INDENT>return '<STR_LIT>'<EOL><DEDENT>return text<EOL>", "docstring": "Print a record.\n\n:param format: 1 XML, 2 HTML (not implemented)\n:param tags: list of tags to be printed", "id": "f7891:m32"}
{"signature": "def _get_default_delivery_medium(self):", "body": "medium_options = (<EOL>self._conversation.self_conversation_state.delivery_medium_option<EOL>)<EOL>try:<EOL><INDENT>default_medium = medium_options[<NUM_LIT:0>].delivery_medium<EOL><DEDENT>except IndexError:<EOL><INDENT>logger.warning('<STR_LIT>', self.id_)<EOL>default_medium = hangouts_pb2.DeliveryMedium(<EOL>medium_type=hangouts_pb2.DELIVERY_MEDIUM_BABEL<EOL>)<EOL><DEDENT>for medium_option in medium_options:<EOL><INDENT>if medium_option.current_default:<EOL><INDENT>default_medium = medium_option.delivery_medium<EOL><DEDENT><DEDENT>return default_medium<EOL>", "docstring": "Return default DeliveryMedium to use for sending messages.\n\n        Use the first option, or an option that's marked as the current\n        default.", "id": "f10020:c0:m17"}
{"signature": "def setIdentityPolicyInstance(self, identityPolicyObj):", "body": "assert not self.identityPolicy<EOL>assert isinstance(identityPolicyObj, RegionIdentityPolicyBase)<EOL>self.identityPolicy = identityPolicyObj<EOL>return<EOL>", "docstring": "TestRegion command that sets identity policy instance.  The instance\n        MUST be derived from TestRegion's RegionIdentityPolicyBase class.\n\n        Users MUST set the identity instance BEFORE running the network\n\n        Exception: AssertionError if identity policy instance has already been set\n                   or if the passed-in instance is not derived from\n                   RegionIdentityPolicyBase.", "id": "f17625:c1:m10"}
{"signature": "def dictVals(l,key):", "body": "dicts=dictFlat(l)<EOL>vals=np.empty(len(dicts))*np.nan<EOL>for i in range(len(dicts)):<EOL><INDENT>if key in dicts[i]:<EOL><INDENT>vals[i]=dicts[i][key]<EOL><DEDENT><DEDENT>return vals<EOL>", "docstring": "Return all 'key' from a list of dicts. (or list of list of dicts)", "id": "f11406:m4"}
{"signature": "def _parse_actual_results(self, actual_results):", "body": "actual_nodes = dict()<EOL>actual_links = dict()<EOL>for bolt in actual_results.topology.bolts:<EOL><INDENT>name = bolt.comp.name<EOL>if name not in actual_links:<EOL><INDENT>actual_links[name] = set()<EOL><DEDENT>for input in bolt.inputs:<EOL><INDENT>actual_links[name].add(input.stream.component_name)<EOL><DEDENT><DEDENT>for instance in actual_results.instances:<EOL><INDENT>name = instance.info.component_name<EOL>if name not in actual_nodes:<EOL><INDENT>actual_nodes[name] = <NUM_LIT:0><EOL><DEDENT>else:<EOL><INDENT>actual_nodes[name] += <NUM_LIT:1><EOL><DEDENT><DEDENT>return actual_nodes, actual_links<EOL>", "docstring": "Parse protobuf messege and generate actual_nodes and actual_links", "id": "f7273:c0:m4"}
{"signature": "def load_key(pubkey):", "body": "try:<EOL><INDENT>return load_pem_public_key(pubkey.encode(), default_backend())<EOL><DEDENT>except ValueError:<EOL><INDENT>pubkey = pubkey.replace('<STR_LIT>', '<STR_LIT>').replace('<STR_LIT>', '<STR_LIT>')<EOL>return load_pem_public_key(pubkey.encode(), default_backend())<EOL><DEDENT>", "docstring": "Load public RSA key.\n\n    Work around keys with incorrect header/footer format.\n\n    Read more about RSA encryption with cryptography:\n    https://cryptography.io/latest/hazmat/primitives/asymmetric/rsa/", "id": "f15410:m0"}
{"signature": "def isPortAvailable(port='<STR_LIT>'):", "body": "isPortAvailable = serial.tools.list_ports.grep(port)<EOL>try:<EOL><INDENT>next(isPortAvailable)<EOL>available = True<EOL><DEDENT>except StopIteration:<EOL><INDENT>available = False<EOL><DEDENT>return available<EOL>", "docstring": "Checks whether specified port is available.\n\nSource code derived from @lqdev suggestion per #38\n\nArgs:\n    port: Serial port location i.e. 'COM1'. Default is /dev/ttyUSB0\n\nReturns:\n    available: Boolean value indicating presence of port", "id": "f4902:c3:m1"}
{"signature": "def _run_command(self, *args, **kwargs):", "body": "<EOL>use_input = kwargs.pop('<STR_LIT>', True)<EOL>capturefile = None<EOL>if environment.flags['<STR_LIT>'] is True:<EOL><INDENT>kwargs.setdefault('<STR_LIT>', PIPE)<EOL>kwargs.setdefault('<STR_LIT>', PIPE)<EOL><DEDENT>elif environment.flags['<STR_LIT>'] == \"<STR_LIT:file>\":<EOL><INDENT>if '<STR_LIT>' in kwargs and '<STR_LIT>' in kwargs:<EOL><INDENT>pass<EOL><DEDENT>else:<EOL><INDENT>fn = environment.flags['<STR_LIT>']<EOL>capturefile = file(fn, \"<STR_LIT:w>\")   <EOL>if '<STR_LIT>' in kwargs and '<STR_LIT>' not in kwargs:<EOL><INDENT>kwargs.setdefault('<STR_LIT>', capturefile)<EOL><DEDENT>else:<EOL><INDENT>kwargs.setdefault('<STR_LIT>', STDOUT)<EOL>kwargs.setdefault('<STR_LIT>', capturefile)<EOL><DEDENT><DEDENT><DEDENT>try:<EOL><INDENT>p = self.Popen(*args, **kwargs)<EOL>out, err = p.communicate(use_input=use_input) <EOL><DEDENT>except:<EOL><INDENT>if capturefile is not None:<EOL><INDENT>logger.error(\"<STR_LIT>\", capturefile)<EOL><DEDENT>raise<EOL><DEDENT>finally:<EOL><INDENT>if capturefile is not None:<EOL><INDENT>capturefile.close()<EOL><DEDENT><DEDENT>rc = p.returncode<EOL>return (rc, out, err), p<EOL>", "docstring": "Execute the command; see the docs for __call__.\n\n        :Returns: a tuple of the *results* tuple ``(rc, stdout, stderr)`` and\n                  the :class:`Popen` instance.", "id": "f6864:c0:m3"}
{"signature": "def col (loc,strg):", "body": "return (loc<len(strg) and strg[loc] == '<STR_LIT:\\n>') and <NUM_LIT:1> or loc - strg.rfind(\"<STR_LIT:\\n>\", <NUM_LIT:0>, loc)<EOL>", "docstring": "Returns current column within a string, counting newlines as line separators.\n   The first column is number 1.\n\n   Note: the default parsing behavior is to expand tabs in the input string\n   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information\n   on parsing strings containing <TAB>s, and suggested methods to maintain a\n   consistent view of the parsed string, the parse location, and line and column\n   positions within the parsed string.", "id": "f17196:m1"}
{"signature": "def start_master_nodes(masters, cl_args):", "body": "pids = []<EOL>for master in masters:<EOL><INDENT>Log.info(\"<STR_LIT>\" % master)<EOL>cmd = \"<STR_LIT>\"% (get_nomad_path(cl_args), get_nomad_master_config_file(cl_args))<EOL>if not is_self(master):<EOL><INDENT>cmd = ssh_remote_execute(cmd, master, cl_args)<EOL><DEDENT>Log.debug(cmd)<EOL>pid = subprocess.Popen(cmd,<EOL>shell=True,<EOL>stdout=subprocess.PIPE,<EOL>stderr=subprocess.PIPE)<EOL>pids.append({\"<STR_LIT>\": pid, \"<STR_LIT>\": master})<EOL><DEDENT>errors = []<EOL>for entry in pids:<EOL><INDENT>pid = entry[\"<STR_LIT>\"]<EOL>return_code = pid.wait()<EOL>output = pid.communicate()<EOL>Log.debug(\"<STR_LIT>\" % (return_code, output))<EOL>if return_code != <NUM_LIT:0>:<EOL><INDENT>errors.append(\"<STR_LIT>\" % (entry[\"<STR_LIT>\"], output[<NUM_LIT:1>]))<EOL><DEDENT><DEDENT>if errors:<EOL><INDENT>for error in errors:<EOL><INDENT>Log.error(error)<EOL><DEDENT>sys.exit(-<NUM_LIT:1>)<EOL><DEDENT>Log.info(\"<STR_LIT>\")<EOL>", "docstring": "Start master nodes", "id": "f7362:m24"}
{"signature": "def _SkipFieldContents(tokenizer):", "body": "<EOL>if tokenizer.TryConsume('<STR_LIT::>') and not tokenizer.LookingAt(<EOL>'<STR_LIT:{>') and not tokenizer.LookingAt('<STR_LIT:<>'):<EOL><INDENT>_SkipFieldValue(tokenizer)<EOL><DEDENT>else:<EOL><INDENT>_SkipFieldMessage(tokenizer)<EOL><DEDENT>", "docstring": "Skips over contents (value or message) of a field.\n\n    Args:\n      tokenizer: A tokenizer to parse the field name and values.", "id": "f8657:m9"}
{"signature": "def remove_from_string(string, values):", "body": "for v in values:<EOL><INDENT>string = string.replace(v, '<STR_LIT>')<EOL><DEDENT>return string<EOL>", "docstring": "Parameters\n----------\nstring:\nvalues:\n\nReturns\n-------", "id": "f4077:m9"}
{"signature": "def format(self, o, context, maxlevels, level):", "body": "return _safe_repr(o, context, maxlevels, level)<EOL>", "docstring": "Format o for a specific context, returning a string\n        and flags indicating whether the representation is 'readable'\n        and whether the o represents a recursive construct.", "id": "f16399:c0:m7"}
{"signature": "def debug_print(lst, lvl=<NUM_LIT:0>):", "body": "pad = '<STR_LIT>'.join(['<STR_LIT>'] * lvl)<EOL>t = type(lst)<EOL>if t is list:<EOL><INDENT>for p in lst:<EOL><INDENT>debug_print(p, lvl)<EOL><DEDENT><DEDENT>elif hasattr(lst, '<STR_LIT>'):<EOL><INDENT>print(pad, t)<EOL>debug_print(list(flatten(lst.tokens)), lvl + <NUM_LIT:1>)<EOL><DEDENT>", "docstring": "Print scope tree\n    args:\n        lst (list): parse result\n        lvl (int): current nesting level", "id": "f12430:m5"}
{"signature": "def prepare_custom_grouping(self, context):", "body": "self.custom_grouper.prepare(context)<EOL>", "docstring": "Prepares for custom grouping for this component\n\n        :param context: Topology context", "id": "f7482:c0:m17"}
{"signature": "def load_ipython_extension(ip):", "body": "global _loaded<EOL>if not _loaded:<EOL><INDENT>_loaded = True<EOL>from lancet import launch<EOL>if sys.version_info[<NUM_LIT:0>] == <NUM_LIT:2>:<EOL><INDENT>launch.input = lambda *args, **kwargs: raw_input(*args, **kwargs)<EOL><DEDENT>plaintext_formatter = ip.display_formatter.formatters['<STR_LIT>']<EOL>plaintext_formatter.for_type(Args, repr_pretty_annotated)<EOL>plaintext_formatter.for_type(Command, repr_pretty_unannotated)<EOL>plaintext_formatter.for_type(Launcher, repr_pretty_unannotated)<EOL>plaintext_formatter.for_type(FileType, repr_pretty_unannotated)<EOL>plaintext_formatter.for_type(review_and_launch, repr_pretty_unannotated)<EOL><DEDENT>", "docstring": "IPython pretty printing support (optional). To load the extension\nyou may execute the following in IPython:\n\n%load_ext lancet", "id": "f12321:m2"}
{"signature": "@property<EOL><INDENT>def path(self):<DEDENT>", "body": "assert self.abs_path and self.abs_bundle_path, \"<STR_LIT>\"<EOL>return self.abs_bundle_path<EOL>", "docstring": "Check if absolute path is not resolved yet", "id": "f4954:c0:m1"}
{"signature": "def write(self, file):", "body": "if self.type not in self.dx_types:<EOL><INDENT>raise ValueError((\"<STR_LIT>\"<EOL>\"<STR_LIT>\"<EOL>\"<STR_LIT>\").format(<EOL>self.type, list(self.dx_types.keys())))<EOL><DEDENT>typelabel = (self.typequote+self.type+self.typequote)<EOL>DXclass.write(self,file,<EOL>'<STR_LIT>'.format(<EOL>typelabel, self.array.size))<EOL>fmt_string = \"<STR_LIT>\"<EOL>if (self.array.dtype.kind == '<STR_LIT:f>' or self.array.dtype.kind == '<STR_LIT:c>'):<EOL><INDENT>precision = numpy.finfo(self.array.dtype).precision<EOL>fmt_string = \"<STR_LIT>\"+\"<STR_LIT>\".format(precision)+\"<STR_LIT>\"<EOL><DEDENT>values_per_line = <NUM_LIT:3><EOL>values = self.array.flat<EOL>while <NUM_LIT:1>:<EOL><INDENT>try:<EOL><INDENT>for i in range(values_per_line):<EOL><INDENT>file.write(fmt_string.format(next(values)) + \"<STR_LIT:\\t>\")<EOL><DEDENT>file.write('<STR_LIT:\\n>')<EOL><DEDENT>except StopIteration:<EOL><INDENT>file.write('<STR_LIT:\\n>')<EOL>break<EOL><DEDENT><DEDENT>file.write('<STR_LIT>')<EOL>", "docstring": "Write the *class array* section.\n\n        Parameters\n        ----------\n        file : file\n\n        Raises\n        ------\n        ValueError\n             If the `dxtype` is not a valid type, :exc:`ValueError` is\n             raised.", "id": "f15445:c3:m1"}
{"signature": "def make_present_participles(verbs):", "body": "res = []<EOL>for verb in verbs:<EOL><INDENT>parts = verb.split()<EOL>if parts[<NUM_LIT:0>].endswith(\"<STR_LIT:e>\"):<EOL><INDENT>parts[<NUM_LIT:0>] = parts[<NUM_LIT:0>][:-<NUM_LIT:1>] + \"<STR_LIT>\"<EOL><DEDENT>else:<EOL><INDENT>parts[<NUM_LIT:0>] = parts[<NUM_LIT:0>] + \"<STR_LIT>\"<EOL><DEDENT>res.append(\"<STR_LIT:U+0020>\".join(parts))<EOL><DEDENT>return res<EOL>", "docstring": "Make the list of verbs into present participles\n\n    E.g.:\n\n        empower -> empowering\n        drive -> driving", "id": "f751:m1"}
{"signature": "def set_pixel(self, x, y, color):", "body": "self._set_pixel_and_convert_color(x, y, color)<EOL>", "docstring": "set the pixel at ``(x, y)`` position to :paramref:`color`\n\n        If ``(x, y)`` is out of the :ref:`bounds <png-builder-bounds>`\n        this does not change the image.\n\n        .. seealso:: :meth:`set_color_in_grid`", "id": "f550:c0:m7"}
{"signature": "def create(self, *args, **kwargs):", "body": "data = self.get_data('<STR_LIT>',<EOL>type=POST,<EOL>params={'<STR_LIT:name>': self.name,<EOL>'<STR_LIT>': self.region,<EOL>'<STR_LIT>': self.size_gigabytes,<EOL>'<STR_LIT:description>': self.description,<EOL>'<STR_LIT>': self.filesystem_type,<EOL>'<STR_LIT>': self.filesystem_label<EOL>})<EOL>if data:<EOL><INDENT>self.id = data['<STR_LIT>']['<STR_LIT:id>']<EOL>self.created_at = data['<STR_LIT>']['<STR_LIT>']<EOL><DEDENT>return self<EOL>", "docstring": "Creates a Block Storage volume\n\nNote: Every argument and parameter given to this method will be\nassigned to the object.\n\nArgs:\n    name: string - a name for the volume\n    region: string - slug identifier for the region\n    size_gigabytes: int - size of the Block Storage volume in GiB\n    filesystem_type: string, optional - name of the filesystem type the\n        volume will be formated with ('ext4' or 'xfs')\n    filesystem_label: string, optional - the label to be applied to the\n        filesystem, only used in conjunction with filesystem_type\n\nOptional Args:\n    description: string - text field to describe a volume", "id": "f1478:c0:m3"}
{"signature": "def _format_id(self, payload):", "body": "if '<STR_LIT:id>' in payload:<EOL><INDENT>return str(payload['<STR_LIT:id>'])<EOL><DEDENT>if '<STR_LIT>' in payload:<EOL><INDENT>return '<STR_LIT:U+0020>'.join([six.text_type(item['<STR_LIT:id>']) for item in payload['<STR_LIT>']])<EOL><DEDENT>raise MultipleRelatedError('<STR_LIT>')<EOL>", "docstring": "Echos only the id", "id": "f3341:c0:m6"}
{"signature": "def rename(self, name, wait=True):", "body": "return self._action('<STR_LIT>', name=name, wait=wait)<EOL>", "docstring": "Change the name of this droplet\n\nParameters\n----------\nname: str\n    New name for the droplet\nwait: bool, default True\n    Whether to block until the pending action is completed\n\nRaises\n------\nAPIError if region does not support private networking", "id": "f4446:c1:m18"}
{"signature": "def listen(func):", "body": "import argparse<EOL>import beanstalkc as bean<EOL>parser = argparse.ArgumentParser(description=\"<STR_LIT>\")<EOL>parser.add_argument('<STR_LIT>', '<STR_LIT>', type=int, help=\"<STR_LIT>\")<EOL>parser.add_argument('<STR_LIT>', '<STR_LIT>', type=int, help=\"<STR_LIT>\")<EOL>args = vars(parser.parse_args())<EOL>port = int(args.get('<STR_LIT:port>') or DEFAULT_BEANSTALKD)<EOL>proc = int(args.get('<STR_LIT>') or <NUM_LIT:1>)<EOL>def _listen(func, index):<EOL><INDENT>sys.stdout = open('<STR_LIT>'.format(index), \"<STR_LIT:w>\", buffering=<NUM_LIT:0>)<EOL>sys.stderr = open('<STR_LIT>'.format(index), \"<STR_LIT:w>\", buffering=<NUM_LIT:0>)<EOL>bsd = bean.Connection(host=LOCALHOST, port=port)<EOL>while True:<EOL><INDENT>job = bsd.reserve()<EOL>func(json.loads(job.body))<EOL><DEDENT><DEDENT>log.info('<STR_LIT>')<EOL>for i in range(proc):<EOL><INDENT>log.info('<STR_LIT:{}>'.format(i))<EOL>Process(target=_listen, args=(func,i)).start()<EOL><DEDENT>", "docstring": "This function is for use in external scripts in order to listen on the\nqueue and perform tasks. In particular, create a function which takes the\narguments you would pass to `launch_all`s jobs parameter, then at the\nbottom of the script add:\n\nif __name__ == '__main__':\n    peri.test.beanstalk.listen(my_function_name)", "id": "f5762:m3"}
{"signature": "def calculate(self, T, P, zs, ws, method):", "body": "if method == SIMPLE:<EOL><INDENT>ks = [i(T, P) for i in self.ThermalConductivityLiquids]<EOL>return mixing_simple(zs, ks)<EOL><DEDENT>elif method == DIPPR_9H:<EOL><INDENT>ks = [i(T, P) for i in self.ThermalConductivityLiquids]<EOL>return DIPPR9H(ws, ks)<EOL><DEDENT>elif method == FILIPPOV:<EOL><INDENT>ks = [i(T, P) for i in self.ThermalConductivityLiquids]<EOL>return Filippov(ws, ks)<EOL><DEDENT>elif method == MAGOMEDOV:<EOL><INDENT>k_w = self.ThermalConductivityLiquids[self.index_w](T, P)<EOL>ws = list(ws) ; ws.pop(self.index_w)<EOL>return thermal_conductivity_Magomedov(T, P, ws, self.wCASs, k_w)<EOL><DEDENT>else:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT>", "docstring": "r'''Method to calculate thermal conductivity of a liquid mixture at \n        temperature `T`, pressure `P`, mole fractions `zs` and weight fractions\n        `ws` with a given method.\n\n        This method has no exception handling; see `mixture_property`\n        for that.\n\n        Parameters\n        ----------\n        T : float\n            Temperature at which to calculate the property, [K]\n        P : float\n            Pressure at which to calculate the property, [Pa]\n        zs : list[float]\n            Mole fractions of all species in the mixture, [-]\n        ws : list[float]\n            Weight fractions of all species in the mixture, [-]\n        method : str\n            Name of the method to use\n\n        Returns\n        -------\n        k : float\n            Thermal conductivity of the liquid mixture, [W/m/K]", "id": "f15790:c1:m2"}
{"signature": "def li_regularizer(scale, scope=None):", "body": "if isinstance(scale, numbers.Integral):<EOL><INDENT>raise ValueError('<STR_LIT>' % scale)<EOL><DEDENT>if isinstance(scale, numbers.Real):<EOL><INDENT>if scale < <NUM_LIT:0.>:<EOL><INDENT>raise ValueError('<STR_LIT>' % scale)<EOL><DEDENT>if scale >= <NUM_LIT:1.>:<EOL><INDENT>raise ValueError('<STR_LIT>' % scale)<EOL><DEDENT>if scale == <NUM_LIT:0.>:<EOL><INDENT>tl.logging.info('<STR_LIT>')<EOL>return lambda _, name=None: None<EOL><DEDENT><DEDENT>def li(weights):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>with tf.name_scope('<STR_LIT>') as scope:<EOL><INDENT>my_scale = ops.convert_to_tensor(scale, dtype=weights.dtype.base_dtype, name='<STR_LIT>')<EOL>standard_ops_fn = standard_ops.multiply<EOL>return standard_ops_fn(<EOL>my_scale, standard_ops.reduce_sum(standard_ops.sqrt(standard_ops.reduce_sum(tf.square(weights), <NUM_LIT:1>))),<EOL>name=scope<EOL>)<EOL><DEDENT><DEDENT>return li<EOL>", "docstring": "Li regularization removes the neurons of previous layer. The `i` represents `inputs`.\n    Returns a function that can be used to apply group li regularization to weights.\n    The implementation follows `TensorFlow contrib <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/regularizers.py>`__.\n\n    Parameters\n    ----------\n    scale : float\n        A scalar multiplier `Tensor`. 0.0 disables the regularizer.\n    scope: str\n        An optional scope name for this function.\n\n    Returns\n    --------\n    A function with signature `li(weights, name=None)` that apply Li regularization.\n\n    Raises\n    ------\n    ValueError : if scale is outside of the range [0.0, 1.0] or if scale is not a float.", "id": "f11218:m12"}
{"signature": "def is_declared(self, expression_var):", "body": "if not isinstance(expression_var, Variable):<EOL><INDENT>raise ValueError(f'<STR_LIT>')<EOL><DEDENT>return any(expression_var is x for x in self.get_declared_variables())<EOL>", "docstring": "True if expression_var is declared in this constraint set", "id": "f16985:c0:m17"}
{"signature": "def enrolled_courses(self, enterprise_customer_user):", "body": "courses_string = mark_safe(self.get_enrolled_course_string(enterprise_customer_user))<EOL>return courses_string or '<STR_LIT:None>'<EOL>", "docstring": "Return a string representing the courses a given EnterpriseCustomerUser is enrolled in\n\nArgs:\n    enterprise_customer_user: The instance of EnterpriseCustomerUser\n        being rendered with this admin form.", "id": "f16064:c6:m1"}
{"signature": "def add_folder_download_callback(callback):", "body": "session.folder_download_callbacks.append(callback)<EOL>", "docstring": "Pass a function to be called when an folder has finished downloading,\nwhich happens after all of its items and recursive children folders\nhave downloaded.\nThis can be used for performing notifications of download progress or\ncalling additional API functions.\n\n:param callback: A function that takes four arguments. The first argument\n    is the communicator object of the current pydas context, the second is\n    the currently active API token, the third is the dict of folder info,\n    the fourth argument is the local download path of the folder.\n:type callback: (Communicator, string, dict, string) -> unknown", "id": "f8359:m5"}
{"signature": "def postprocess(self, obj, mapping, **kwargs):", "body": "pass<EOL>", "docstring": "Post-processing hook.  Called by map_to_dictionary()", "id": "f2914:c4:m16"}
{"signature": "def Chueh_Prausnitz_Vc(zs, Vcs, nus):", "body": "if not none_and_length_check([zs, Vcs]): <EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT>denominator = sum(zs[i]*Vcs[i]**(<NUM_LIT:2>/<NUM_LIT>) for i in range(len(zs)))<EOL>Vcm = <NUM_LIT:0><EOL>for i in range(len(zs)):<EOL><INDENT>Vcm += zs[i]*Vcs[i]**(<NUM_LIT:2>/<NUM_LIT>)*Vcs[i]/denominator<EOL>for j in range(len(zs)):<EOL><INDENT>Vcm += (zs[i]*Vcs[i]**(<NUM_LIT:2>/<NUM_LIT>)/denominator)*(zs[j]*Vcs[j]**(<NUM_LIT:2>/<NUM_LIT>)/denominator)*nus[i][j]/<NUM_LIT><EOL><DEDENT><DEDENT>return Vcm<EOL>", "docstring": "r'''Calculates critical volume of a mixture according to\n    mixing rules in [1]_ with an interaction parameter.\n\n    .. math::\n        V_{cm} = \\sum_i^n \\theta_i V_{ci} + \\sum_i^n\\sum_j^n(\\theta_i \\theta_j \\nu_{ij})V_{ref}\n        \\theta = \\frac{x_i V_{ci}^{2/3}}{\\sum_{j=1}^n x_j V_{cj}^{2/3}}\n\n    Parameters\n    ----------\n    zs : float\n        Mole fractions of all components\n    Vcs : float\n        Critical volumes of all components, [m^3/mol]\n    nus : matrix\n        Interaction parameters, [cm^3/mol]\n\n    Returns\n    -------\n    Vcm : float\n        Critical volume of the mixture, [m^3/mol]\n\n    Notes\n    -----\n    All parameters, even if zero, must be given to this function.\n    nu parameters are in cm^3/mol, but are converted to m^3/mol inside the function\n\n\n    Examples\n    --------\n    1-butanol/benzene 0.4271/0.5729 mixture, Vcm = 268.096 mL/mol.\n\n    >>> Chueh_Prausnitz_Vc([0.4271, 0.5729], [0.000273, 0.000256], [[0, 5.61847], [5.61847, 0]])\n    0.00026620503424517445\n\n    References\n    ----------\n    .. [1] Chueh, P. L., and J. M. Prausnitz. \"Vapor-Liquid Equilibria at High\n       Pressures: Calculation of Critical Temperatures, Volumes, and Pressures\n       of Nonpolar Mixtures.\" AIChE Journal 13, no. 6 (November 1, 1967):\n       1107-13. doi:10.1002/aic.690130613.\n    .. [2] Najafi, Hamidreza, Babak Maghbooli, and Mohammad Amin Sobati.\n       \"Prediction of True Critical Volume of Multi-Component Mixtures:\n       Extending Fast Estimation Methods.\" Fluid Phase Equilibria 386\n       (January 25, 2015): 13-29. doi:10.1016/j.fluid.2014.11.008.", "id": "f15798:m16"}
{"signature": "def signed_session(self, session=None):", "body": "if session:<EOL><INDENT>session = super(ClientCertAuthentication, self).signed_session(session)<EOL><DEDENT>else:<EOL><INDENT>session = super(ClientCertAuthentication, self).signed_session()<EOL><DEDENT>if self.cert is not None:<EOL><INDENT>session.cert = self.cert<EOL><DEDENT>if self.ca_cert is not None:<EOL><INDENT>session.verify = self.ca_cert<EOL><DEDENT>if self.no_verify:<EOL><INDENT>session.verify = False<EOL><DEDENT>return session<EOL>", "docstring": "Create requests session with any required auth headers\n        applied.\n\n        :rtype: requests.Session.", "id": "f2347:c0:m1"}
{"signature": "def plot_coarsened(self, **kwargs):", "body": "ax = kwargs.pop('<STR_LIT>', None)<EOL>columns = kwargs.pop('<STR_LIT>', Ellipsis)         <EOL>if columns is Ellipsis or columns is None:<EOL><INDENT>columns = numpy.arange(self.array.shape[<NUM_LIT:0>])<EOL><DEDENT>if len(columns) < <NUM_LIT:2>:<EOL><INDENT>raise MissingDataError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL><DEDENT>color = kwargs.pop('<STR_LIT>', self.default_color_cycle)<EOL>try:<EOL><INDENT>cmap = matplotlib.cm.get_cmap(color)<EOL>colors = cmap(matplotlib.colors.Normalize()(numpy.arange(len(columns[<NUM_LIT:1>:]), dtype=float)))<EOL><DEDENT>except TypeError:<EOL><INDENT>colors = cycle(utilities.asiterable(color))<EOL><DEDENT>if ax is None:<EOL><INDENT>ax = plt.gca()<EOL><DEDENT>t = columns[<NUM_LIT:0>]<EOL>kwargs['<STR_LIT>'] = True<EOL>kwargs['<STR_LIT>'] = ax<EOL>for column, color in zip(columns[<NUM_LIT:1>:], colors):<EOL><INDENT>kwargs['<STR_LIT>'] = color<EOL>self.errorbar(columns=[t, column, column], **kwargs)<EOL><DEDENT>return ax<EOL>", "docstring": "Plot data like :meth:`XVG.plot` with the range of **all** data shown.\n\n        Data are reduced to *maxpoints* (good results are obtained\n        with low values such as 100) and the actual range of observed\n        data is plotted as a translucent error band around the mean.\n\n        Each column in *columns* (except the abscissa, i.e. the first\n        column) is decimated (with :meth:`XVG.decimate`) and the range\n        of data is plotted alongside the mean using\n        :meth:`XVG.errorbar` (see for arguments). Additional\n        arguments:\n\n        :Kewords:\n           *maxpoints*\n                number of points (bins) to coarsen over\n           *color*\n                single color (used for all plots); sequence of colors\n                (will be repeated as necessary); or a matplotlib\n                colormap (e.g. \"jet\", see :mod:`matplotlib.cm`). The\n                default is to use the :attr:`XVG.default_color_cycle`.\n           *method*\n                Method to coarsen the data. See :meth:`XVG.decimate`\n\n        The *demean* keyword has no effect as it is required to be ``True``.\n\n        .. SeeAlso:: :meth:`XVG.plot`, :meth:`XVG.errorbar` and :meth:`XVG.decimate`", "id": "f6858:c0:m18"}
{"signature": "def authorize_view(self):", "body": "args = request.args.to_dict()<EOL>args['<STR_LIT>'] = request.args.getlist('<STR_LIT>')<EOL>return_url = args.pop('<STR_LIT>', None)<EOL>if return_url is None:<EOL><INDENT>return_url = request.referrer or '<STR_LIT:/>'<EOL><DEDENT>flow = self._make_flow(return_url=return_url, **args)<EOL>auth_url = flow.step1_get_authorize_url()<EOL>return redirect(auth_url)<EOL>", "docstring": "Flask view that starts the authorization flow.\n\n        Starts flow by redirecting the user to the OAuth2 provider.", "id": "f2464:c0:m6"}
{"signature": "def _row_should_be_placed(self, row, position):", "body": "placed_row = self._rows_in_grid.get(row)<EOL>return placed_row is None or placed_row.y < position.y<EOL>", "docstring": ":return: whether to place this instruction", "id": "f553:c3:m5"}
{"signature": "@staticmethod<EOL><INDENT>def create(dataset, symbol, degree):<DEDENT>", "body": "x_vals = dataset.data['<STR_LIT:T>'].tolist()<EOL>y_vals = dataset.data[symbol].tolist()<EOL>coeffs = np.polyfit(x_vals, y_vals, degree)<EOL>result = PolynomialModelT(dataset.material,<EOL>dataset.names_dict[symbol],<EOL>symbol, dataset.display_symbols_dict[symbol],<EOL>dataset.units_dict[symbol],<EOL>None, [dataset.name], coeffs)<EOL>result.state_schema['<STR_LIT:T>']['<STR_LIT>'] = float(min(x_vals))<EOL>result.state_schema['<STR_LIT:T>']['<STR_LIT>'] = float(max(x_vals))<EOL>return result<EOL>", "docstring": "Create a model object from the data set for the property specified by\nthe supplied symbol, using the specified polynomial degree.\n\n:param dataset: a DataSet object\n:param symbol: the symbol of the property to be described, e.g. 'rho'\n:param degree: the polynomial degree to use\n\n:returns: a new PolynomialModelT object", "id": "f15850:c0:m0"}
{"signature": "def _generateFileFromProb(filename, numRecords, categoryList, initProb, <EOL>firstOrderProb, secondOrderProb, seqLen, numNoise=<NUM_LIT:0>, resetsEvery=None):", "body": "<EOL>print(\"<STR_LIT>\" % (filename))<EOL>fields = [('<STR_LIT>', '<STR_LIT:int>', '<STR_LIT:R>'), <EOL>('<STR_LIT>', '<STR_LIT:string>', '<STR_LIT>'),<EOL>('<STR_LIT>', '<STR_LIT:float>', '<STR_LIT>')]<EOL>scriptDir = os.path.dirname(__file__)<EOL>pathname = os.path.join(scriptDir, '<STR_LIT>', filename)<EOL>outFile = FileRecordStream(pathname, write=True, fields=fields)<EOL>initCumProb = initProb.cumsum()<EOL>firstOrderCumProb = dict()<EOL>for (key,value) in firstOrderProb.items():<EOL><INDENT>firstOrderCumProb[key] = value.cumsum()<EOL><DEDENT>if secondOrderProb is not None:<EOL><INDENT>secondOrderCumProb = dict()<EOL>for (key,value) in secondOrderProb.items():<EOL><INDENT>secondOrderCumProb[key] = value.cumsum()<EOL><DEDENT><DEDENT>else:<EOL><INDENT>secondOrderCumProb = None<EOL><DEDENT>elementsInSeq = []<EOL>numElementsSinceReset = <NUM_LIT:0><EOL>maxCatIdx = len(categoryList) - <NUM_LIT:1><EOL>for _ in range(numRecords):<EOL><INDENT>if numElementsSinceReset == <NUM_LIT:0>:<EOL><INDENT>reset = <NUM_LIT:1><EOL><DEDENT>else:<EOL><INDENT>reset = <NUM_LIT:0><EOL><DEDENT>rand = numpy.random.rand()<EOL>if secondOrderCumProb is None:<EOL><INDENT>if len(elementsInSeq) == <NUM_LIT:0>:<EOL><INDENT>catIdx = numpy.searchsorted(initCumProb, rand)<EOL><DEDENT>elif len(elementsInSeq) >= <NUM_LIT:1> and(seqLen is None or len(elementsInSeq) < seqLen-numNoise):<EOL><INDENT>catIdx = numpy.searchsorted(firstOrderCumProb[str(elementsInSeq[-<NUM_LIT:1>])], <EOL>rand)<EOL><DEDENT>else:   <EOL><INDENT>catIdx = numpy.random.randint(len(categoryList))<EOL><DEDENT><DEDENT>else:<EOL><INDENT>if len(elementsInSeq) == <NUM_LIT:0>:<EOL><INDENT>catIdx = numpy.searchsorted(initCumProb, rand)<EOL><DEDENT>elif len(elementsInSeq) == <NUM_LIT:1>:<EOL><INDENT>catIdx = numpy.searchsorted(firstOrderCumProb[str(elementsInSeq)], rand)<EOL><DEDENT>elif (len(elementsInSeq) >=<NUM_LIT:2>) and(seqLen is None or len(elementsInSeq) < seqLen-numNoise):<EOL><INDENT>catIdx = numpy.searchsorted(secondOrderCumProb[str(elementsInSeq[-<NUM_LIT:2>:])], rand)<EOL><DEDENT>else:   <EOL><INDENT>catIdx = numpy.random.randint(len(categoryList))<EOL><DEDENT><DEDENT>catIdx = min(maxCatIdx, catIdx)<EOL>outFile.appendRecord([reset, categoryList[catIdx], catIdx])    <EOL>elementsInSeq.append(catIdx)<EOL>numElementsSinceReset += <NUM_LIT:1><EOL>if resetsEvery is not None and numElementsSinceReset == resetsEvery:<EOL><INDENT>numElementsSinceReset = <NUM_LIT:0><EOL>elementsInSeq = []<EOL><DEDENT>if seqLen is not None and (len(elementsInSeq) == seqLen+numNoise):<EOL><INDENT>elementsInSeq = []<EOL><DEDENT><DEDENT>outFile.close()<EOL>", "docstring": "Generate a set of records reflecting a set of probabilities.\n\n    Parameters:\n    ----------------------------------------------------------------\n    filename:         name of .csv file to generate\n    numRecords:       number of records to generate\n    categoryList:     list of category names\n    initProb:         Initial probability for each category. This is a vector\n                        of length len(categoryList).\n    firstOrderProb:   A dictionary of the 1st order probabilities. The key\n                        is the 1st element of the sequence, the value is\n                        the probability of each 2nd element given the first. \n    secondOrderProb:  A dictionary of the 2nd order probabilities. The key\n                        is the first 2 elements of the sequence, the value is\n                        the probability of each possible 3rd element given the \n                        first two. If this is None, then the sequences will be\n                        first order only. \n    seqLen:           Desired length of each sequence. The 1st element will\n                        be generated using the initProb, the 2nd element by the\n                        firstOrder table, and the 3rd and all successive \n                        elements by the secondOrder table. None means infinite\n                        length. \n    numNoise:         Number of noise elements to place between each \n                        sequence. The noise elements are evenly distributed from \n                        all categories. \n    resetsEvery:      If not None, generate a reset every N records\n\n\n    Here is an example of some parameters:\n\n    categoryList:     ['cat1', 'cat2', 'cat3']\n\n    initProb:         [0.7, 0.2, 0.1]\n\n    firstOrderProb:   {'[0]': [0.3, 0.3, 0.4],\n                       '[1]': [0.3, 0.3, 0.4],\n                       '[2]': [0.3, 0.3, 0.4]}\n\n    secondOrderProb:  {'[0,0]': [0.3, 0.3, 0.4],\n                       '[0,1]': [0.3, 0.3, 0.4],\n                       '[0,2]': [0.3, 0.3, 0.4],\n                       '[1,0]': [0.3, 0.3, 0.4],\n                       '[1,1]': [0.3, 0.3, 0.4],\n                       '[1,2]': [0.3, 0.3, 0.4],\n                       '[2,0]': [0.3, 0.3, 0.4],\n                       '[2,1]': [0.3, 0.3, 0.4],\n                       '[2,2]': [0.3, 0.3, 0.4]}", "id": "f17515:m3"}
{"signature": "@property<EOL><INDENT>def is_arithmetic(self):<DEDENT>", "body": "return self.semantics in (<EOL>'<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>')<EOL>", "docstring": "True if the instruction is an arithmetic operation", "id": "f13733:c4:m36"}
{"signature": "def _getarray(loci, snames):", "body": "<EOL>lxs = np.zeros((len(snames), len(loci)), dtype=np.uint64)<EOL>for loc in xrange(len(loci)):<EOL><INDENT>for seq in loci[loc].split(\"<STR_LIT:\\n>\"):<EOL><INDENT>if \"<STR_LIT>\" not in seq:<EOL><INDENT>lxs[snames.index(seq.split()[<NUM_LIT:0>][:]), loc] += <NUM_LIT:1><EOL><DEDENT><DEDENT><DEDENT>return lxs, snames<EOL>", "docstring": "parse loci list and return presence/absence matrix\nordered by the tips on the tree or list of names.", "id": "f5320:m4"}
{"signature": "@main.command()<EOL>@click.argument('<STR_LIT>')<EOL>@click.option('<STR_LIT>', '<STR_LIT>', type=click.File('<STR_LIT:w>'))<EOL>@click.option('<STR_LIT>', '<STR_LIT>')<EOL>@click.option('<STR_LIT>', '<STR_LIT>', is_flag=True)<EOL>def belns(keyword: str, file: TextIO, encoding: Optional[str], use_names: bool):", "body": "directory = get_data_dir(keyword)<EOL>obo_url = f'<STR_LIT>'<EOL>obo_path = os.path.join(directory, f'<STR_LIT>')<EOL>obo_cache_path = os.path.join(directory, f'<STR_LIT>')<EOL>obo_getter = make_obo_getter(obo_url, obo_path, preparsed_path=obo_cache_path)<EOL>graph = obo_getter()<EOL>convert_obo_graph_to_belns(<EOL>graph,<EOL>file=file,<EOL>encoding=encoding,<EOL>use_names=use_names,<EOL>)<EOL>", "docstring": "Write as a BEL namespace.", "id": "f1589:m2"}
{"signature": "def __init__(self, procs=<NUM_LIT:10>, workspace_url: str=None, policy: str='<STR_LIT>'):", "body": "self._accounts = dict()<EOL>self._serializer = PickleSerializer()<EOL>self._config_procs = procs<EOL>constraints = ConstraintSet()<EOL>world = evm.EVMWorld(constraints)<EOL>initial_state = State(constraints, world)<EOL>super().__init__(initial_state, workspace_url=workspace_url, policy=policy)<EOL>self.constraints = ConstraintSet()<EOL>self.detectors = {}<EOL>self.metadata: Dict[int, SolidityMetadata] = {}<EOL>self.context['<STR_LIT>'] = {}<EOL>self.context['<STR_LIT>']['<STR_LIT>'] = set()<EOL>self.context['<STR_LIT>']['<STR_LIT>'] = set()<EOL>self.context['<STR_LIT>']['<STR_LIT>'] = <NUM_LIT:0><EOL>self.context['<STR_LIT>']['<STR_LIT>'] = dict()<EOL>self.context['<STR_LIT>']['<STR_LIT>'] = set()<EOL>self._executor.subscribe('<STR_LIT>', self._load_state_callback)<EOL>self._executor.subscribe('<STR_LIT>', self._terminate_state_callback)<EOL>self._executor.subscribe('<STR_LIT>', self._did_evm_execute_instruction_callback)<EOL>self._executor.subscribe('<STR_LIT>', self._did_evm_read_code)<EOL>self._executor.subscribe('<STR_LIT>', self._on_symbolic_sha3_callback)<EOL>self._executor.subscribe('<STR_LIT>', self._on_concrete_sha3_callback)<EOL>self.subscribe('<STR_LIT>', self._generate_testcase_callback)<EOL>", "docstring": "A Manticore EVM manager\n:param procs:, number of workers to use in the exploration\n:param workspace_url: workspace folder name\n:param policy: scheduling priority", "id": "f16997:c0:m13"}
{"signature": "@deprecated(details='<STR_LIT>')<EOL>def tradesWS(symbols=None, on_data=None):", "body": "symbols = _strToList(symbols)<EOL>sendinit = ({'<STR_LIT>': symbols, '<STR_LIT>': ['<STR_LIT>']},)<EOL>return _stream(_wsURL('<STR_LIT>'), sendinit, on_data)<EOL>", "docstring": "https://iextrading.com/developer/docs/#trades", "id": "f2334:m4"}
{"signature": "def _run_io_threads(self, handler):", "body": "reader = ReadingThread(self.settings, handler, daemon = self.daemon,<EOL>exc_queue = self.exc_queue)<EOL>writter = WrittingThread(self.settings, handler, daemon = self.daemon,<EOL>exc_queue = self.exc_queue)<EOL>self.io_threads += [reader, writter]<EOL>reader.start()<EOL>writter.start()<EOL>", "docstring": "Start threads for an IOHandler.", "id": "f15264:c5:m4"}
{"signature": "@property<EOL><INDENT>def instructions(self):<DEDENT>", "body": "x = self.x<EOL>y = self.y<EOL>result = []<EOL>for instruction in self._row.instructions:<EOL><INDENT>instruction_in_grid = InstructionInGrid(instruction, Point(x, y))<EOL>x += instruction_in_grid.width<EOL>result.append(instruction_in_grid)<EOL><DEDENT>return result<EOL>", "docstring": "The instructions in a grid.\n\n        :return: the :class:`instructions in a grid <InstructionInGrid>` of\n          this row\n        :rtype: list", "id": "f553:c2:m2"}
{"signature": "def remove_out_of_image(self, fully=True, partly=False):", "body": "bbs_clean = [bb for bb in self.bounding_boxes<EOL>if not bb.is_out_of_image(self.shape, fully=fully, partly=partly)]<EOL>return BoundingBoxesOnImage(bbs_clean, shape=self.shape)<EOL>", "docstring": "Remove all bounding boxes that are fully or partially outside of the image.\n\nParameters\n----------\nfully : bool, optional\n    Whether to remove bounding boxes that are fully outside of the image.\n\npartly : bool, optional\n    Whether to remove bounding boxes that are partially outside of the image.\n\nReturns\n-------\nimgaug.BoundingBoxesOnImage\n    Reduced set of bounding boxes, with those that were fully/partially outside of\n    the image removed.", "id": "f16277:c1:m8"}
{"signature": "def get_thermostat(self, serial_number):", "body": "return NuHeatThermostat(self, serial_number)<EOL>", "docstring": "Get a thermostat object by serial number\n\n:param serial_number: The serial number / ID of the desired thermostat", "id": "f1222:c0:m3"}
{"signature": "def _trimSegmentsInCell(self, colIdx, cellIdx, segList, minPermanence,<EOL>minNumSyns):", "body": "<EOL>if minPermanence is None:<EOL><INDENT>minPermanence = self.connectedPerm<EOL><DEDENT>if minNumSyns is None:<EOL><INDENT>minNumSyns = self.activationThreshold<EOL><DEDENT>nSegsRemoved, nSynsRemoved = <NUM_LIT:0>, <NUM_LIT:0><EOL>segsToDel = [] <EOL>for segment in segList:<EOL><INDENT>synsToDel = [syn for syn in segment.syns if syn[<NUM_LIT:2>] < minPermanence]<EOL>if len(synsToDel) == len(segment.syns):<EOL><INDENT>segsToDel.append(segment) <EOL><DEDENT>else:<EOL><INDENT>if len(synsToDel) > <NUM_LIT:0>:<EOL><INDENT>for syn in synsToDel: <EOL><INDENT>segment.syns.remove(syn)<EOL>nSynsRemoved += <NUM_LIT:1><EOL><DEDENT><DEDENT>if len(segment.syns) < minNumSyns:<EOL><INDENT>segsToDel.append(segment)<EOL><DEDENT><DEDENT><DEDENT>nSegsRemoved += len(segsToDel)<EOL>for seg in segsToDel: <EOL><INDENT>self._cleanUpdatesList(colIdx, cellIdx, seg)<EOL>self.cells[colIdx][cellIdx].remove(seg)<EOL>nSynsRemoved += len(seg.syns)<EOL><DEDENT>return nSegsRemoved, nSynsRemoved<EOL>", "docstring": "This method goes through a list of segments for a given cell and\ndeletes all synapses whose permanence is less than minPermanence and deletes\nany segments that have less than minNumSyns synapses remaining.\n\n:param colIdx        Column index\n:param cellIdx       Cell index within the column\n:param segList       List of segment references\n:param minPermanence Any syn whose permamence is 0 or < minPermanence will\n                     be deleted.\n:param minNumSyns    Any segment with less than minNumSyns synapses remaining\n                     in it will be deleted.\n\n:returns: tuple (numSegsRemoved, numSynsRemoved)", "id": "f17565:c0:m65"}
{"signature": "@register.tag<EOL>def gauges(parser, token):", "body": "bits = token.split_contents()<EOL>if len(bits) > <NUM_LIT:1>:<EOL><INDENT>raise TemplateSyntaxError(\"<STR_LIT>\" % bits[<NUM_LIT:0>])<EOL><DEDENT>return GaugesNode()<EOL>", "docstring": "Gaug.es template tag.\n\nRenders Javascript code to gaug.es testing.  You must supply\nyour Site ID account number in the ``GAUGES_SITE_ID``\nsetting.", "id": "f14861:m0"}
{"signature": "def GetIndentLevel(line):", "body": "indent = Match(r'<STR_LIT>', line)<EOL>if indent:<EOL><INDENT>return len(indent.group(<NUM_LIT:1>))<EOL><DEDENT>else:<EOL><INDENT>return <NUM_LIT:0><EOL><DEDENT>", "docstring": "Return the number of leading spaces in line.\n\n    Args:\n      line: A string to check.\n\n    Returns:\n      An integer count of leading spaces, possibly zero.", "id": "f7245:m36"}
{"signature": "def read(self, bands=None, **kwargs):", "body": "arr = self<EOL>if bands is not None:<EOL><INDENT>arr = self[bands, ...]<EOL><DEDENT>return arr.compute(scheduler=threaded_get)<EOL>", "docstring": "Reads data from a dask array and returns the computed ndarray matching the given bands\n\n        Args:\n            bands (list): band indices to read from the image. Returns bands in the order specified in the list of bands.\n\n        Returns:\n            ndarray: a numpy array of image data", "id": "f7081:c1:m2"}
{"signature": "def mu(self, **state):", "body": "raise NotImplementedError()<EOL>", "docstring": "Calculate the mu value given the material state.\n\n:param **state: material state\n\n:returns: float", "id": "f15830:c0:m5"}
{"signature": "@tornado.gen.coroutine<EOL><INDENT>def fetch_backpressure(self, cluster, metric, topology, component, instance,timerange, is_max, environ=None):<DEDENT>", "body": "instances = yield get_instances(cluster, environ, topology)<EOL>if component != \"<STR_LIT:*>\":<EOL><INDENT>filtered_inst = [instance for instance in instances if instance.split(\"<STR_LIT:_>\")[<NUM_LIT:2>] == component]<EOL><DEDENT>else:<EOL><INDENT>filtered_inst = instances<EOL><DEDENT>futures_dict = {}<EOL>for inst in filtered_inst:<EOL><INDENT>query = queries.get(metric).format(inst)<EOL>futures_dict[inst] = get_metrics(cluster, environ, topology, timerange, query)<EOL><DEDENT>res = yield futures_dict<EOL>if not is_max:<EOL><INDENT>timelines = []<EOL>for key in res:<EOL><INDENT>result = res[key]<EOL>if len(result[\"<STR_LIT>\"]) > <NUM_LIT:0>:<EOL><INDENT>result[\"<STR_LIT>\"][<NUM_LIT:0>][\"<STR_LIT>\"] = key<EOL><DEDENT>timelines.extend(result[\"<STR_LIT>\"])<EOL><DEDENT>result = self.get_metric_response(timerange, timelines, is_max)<EOL><DEDENT>else:<EOL><INDENT>data = self.compute_max(res.values())<EOL>result = self.get_metric_response(timerange, data, is_max)<EOL><DEDENT>raise tornado.gen.Return(result)<EOL>", "docstring": ":param cluster:\n:param metric:\n:param topology:\n:param component:\n:param instance:\n:param timerange:\n:param isMax:\n:param environ:\n:return:", "id": "f7414:c0:m2"}
{"signature": "def scale_out(self, blocks=<NUM_LIT:1>):", "body": "r = []<EOL>for i in range(blocks):<EOL><INDENT>if self.provider:<EOL><INDENT>block = self.provider.submit(self.launch_cmd, <NUM_LIT:1>, self.workers_per_node)<EOL>logger.debug(\"<STR_LIT>\".format(i, block))<EOL>if not block:<EOL><INDENT>raise(ScalingFailed(self.provider.label,<EOL>\"<STR_LIT>\"))<EOL><DEDENT>self.engines.extend([block])<EOL>r.extend([block])<EOL><DEDENT><DEDENT>else:<EOL><INDENT>logger.error(\"<STR_LIT>\")<EOL>r = None<EOL><DEDENT>return r<EOL>", "docstring": "Scales out the number of active workers by 1.\n\n        This method is notImplemented for threads and will raise the error if called.\n\n        Parameters:\n            blocks : int\n               Number of blocks to be provisioned.", "id": "f2813:c0:m8"}
{"signature": "def _dispatch(self, event, listener, *args, **kwargs):", "body": "if (<EOL>asyncio.iscoroutinefunction(listener) or<EOL>isinstance(listener, functools.partial) and<EOL>asyncio.iscoroutinefunction(listener.func)<EOL>):<EOL><INDENT>return self._dispatch_coroutine(event, listener, *args, **kwargs)<EOL><DEDENT>return self._dispatch_function(event, listener, *args, **kwargs)<EOL>", "docstring": "Dispatch an event to a listener.\n\n        Args:\n            event (str): The name of the event that triggered this call.\n            listener (def or async def): The listener to trigger.\n            *args: Any number of positional arguments.\n            **kwargs: Any number of keyword arguments.\n\n        This method inspects the listener. If it is a def it dispatches the\n        listener to a method that will execute that def. If it is an async def\n        it dispatches it to a method that will schedule the resulting coro with\n        the event loop.", "id": "f11647:c0:m11"}
{"signature": "def get_y(self):", "body": "return self._y<EOL>", "docstring": "Return the bottom coord of the rectangle", "id": "f17197:c2:m7"}
{"signature": "def to_special_value(self, value):", "body": "if isinstance(value, utils.NoAssert):<EOL><INDENT>return self.spdx_namespace.noassertion<EOL><DEDENT>elif isinstance(value, utils.SPDXNone):<EOL><INDENT>return self.spdx_namespace.none<EOL><DEDENT>else:<EOL><INDENT>return Literal(value)<EOL><DEDENT>", "docstring": "Return proper spdx term or Literal", "id": "f3742:c0:m2"}
{"signature": "def get_heron_dir():", "body": "go_above_dirs = <NUM_LIT:9><EOL>path = \"<STR_LIT:/>\".join(os.path.realpath(__file__).split('<STR_LIT:/>')[:-go_above_dirs])<EOL>return normalized_class_path(path)<EOL>", "docstring": "This will extract heron directory from .pex file.\n\nFor example,\nwhen __file__ is '/Users/heron-user/bin/heron/heron/tools/common/src/python/utils/config.pyc', and\nits real path is '/Users/heron-user/.heron/bin/heron/tools/common/src/python/utils/config.pyc',\nthe internal variable ``path`` would be '/Users/heron-user/.heron', which is the heron directory\n\nThis means the variable `go_above_dirs` below is 9.\n\n:return: root location of the .pex file", "id": "f7415:m6"}
{"signature": "def sync_update_current_price_info(self):", "body": "loop = asyncio.get_event_loop()<EOL>task = loop.create_task(self.update_current_price_info())<EOL>loop.run_until_complete(task)<EOL>", "docstring": "Update current price info.", "id": "f12605:c1:m3"}
{"signature": "def filter_gaussian(self,sigmaMs=<NUM_LIT:100>,applyFiltered=False,applyBaseline=False):", "body": "if sigmaMs==<NUM_LIT:0>:<EOL><INDENT>return self.dataY<EOL><DEDENT>filtered=cm.filter_gaussian(self.dataY,sigmaMs)<EOL>if applyBaseline:<EOL><INDENT>self.dataY=self.dataY-filtered<EOL><DEDENT>elif applyFiltered:<EOL><INDENT>self.dataY=filtered<EOL><DEDENT>else:<EOL><INDENT>return filtered<EOL><DEDENT>", "docstring": "RETURNS filtered trace. Desn't filter it in place.", "id": "f11408:c0:m13"}
{"signature": "def cancel(self, job_ids):", "body": "job_id_list = '<STR_LIT:U+0020>'.join(job_ids)<EOL>cmd = \"<STR_LIT>\".format(job_id_list)<EOL>retcode, stdout, stderr = super().execute_wait(cmd, <NUM_LIT:3>)<EOL>rets = None<EOL>if retcode == <NUM_LIT:0>:<EOL><INDENT>for jid in job_ids:<EOL><INDENT>self.resources[jid]['<STR_LIT:status>'] = \"<STR_LIT>\"<EOL><DEDENT>rets = [True for i in job_ids]<EOL><DEDENT>else:<EOL><INDENT>rets = [False for i in job_ids]<EOL><DEDENT>return rets<EOL>", "docstring": "Cancels the resources identified by the job_ids provided by the user.\n\n        Args:\n             - job_ids (list): A list of job identifiers\n\n        Returns:\n             - A list of status from cancelling the job which can be True, False\n\n        Raises:\n             - ExecutionProviderException or its subclasses", "id": "f2787:c0:m4"}
{"signature": "def lab_to_rgb(l, a, b):", "body": "y = (l + <NUM_LIT:16>) / <NUM_LIT><EOL>x = a / <NUM_LIT> + y<EOL>z = y - b / <NUM_LIT><EOL>v = [x, y, z]<EOL>for i in _range(<NUM_LIT:3>):<EOL><INDENT>if pow(v[i], <NUM_LIT:3>) > <NUM_LIT>:<EOL><INDENT>v[i] = pow(v[i], <NUM_LIT:3>)<EOL><DEDENT>else:<EOL><INDENT>v[i] = (v[i] - <NUM_LIT:16> / <NUM_LIT>) / <NUM_LIT><EOL><DEDENT><DEDENT>x = v[<NUM_LIT:0>] * <NUM_LIT> / <NUM_LIT:100><EOL>y = v[<NUM_LIT:1>] * <NUM_LIT> / <NUM_LIT:100><EOL>z = v[<NUM_LIT:2>] * <NUM_LIT> / <NUM_LIT:100><EOL>r = x * <NUM_LIT> + y * -<NUM_LIT> + z * -<NUM_LIT><EOL>g = x * -<NUM_LIT> + y * <NUM_LIT> + z * <NUM_LIT><EOL>b = x * <NUM_LIT> + y * -<NUM_LIT> + z * <NUM_LIT><EOL>v = [r, g, b]<EOL>for i in _range(<NUM_LIT:3>):<EOL><INDENT>if v[i] > <NUM_LIT>:<EOL><INDENT>v[i] = <NUM_LIT> * pow(v[i], <NUM_LIT:1> / <NUM_LIT>) - <NUM_LIT><EOL><DEDENT>else:<EOL><INDENT>v[i] = <NUM_LIT> * v[i]<EOL><DEDENT><DEDENT>r, g, b = v[<NUM_LIT:0>], v[<NUM_LIT:1>], v[<NUM_LIT:2>]<EOL>return r, g, b<EOL>", "docstring": "Converts CIE Lab to RGB components.\n\n    First we have to convert to XYZ color space.\n    Conversion involves using a white point,\n    in this case D65 which represents daylight illumination.\n\n    Algorithm adopted from:\n    http://www.easyrgb.com/math.php", "id": "f11536:m4"}
{"signature": "@build.command()<EOL>@click.pass_context<EOL>@clean_outputs<EOL>def bookmark(ctx):", "body": "user, project_name, _build = get_build_or_local(ctx.obj.get('<STR_LIT>'), ctx.obj.get('<STR_LIT>'))<EOL>try:<EOL><INDENT>PolyaxonClient().build_job.bookmark(user, project_name, _build)<EOL><DEDENT>except (PolyaxonHTTPError, PolyaxonShouldExitError, PolyaxonClientException) as e:<EOL><INDENT>Printer.print_error('<STR_LIT>'.format(_build))<EOL>Printer.print_error('<STR_LIT>'.format(e))<EOL>sys.exit(<NUM_LIT:1>)<EOL><DEDENT>Printer.print_success(\"<STR_LIT>\")<EOL>", "docstring": "Bookmark build job.\n\n    Uses [Caching](/references/polyaxon-cli/#caching)\n\n    Examples:\n\n    \\b\n    ```bash\n    $ polyaxon build bookmark\n    ```\n\n    \\b\n    ```bash\n    $ polyaxon build -b 2 bookmark\n    ```", "id": "f1045:m6"}
{"signature": "def kill_all(self):", "body": "for pid in self.children:<EOL><INDENT>try:<EOL><INDENT>os.kill(pid, signal.SIGTRAP)<EOL><DEDENT>except OSError:<EOL><INDENT>continue<EOL><DEDENT><DEDENT>self.join()<EOL>", "docstring": "kill all slaves and reap the monitor", "id": "f2547:c2:m3"}
{"signature": "def get_meta_content(self, meta_name):", "body": "meta = self.parser.css_select(self.article.doc, meta_name)<EOL>content = None<EOL>if meta is not None and len(meta) > <NUM_LIT:0>:<EOL><INDENT>content = self.parser.getAttribute(meta[<NUM_LIT:0>], '<STR_LIT:content>')<EOL><DEDENT>if content:<EOL><INDENT>return content.strip()<EOL><DEDENT>return '<STR_LIT>'<EOL>", "docstring": "Extract a given meta content form document", "id": "f14082:c0:m4"}
{"signature": "@map_types<EOL><INDENT>def __init__(self,<EOL>centre: dim.Position = (<NUM_LIT:0.0>, <NUM_LIT:0.0>),<EOL>einstein_radius: dim.Length = <NUM_LIT:1.0>,<EOL>core_radius: dim.Length = <NUM_LIT>):<DEDENT>", "body": "super(SphericalCoredIsothermal, self).__init__(centre=centre, einstein_radius=einstein_radius, slope=<NUM_LIT>,<EOL>core_radius=core_radius)<EOL>", "docstring": "Represents a cored spherical isothermal density distribution, which is equivalent to the elliptical power-law\ndensity distribution for the value slope: float = 2.0\n\nParameters\n----------\ncentre: (float, float)\n    The (y,x) arc-second coordinates of the profile centre.\neinstein_radius : float\n    The arc-second Einstein radius.\ncore_radius : float\n    The arc-second radius of the inner core.", "id": "f5952:c8:m0"}
{"signature": "def run(self, clock, generalLedger):", "body": "if not self._meet_execution_criteria(clock.timestep_ix):<EOL><INDENT>return<EOL><DEDENT>if self.description is None:<EOL><INDENT>tx_name = self.name<EOL><DEDENT>else:<EOL><INDENT>tx_name = self.description<EOL><DEDENT>if self._months_executed == <NUM_LIT:0>:<EOL><INDENT>generalLedger.create_transaction(<EOL>tx_name,<EOL>description='<STR_LIT>',<EOL>tx_date=clock.get_datetime(),<EOL>dt_account=self.bank_account,<EOL>cr_account=self.loan_account,<EOL>source=self.path,<EOL>amount=self.amount)<EOL><DEDENT>else:<EOL><INDENT>curr_interest_amount = (self._amount_left *<EOL>self.interest_rate) / <NUM_LIT><EOL>generalLedger.create_transaction(<EOL>tx_name,<EOL>description='<STR_LIT>',<EOL>tx_date=clock.get_datetime(),<EOL>dt_account=self.interest_account,<EOL>cr_account=self.loan_account,<EOL>source=self.path,<EOL>amount=curr_interest_amount)<EOL>generalLedger.create_transaction(<EOL>tx_name,<EOL>description='<STR_LIT>',<EOL>tx_date=clock.get_datetime(),<EOL>dt_account=self.loan_account,<EOL>cr_account=self.bank_account,<EOL>source=self.path,<EOL>amount=self._monthly_payment)<EOL>self._amount_left += curr_interest_amount - self._monthly_payment<EOL><DEDENT>self._months_executed += self.interval<EOL>", "docstring": "Execute the activity at the current clock cycle.\n\n:param clock: The clock containing the current execution time and\n  period information.\n:param generalLedger: The general ledger into which to create the\n  transactions.", "id": "f15837:c1:m8"}
{"signature": "def yesterdayDF(symbol, token='<STR_LIT>', version='<STR_LIT>'):", "body": "y = yesterday(symbol, token, version)<EOL>if y:<EOL><INDENT>df = pd.io.json.json_normalize(y)<EOL>_toDatetime(df)<EOL>_reindex(df, '<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>df = pd.DataFrame()<EOL><DEDENT>return df<EOL>", "docstring": "This returns previous day adjusted price data for one or more stocks\n\n    https://iexcloud.io/docs/api/#previous-day-prices\n    Available after 4am ET Tue-Sat\n\n    Args:\n        symbol (string); Ticker to request\n        token (string); Access token\n        version (string); API version\n\n    Returns:\n        DataFrame: result", "id": "f2330:m70"}
{"signature": "def unpickle(pickled_string):", "body": "try:<EOL><INDENT>obj = loads(pickled_string)<EOL><DEDENT>except Exception as e:<EOL><INDENT>raise UnpickleError('<STR_LIT>', pickled_string, e)<EOL><DEDENT>return obj<EOL>", "docstring": "Unpickles a string, but raises a unified UnpickleError in case anything\n    fails.\n    This is a helper method to not have to deal with the fact that `loads()`\n    potentially raises many types of exceptions (e.g. AttributeError,\n    IndexError, TypeError, KeyError, etc.)", "id": "f3055:m0"}
{"signature": "def plot_intensities(<EOL>light_profile, grid, mask=None, extract_array_from_mask=False, zoom_around_mask=False, positions=None, <EOL>as_subplot=False,<EOL>units='<STR_LIT>', kpc_per_arcsec=None, figsize=(<NUM_LIT:7>, <NUM_LIT:7>), aspect='<STR_LIT>',<EOL>cmap='<STR_LIT>', norm='<STR_LIT>', norm_min=None, norm_max=None, linthresh=<NUM_LIT>, linscale=<NUM_LIT>,<EOL>cb_ticksize=<NUM_LIT:10>, cb_fraction=<NUM_LIT>, cb_pad=<NUM_LIT>, cb_tick_values=None, cb_tick_labels=None,<EOL>title='<STR_LIT>', titlesize=<NUM_LIT:16>, xlabelsize=<NUM_LIT:16>, ylabelsize=<NUM_LIT:16>, xyticksize=<NUM_LIT:16>,<EOL>mask_pointsize=<NUM_LIT:10>, position_pointsize=<NUM_LIT>, grid_pointsize=<NUM_LIT:1>,<EOL>output_path=None, output_format='<STR_LIT>', output_filename='<STR_LIT>'):", "body": "intensities = light_profile.intensities_from_grid(grid=grid)<EOL>intensities = grid.scaled_array_2d_from_array_1d(intensities)<EOL>array_plotters.plot_array(<EOL>array=intensities, mask=mask, extract_array_from_mask=extract_array_from_mask,<EOL>zoom_around_mask=zoom_around_mask, positions=positions, as_subplot=as_subplot,<EOL>units=units, kpc_per_arcsec=kpc_per_arcsec, figsize=figsize, aspect=aspect,<EOL>cmap=cmap, norm=norm, norm_min=norm_min, norm_max=norm_max, linthresh=linthresh, linscale=linscale,<EOL>cb_ticksize=cb_ticksize, cb_fraction=cb_fraction, cb_pad=cb_pad, <EOL>cb_tick_values=cb_tick_values, cb_tick_labels=cb_tick_labels,<EOL>title=title, titlesize=titlesize, xlabelsize=xlabelsize, ylabelsize=ylabelsize, xyticksize=xyticksize,<EOL>mask_pointsize=mask_pointsize, position_pointsize=position_pointsize, grid_pointsize=grid_pointsize,<EOL>output_path=output_path, output_format=output_format, output_filename=output_filename)<EOL>", "docstring": "Plot the intensities (e.g. the image) of a light profile, on a regular grid of (y,x) coordinates.\n\n    Set *autolens.hyper.array.plotters.array_plotters* for a description of all innput parameters not described below.\n\n    Parameters\n    -----------\n    light_profile : model.profiles.light_profiles.LightProfile\n        The light profile whose intensities are plotted.\n    grid : ndarray or hyper.array.grid_stacks.RegularGrid\n        The (y,x) coordinates of the grid, in an array of shape (total_coordinates, 2)", "id": "f5953:m0"}
{"signature": "def _save_without_sending(self, *args, **kwargs):", "body": "self.do_not_send = True<EOL>super(MailerMessage, self).save(*args, **kwargs)<EOL>", "docstring": "Saves the MailerMessage instance without sending the e-mail. This ensures\nother models (e.g. `Attachment`) have something to relate to in the database.", "id": "f12195:c1:m2"}
{"signature": "@click.group()<EOL>@click.option('<STR_LIT>', '<STR_LIT>', is_flag=True, default=False, help='<STR_LIT>')<EOL>@click.pass_context<EOL>@clean_outputs<EOL>def cli(context, verbose):", "body": "configure_logger(verbose or GlobalConfigManager.get_value('<STR_LIT>'))<EOL>non_check_cmds = ['<STR_LIT>', '<STR_LIT:version>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>']<EOL>if context.invoked_subcommand not in non_check_cmds:<EOL><INDENT>check_cli_version()<EOL><DEDENT>", "docstring": "Polyaxon CLI tool to:\n\n        * Parse, Validate, and Check Polyaxonfiles.\n\n        * Interact with Polyaxon server.\n\n        * Run and Monitor experiments.\n\n    Check the help available for each command listed below.", "id": "f1033:m0"}
{"signature": "def digest_auth_user(self, realm, user_name, environ):", "body": "user = self._get_realm_entry(realm, user_name)<EOL>if user is None:<EOL><INDENT>return False<EOL><DEDENT>password = user.get(\"<STR_LIT:password>\")<EOL>environ[\"<STR_LIT>\"] = user.get(\"<STR_LIT>\", [])<EOL>return self._compute_http_digest_a1(realm, user_name, password)<EOL>", "docstring": "Computes digest hash A1 part.", "id": "f8587:c0:m7"}
{"signature": "def count_node_match(self, pattern, adict=None):", "body": "mydict = self._filetree if adict is None else adict<EOL>k = <NUM_LIT:0><EOL>if isinstance(mydict, dict):<EOL><INDENT>names = mydict.keys()<EOL>k += len(filter_list(names, pattern))<EOL>for nom in names:<EOL><INDENT>k += self.count_node_match(pattern, mydict[nom])<EOL><DEDENT><DEDENT>else:<EOL><INDENT>k = len(filter_list(mydict, pattern))<EOL><DEDENT>return k<EOL>", "docstring": "Return the number of nodes that match the pattern.\n\n:param pattern:\n\n:param adict:\n:return: int", "id": "f4058:c1:m13"}
{"signature": "def get_many_mock_pplans():", "body": "pplans_lst = []<EOL>for i in range(<NUM_LIT:10>):<EOL><INDENT>_id = \"<STR_LIT>\" + str(i)<EOL>pplan = get_mock_pplan(stmgrs=[get_mock_stmgr(id=_id)])<EOL>pplans_lst.append(pplan)<EOL><DEDENT>return pplans_lst<EOL>", "docstring": "Returns a list of 10 PhysicalPlan objects, differing just by stream manager id", "id": "f7463:m14"}
{"signature": "def read_exactly(self, num_bytes):", "body": "output = b'<STR_LIT>'<EOL>remaining = num_bytes<EOL>while remaining > <NUM_LIT:0>:<EOL><INDENT>output += self.read(remaining)<EOL>remaining = num_bytes - len(output)<EOL><DEDENT>return output<EOL>", "docstring": "Reads exactly the specified number of bytes from the socket\n\n:param num_bytes:\n    An integer - the exact number of bytes to read\n\n:return:\n    A byte string of the data that was read", "id": "f9507:c1:m8"}
{"signature": "def __len__(self):", "body": "return self.variables.__len__()<EOL>", "docstring": "The number of variables.", "id": "f11942:c0:m3"}
{"signature": "def _parallel_bls_worker(task):", "body": "try:<EOL><INDENT>times, mags, errs = task[:<NUM_LIT:3>]<EOL>magsarefluxes = task[<NUM_LIT:3>]<EOL>minfreq, nfreq, stepsize = task[<NUM_LIT:4>:<NUM_LIT:7>]<EOL>ndurations, mintransitduration, maxtransitduration = task[<NUM_LIT:7>:<NUM_LIT:10>]<EOL>blsobjective, blsmethod, blsoversample = task[<NUM_LIT:10>:]<EOL>frequencies = minfreq + nparange(nfreq)*stepsize<EOL>periods = <NUM_LIT:1.0>/frequencies<EOL>durations = nplinspace(mintransitduration*periods.min(),<EOL>maxtransitduration*periods.min(),<EOL>ndurations)<EOL>if magsarefluxes:<EOL><INDENT>blsmodel = BoxLeastSquares(<EOL>times*u.day,<EOL>mags*u.dimensionless_unscaled,<EOL>dy=errs*u.dimensionless_unscaled<EOL>)<EOL><DEDENT>else:<EOL><INDENT>blsmodel = BoxLeastSquares(<EOL>times*u.day,<EOL>mags*u.mag,<EOL>dy=errs*u.mag<EOL>)<EOL><DEDENT>blsresult = blsmodel.power(<EOL>periods*u.day,<EOL>durations*u.day,<EOL>objective=blsobjective,<EOL>method=blsmethod,<EOL>oversample=blsoversample<EOL>)<EOL>return {<EOL>'<STR_LIT>': blsresult,<EOL>'<STR_LIT>': blsmodel,<EOL>'<STR_LIT>': durations,<EOL>'<STR_LIT>': nparray(blsresult.power)<EOL>}<EOL><DEDENT>except Exception as e:<EOL><INDENT>LOGEXCEPTION('<STR_LIT>' %<EOL>(frequencies[<NUM_LIT:0>], frequencies[-<NUM_LIT:1>]))<EOL>return {<EOL>'<STR_LIT>': None,<EOL>'<STR_LIT>': None,<EOL>'<STR_LIT>': durations,<EOL>'<STR_LIT>': nparray([npnan for x in range(nfreq)]),<EOL>}<EOL><DEDENT>", "docstring": "This wraps Astropy's BoxLeastSquares for use with bls_parallel_pfind below.\n\n`task` is a tuple::\n\n    task[0] = times\n    task[1] = mags\n    task[2] = errs\n    task[3] = magsarefluxes\n\n    task[4] = minfreq\n    task[5] = nfreq\n    task[6] = stepsize\n\n    task[7] = ndurations\n    task[8] = mintransitduration\n    task[9] = maxtransitduration\n\n    task[10] = blsobjective\n    task[11] = blsmethod\n    task[12] = blsoversample", "id": "f14765:m1"}
{"signature": "def aad_cache():", "body": "return jsonpickle.decode(get_config_value('<STR_LIT>', fallback=None)),jsonpickle.decode(get_config_value('<STR_LIT>', fallback=None))<EOL>", "docstring": "AAD token cache.", "id": "f2344:m11"}
{"signature": "def get_trip_points(cur, route_id, offset=<NUM_LIT:0>, tripid_glob='<STR_LIT>'):", "body": "extra_where = '<STR_LIT>'<EOL>if tripid_glob:<EOL><INDENT>extra_where = \"<STR_LIT>\" % tripid_glob<EOL><DEDENT>cur.execute('<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>'<EOL>'<STR_LIT>' % extra_where, (route_id, offset))<EOL>stop_points = [dict(seq=row[<NUM_LIT:0>], lat=row[<NUM_LIT:1>], lon=row[<NUM_LIT:2>]) for row in cur]<EOL>return stop_points<EOL>", "docstring": "Get all scheduled stops on a particular route_id.\n\n    Given a route_id, return the trip-stop-list with\n    latitude/longitudes.  This is a bit more tricky than it seems,\n    because we have to go from table route->trips->stop_times.  This\n    functions finds an arbitrary trip (in trip table) with this route ID\n    and, and then returns all stop points for that trip.\n\n    Parameters\n    ----------\n    cur : sqlite3.Cursor\n        cursor to sqlite3 DB containing GTFS\n    route_id : string or any\n        route_id to get stop points of\n    offset : int\n        LIMIT offset if you don't want the first trip returned.\n    tripid_glob : string\n        If given, allows you to limit tripids which can be selected.\n        Mainly useful in debugging.\n\n    Returns\n    -------\n    stop-list\n        List of stops in stop-seq format.", "id": "f12861:m9"}
{"signature": "def interpolate(self, T, name):", "body": "key = (name, self.interpolation_T, self.interpolation_property, self.interpolation_property_inv)<EOL><INDENT>if isinstance(self.tabular_data_interpolators, dict) and key in self.tabular_data_interpolators:<EOL><INDENT>extrapolator, spline = self.tabular_data_interpolators[key]<EOL><DEDENT><DEDENT>if key in self.tabular_data_interpolators:<EOL><INDENT>extrapolator, spline = self.tabular_data_interpolators[key]<EOL><DEDENT>else:<EOL><INDENT>Ts, properties = self.tabular_data[name]<EOL>if self.interpolation_T:  <EOL><INDENT>Ts2 = [self.interpolation_T(T2) for T2 in Ts]<EOL><DEDENT>else:<EOL><INDENT>Ts2 = Ts<EOL><DEDENT>if self.interpolation_property:  <EOL><INDENT>properties2 = [self.interpolation_property(p) for p in properties]<EOL><DEDENT>else:<EOL><INDENT>properties2 = properties<EOL><DEDENT>extrapolator = interp1d(Ts2, properties2, fill_value='<STR_LIT>')<EOL>if len(properties) >= <NUM_LIT:5>:<EOL><INDENT>spline = interp1d(Ts2, properties2, kind='<STR_LIT>')<EOL><DEDENT>else:<EOL><INDENT>spline = None<EOL>", "docstring": "r'''Method to perform interpolation on a given tabular data set\n        previously added via :obj:`set_tabular_data`. This method will create the\n        interpolators the first time it is used on a property set, and store\n        them for quick future use.\n\n        Interpolation is cubic-spline based if 5 or more points are available,\n        and linearly interpolated if not. Extrapolation is always performed\n        linearly. This function uses the transforms `interpolation_T`,\n        `interpolation_property`, and `interpolation_property_inv` if set. If\n        any of these are changed after the interpolators were first created,\n        new interpolators are created with the new transforms.\n        All interpolation is performed via the `interp1d` function.\n\n        Parameters\n        ----------\n        T : float\n            Temperature at which to interpolate the property, [K]\n        name : str\n            The name assigned to the tabular data set\n\n        Returns\n        -------\n        prop : float\n            Calculated property, [`units`]", "id": "f15806:c0:m9"}
{"signature": "def paintEvent( self, e ):", "body": "<EOL>if DEBUG: print('<STR_LIT>', self,self.get_width_height())<EOL>if type(self.replot) is bool: <EOL><INDENT>if self.replot:<EOL><INDENT>FigureCanvasAgg.draw(self)<EOL><DEDENT>if QtCore.QSysInfo.ByteOrder == QtCore.QSysInfo.LittleEndian:<EOL><INDENT>stringBuffer = self.renderer._renderer.tostring_bgra()<EOL><DEDENT>else:<EOL><INDENT>stringBuffer = self.renderer._renderer.tostring_argb()<EOL><DEDENT>qImage = QtGui.QImage(stringBuffer, self.renderer.width,<EOL>self.renderer.height,<EOL>QtGui.QImage.Format_ARGB32)<EOL>p = QtGui.QPainter(self)<EOL>p.drawPixmap(QtCore.QPoint(<NUM_LIT:0>, <NUM_LIT:0>), QtGui.QPixmap.fromImage(qImage))<EOL>if self.drawRect:<EOL><INDENT>p.setPen( QtGui.QPen( QtCore.Qt.black, <NUM_LIT:1>, QtCore.Qt.DotLine ) )<EOL>p.drawRect( self.rect[<NUM_LIT:0>], self.rect[<NUM_LIT:1>], self.rect[<NUM_LIT:2>], self.rect[<NUM_LIT:3>] )<EOL><DEDENT>p.end()<EOL><DEDENT>else:<EOL><INDENT>bbox = self.replot<EOL>l, b, r, t = bbox.extents<EOL>w = int(r) - int(l)<EOL>h = int(t) - int(b)<EOL>t = int(b) + h<EOL>reg = self.copy_from_bbox(bbox)<EOL>stringBuffer = reg.to_string_argb()<EOL>qImage = QtGui.QImage(stringBuffer, w, h, QtGui.QImage.Format_ARGB32)<EOL>pixmap = QtGui.QPixmap.fromImage(qImage)<EOL>p = QtGui.QPainter( self )<EOL>p.drawPixmap(QtCore.QPoint(l, self.renderer.height-t), pixmap)<EOL>p.end()<EOL><DEDENT>self.replot = False<EOL>self.drawRect = False<EOL>", "docstring": "Draw to the Agg backend and then copy the image to the qt.drawable.\nIn Qt, all drawing should be done inside of here when a widget is\nshown onscreen.", "id": "f17213:c2:m3"}
{"signature": "def derive_private_key(self, sequence):", "body": "encoded = \"<STR_LIT>\" % (str(self), sequence)<EOL>a = bytes(encoded, \"<STR_LIT:ascii>\")<EOL>s = hashlib.sha256(hashlib.sha512(a).digest()).digest()<EOL>return PrivateKey(hexlify(s).decode(\"<STR_LIT:ascii>\"), prefix=self.pubkey.prefix)<EOL>", "docstring": "Derive new private key from this private key and an arbitrary\n            sequence number", "id": "f8268:c5:m7"}
{"signature": "def p_list_0(self, p):", "body": "if DEBUG:<EOL><INDENT>self.print_p(p)<EOL><DEDENT>p[<NUM_LIT:0>] = p[<NUM_LIT:2>]<EOL>", "docstring": "list : LEFTBRACKET listitems RIGHTBRACKET\n     | LEFTBRACKET listitems COMMA RIGHTBRACKET", "id": "f11423:c0:m14"}
{"signature": "def load_ptb_dataset(path='<STR_LIT:data>'):", "body": "path = os.path.join(path, '<STR_LIT>')<EOL>logging.info(\"<STR_LIT>\".format(path))<EOL>filename = '<STR_LIT>'<EOL>url = '<STR_LIT>'<EOL>maybe_download_and_extract(filename, path, url, extract=True)<EOL>data_path = os.path.join(path, '<STR_LIT>', '<STR_LIT:data>')<EOL>train_path = os.path.join(data_path, \"<STR_LIT>\")<EOL>valid_path = os.path.join(data_path, \"<STR_LIT>\")<EOL>test_path = os.path.join(data_path, \"<STR_LIT>\")<EOL>word_to_id = nlp.build_vocab(nlp.read_words(train_path))<EOL>train_data = nlp.words_to_word_ids(nlp.read_words(train_path), word_to_id)<EOL>valid_data = nlp.words_to_word_ids(nlp.read_words(valid_path), word_to_id)<EOL>test_data = nlp.words_to_word_ids(nlp.read_words(test_path), word_to_id)<EOL>vocab_size = len(word_to_id)<EOL>return train_data, valid_data, test_data, vocab_size<EOL>", "docstring": "Load Penn TreeBank (PTB) dataset.\n\n    It is used in many LANGUAGE MODELING papers,\n    including \"Empirical Evaluation and Combination of Advanced Language\n    Modeling Techniques\", \"Recurrent Neural Network Regularization\".\n    It consists of 929k training words, 73k validation words, and 82k test\n    words. It has 10k words in its vocabulary.\n\n    Parameters\n    ----------\n    path : str\n        The path that the data is downloaded to, defaults is ``data/ptb/``.\n\n    Returns\n    --------\n    train_data, valid_data, test_data : list of int\n        The training, validating and testing data in integer format.\n    vocab_size : int\n        The vocabulary size.\n\n    Examples\n    --------\n    >>> train_data, valid_data, test_data, vocab_size = tl.files.load_ptb_dataset()\n\n    References\n    ---------------\n    - ``tensorflow.models.rnn.ptb import reader``\n    - `Manual download <http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz>`__\n\n    Notes\n    ------\n    - If you want to get the raw data, see the source code.", "id": "f11137:m5"}
{"signature": "def __call__(self):", "body": "b=self._base<EOL>vmin, vmax = self.axis.get_view_interval()<EOL>if vmin <= <NUM_LIT:0.0>:<EOL><INDENT>vmin = self.axis.get_minpos()<EOL>if vmin <= <NUM_LIT:0.0>:<EOL><INDENT>raise ValueError(<EOL>\"<STR_LIT>\")<EOL><DEDENT><DEDENT>vmin = math.log(vmin)/math.log(b)<EOL>vmax = math.log(vmax)/math.log(b)<EOL>if vmax<vmin:<EOL><INDENT>vmin, vmax = vmax, vmin<EOL><DEDENT>numdec = math.floor(vmax)-math.ceil(vmin)<EOL>if self._subs is None: <EOL><INDENT>if numdec><NUM_LIT:10>: subs = np.array([<NUM_LIT:1.0>])<EOL>elif numdec><NUM_LIT:6>: subs = np.arange(<NUM_LIT>, b, <NUM_LIT>)<EOL>else: subs = np.arange(<NUM_LIT>, b)<EOL><DEDENT>else:<EOL><INDENT>subs = self._subs<EOL><DEDENT>stride = <NUM_LIT:1><EOL>while numdec/stride+<NUM_LIT:1> > self.numticks:<EOL><INDENT>stride += <NUM_LIT:1><EOL><DEDENT>decades = np.arange(math.floor(vmin),<EOL>math.ceil(vmax)+stride, stride)<EOL>if len(subs) > <NUM_LIT:1> or (len(subs == <NUM_LIT:1>) and subs[<NUM_LIT:0>] != <NUM_LIT:1.0>):<EOL><INDENT>ticklocs = []<EOL>for decadeStart in b**decades:<EOL><INDENT>ticklocs.extend( subs*decadeStart )<EOL><DEDENT><DEDENT>else:<EOL><INDENT>ticklocs = b**decades<EOL><DEDENT>return np.array(ticklocs)<EOL>", "docstring": "Return the locations of the ticks", "id": "f17239:c19:m4"}
{"signature": "def value(self, state):", "body": "abstract<EOL>", "docstring": "For optimization problems, each state has a value.  Hill-climbing\n        and related algorithms try to maximize this value.", "id": "f1675:c0:m5"}
{"signature": "@property<EOL><INDENT>def infos(self):<DEDENT>", "body": "data = {<EOL>\"<STR_LIT>\": {<EOL>\"<STR_LIT:description>\": self.meta_description,<EOL>\"<STR_LIT>\": self.meta_lang,<EOL>\"<STR_LIT>\": self.meta_keywords,<EOL>\"<STR_LIT>\": self.meta_favicon,<EOL>\"<STR_LIT>\": self.canonical_link,<EOL>\"<STR_LIT>\": self.meta_encoding<EOL>},<EOL>\"<STR_LIT:image>\": None,<EOL>\"<STR_LIT>\": self.domain,<EOL>\"<STR_LIT:title>\": self.title,<EOL>\"<STR_LIT>\": self.cleaned_text,<EOL>\"<STR_LIT>\": self.opengraph,<EOL>\"<STR_LIT>\": self.tags,<EOL>\"<STR_LIT>\": self.tweets,<EOL>\"<STR_LIT>\": [],<EOL>\"<STR_LIT>\": self.links,<EOL>\"<STR_LIT>\": self.authors,<EOL>\"<STR_LIT>\": self.publish_date<EOL>}<EOL>if self.top_image is not None:<EOL><INDENT>data['<STR_LIT:image>'] = {<EOL>'<STR_LIT:url>': self.top_image.src,<EOL>'<STR_LIT:width>': self.top_image.width,<EOL>'<STR_LIT>': self.top_image.height,<EOL>'<STR_LIT:type>': '<STR_LIT:image>'<EOL>}<EOL><DEDENT>for movie in self.movies:<EOL><INDENT>data['<STR_LIT>'].append({<EOL>'<STR_LIT>': movie.embed_type,<EOL>'<STR_LIT>': movie.provider,<EOL>'<STR_LIT:width>': movie.width,<EOL>'<STR_LIT>': movie.height,<EOL>'<STR_LIT>': movie.embed_code,<EOL>'<STR_LIT:src>': movie.src,<EOL>})<EOL><DEDENT>return data<EOL>", "docstring": "dict: The summation of all data available about the extracted article\n\n            Note:\n                Read only", "id": "f14091:c0:m27"}
{"signature": "def __init__(self, took=None, total_num_hits=None, max_score=None, hits=None, **kwargs):", "body": "super(DatasetSearchResult, self).__init__(<EOL>took=took, total_num_hits=total_num_hits, max_score=max_score,<EOL>hits=self._get_object(DatasetSearchHit, hits), **kwargs)<EOL>", "docstring": "Constructor.\n\n:param took: Number of milliseconds that the query took to execute.\n:param total_num_hits: Total number of hits.\n:param max_score: The maximum score.\n:param hits: List of :class:`DatasetSearchHit` objects.", "id": "f3520:c0:m0"}
{"signature": "@blog.command()<EOL>@click.pass_context<EOL>def preview(context):", "body": "config = context.obj<EOL>pelican(config, '<STR_LIT>', '<STR_LIT>')<EOL>server_proc = None<EOL>os.chdir(config['<STR_LIT>'])<EOL>try:<EOL><INDENT>try:<EOL><INDENT>command = '<STR_LIT>' + str(PORT)<EOL>server_proc = run(command, bg=True)<EOL>time.sleep(<NUM_LIT:3>)<EOL>click.launch('<STR_LIT>')<EOL>time.sleep(<NUM_LIT:5>)<EOL>pelican(config, '<STR_LIT>')<EOL><DEDENT>except Exception:<EOL><INDENT>if server_proc is not None:<EOL><INDENT>server_proc.kill()<EOL><DEDENT>raise<EOL><DEDENT><DEDENT>except KeyboardInterrupt:<EOL><INDENT>abort(context)<EOL><DEDENT>", "docstring": "Opens local preview of your blog website", "id": "f1830:m0"}
{"signature": "def simulate_static(self, steps, time, solution = solve_type.FAST, collect_dynamic = False):", "body": "if (self._ccore_network_pointer is not None):<EOL><INDENT>ccore_instance_dynamic = wrapper.sync_simulate_static(self._ccore_network_pointer, steps, time, solution, collect_dynamic);<EOL>return sync_dynamic(None, None, ccore_instance_dynamic);<EOL><DEDENT>dyn_phase = [];<EOL>dyn_time = [];<EOL>if (collect_dynamic == True):<EOL><INDENT>dyn_phase.append(self._phases);<EOL>dyn_time.append(<NUM_LIT:0>);<EOL><DEDENT>step = time / steps;<EOL>int_step = step / <NUM_LIT>;<EOL>for t in numpy.arange(step, time + step, step):<EOL><INDENT>self._phases = self._calculate_phases(solution, t, step, int_step);<EOL>if (collect_dynamic == True):<EOL><INDENT>dyn_phase.append(self._phases);<EOL>dyn_time.append(t);<EOL><DEDENT><DEDENT>if (collect_dynamic != True):<EOL><INDENT>dyn_phase.append(self._phases);<EOL>dyn_time.append(time);<EOL><DEDENT>output_sync_dynamic = sync_dynamic(dyn_phase, dyn_time);<EOL>return output_sync_dynamic<EOL>", "docstring": "!\n        @brief Performs static simulation of oscillatory network.\n\n        @param[in] steps (uint): Number steps of simulations during simulation.\n        @param[in] time (double): Time of simulation.\n        @param[in] solution (solve_type): Type of solution.\n        @param[in] collect_dynamic (bool): If True - returns whole dynamic of oscillatory network, otherwise returns only last values of dynamics.\n\n        @return (list) Dynamic of oscillatory network. If argument 'collect_dynamic' = True, than return dynamic for the whole simulation time,\n                otherwise returns only last values (last step of simulation) of dynamic.\n\n        @see simulate()\n        @see simulate_dynamic()", "id": "f15630:c3:m7"}
{"signature": "def get_tracking_clans(self, **params: keys):", "body": "url = self.api.CLAN + '<STR_LIT>'<EOL>return self._get_model(url, **params)<EOL>", "docstring": "Get a list of clans that are being\n        tracked by having either cr-api.com or\n        royaleapi.com in the description\n\n        Parameters\n        ----------\n        \\*\\*keys: Optional[list] = None\n            Filter which keys should be included in the\n            response\n        \\*\\*exclude: Optional[list] = None\n            Filter which keys should be excluded from the\n            response\n        \\*\\*max: Optional[int] = None\n            Limit the number of items returned in the response\n        \\*\\*page: Optional[int] = None\n            Works with max, the zero-based page of the\n            items\n        \\*\\*timeout: Optional[int] = None\n            Custom timeout that overwrites Client.timeout", "id": "f10347:c0:m25"}
{"signature": "def sccs_bit_sync(y,Ns):", "body": "<EOL>rx_symb_d = np.zeros(int(np.fix(len(y)/Ns)))<EOL>track = np.zeros(int(np.fix(len(y)/Ns)))<EOL>bit_count = -<NUM_LIT:1><EOL>y_abs = np.zeros(len(y))<EOL>clk = np.zeros(len(y))<EOL>k = Ns+<NUM_LIT:1> <EOL>for i in range(len(y)):<EOL><INDENT>if i >= Ns: <EOL><INDENT>y_abs[i] = np.abs(np.sum(y[i-Ns+<NUM_LIT:1>:i+<NUM_LIT:1>]))<EOL>if (k == <NUM_LIT:0>):<EOL><INDENT>w_hat = y_abs[i-<NUM_LIT:2>:i+<NUM_LIT:1>]<EOL>bit_count += <NUM_LIT:1><EOL>if w_hat[<NUM_LIT:1>] != <NUM_LIT:0>:<EOL><INDENT>if w_hat[<NUM_LIT:0>] < w_hat[<NUM_LIT:2>]:<EOL><INDENT>k = Ns-<NUM_LIT:1><EOL>clk[i-<NUM_LIT:2>] = <NUM_LIT:1><EOL>rx_symb_d[bit_count] = y[i-<NUM_LIT:2>-int(np.round(Ns/<NUM_LIT:2>))-<NUM_LIT:1>]<EOL><DEDENT>elif w_hat[<NUM_LIT:0>] > w_hat[<NUM_LIT:2>]:<EOL><INDENT>k = Ns+<NUM_LIT:1><EOL>clk[i] = <NUM_LIT:1><EOL>rx_symb_d[bit_count] = y[i-int(np.round(Ns/<NUM_LIT:2>))-<NUM_LIT:1>]<EOL><DEDENT>else:<EOL><INDENT>k = Ns<EOL>clk[i-<NUM_LIT:1>] = <NUM_LIT:1><EOL>rx_symb_d[bit_count] = y[i-<NUM_LIT:1>-int(np.round(Ns/<NUM_LIT:2>))-<NUM_LIT:1>]<EOL><DEDENT><DEDENT>else:<EOL><INDENT>k = Ns<EOL>clk[i-<NUM_LIT:1>] = <NUM_LIT:1><EOL>rx_symb_d[bit_count] = y[i-<NUM_LIT:1>-int(np.round(Ns/<NUM_LIT:2>))]<EOL><DEDENT>track[bit_count] = np.mod(i,Ns)<EOL><DEDENT><DEDENT>k -= <NUM_LIT:1><EOL><DEDENT>rx_symb_d = rx_symb_d[:bit_count]<EOL>return rx_symb_d, clk, track<EOL>", "docstring": "rx_symb_d,clk,track = sccs_bit_sync(y,Ns)\n\n//////////////////////////////////////////////////////\n Symbol synchronization algorithm using SCCS\n//////////////////////////////////////////////////////\n     y = baseband NRZ data waveform\n    Ns = nominal number of samples per symbol\n\nReworked from ECE 5675 Project\nTranslated from m-code version\nMark Wickert April 2014", "id": "f14895:m6"}
{"signature": "@property<EOL><INDENT>def parse_headers(self):<DEDENT>", "body": "return self._parse_headers<EOL>", "docstring": "bool: Specify if headers should be pulled or not in the cleaned_text\n            output\n\n            Note:\n                Defaults to `True`", "id": "f14092:c3:m44"}
{"signature": "def get_medians(self):", "body": "return self.__medians<EOL>", "docstring": "!\n        @brief Returns list of centers of allocated clusters.\n\n        @see process()\n        @see get_clusters()", "id": "f15586:c0:m3"}
{"signature": "def __getitem__(self, stream_id):", "body": "if stream_id not in self.get_out_streamids():<EOL><INDENT>raise ValueError(\"<STR_LIT>\" % stream_id)<EOL><DEDENT>component_id = self.name or self<EOL>return GlobalStreamId(componentId=component_id, streamId=stream_id)<EOL>", "docstring": "Get GlobalStreamId for a given stream_id", "id": "f7234:c0:m13"}
{"signature": "def domain_name():", "body": "result = random.choice(get_dictionary('<STR_LIT>')).strip()<EOL>result += '<STR_LIT:.>' + top_level_domain()<EOL>return result.lower()<EOL>", "docstring": "Return a random domain name.\n\n    Lowercased result of :py:func:`~forgery_py.forgery.name.company_name()`\n    plus :py:func:`~top_level_domain()`.", "id": "f14649:m2"}
{"signature": "def get_evidences_by_pmid(graph: BELGraph, pmids: Union[str, Iterable[str]]):", "body": "result = defaultdict(set)<EOL>for _, _, _, data in filter_edges(graph, build_pmid_inclusion_filter(pmids)):<EOL><INDENT>result[data[CITATION][CITATION_REFERENCE]].add(data[EVIDENCE])<EOL><DEDENT>return dict(result)<EOL>", "docstring": "Get a dictionary from the given PubMed identifiers to the sets of all evidence strings associated with each\n    in the graph.\n\n    :param graph: A BEL graph\n    :param pmids: An iterable of PubMed identifiers, as strings. Is consumed and converted to a set.\n    :return: A dictionary of {pmid: set of all evidence strings}\n    :rtype: dict", "id": "f9403:m12"}
{"signature": "def error(self, text):", "body": "msg = '<STR_LIT>'.format(<EOL>self._dev, self._spi_speed, text)<EOL>log.error(msg)<EOL>raise IOError(msg)<EOL>", "docstring": "SHOULD BE PRIVATE", "id": "f2108:c0:m3"}
{"signature": "def as_xml(self, parent = None):", "body": "if parent is not None:<EOL><INDENT>element = ElementTree.SubElement(parent, ITEM_TAG)<EOL><DEDENT>else:<EOL><INDENT>element = ElementTree.Element(ITEM_TAG)<EOL><DEDENT>element.set(\"<STR_LIT>\", str(self.jid))<EOL>if self.name is not None:<EOL><INDENT>element.set(\"<STR_LIT:name>\", self.name)<EOL><DEDENT>if self.subscription is not None:<EOL><INDENT>element.set(\"<STR_LIT>\", self.subscription)<EOL><DEDENT>if self.ask:<EOL><INDENT>element.set(\"<STR_LIT>\", self.ask)<EOL><DEDENT>if self.approved:<EOL><INDENT>element.set(\"<STR_LIT>\", \"<STR_LIT:true>\")<EOL><DEDENT>for group in self.groups:<EOL><INDENT>ElementTree.SubElement(element, GROUP_TAG).text = group<EOL><DEDENT>return element<EOL>", "docstring": "Make an XML element from self.\n\n        :Parameters:\n            - `parent`: Parent element\n        :Types:\n            - `parent`: :etree:`ElementTree.Element`", "id": "f15245:c3:m2"}
{"signature": "def _cast(self, out, peek=None):", "body": "<EOL>if not out:<EOL><INDENT>if '<STR_LIT>' not in response:<EOL><INDENT>response['<STR_LIT>'] = <NUM_LIT:0><EOL><DEDENT>return []<EOL><DEDENT>if isinstance(out, (tuple, list))and isinstance(out[<NUM_LIT:0>], (bytes, unicode)):<EOL><INDENT>out = out[<NUM_LIT:0>][<NUM_LIT:0>:<NUM_LIT:0>].join(out)  <EOL><DEDENT>if isinstance(out, unicode):<EOL><INDENT>out = out.encode(response.charset)<EOL><DEDENT>if isinstance(out, bytes):<EOL><INDENT>if '<STR_LIT>' not in response:<EOL><INDENT>response['<STR_LIT>'] = len(out)<EOL><DEDENT>return [out]<EOL><DEDENT>if isinstance(out, HTTPError):<EOL><INDENT>out.apply(response)<EOL>out = self.error_handler.get(out.status_code,<EOL>self.default_error_handler)(out)<EOL>return self._cast(out)<EOL><DEDENT>if isinstance(out, HTTPResponse):<EOL><INDENT>out.apply(response)<EOL>return self._cast(out.body)<EOL><DEDENT>if hasattr(out, '<STR_LIT>'):<EOL><INDENT>if '<STR_LIT>' in request.environ:<EOL><INDENT>return request.environ['<STR_LIT>'](out)<EOL><DEDENT>elif hasattr(out, '<STR_LIT>') or not hasattr(out, '<STR_LIT>'):<EOL><INDENT>return WSGIFileWrapper(out)<EOL><DEDENT><DEDENT>try:<EOL><INDENT>iout = iter(out)<EOL>first = next(iout)<EOL>while not first:<EOL><INDENT>first = next(iout)<EOL><DEDENT><DEDENT>except StopIteration:<EOL><INDENT>return self._cast('<STR_LIT>')<EOL><DEDENT>except HTTPResponse:<EOL><INDENT>first = _e()<EOL><DEDENT>except (KeyboardInterrupt, SystemExit, MemoryError):<EOL><INDENT>raise<EOL><DEDENT>except:<EOL><INDENT>if not self.catchall: raise<EOL>first = HTTPError(<NUM_LIT>, '<STR_LIT>', _e(), format_exc())<EOL><DEDENT>if isinstance(first, HTTPResponse):<EOL><INDENT>return self._cast(first)<EOL><DEDENT>elif isinstance(first, bytes):<EOL><INDENT>new_iter = itertools.chain([first], iout)<EOL><DEDENT>elif isinstance(first, unicode):<EOL><INDENT>encoder = lambda x: x.encode(response.charset)<EOL>new_iter = imap(encoder, itertools.chain([first], iout))<EOL><DEDENT>else:<EOL><INDENT>msg = '<STR_LIT>' % type(first)<EOL>return self._cast(HTTPError(<NUM_LIT>, msg))<EOL><DEDENT>if hasattr(out, '<STR_LIT>'):<EOL><INDENT>new_iter = _closeiter(new_iter, out.close)<EOL><DEDENT>return new_iter<EOL>", "docstring": "Try to convert the parameter into something WSGI compatible and set\n        correct HTTP headers when possible.\n        Support: False, str, unicode, dict, HTTPResponse, HTTPError, file-like,\n        iterable of strings and iterable of unicodes", "id": "f12971:c11:m25"}
{"signature": "def _on_complete_hook(self, my_task):", "body": "<EOL>for child in my_task.children:<EOL><INDENT>child.task_spec._update(child)<EOL><DEDENT>", "docstring": "A hook into _on_complete() that does the task specific work.\n\n:type  my_task: Task\n:param my_task: The associated task in the task tree.\n:rtype:  bool\n:returns: True on success, False otherwise.", "id": "f7760:c0:m20"}
{"signature": "def LOOP(cpu, dest):", "body": "counter_name = {<NUM_LIT:16>: '<STR_LIT>', <NUM_LIT:32>: '<STR_LIT>', <NUM_LIT:64>: '<STR_LIT>'}[cpu.address_bit_size]<EOL>counter = cpu.write_register(counter_name, cpu.read_register(counter_name) - <NUM_LIT:1>)<EOL>cpu.PC = Operators.ITEBV(cpu.address_bit_size, counter == <NUM_LIT:0>, (cpu.PC + dest.read()) & ((<NUM_LIT:1> << dest.size) - <NUM_LIT:1>), cpu.PC + cpu.instruction.size)<EOL>", "docstring": "Loops according to ECX counter.\n\nPerforms a loop operation using the ECX or CX register as a counter.\nEach time the LOOP instruction is executed, the count register is decremented,\nthen checked for 0. If the count is 0, the loop is terminated and program\nexecution continues with the instruction following the LOOP instruction.\nIf the count is not zero, a near jump is performed to the destination\n(target) operand, which is presumably the instruction at the beginning\nof the loop. If the address-size attribute is 32 bits, the ECX register\nis used as the count register; otherwise the CX register is used::\n\n        IF address_bit_size  =  32\n        THEN\n            Count is ECX;\n        ELSE (* address_bit_size  =  16 *)\n            Count is CX;\n        FI;\n        Count  =  Count - 1;\n\n        IF (Count  0)  =  1\n        THEN\n            EIP  =  EIP + SignExtend(DEST);\n            IF OperandSize  =  16\n            THEN\n                EIP  =  EIP AND 0000FFFFH;\n            FI;\n        ELSE\n            Terminate loop and continue program execution at EIP;\n        FI;\n\n:param cpu: current CPU.\n:param dest: destination operand.", "id": "f16975:c2:m149"}
{"signature": "def set_xscale(self, value, **kwargs):", "body": "self.xaxis.set_scale(value, **kwargs)<EOL>self.autoscale_view()<EOL>self._update_transScale()<EOL>", "docstring": "call signature::\n\n  set_xscale(value)\n\nSet the scaling of the x-axis: %(scale)s\n\nACCEPTS: [%(scale)s]\n\nDifferent kwargs are accepted, depending on the scale:\n%(scale_docs)s", "id": "f17238:c1:m82"}
{"signature": "def db_connect(method):", "body": "@functools.wraps(method)<EOL>def wrapped(self, *args, **kwargs):<EOL><INDENT>connect = db_connection_factory()<EOL>with connect() as db_connection:<EOL><INDENT>with db_connection.cursor() as cursor:<EOL><INDENT>return method(self, cursor, *args, **kwargs)<EOL><DEDENT><DEDENT><DEDENT>return wrapped<EOL>", "docstring": "Decorator for methods that need to use the database\n\n    Example:\n    @db_connect\n    def setUp(self, cursor):\n        cursor.execute(some_sql)\n        # some other code", "id": "f15186:m3"}
{"signature": "def fit(self, X, y=None):", "body": "self.kmeans_fit_ = copy(self.kmeans)<EOL>X = as_features(X, stack=True)<EOL>self.kmeans_fit_.fit(X.stacked_features) <EOL>return self<EOL>", "docstring": "Choose the codewords based on a training set.\n\nParameters\n----------\nX : :class:`skl_groups.features.Features` or list of arrays of shape ``[n_samples[i], n_features]``\n    Training set. If a Features object, it will be stacked.", "id": "f14628:c0:m5"}
{"signature": "def find_swig():", "body": "for executable in (\"<STR_LIT>\", \"<STR_LIT>\"):<EOL><INDENT>if find_executable(executable):<EOL><INDENT>return executable<EOL><DEDENT><DEDENT>raise Exception(\"<STR_LIT>\")<EOL>", "docstring": "Find SWIG executable path", "id": "f4967:m3"}
{"signature": "def set(self, model, value):", "body": "self.validate(value)<EOL>self._pop(model)<EOL>value = self.serialize(value)<EOL>model.tags.append(value)<EOL>", "docstring": "Set tag on model object.", "id": "f10743:c1:m5"}
{"signature": "def parse_colors(sequence):", "body": "return '<STR_LIT>'.join(escape_codes[n] for n in sequence.split('<STR_LIT:U+002C>') if n)<EOL>", "docstring": "Return escape codes from a color sequence.", "id": "f10290:m1"}
{"signature": "def setBoostStrength(self, boostStrength):", "body": "self._boostStrength = boostStrength<EOL>", "docstring": "Sets the maximum boost value.\n:param boostStrength: (float) value to set", "id": "f17561:c4:m22"}
{"signature": "def simulate(self, n=<NUM_LIT:1>, t=None):", "body": "if not self._initialized:<EOL><INDENT>msg = (\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL>raise QueueingToolError(msg)<EOL><DEDENT>if t is None:<EOL><INDENT>for dummy in range(n):<EOL><INDENT>self._simulate_next_event(slow=False)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>now = self._t<EOL>while self._t < now + t:<EOL><INDENT>self._simulate_next_event(slow=False)<EOL><DEDENT><DEDENT>", "docstring": "Simulates the network forward.\n\n        Simulates either a specific number of events or for a specified\n        amount of simulation time.\n\n        Parameters\n        ----------\n        n : int (optional, default: 1)\n            The number of events to simulate. If ``t`` is not given\n            then this parameter is used.\n        t : float (optional)\n            The amount of simulation time to simulate forward. If\n            given, ``t`` is used instead of ``n``.\n\n        Raises\n        ------\n        QueueingToolError\n            Will raise a :exc:`.QueueingToolError` if the\n            ``QueueNetwork`` has not been initialized. Call\n            :meth:`.initialize` before calling this method.\n\n        Examples\n        --------\n        Let ``net`` denote your instance of a ``QueueNetwork``. Before\n        you simulate, you need to initialize the network, which allows\n        arrivals from outside the network. To initialize with 2 (random\n        chosen) edges accepting arrivals run:\n\n        >>> import queueing_tool as qt\n        >>> g = qt.generate_pagerank_graph(100, seed=50)\n        >>> net = qt.QueueNetwork(g, seed=50)\n        >>> net.initialize(2)\n\n        To simulate the network 50000 events run:\n\n        >>> net.num_events\n        0\n        >>> net.simulate(50000)\n        >>> net.num_events\n        50000\n\n        To simulate the network for at least 75 simulation time units\n        run:\n\n        >>> t0 = net.current_time\n        >>> net.simulate(t=75)\n        >>> t1 = net.current_time\n        >>> t1 - t0 # doctest: +ELLIPSIS\n        75...", "id": "f14533:c1:m22"}
{"signature": "def cmd_partition(opts):", "body": "config = load_config(opts.config)<EOL>b = get_blockade(config, opts)<EOL>if opts.random:<EOL><INDENT>if opts.partitions:<EOL><INDENT>raise BlockadeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL><DEDENT>b.random_partition()<EOL><DEDENT>else:<EOL><INDENT>partitions = []<EOL>for partition in opts.partitions:<EOL><INDENT>names = []<EOL>for name in partition.split(\"<STR_LIT:U+002C>\"):<EOL><INDENT>name = name.strip()<EOL>if name:<EOL><INDENT>names.append(name)<EOL><DEDENT><DEDENT>partitions.append(names)<EOL><DEDENT>if not partitions:<EOL><INDENT>raise BlockadeError(\"<STR_LIT>\"<EOL>\"<STR_LIT>\")<EOL><DEDENT>b.partition(partitions)<EOL><DEDENT>", "docstring": "Partition the network between containers\n\n    Replaces any existing partitions outright. Any containers NOT specified\n    in arguments will be globbed into a single implicit partition. For\n    example if you have three containers: c1, c2, and c3 and you run:\n\n        blockade partition c1\n\n    The result will be a partition with just c1 and another partition with\n    c2 and c3.\n\n    Alternatively, --random may be specified, and zero or more random\n    partitions will be generated by blockade.", "id": "f342:m20"}
{"signature": "def poll(self):", "body": "assert(len(self.events_scan) != <NUM_LIT:0>)<EOL>p, events_current = self.events_scan.pop_min()<EOL>return p, events_current<EOL>", "docstring": "Get, and remove, the first (lowest) item from this queue.\n\n:return: the first (lowest) item from this queue.\n:rtype: Point, Event pair.", "id": "f16268:c2:m2"}
{"signature": "def handle(self, *args, **options):", "body": "self.stdout.write('<STR_LIT>')<EOL>for model in get_registered_model():<EOL><INDENT>if options.get('<STR_LIT>', None) and not (model.__name__ in<EOL>options['<STR_LIT>']):<EOL><INDENT>continue<EOL><DEDENT>clear_index(model)<EOL>self.stdout.write('<STR_LIT>'.format(model.__name__))<EOL><DEDENT>", "docstring": "Run the management command.", "id": "f11859:c0:m1"}
{"signature": "def _getTopDownMapping(self):", "body": "<EOL>if self._topDownMappingM is None:<EOL><INDENT>if self.periodic:<EOL><INDENT>self._topDownValues = numpy.arange(self.minval + self.resolution / <NUM_LIT>,<EOL>self.maxval,<EOL>self.resolution)<EOL><DEDENT>else:<EOL><INDENT>self._topDownValues = numpy.arange(self.minval,<EOL>self.maxval + self.resolution / <NUM_LIT>,<EOL>self.resolution)<EOL><DEDENT>numCategories = len(self._topDownValues)<EOL>self._topDownMappingM = SM32(numCategories, self.n)<EOL>outputSpace = numpy.zeros(self.n, dtype=GetNTAReal())<EOL>for i in range(numCategories):<EOL><INDENT>value = self._topDownValues[i]<EOL>value = max(value, self.minval)<EOL>value = min(value, self.maxval)<EOL>self.encodeIntoArray(value, outputSpace, learn=False)<EOL>self._topDownMappingM.setRowFromDense(i, outputSpace)<EOL><DEDENT><DEDENT>return self._topDownMappingM<EOL>", "docstring": "Return the interal _topDownMappingM matrix used for handling the\n        bucketInfo() and topDownCompute() methods. This is a matrix, one row per\n        category (bucket) where each row contains the encoded output for that\n        category.", "id": "f17548:c0:m12"}
{"signature": "def plot_origin(array, origin, units, kpc_per_arcsec, zoom_offset_arcsec):", "body": "if origin is not None:<EOL><INDENT>origin_grid = np.asarray(origin)<EOL>if zoom_offset_arcsec is not None:<EOL><INDENT>origin_grid -= zoom_offset_arcsec<EOL><DEDENT>origin_units = convert_grid_units(array=array, grid_arcsec=origin_grid, units=units,<EOL>kpc_per_arcsec=kpc_per_arcsec)<EOL>plt.scatter(y=origin_units[<NUM_LIT:0>], x=origin_units[<NUM_LIT:1>], s=<NUM_LIT>, c='<STR_LIT:k>', marker='<STR_LIT:x>')<EOL><DEDENT>", "docstring": "Plot the (y,x) origin ofo the array's coordinates as a 'x'.\n\n    Parameters\n    -----------\n    array : data.array.scaled_array.ScaledArray\n        The 2D array of data which is plotted.\n    origin : (float, float).\n        The origin of the coordinate system of the array, which is plotted as an 'x' on the image if input.\n    units : str\n        The units of the y / x axis of the plots, in arc-seconds ('arcsec') or kiloparsecs ('kpc').\n    kpc_per_arcsec : float or None\n        The conversion factor between arc-seconds and kiloparsecs, required to plot the units in kpc.", "id": "f5985:m8"}
{"signature": "def initiate(self, transport, to = None):", "body": "if to is None:<EOL><INDENT>to = JID(self.me.domain)<EOL><DEDENT>return StreamBase.initiate(self, transport, to)<EOL>", "docstring": "Initiate an XMPP connection over the `transport`.\n\n        :Parameters:\n            - `transport`: an XMPP transport instance\n            - `to`: peer name (defaults to own jid domain part)", "id": "f15309:c0:m1"}
{"signature": "def roi_pooling(input, rois, pool_height, pool_width):", "body": "<EOL>out = roi_pooling_module.roi_pooling(input, rois, pool_height=pool_height, pool_width=pool_width)<EOL>output, argmax_output = out[<NUM_LIT:0>], out[<NUM_LIT:1>]<EOL>return output<EOL>", "docstring": "returns a tensorflow operation for computing the Region of Interest Pooling\n\n@arg input: feature maps on which to perform the pooling operation\n@arg rois: list of regions of interest in the format (feature map index, upper left, bottom right)\n@arg pool_width: size of the pooling sections", "id": "f11142:m0"}
{"signature": "def __init__(self, shell):", "body": "super(OctaveMagics, self).__init__(shell)<EOL>self._oct = oct2py.octave<EOL>self._display = display<EOL>", "docstring": "Parameters\n----------\nshell : IPython shell", "id": "f418:c0:m0"}
{"signature": "def debug(*args, **kwargs):", "body": "if not (DEBUG and args):<EOL><INDENT>return None<EOL><DEDENT>parent = kwargs.get('<STR_LIT>', None)<EOL>with suppress(KeyError):<EOL><INDENT>kwargs.pop('<STR_LIT>')<EOL><DEDENT>backlevel = kwargs.get('<STR_LIT>', <NUM_LIT:1>)<EOL>with suppress(KeyError):<EOL><INDENT>kwargs.pop('<STR_LIT>')<EOL><DEDENT>frame = inspect.currentframe()<EOL>while backlevel > <NUM_LIT:0>:<EOL><INDENT>frame = frame.f_back<EOL>backlevel -= <NUM_LIT:1><EOL><DEDENT>fname = os.path.split(frame.f_code.co_filename)[-<NUM_LIT:1>]<EOL>lineno = frame.f_lineno<EOL>if parent:<EOL><INDENT>func = '<STR_LIT>'.format(parent.__class__.__name__, frame.f_code.co_name)<EOL><DEDENT>else:<EOL><INDENT>func = frame.f_code.co_name<EOL><DEDENT>lineinfo = '<STR_LIT>'.format(<EOL>C(fname, '<STR_LIT>'),<EOL>C(str(lineno).ljust(<NUM_LIT:4>), '<STR_LIT>'),<EOL>C().join(C(func, '<STR_LIT>'), '<STR_LIT>').ljust(<NUM_LIT:20>)<EOL>)<EOL>pargs = list(C(a, '<STR_LIT>').str() for a in args)<EOL>pargs[<NUM_LIT:0>] = '<STR_LIT>'.join((lineinfo, pargs[<NUM_LIT:0>]))<EOL>print_err(*pargs, **kwargs)<EOL>", "docstring": "Print a message only if DEBUG is truthy.", "id": "f9614:m1"}
{"signature": "def _merge_config(self, config, templates):", "body": "if not templates:<EOL><INDENT>return config<EOL><DEDENT>if not isinstance(templates, list):<EOL><INDENT>raise TypeError('<STR_LIT>')<EOL><DEDENT>result = {}<EOL>config_list = templates + [config]<EOL>for merging in config_list:<EOL><INDENT>result = merge_config(result, self._load(merging), self.list_identifiers)<EOL><DEDENT>return result<EOL>", "docstring": "Merges config with templates", "id": "f11894:c0:m2"}
{"signature": "def __getitem__(self, index):", "body": "return self._row[index]<EOL>", "docstring": "Retrieve the row at index.", "id": "f3227:c0:m5"}
{"signature": "def GetSelectionPattern(self) -> SelectionPattern:", "body": "return self.GetPattern(PatternId.SelectionPattern)<EOL>", "docstring": "Return `SelectionPattern` if it supports the pattern else None(Conditional support according to MSDN).", "id": "f1782:c118:m2"}
{"signature": "def __ror__(self, other):", "body": "return np.bitwise_or(other, self)<EOL>", "docstring": "x.__ror__(y) <==> y|x", "id": "f4853:c1:m49"}
{"signature": "def _new_pattern_collection(self):", "body": "return self._spec.new_pattern_collection()<EOL>", "docstring": "Create a new pattern collection.\n\n        :return: a new specified pattern collection for\n          :meth:`knitting_pattern_set`", "id": "f560:c1:m9"}
{"signature": "@property<EOL><INDENT>def connection(self) -> str:<DEDENT>", "body": "return str(self.engine.url)<EOL>", "docstring": "Return this manager's connection string.", "id": "f1578:c0:m1"}
{"signature": "@classmethod<EOL><INDENT>def load(klass, filename, inject_env=True):<DEDENT>", "body": "p = PipfileParser(filename=filename)<EOL>pipfile = klass(filename=filename)<EOL>pipfile.data = p.parse(inject_env=inject_env)<EOL>return pipfile<EOL>", "docstring": "Load a Pipfile from a given filename.", "id": "f10:c1:m2"}
{"signature": "@yaz.task(choice__choices=[\"<STR_LIT:yes>\", \"<STR_LIT>\", \"<STR_LIT>\"])<EOL><INDENT>def required_choice(self, choice):<DEDENT>", "body": "return self.choices[choice]<EOL>", "docstring": "This is the documentation for the required_choice task", "id": "f5613:c0:m0"}
{"signature": "@property<EOL><INDENT>def xp(self):<DEDENT>", "body": "return self._xp<EOL>", "docstring": "Particle coordinates :math:`x'`.", "id": "f12082:c0:m7"}
{"signature": "@classmethod<EOL><INDENT>async def get_session(cls, view):<DEDENT>", "body": "session = cls(view)<EOL>session.key = await session.get_key()<EOL>session._data = await session.load() or {}<EOL>return session<EOL>", "docstring": "Every request have a session instance\n:param view:\n:return:", "id": "f10992:c0:m9"}
{"signature": "def _unicode(string):", "body": "for encoding in ['<STR_LIT:utf-8>', '<STR_LIT>']:<EOL><INDENT>try:<EOL><INDENT>result = unicode(string, encoding)<EOL>return result<EOL><DEDENT>except UnicodeDecodeError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT>result = unicode(string, '<STR_LIT:utf-8>', '<STR_LIT:replace>')<EOL>return result<EOL>", "docstring": "Try to convert a string to unicode using different encodings", "id": "f1791:m2"}
{"signature": "def initialize(self, tracker):", "body": "self.tracker = tracker<EOL>", "docstring": "initialize", "id": "f7328:c0:m0"}
{"signature": "def prepare_allele_name(self, allele_name):", "body": "return allele_name.replace(\"<STR_LIT:*>\", \"<STR_LIT>\")<EOL>", "docstring": "How does the predictor expect to see allele names?", "id": "f934:c0:m2"}
{"signature": "def fail(self, msg=None):", "body": "raise self.failureException(msg)<EOL>", "docstring": "Fail immediately, with the given message.", "id": "f16396:c5:m22"}
{"signature": "def store(self, data, key=None, *args, **kwargs):", "body": "list.__init__(self, data)<EOL>self._store_items(self._cache_key(key))<EOL>", "docstring": "Cache the list\n\n            :param list data: List of objects to cache", "id": "f8243:c2:m3"}
{"signature": "def split_segments(text, closing_paren=False):", "body": "buf = StringIO()<EOL>segments = []<EOL>combinators = []<EOL>last_group = False<EOL>iterator = iter(text)<EOL>last_negation = False<EOL>for character in iterator:<EOL><INDENT>if character in COMBINATORS:<EOL><INDENT>if last_negation:<EOL><INDENT>buf.write(constants.OPERATOR_NEGATION)<EOL><DEDENT>val = buf.getvalue()<EOL>reset_stringio(buf)<EOL>if not last_group and not len(val):<EOL><INDENT>raise ValueError('<STR_LIT>' % character)<EOL><DEDENT>if len(val):<EOL><INDENT>segments.append(parse_segment(val))<EOL><DEDENT>combinators.append(COMBINATORS[character])<EOL><DEDENT>elif character == constants.GROUP_BEGIN:<EOL><INDENT>if buf.tell():<EOL><INDENT>raise ValueError('<STR_LIT>' % character)<EOL><DEDENT>seg = split_segments(iterator, True)<EOL>if last_negation:<EOL><INDENT>seg = UnarySegmentCombinator(seg)<EOL><DEDENT>segments.append(seg)<EOL>last_group = True<EOL>continue<EOL><DEDENT>elif character == constants.GROUP_END:<EOL><INDENT>val = buf.getvalue()<EOL>if not buf.tell() or not closing_paren:<EOL><INDENT>raise ValueError('<STR_LIT>' % character)<EOL><DEDENT>segments.append(parse_segment(val))<EOL>return combine(segments, combinators)<EOL><DEDENT>elif character == constants.OPERATOR_NEGATION and not buf.tell():<EOL><INDENT>last_negation = True<EOL>continue<EOL><DEDENT>else:<EOL><INDENT>if last_negation:<EOL><INDENT>buf.write(constants.OPERATOR_NEGATION)<EOL><DEDENT>if last_group:<EOL><INDENT>raise ValueError('<STR_LIT>' % character)<EOL><DEDENT>buf.write(character)<EOL><DEDENT>last_negation = False<EOL>last_group = False<EOL><DEDENT>else:<EOL><INDENT>if closing_paren:<EOL><INDENT>raise ValueError('<STR_LIT>' % constants.GROUP_END)<EOL><DEDENT>if not last_group:<EOL><INDENT>segments.append(parse_segment(buf.getvalue()))<EOL><DEDENT><DEDENT>return combine(segments, combinators)<EOL>", "docstring": "Return objects representing segments.", "id": "f10131:m2"}
{"signature": "def run(self):", "body": "octave = Oct2Py()<EOL>octave.push('<STR_LIT:name>', self.getName())<EOL>name = octave.pull('<STR_LIT:name>')<EOL>now = datetime.datetime.now()<EOL>print(\"<STR_LIT>\".format(self.getName(), name, now))<EOL>octave.exit()<EOL>try:<EOL><INDENT>assert self.getName() == name<EOL><DEDENT>except AssertionError:  <EOL><INDENT>raise Oct2PyError('<STR_LIT>')<EOL><DEDENT>return<EOL>", "docstring": "Create a unique instance of Octave and verify namespace uniqueness.\n\nRaises\n======\nOct2PyError\n    If the thread does not sucessfully demonstrate independence", "id": "f422:c0:m0"}
{"signature": "def monitor(pid, task_id, monitoring_hub_url, run_id, sleep_dur=<NUM_LIT:10>):", "body": "import psutil<EOL>radio = UDPRadio(monitoring_hub_url,<EOL>source_id=task_id)<EOL>simple = [\"<STR_LIT>\", '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT:name>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT:status>', '<STR_LIT:username>']<EOL>summable_values = ['<STR_LIT>', '<STR_LIT>', '<STR_LIT>']<EOL>pm = psutil.Process(pid)<EOL>pm.cpu_percent()<EOL>first_msg = True<EOL>while True:<EOL><INDENT>try:<EOL><INDENT>d = {\"<STR_LIT>\" + str(k): v for k, v in pm.as_dict().items() if k in simple}<EOL>d[\"<STR_LIT>\"] = run_id<EOL>d[\"<STR_LIT>\"] = task_id<EOL>d['<STR_LIT>'] = sleep_dur<EOL>d['<STR_LIT>'] = first_msg<EOL>d['<STR_LIT>'] = datetime.datetime.now()<EOL>children = pm.children(recursive=True)<EOL>d[\"<STR_LIT>\"] = psutil.cpu_count()<EOL>d['<STR_LIT>'] = pm.memory_info().vms<EOL>d['<STR_LIT>'] = pm.memory_info().rss<EOL>d['<STR_LIT>'] = pm.cpu_times().user<EOL>d['<STR_LIT>'] = pm.cpu_times().system<EOL>d['<STR_LIT>'] = len(children)<EOL>try:<EOL><INDENT>d['<STR_LIT>'] = pm.io_counters().write_bytes<EOL>d['<STR_LIT>'] = pm.io_counters().read_bytes<EOL><DEDENT>except psutil._exceptions.AccessDenied:<EOL><INDENT>d['<STR_LIT>'] = <NUM_LIT:0><EOL>d['<STR_LIT>'] = <NUM_LIT:0><EOL><DEDENT>for child in children:<EOL><INDENT>for k, v in child.as_dict(attrs=summable_values).items():<EOL><INDENT>d['<STR_LIT>' + str(k)] += v<EOL><DEDENT>d['<STR_LIT>'] += child.cpu_times().user<EOL>d['<STR_LIT>'] += child.cpu_times().system<EOL>d['<STR_LIT>'] += child.memory_info().vms<EOL>d['<STR_LIT>'] += child.memory_info().rss<EOL>try:<EOL><INDENT>d['<STR_LIT>'] += child.io_counters().write_bytes<EOL>d['<STR_LIT>'] += child.io_counters().read_bytes<EOL><DEDENT>except psutil._exceptions.AccessDenied:<EOL><INDENT>d['<STR_LIT>'] += <NUM_LIT:0><EOL>d['<STR_LIT>'] += <NUM_LIT:0><EOL><DEDENT><DEDENT><DEDENT>finally:<EOL><INDENT>radio.send(MessageType.TASK_INFO, task_id, d)<EOL>time.sleep(sleep_dur)<EOL>first_msg = False<EOL><DEDENT><DEDENT>", "docstring": "Internal\n    Monitors the Parsl task's resources by pointing psutil to the task's pid and watching it and its children.", "id": "f2764:m2"}
{"signature": "@memoize()<EOL><INDENT>def _skew(self, x, z, d=<NUM_LIT:0>):<DEDENT>", "body": "<EOL>kval = (np.tanh(self._poly(z, self._kurtosis_coeffs(d)))+<NUM_LIT:1>)/<NUM_LIT><EOL>bdpoly = np.array([<EOL>-<NUM_LIT>,  <NUM_LIT>, -<NUM_LIT>,<EOL>-<NUM_LIT>, <NUM_LIT>, <NUM_LIT><EOL>])<EOL>top = np.polyval(bdpoly, kval)<EOL>skew = self._poly(z, self._skew_coeffs(d))<EOL>skewval = top*(np.tanh(skew) + <NUM_LIT:1>) - top<EOL>return skewval*(<NUM_LIT:3>*x - x**<NUM_LIT:3>)<EOL>", "docstring": "returns the kurtosis parameter for direction d, d=0 is rho, d=1 is z", "id": "f5755:c8:m7"}
{"signature": "def GetActiveComposition(self) -> TextRange:", "body": "textRange = self.pattern.GetActiveComposition()<EOL>if textRange:<EOL><INDENT>return TextRange(textRange=textRange)<EOL><DEDENT>", "docstring": "Call IUIAutomationTextEditPattern::GetActiveComposition.\nReturn `TextRange` or None, the active composition.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationtexteditpattern-getactivecomposition", "id": "f1782:c69:m1"}
{"signature": "def _parse_document_id(elm_tree):", "body": "xpath = '<STR_LIT>'<EOL>return [x for x in elm_tree.xpath(xpath, namespaces=COLLECTION_NSMAP)][<NUM_LIT:0>]<EOL>", "docstring": "Given the parsed xml to an `ElementTree`,\n    parse the id from the content.", "id": "f12747:m0"}
{"signature": "def create_or_update_kv2(self, path, data, cas=None, mount_path='<STR_LIT>'):", "body": "params = {<EOL>'<STR_LIT>': {},<EOL>'<STR_LIT:data>': data<EOL>}<EOL>if cas is not None:<EOL><INDENT>params['<STR_LIT>']['<STR_LIT>'] = cas<EOL><DEDENT>write_path = '<STR_LIT>'.format(mount_path, path)<EOL>return self.write(write_path, **params)<EOL>", "docstring": "Create or update some data in a key/value version 2 secret engine.\n\n:raises CasError:\n    Raises an error if the ``cas`` value, when provided, doesn't match\n    Vault's version for the key.", "id": "f13708:c2:m8"}
{"signature": "def __init__(self, t, fmt, tz=None):", "body": "if tz is None: tz = _get_rc_timezone()<EOL>self.t = t<EOL>self.fmt = fmt<EOL>self.tz = tz<EOL>", "docstring": "*t* is a sequence of dates (floating point days).  *fmt* is a\n:func:`strftime` format string.", "id": "f17189:c2:m0"}
{"signature": "def quietParts(data,percentile=<NUM_LIT:10>):", "body": "nChunks=int(len(Y)/CHUNK_POINTS)<EOL>chunks=np.reshape(Y[:nChunks*CHUNK_POINTS],(nChunks,CHUNK_POINTS))<EOL>variances=np.var(chunks,axis=<NUM_LIT:1>)<EOL>percentiles=np.empty(len(variances))<EOL>for i,variance in enumerate(variances):<EOL><INDENT>percentiles[i]=sorted(variances).index(variance)/len(variances)*<NUM_LIT:100><EOL><DEDENT>selected=chunks[np.where(percentiles<=percentile)[<NUM_LIT:0>]].flatten()<EOL>return selected<EOL>", "docstring": "Given some data (Y) break it into chunks and return just the quiet ones.\nReturns data where the variance for its chunk size is below the given percentile.\nCHUNK_POINTS should be adjusted so it's about 10ms of data.", "id": "f11388:m0"}
{"signature": "def pathjoin(*args, **kwargs):", "body": "log.debug('<STR_LIT>' % list(args))<EOL>def _pathjoin(*args, **kwargs):<EOL><INDENT>len_ = len(args) - <NUM_LIT:1><EOL>if len_ < <NUM_LIT:0>:<EOL><INDENT>raise Exception('<STR_LIT>')<EOL><DEDENT>elif len_ == <NUM_LIT:0>:<EOL><INDENT>if not isinstance(args, basestring):<EOL><INDENT>if hasattr(args, '<STR_LIT>'):<EOL><INDENT>_args = args<EOL>_args<EOL>args = args[<NUM_LIT:0>]<EOL><DEDENT><DEDENT><DEDENT>for i, arg in enumerate(args):<EOL><INDENT>if not i:<EOL><INDENT>yield arg.rstrip('<STR_LIT:/>')<EOL><DEDENT>elif i == len_:<EOL><INDENT>yield arg.lstrip('<STR_LIT:/>')<EOL><DEDENT>else:<EOL><INDENT>yield arg.strip('<STR_LIT:/>')<EOL><DEDENT><DEDENT><DEDENT>joined_path = u'<STR_LIT:/>'.join(_pathjoin(*args))<EOL>return sanitize_path(joined_path)<EOL>", "docstring": "Arguments:\n    args (list): *args list of paths\n        if len(args) == 1, args[0] is not a string, and args[0] is iterable,\n        set args to args[0].\n\nBasically::\n\n    joined_path = u'/'.join(\n        [args[0].rstrip('/')] +\n        [a.strip('/') for a in args[1:-1]] +\n        [args[-1].lstrip('/')])", "id": "f12972:m0"}
{"signature": "def check(self, query):", "body": "if query.get_type() in {Keyword.LIST, Keyword.DROP}:<EOL><INDENT>series = query.series_stmt<EOL><DEDENT>else:<EOL><INDENT>series = query.from_stmt<EOL><DEDENT>if len(series) >= self.min_series_name_length:<EOL><INDENT>return Ok(True)<EOL><DEDENT>return Err(\"<STR_LIT>\")<EOL>", "docstring": ":param query:", "id": "f1432:c0:m3"}
{"signature": "async def update_state(self, data):", "body": "guild_id = int(data['<STR_LIT>'])<EOL>if guild_id in self.players:<EOL><INDENT>player = self.players.get(guild_id)<EOL>player.position = data['<STR_LIT:state>'].get('<STR_LIT>', <NUM_LIT:0>)<EOL>player.position_timestamp = data['<STR_LIT:state>']['<STR_LIT:time>']<EOL><DEDENT>", "docstring": "Updates a player's state when a payload with opcode ``playerUpdate`` is received.", "id": "f6348:c0:m4"}
{"signature": "def serialize_all(nodes, stream=None, Dumper=Dumper,<EOL>canonical=None, indent=None, width=None,<EOL>allow_unicode=None, line_break=None,<EOL>encoding='<STR_LIT:utf-8>', explicit_start=None, explicit_end=None,<EOL>version=None, tags=None):", "body": "getvalue = None<EOL>if stream is None:<EOL><INDENT>if encoding is None:<EOL><INDENT>from StringIO import StringIO<EOL><DEDENT>else:<EOL><INDENT>from cStringIO import StringIO<EOL><DEDENT>stream = StringIO()<EOL>getvalue = stream.getvalue<EOL><DEDENT>dumper = Dumper(stream, canonical=canonical, indent=indent, width=width,<EOL>allow_unicode=allow_unicode, line_break=line_break,<EOL>encoding=encoding, version=version, tags=tags,<EOL>explicit_start=explicit_start, explicit_end=explicit_end)<EOL>try:<EOL><INDENT>dumper.open()<EOL>for node in nodes:<EOL><INDENT>dumper.serialize(node)<EOL><DEDENT>dumper.close()<EOL><DEDENT>finally:<EOL><INDENT>dumper.dispose()<EOL><DEDENT>if getvalue:<EOL><INDENT>return getvalue()<EOL><DEDENT>", "docstring": "Serialize a sequence of representation trees into a YAML stream.\nIf stream is None, return the produced string instead.", "id": "f8321:m9"}
{"signature": "@map_types<EOL><INDENT>def __init__(self,<EOL>magnitude: float = <NUM_LIT>,<EOL>phi: float = <NUM_LIT:0.0>):<DEDENT>", "body": "super(ExternalShear, self).__init__(centre=(<NUM_LIT:0.0>, <NUM_LIT:0.0>), phi=phi, axis_ratio=<NUM_LIT:1.0>)<EOL>self.magnitude = magnitude<EOL>", "docstring": "An external shear term, to model the line-of-sight contribution of other galaxies / satellites.\n\nParameters\n----------\nmagnitude : float\n    The overall magnitude of the shear (gamma).\nphi : float\n    The rotation axis of the shear.", "id": "f5952:c28:m0"}
{"signature": "@event_handler(DisconnectedEvent)<EOL><INDENT>def handle_disconnected(self, event):<DEDENT>", "body": "return QUIT<EOL>", "docstring": "Quit the main loop upon disconnection.", "id": "f15229:c0:m6"}
{"signature": "@superuser.command()<EOL>@click.argument('<STR_LIT:username>', type=str)<EOL>@clean_outputs<EOL>def grant(username):", "body": "try:<EOL><INDENT>PolyaxonClient().user.grant_superuser(username)<EOL><DEDENT>except (PolyaxonHTTPError, PolyaxonShouldExitError, PolyaxonClientException) as e:<EOL><INDENT>Printer.print_error('<STR_LIT>'.format(username))<EOL>Printer.print_error('<STR_LIT>'.format(e))<EOL>sys.exit(<NUM_LIT:1>)<EOL><DEDENT>Printer.print_success(<EOL>\"<STR_LIT>\".format(username))<EOL>", "docstring": "Grant superuser role to a user.\n\n    Example:\n\n    \\b\n    ```bash\n    $ polyaxon superuser grant david\n    ```", "id": "f1065:m1"}
{"signature": "def incomeStatement(symbol, token='<STR_LIT>', version='<STR_LIT>'):", "body": "_raiseIfNotStr(symbol)<EOL>return _getJson('<STR_LIT>' + symbol + '<STR_LIT>', token, version)<EOL>", "docstring": "Pulls income statement data. Available quarterly (4 quarters) or annually (4 years).\n\n    https://iexcloud.io/docs/api/#income-statement\n    Updates at 8am, 9am UTC daily\n\n    Args:\n        symbol (string); Ticker to request\n        token (string); Access token\n        version (string); API version\n\n    Returns:\n        dict: result", "id": "f2330:m39"}
{"signature": "def _is_compound_mass_temperature_tuple(self, value):", "body": "if not type(value) is tuple:<EOL><INDENT>return False<EOL><DEDENT>elif not len(value) == <NUM_LIT:3>:<EOL><INDENT>return False<EOL><DEDENT>elif not type(value[<NUM_LIT:0>]) is str:<EOL><INDENT>return False<EOL><DEDENT>elif not type(value[<NUM_LIT:1>]) is float andnot type(value[<NUM_LIT:1>]) is numpy.float64 andnot type(value[<NUM_LIT:1>]) is numpy.float32:<EOL><INDENT>return False<EOL><DEDENT>elif not type(value[<NUM_LIT:1>]) is float andnot type(value[<NUM_LIT:1>]) is numpy.float64 andnot type(value[<NUM_LIT:1>]) is numpy.float32:<EOL><INDENT>return False<EOL><DEDENT>else:<EOL><INDENT>return True<EOL><DEDENT>", "docstring": "Determines whether value is a tuple of the format\n        (compound(str), mass(float), temperature(float)).\n\n        :param value: The value to be tested.\n\n        :returns: True or False", "id": "f15823:c1:m9"}
{"signature": "def mpsse_read_gpio(self):", "body": "<EOL>self._write('<STR_LIT>')<EOL>data = self._poll_read(<NUM_LIT:2>)<EOL>low_byte = ord(data[<NUM_LIT:0>])<EOL>high_byte = ord(data[<NUM_LIT:1>])<EOL>logger.debug('<STR_LIT>'.format(<EOL>low_byte, high_byte))<EOL>return (high_byte << <NUM_LIT:8>) | low_byte<EOL>", "docstring": "Read both GPIO bus states and return a 16 bit value with their state.\n        D0-D7 are the lower 8 bits and C0-C7 are the upper 8 bits.", "id": "f8002:c0:m8"}
{"signature": "def FindText(self, text: str, backward: bool, ignoreCase: bool) -> '<STR_LIT>':", "body": "textRange = self.textRange.FindText(text, int(backward), int(ignoreCase))<EOL>if textRange:<EOL><INDENT>return TextRange(textRange=textRange)<EOL><DEDENT>", "docstring": "Call IUIAutomationTextRange::FindText.\ntext: str,\nbackward: bool, True if the last occurring text range should be returned instead of the first; otherwise False.\nignoreCase: bool, True if case should be ignored; otherwise False.\nreturn `TextRange` or None, a text range subset that contains the specified text.\nRefer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationtextrange-findtext", "id": "f1782:c67:m7"}
{"signature": "def sub_to_image_grid(func):", "body": "@wraps(func)<EOL>def wrapper(grid, galaxies, *args, **kwargs):<EOL><INDENT>\"\"\"<STR_LIT>\"\"\"<EOL>result = func(grid, galaxies, *args, *kwargs)<EOL>if isinstance(grid, SubGrid):<EOL><INDENT>return grid.regular_data_1d_from_sub_data_1d(result)<EOL><DEDENT>else:<EOL><INDENT>return result<EOL><DEDENT><DEDENT>return wrapper<EOL>", "docstring": "Wrap the function in a function that, if the grid is a sub-grid (grids.SubGrid), rebins the computed \\\nvalues to the sub-grids corresponding regular-grid by taking the mean of each set of sub-gridded values.\n\nParameters\n----------\nfunc : (profiles, *args, **kwargs) -> Object\n    A function that requires the sub-grid and galaxies.", "id": "f5990:m0"}
{"signature": "@property<EOL><INDENT>def albums(self):<DEDENT>", "body": "root_path = self.path if self.path != '<STR_LIT:.>' else '<STR_LIT>'<EOL>return [self.gallery.albums[join(root_path, path)]<EOL>for path in self.subdirs]<EOL>", "docstring": "List of :class:`~sigal.gallery.Album` objects for each\n        sub-directory.", "id": "f13464:c3:m11"}
{"signature": "def set_default_account(self, account):", "body": "self.account_class(account)<EOL>self.config[\"<STR_LIT>\"] = account<EOL>", "docstring": "Set the default account to be used", "id": "f8253:c0:m7"}
{"signature": "def __splitting_criterion(self, clusters, centers):", "body": "if self.__criterion == splitting_type.BAYESIAN_INFORMATION_CRITERION:<EOL><INDENT>return self.__bayesian_information_criterion(clusters, centers)<EOL><DEDENT>elif self.__criterion == splitting_type.MINIMUM_NOISELESS_DESCRIPTION_LENGTH:<EOL><INDENT>return self.__minimum_noiseless_description_length(clusters, centers)<EOL><DEDENT>else:<EOL><INDENT>assert <NUM_LIT:0>;<EOL><DEDENT>", "docstring": "!\n        @brief Calculates splitting criterion for input clusters.\n\n        @param[in] clusters (list): Clusters for which splitting criterion should be calculated.\n        @param[in] centers (list): Centers of the clusters.\n\n        @return (double) Returns splitting criterion. High value of splitting cretion means that current structure is much better.\n\n        @see __bayesian_information_criterion(clusters, centers)\n        @see __minimum_noiseless_description_length(clusters, centers)", "id": "f15593:c1:m8"}
{"signature": "def start(self):", "body": "self.thread = Thread(target=self.main)<EOL>self.thread.daemon = True<EOL>self.thread.start()<EOL>", "docstring": "master only", "id": "f2547:c3:m3"}
{"signature": "def add_context(self, err_context, succ_context=None):", "body": "self.err_context = err_context<EOL>self.succ_context = succ_context<EOL>", "docstring": "Prepend msg to add some context information\n\n        :param pmsg: context info\n        :return: None", "id": "f7379:c1:m4"}
{"signature": "def delete(self):", "body": "i = self.index()<EOL>if i != None: del self.canvas.layers[i]<EOL>", "docstring": "Removes this layer from the canvas.", "id": "f11554:c2:m3"}
{"signature": "def get_heron_libs(local_jars):", "body": "heron_lib_dir = get_heron_lib_dir()<EOL>heron_libs = [os.path.join(heron_lib_dir, f) for f in local_jars]<EOL>return heron_libs<EOL>", "docstring": "Get all the heron lib jars with the absolute paths", "id": "f7415:m15"}
{"signature": "def read_until(self, marker):", "body": "if not isinstance(marker, byte_cls) and not isinstance(marker, Pattern):<EOL><INDENT>raise TypeError(pretty_message(<EOL>'''<STR_LIT>''',<EOL>type_name(marker)<EOL>))<EOL><DEDENT>output = b'<STR_LIT>'<EOL>is_regex = isinstance(marker, Pattern)<EOL>while True:<EOL><INDENT>if len(self._decrypted_bytes) > <NUM_LIT:0>:<EOL><INDENT>chunk = self._decrypted_bytes<EOL>self._decrypted_bytes = b'<STR_LIT>'<EOL><DEDENT>else:<EOL><INDENT>if self._ssl is None:<EOL><INDENT>self._raise_closed()<EOL><DEDENT>to_read = libssl.SSL_pending(self._ssl) or <NUM_LIT><EOL>chunk = self.read(to_read)<EOL><DEDENT>offset = len(output)<EOL>output += chunk<EOL>if is_regex:<EOL><INDENT>match = marker.search(output)<EOL>if match is not None:<EOL><INDENT>end = match.end()<EOL>break<EOL><DEDENT><DEDENT>else:<EOL><INDENT>start = max(<NUM_LIT:0>, offset - len(marker) - <NUM_LIT:1>)<EOL>match = output.find(marker, start)<EOL>if match != -<NUM_LIT:1>:<EOL><INDENT>end = match + len(marker)<EOL>break<EOL><DEDENT><DEDENT><DEDENT>self._decrypted_bytes = output[end:] + self._decrypted_bytes<EOL>return output[<NUM_LIT:0>:end]<EOL>", "docstring": "Reads data from the socket until a marker is found. Data read includes\nthe marker.\n\n:param marker:\n    A byte string or regex object from re.compile(). Used to determine\n    when to stop reading. Regex objects are more inefficient since\n    they must scan the entire byte string of read data each time data\n    is read off the socket.\n\n:return:\n    A byte string of the data read, including the marker", "id": "f9532:c1:m7"}
{"signature": "def load_mnist_dataset(shape=(-<NUM_LIT:1>, <NUM_LIT>), path='<STR_LIT:data>'):", "body": "return _load_mnist_dataset(shape, path, name='<STR_LIT>', url='<STR_LIT>')<EOL>", "docstring": "Load the original mnist.\n\n    Automatically download MNIST dataset and return the training, validation and test set with 50000, 10000 and 10000 digit images respectively.\n\n    Parameters\n    ----------\n    shape : tuple\n        The shape of digit images (the default is (-1, 784), alternatively (-1, 28, 28, 1)).\n    path : str\n        The path that the data is downloaded to.\n\n    Returns\n    -------\n    X_train, y_train, X_val, y_val, X_test, y_test: tuple\n        Return splitted training/validation/test set respectively.\n\n    Examples\n    --------\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1,784), path='datasets')\n    >>> X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))", "id": "f11137:m0"}
{"signature": "@staticmethod<EOL><INDENT>def animate_cluster_allocation(dataset, analyser, animation_velocity = <NUM_LIT>, tolerance = <NUM_LIT:0.1>, save_movie = None, title = None):<DEDENT>", "body": "figure = plt.figure();<EOL>def init_frame():<EOL><INDENT>return frame_generation(<NUM_LIT:0>);<EOL><DEDENT>def frame_generation(index_dynamic):<EOL><INDENT>figure.clf();<EOL>if (title is not None):<EOL><INDENT>figure.suptitle(title, fontsize = <NUM_LIT>, fontweight = '<STR_LIT>');<EOL><DEDENT>ax1 = figure.add_subplot(<NUM_LIT>, projection='<STR_LIT>');<EOL>clusters = analyser.allocate_clusters(eps = tolerance, iteration = index_dynamic);<EOL>dynamic = analyser.output[index_dynamic];<EOL>visualizer = cluster_visualizer(size_row = <NUM_LIT:2>);<EOL>visualizer.append_clusters(clusters, dataset);<EOL>artist1, = ax1.plot(dynamic, [<NUM_LIT:1.0>] * len(dynamic), marker = '<STR_LIT:o>', color = '<STR_LIT>', ls = '<STR_LIT>');<EOL>visualizer.show(figure, display = False);<EOL>artist2 = figure.gca();<EOL>return [ artist1, artist2 ];<EOL><DEDENT>cluster_animation = animation.FuncAnimation(figure, frame_generation, len(analyser), interval = animation_velocity, init_func = init_frame, repeat_delay = <NUM_LIT>);<EOL>if (save_movie is not None):<EOL><INDENT>plt.rcParams['<STR_LIT>'] = '<STR_LIT>';<EOL>ffmpeg_writer = animation.FFMpegWriter(fps = <NUM_LIT:15>);<EOL>cluster_animation.save(save_movie, writer = ffmpeg_writer);<EOL>", "docstring": "!\n        @brief Shows animation of output dynamic (output of each oscillator) during simulation on a circle from [0; 2pi].\n\n        @param[in] dataset (list): Input data that was used for processing by the network.\n        @param[in] analyser (syncnet_analyser): Output dynamic analyser of the Sync network.\n        @param[in] animation_velocity (uint): Interval between frames in milliseconds.\n        @param[in] tolerance (double): Tolerance level that define maximal difference between phases of oscillators in one cluster.\n        @param[in] save_movie (string): If it is specified then animation will be stored to file that is specified in this parameter.\n        @param[in] title (string): If it is specified then title will be displayed on the animation plot.", "id": "f15545:c1:m0"}
{"signature": "@coroutine<EOL>def patch_anchors(parser, show_progressbar):", "body": "files = defaultdict(list)<EOL>try:<EOL><INDENT>while True:<EOL><INDENT>pentry = (yield)<EOL>try:<EOL><INDENT>fname, anchor = pentry.path.split(\"<STR_LIT:#>\")<EOL>files[fname].append(<EOL>TOCEntry(name=pentry.name, type=pentry.type, anchor=anchor)<EOL>)<EOL><DEDENT>except ValueError:<EOL><INDENT>pass<EOL><DEDENT><DEDENT><DEDENT>except GeneratorExit:<EOL><INDENT>pass<EOL><DEDENT>def patch_files(files):<EOL><INDENT>for fname, entries in files:<EOL><INDENT>full_path = os.path.join(parser.doc_path, fname)<EOL>with codecs.open(full_path, mode=\"<STR_LIT:r>\", encoding=\"<STR_LIT:utf-8>\") as fp:<EOL><INDENT>soup = BeautifulSoup(fp, \"<STR_LIT>\")<EOL>for entry in entries:<EOL><INDENT>if not parser.find_and_patch_entry(soup, entry):<EOL><INDENT>log.debug(<EOL>\"<STR_LIT>\".format(<EOL>entry.anchor, click.format_filename(fname)<EOL>)<EOL>)<EOL><DEDENT><DEDENT><DEDENT>with open(full_path, mode=\"<STR_LIT:wb>\") as fp:<EOL><INDENT>fp.write(soup.encode(\"<STR_LIT:utf-8>\"))<EOL><DEDENT><DEDENT><DEDENT>if show_progressbar is True:<EOL><INDENT>with click.progressbar(<EOL>files.items(),<EOL>width=<NUM_LIT:0>,<EOL>length=len(files),<EOL>label=\"<STR_LIT>\",<EOL>) as pbar:<EOL><INDENT>patch_files(pbar)<EOL><DEDENT><DEDENT>else:<EOL><INDENT>patch_files(files.items())<EOL><DEDENT>", "docstring": "Consume ``ParseEntry``s then patch docs for TOCs by calling\n*parser*'s ``find_and_patch_entry``.", "id": "f5834:m1"}
{"signature": "def ensure_app_config_dir(appname, *args):", "body": "from ubelt import util_path<EOL>dpath = get_app_config_dir(appname, *args)<EOL>util_path.ensuredir(dpath)<EOL>return dpath<EOL>", "docstring": "Calls `get_app_config_dir` but ensures the directory exists.\n\nArgs:\n    appname (str): the name of the application\n    *args: any other subdirectories may be specified\n\nSeeAlso:\n    get_app_config_dir\n\nExample:\n    >>> import ubelt as ub\n    >>> dpath = ub.ensure_app_config_dir('ubelt')\n    >>> assert exists(dpath)", "id": "f5145:m6"}
{"signature": "def _login(self, session, get_request=False):", "body": "req = session.post(self._login_url, data=self._logindata)<EOL>if _LOGIN_ERROR_STRING in req.text orreq.status_code == <NUM_LIT> orreq.url == _LOGIN_URL:<EOL><INDENT>err_mess = \"<STR_LIT>\"<EOL>if _LOGIN_LOCKED_MESS in req.text:<EOL><INDENT>err_mess += \"<STR_LIT>\" + _LOGIN_LOCKED_MESS_ENG<EOL>self._suspended = True<EOL>raise self.AccountSuspendedError(err_mess)<EOL><DEDENT>raise self.LoginError(err_mess)<EOL><DEDENT>self._suspended = False  <EOL>return (session, req) if get_request else session<EOL>", "docstring": "Return a session for yesss.at.", "id": "f5903:c0:m1"}
{"signature": "def chdir(self, new_pwd, relative=True):", "body": "if new_pwd and self.pwd and relative:<EOL><INDENT>new_pwd = os.path.join(self.pwd, new_pwd)<EOL><DEDENT>self.pwd = new_pwd<EOL>", "docstring": "Parameters\n----------\nnew_pwd: str,\n    Directory to change to\nrelative: bool, default True\n    If True then the given directory is treated as relative to the\n    current directory", "id": "f4453:c0:m3"}
{"signature": "def __init__(self, plane_redshifts, cosmology):", "body": "self.plane_redshifts = plane_redshifts<EOL>self.cosmology = cosmology<EOL>", "docstring": "Abstract Ray tracer for lens systems with any number of planes.\n\n        From the galaxies of the tracer's planes, their grid-stack(s) and the cosmology physically derived quantities \\\n        (e.g. surface density, angular diameter distances, critical surface densities) can be computed.\n\n        Parameters\n        ----------\n        plane_redshifts : [pl.Plane] or [pl.PlaneStack]\n            The list of the tracer's planes in ascending redshift order.\n        cosmology : astropy.cosmology\n            The cosmology of the ray-tracing calculation.", "id": "f5972:c0:m0"}
{"signature": "@view_config(<EOL>route_name='<STR_LIT>',<EOL>request_method='<STR_LIT:POST>')<EOL>def dropbox_submission(dropbox, request):", "body": "try:<EOL><INDENT>data = dropbox_schema.deserialize(request.POST)<EOL><DEDENT>except Exception:<EOL><INDENT>return HTTPFound(location=request.route_url('<STR_LIT>'))<EOL><DEDENT>dropbox.message = data.get('<STR_LIT:message>')<EOL>if '<STR_LIT>' in dropbox.settings:<EOL><INDENT>dropbox.from_watchdog = is_equal(<EOL>unicode(dropbox.settings['<STR_LIT>']),<EOL>data.pop('<STR_LIT>', u'<STR_LIT>'))<EOL><DEDENT>if data.get('<STR_LIT>') is not None:<EOL><INDENT>dropbox.add_attachment(data['<STR_LIT>'])<EOL><DEDENT>dropbox.submit()<EOL>drop_url = request.route_url('<STR_LIT>', drop_id=dropbox.drop_id)<EOL>print(\"<STR_LIT>\" % drop_url)<EOL>return HTTPFound(location=drop_url)<EOL>", "docstring": "handles the form submission, redirects to the dropbox's status page.", "id": "f10069:m3"}
{"signature": "def can_zoom(self):", "body": "return False<EOL>", "docstring": "Return True if this axes support the zoom box", "id": "f17181:c0:m17"}
{"signature": "def plain(self, markup):", "body": "<EOL>if self.full_strip:<EOL><INDENT>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL><DEDENT>else:<EOL><INDENT>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)<EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)<EOL><DEDENT>markup = re.sub(self.re[\"<STR_LIT>\"], \"<STR_LIT>\", markup)<EOL>markup = re.sub(self.re[\"<STR_LIT>\"], \"<STR_LIT>\", markup)<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>if self.full_strip:<EOL><INDENT>markup = re.sub(r\"<STR_LIT>\", \"<STR_LIT>\", markup)<EOL><DEDENT>else:<EOL><INDENT>markup = re.sub(r\"<STR_LIT>\", '<STR_LIT>', markup)<EOL>markup = re.sub(r\"<STR_LIT>\", '<STR_LIT>', markup)    <EOL><DEDENT>markup = re.sub(self.re[\"<STR_LIT>\"], \"<STR_LIT>\", markup)<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)<EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)<EOL>markup = markup.replace(\"<STR_LIT:[>\", \"<STR_LIT>\")<EOL>markup = markup.replace(\"<STR_LIT:]>\", \"<STR_LIT>\")<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT:[>\")<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT:]>\")<EOL>markup = re.sub(self.re[\"<STR_LIT>\"], \"<STR_LIT>\", markup)                  <EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)                      <EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)           <EOL>markup = re.sub(self.ref+\"<STR_LIT>\", \"<STR_LIT>\", markup)        <EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)                 <EOL>markup = re.sub(self.re[\"<STR_LIT>\"], \"<STR_LIT>\", markup)                    <EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)            <EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)                   <EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT>\", markup)         <EOL>markup = re.sub(re.compile(\"<STR_LIT>\", re.DOTALL), \"<STR_LIT>\", markup)  <EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT>\")<EOL>markup = re.sub(\"<STR_LIT>\", \"<STR_LIT:U+0020>\", markup)<EOL>markup = markup.split(\"<STR_LIT:\\n>\")<EOL>for i in range(len(markup)):<EOL><INDENT>if not markup[i].startswith(\"<STR_LIT:U+0020>\"):<EOL><INDENT>markup[i] = re.sub(r\"<STR_LIT>\", \"<STR_LIT:U+0020>\", markup[i])<EOL><DEDENT><DEDENT>markup = \"<STR_LIT:\\n>\".join(markup)<EOL>markup = markup.replace(\"<STR_LIT>\", \"<STR_LIT:.>\")<EOL>if self.full_strip:<EOL><INDENT>markup = strip_tags(markup, exclude=[\"<STR_LIT>\"], linebreaks=True)<EOL><DEDENT>markup = markup.strip()<EOL>return markup<EOL>", "docstring": "Strips Wikipedia markup from given text.\n\n        This creates a \"plain\" version of the markup,\n        stripping images and references and the like.\n        Does some commonsense maintenance as well,\n        like collapsing multiple spaces.\n        If you specified full_strip=False for WikipediaPage instance,\n        some markup is preserved as HTML (links, bold, italic).", "id": "f11594:c8:m5"}
{"signature": "def gradient_optimizer(coro):", "body": "class GradientOptimizer(Optimizer):<EOL><INDENT>@wraps(coro)<EOL>def __init__(self, *args, **kwargs):<EOL><INDENT>self.algorithm = coro(*args, **kwargs)<EOL>self.algorithm.send(None)<EOL>self.operators = []<EOL><DEDENT>def set_transform(self, func):<EOL><INDENT>self.transform = compose(destruct, func, self.restruct)<EOL><DEDENT>def minimize(self, f_df, x0, display=sys.stdout, maxiter=<NUM_LIT>):<EOL><INDENT>self.display = display<EOL>self.theta = x0<EOL>xk = self.algorithm.send(destruct(x0).copy())<EOL>store = defaultdict(list)<EOL>runtimes = []<EOL>if len(self.operators) == <NUM_LIT:0>:<EOL><INDENT>self.operators = [proxops.identity()]<EOL><DEDENT>obj, grad = wrap(f_df, x0)<EOL>transform = compose(destruct, *reversed(self.operators), self.restruct)<EOL>self.optional_print(tp.header(['<STR_LIT>', '<STR_LIT>', '<STR_LIT>', '<STR_LIT>']))<EOL>try:<EOL><INDENT>for k in count():<EOL><INDENT>tstart = perf_counter()<EOL>f = obj(xk)<EOL>df = grad(xk)<EOL>xk = transform(self.algorithm.send(df))<EOL>runtimes.append(perf_counter() - tstart)<EOL>store['<STR_LIT:f>'].append(f)<EOL>self.optional_print(tp.row([k,<EOL>f,<EOL>np.linalg.norm(destruct(df)),<EOL>tp.humantime(runtimes[-<NUM_LIT:1>])]))<EOL>if k >= maxiter:<EOL><INDENT>break<EOL><DEDENT><DEDENT><DEDENT>except KeyboardInterrupt:<EOL><INDENT>pass<EOL><DEDENT>self.optional_print(tp.bottom(<NUM_LIT:4>))<EOL>self.optional_print(u'<STR_LIT>'.format(store['<STR_LIT:f>'][-<NUM_LIT:1>]))<EOL>self.optional_print(u'<STR_LIT>'.format(tp.humantime(sum(runtimes))))<EOL>self.optional_print(u'<STR_LIT>'.format(<EOL>tp.humantime(np.mean(runtimes)),<EOL>tp.humantime(np.std(runtimes)),<EOL>))<EOL>return OptimizeResult({<EOL>'<STR_LIT:x>': self.restruct(xk),<EOL>'<STR_LIT:f>': f,<EOL>'<STR_LIT>': self.restruct(df),<EOL>'<STR_LIT:k>': k,<EOL>'<STR_LIT>': np.array(store['<STR_LIT:f>']),<EOL>})<EOL><DEDENT><DEDENT>return GradientOptimizer<EOL>", "docstring": "Turns a coroutine into a gradient based optimizer.", "id": "f9764:m0"}
{"signature": "def createTMs(includeCPP = True,<EOL>includePy = True,<EOL>numCols = <NUM_LIT:100>,<EOL>cellsPerCol = <NUM_LIT:4>,<EOL>activationThreshold = <NUM_LIT:3>,<EOL>minThreshold = <NUM_LIT:3>,<EOL>newSynapseCount = <NUM_LIT:3>,<EOL>initialPerm = <NUM_LIT>,<EOL>permanenceInc = <NUM_LIT:0.1>,<EOL>permanenceDec = <NUM_LIT:0.0>,<EOL>globalDecay = <NUM_LIT:0.0>,<EOL>pamLength = <NUM_LIT:0>,<EOL>checkSynapseConsistency = True,<EOL>maxInfBacktrack = <NUM_LIT:0>,<EOL>maxLrnBacktrack = <NUM_LIT:0>,<EOL>**kwargs<EOL>):", "body": "<EOL>connectedPerm = <NUM_LIT:0.5><EOL>tms = dict()<EOL>if includeCPP:<EOL><INDENT>if VERBOSITY >= <NUM_LIT:2>:<EOL><INDENT>print(\"<STR_LIT>\")<EOL><DEDENT>cpp_tm = BacktrackingTMCPP(numberOfCols = numCols, cellsPerColumn = cellsPerCol,<EOL>initialPerm = initialPerm, connectedPerm = connectedPerm,<EOL>minThreshold = minThreshold, newSynapseCount = newSynapseCount,<EOL>permanenceInc = permanenceInc, permanenceDec = permanenceDec,<EOL>activationThreshold = activationThreshold,<EOL>globalDecay = globalDecay, burnIn = <NUM_LIT:1>,<EOL>seed=SEED, verbosity=VERBOSITY,<EOL>checkSynapseConsistency = checkSynapseConsistency,<EOL>collectStats = True,<EOL>pamLength = pamLength,<EOL>maxInfBacktrack = maxInfBacktrack,<EOL>maxLrnBacktrack = maxLrnBacktrack,<EOL>)<EOL>cpp_tm.retrieveLearningStates = True<EOL>tms['<STR_LIT>'] = cpp_tm<EOL><DEDENT>if includePy:<EOL><INDENT>if VERBOSITY >= <NUM_LIT:2>:<EOL><INDENT>print(\"<STR_LIT>\")<EOL><DEDENT>py_tm = BacktrackingTM(numberOfCols = numCols,<EOL>cellsPerColumn = cellsPerCol,<EOL>initialPerm = initialPerm,<EOL>connectedPerm = connectedPerm,<EOL>minThreshold = minThreshold,<EOL>newSynapseCount = newSynapseCount,<EOL>permanenceInc = permanenceInc,<EOL>permanenceDec = permanenceDec,<EOL>activationThreshold = activationThreshold,<EOL>globalDecay = globalDecay, burnIn = <NUM_LIT:1>,<EOL>seed=SEED, verbosity=VERBOSITY,<EOL>collectStats = True,<EOL>pamLength = pamLength,<EOL>maxInfBacktrack = maxInfBacktrack,<EOL>maxLrnBacktrack = maxLrnBacktrack,<EOL>)<EOL>tms['<STR_LIT>'] = py_tm<EOL><DEDENT>return tms<EOL>", "docstring": "Create one or more TM instances, placing each into a dict keyed by\n    name.\n\n    Parameters:\n    ------------------------------------------------------------------\n    retval:   tms - dict of TM instances", "id": "f17532:m5"}
{"signature": "def redirect_stream(system, target):", "body": "if target is None:<EOL><INDENT>target_fd = os.open(os.devnull, os.O_RDWR)<EOL><DEDENT>else:<EOL><INDENT>target_fd = target.fileno()<EOL><DEDENT>try:<EOL><INDENT>os.dup2(target_fd, system.fileno())<EOL><DEDENT>except OSError as err:<EOL><INDENT>raise DaemonError('<STR_LIT>'<EOL>.format(system, target, err))<EOL><DEDENT>", "docstring": "Redirect Unix streams\n\n    If None, redirect Stream to /dev/null, else redirect to target.\n\n    :param system: ether sys.stdin, sys.stdout, or sys.stderr\n    :type system: file object\n\n    :param target: File like object, or None\n    :type target: None, File Object\n\n    :return: None\n    :raise: DaemonError", "id": "f733:m5"}
{"signature": "def get_attributes(self, attributes, default='<STR_LIT>'):", "body": "if isinstance(attributes, str):<EOL><INDENT>attributes = [attributes]<EOL><DEDENT>attrs = [getattr(self, attr, default) for attr in attributes]<EOL>if len(attrs) == <NUM_LIT:1>:<EOL><INDENT>return attrs[<NUM_LIT:0>]<EOL><DEDENT>return tuple(attrs)<EOL>", "docstring": "Return the attributes values from this DicomFile\n\n        Parameters\n        ----------\n        attributes: str or list of str\n         DICOM field names\n\n        default: str\n         Default value if the attribute does not exist.\n\n        Returns\n        -------\n        Value of the field or list of values.", "id": "f4071:c0:m1"}
{"signature": "def set_connection(self, i, j):", "body": "if (self.structure != conn_type.DYNAMIC):<EOL><INDENT>raise NameError(\"<STR_LIT>\");<EOL><DEDENT>if (self._conn_represent == conn_represent.MATRIX):<EOL><INDENT>self._osc_conn[i][j] = True;<EOL>self._osc_conn[j][i] = True;<EOL><DEDENT>else:<EOL><INDENT>self._osc_conn[i].append(j);<EOL>self._osc_conn[j].append(i);<EOL><DEDENT>", "docstring": "!\n        @brief Couples two specified oscillators in the network with dynamic connections.\n\n        @param[in] i (uint): index of an oscillator that should be coupled with oscillator 'j' in the network.\n        @param[in] j (uint): index of an oscillator that should be coupled with oscillator 'i' in the network.\n\n        @note This method can be used only in case of DYNAMIC connections, otherwise it throws expection.", "id": "f15681:c4:m14"}
{"signature": "def render_pep440(pieces):", "body": "if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered = pieces[\"<STR_LIT>\"]<EOL>if pieces[\"<STR_LIT>\"] or pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered += plus_or_dot(pieces)<EOL>rendered += \"<STR_LIT>\" % (pieces[\"<STR_LIT>\"], pieces[\"<STR_LIT>\"])<EOL>if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered += \"<STR_LIT>\"<EOL><DEDENT><DEDENT><DEDENT>else:<EOL><INDENT>rendered = \"<STR_LIT>\" % (pieces[\"<STR_LIT>\"],<EOL>pieces[\"<STR_LIT>\"])<EOL>if pieces[\"<STR_LIT>\"]:<EOL><INDENT>rendered += \"<STR_LIT>\"<EOL><DEDENT><DEDENT>return rendered<EOL>", "docstring": "Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]", "id": "f10103:m12"}
{"signature": "def valid_content_type(self, content_type, accept):", "body": "accept_tokens = accept.replace('<STR_LIT:U+0020>', '<STR_LIT>').split('<STR_LIT:;>')<EOL>content_type_tokens = content_type.replace('<STR_LIT:U+0020>', '<STR_LIT>').split('<STR_LIT:;>')<EOL>return (<EOL>all(elem in content_type_tokens for elem in accept_tokens) and<EOL>(content_type_tokens[<NUM_LIT:0>] == '<STR_LIT>' or<EOL>content_type_tokens[<NUM_LIT:0>] == '<STR_LIT>')<EOL>)<EOL>", "docstring": "Check that the server is returning a valid Content-Type\n\n        Args:\n            content_type (str): ``Content-Type:`` header value\n            accept (str): media type to include in the ``Accept:`` header.", "id": "f4931:c10:m1"}
{"signature": "def create(self, list_id, data):", "body": "self.list_id = list_id<EOL>if '<STR_LIT:name>' not in data:<EOL><INDENT>raise KeyError('<STR_LIT>')<EOL><DEDENT>if '<STR_LIT:type>' not in data:<EOL><INDENT>raise KeyError('<STR_LIT>')<EOL><DEDENT>response = self._mc_client._post(url=self._build_path(list_id, '<STR_LIT>'), data=data)<EOL>if response is not None:<EOL><INDENT>self.merge_id = response['<STR_LIT>']<EOL><DEDENT>else:<EOL><INDENT>self.merge_id = None<EOL><DEDENT>return response<EOL>", "docstring": "Add a new merge field for a specific list.\n\n:param list_id: The unique id for the list.\n:type list_id: :py:class:`str`\n:param data: The request body parameters\n:type data: :py:class:`dict`\ndata = {\n    \"name\": string*,\n    \"type\": string*\n}", "id": "f301:c0:m1"}
{"signature": "def set_labels(self, labels):", "body": "if not isinstance(labels, string_types) and len(labels) != self.n_subjs:<EOL><INDENT>raise ValueError('<STR_LIT>'<EOL>'<STR_LIT>'.format(len(labels), self.n_subjs))<EOL><DEDENT>self.labels = labels<EOL>", "docstring": "Parameters\n----------\nlabels: list of int or str\n    This list will be checked to have the same size as\n\nRaises\n------\nValueError\n    if len(labels) != self.n_subjs", "id": "f4085:c0:m9"}
{"signature": "def get_haploid_lik(errors, bfreqs, ustacks, counts):", "body": "hetero = <NUM_LIT:0.><EOL>if errors <= <NUM_LIT:0.>:<EOL><INDENT>score = np.exp(<NUM_LIT:100>)<EOL><DEDENT>else:<EOL><INDENT>lik1 = ((<NUM_LIT:1.>-hetero)*likelihood1(errors, bfreqs, ustacks)) <EOL>liks = lik1<EOL>logliks = np.log(liks[liks > <NUM_LIT:0>])*counts[liks > <NUM_LIT:0>]<EOL>score = -logliks.sum()<EOL><DEDENT>return score<EOL>", "docstring": "Log likelihood score given values [E].", "id": "f5313:m5"}
{"signature": "def submit(self, func, *args, **kwargs):", "body": "task_id = uuid.uuid4()<EOL>logger.debug(\"<STR_LIT>\".format(func, args))<EOL>self.tasks[task_id] = Future()<EOL>fn_buf = pack_apply_message(func, args, kwargs,<EOL>buffer_threshold=<NUM_LIT> * <NUM_LIT>,<EOL>item_threshold=<NUM_LIT>)<EOL>msg = {\"<STR_LIT>\": task_id,<EOL>\"<STR_LIT>\": fn_buf}<EOL>self.outgoing_q.put(msg)<EOL>return self.tasks[task_id]<EOL>", "docstring": "Submits work to the the outgoing_q.\n\n        The outgoing_q is an external process listens on this\n        queue for new work. This method is simply pass through and behaves like a\n        submit call as described here `Python docs: <https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor>`_\n\n        Args:\n            - func (callable) : Callable function\n            - *args (list) : List of arbitrary positional arguments.\n\n        Kwargs:\n            - **kwargs (dict) : A dictionary of arbitrary keyword args for func.\n\n        Returns:\n              Future", "id": "f2812:c0:m6"}
{"signature": "def _groupby_consecutive(txn, max_delta=pd.Timedelta('<STR_LIT>')):", "body": "def vwap(transaction):<EOL><INDENT>if transaction.amount.sum() == <NUM_LIT:0>:<EOL><INDENT>warnings.warn('<STR_LIT>')<EOL>return np.nan<EOL><DEDENT>return (transaction.amount * transaction.price).sum() /transaction.amount.sum()<EOL><DEDENT>out = []<EOL>for sym, t in txn.groupby('<STR_LIT>'):<EOL><INDENT>t = t.sort_index()<EOL>t.index.name = '<STR_LIT>'<EOL>t = t.reset_index()<EOL>t['<STR_LIT>'] = t.amount > <NUM_LIT:0><EOL>t['<STR_LIT>'] = (t.order_sign.shift(<EOL><NUM_LIT:1>) != t.order_sign).astype(int).cumsum()<EOL>t['<STR_LIT>'] = ((t.dt.sub(t.dt.shift(<NUM_LIT:1>))) ><EOL>max_delta).astype(int).cumsum()<EOL>grouped_price = (t.groupby(('<STR_LIT>',<EOL>'<STR_LIT>'))<EOL>.apply(vwap))<EOL>grouped_price.name = '<STR_LIT>'<EOL>grouped_rest = t.groupby(('<STR_LIT>', '<STR_LIT>')).agg({<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': '<STR_LIT>',<EOL>'<STR_LIT>': '<STR_LIT>'})<EOL>grouped = grouped_rest.join(grouped_price)<EOL>out.append(grouped)<EOL><DEDENT>out = pd.concat(out)<EOL>out = out.set_index('<STR_LIT>')<EOL>return out<EOL>", "docstring": "Merge transactions of the same direction separated by less than\n    max_delta time duration.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed round_trips. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n\n    max_delta : pandas.Timedelta (optional)\n        Merge transactions in the same direction separated by less\n        than max_delta time duration.\n\n\n    Returns\n    -------\n    transactions : pd.DataFrame", "id": "f12217:m1"}
{"signature": "@abstractmethod<EOL><INDENT>def invoke_deactivate(self):<DEDENT>", "body": "raise NotImplementedError()<EOL>", "docstring": "Deactivate the instance", "id": "f7466:c0:m15"}
{"signature": "def _modify(item, func):", "body": "result = dict()<EOL>for key in item:<EOL><INDENT>result[func(key)] = item[key]<EOL><DEDENT>return result<EOL>", "docstring": "Modifies each item.keys() string based on the func passed in.\nOften used with inflection's camelize or underscore methods.\n\n:param item: dictionary representing item to be modified\n:param func: function to run on each key string\n:return: dictionary where each key has been modified by func.", "id": "f10858:m0"}
{"signature": "def save(self, filename):", "body": "self.export(filename, file_format=\"<STR_LIT>\")<EOL>", "docstring": "Save a grid object to <filename>.pickle\n\n        Internally, this calls\n        ``Grid.export(filename, format=\"python\")``. A grid can be\n        regenerated from the saved data with ::\n\n           g = Grid(filename=\"grid.pickle\")\n\n        .. note::\n           The pickle format depends on the Python version and\n           therefore it is not guaranteed that a grid saved with, say,\n           Python 2.7 can also be read with Python 3.5. The OpenDX format\n           is a better alternative for portability.", "id": "f15455:c0:m23"}
{"signature": "@property<EOL><INDENT>def legal_status(self):<DEDENT>", "body": "if self.__legal_status:<EOL><INDENT>return self.__legal_status<EOL><DEDENT>else:<EOL><INDENT>self.__legal_status = legal_status(self.CAS, Method='<STR_LIT>')<EOL>return self.__legal_status<EOL><DEDENT>", "docstring": "r'''Dictionary of legal status indicators for the chemical.\n\n        Examples\n        --------\n        >>> pprint(Chemical('benzene').legal_status)\n        {'DSL': 'LISTED',\n         'EINECS': 'LISTED',\n         'NLP': 'UNLISTED',\n         'SPIN': 'LISTED',\n         'TSCA': 'LISTED'}", "id": "f15812:c0:m33"}
{"signature": "def render_page_to_string(request, page_name, data={}):", "body": "return render_to_string(page_name, data, request=request)<EOL>", "docstring": "A shortcut for using ``render_to_string`` with a\n    :class:`RequestContext` automatically.", "id": "f9990:m1"}
{"signature": "def connect_to(self, joint, other_body, offset=(<NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT:0>), other_offset=(<NUM_LIT:0>, <NUM_LIT:0>, <NUM_LIT:0>),<EOL>**kwargs):", "body": "anchor = self.world.move_next_to(self, other_body, offset, other_offset)<EOL>self.world.join(joint, self, other_body, anchor=anchor, **kwargs)<EOL>", "docstring": "Move another body next to this one and join them together.\n\n        This method will move the ``other_body`` so that the anchor points for\n        the joint coincide. It then creates a joint to fasten the two bodies\n        together. See :func:`World.move_next_to` and :func:`World.join`.\n\n        Parameters\n        ----------\n        joint : str\n            The type of joint to use when connecting these bodies.\n        other_body : :class:`Body` or str\n            The other body to join with this one.\n        offset : 3-tuple of float, optional\n            The body-relative offset where the anchor for the joint should be\n            placed. Defaults to (0, 0, 0). See :func:`World.move_next_to` for a\n            description of how offsets are specified.\n        other_offset : 3-tuple of float, optional\n            The offset on the second body where the joint anchor should be\n            placed. Defaults to (0, 0, 0). Like ``offset``, this is given as an\n            offset relative to the size and shape of ``other_body``.", "id": "f14887:c1:m30"}
{"signature": "def copy(self, graph):", "body": "l = self.__class__(graph, self.n)<EOL>l.i = <NUM_LIT:0><EOL>return l<EOL>", "docstring": "Returns a copy of the layout for the given graph.", "id": "f11611:c1:m1"}
{"signature": "def save(self):", "body": "token = models.PasswordResetToken.objects.get(<EOL>key=self.validated_data[\"<STR_LIT:key>\"]<EOL>)<EOL>token.email.user.set_password(self.validated_data[\"<STR_LIT:password>\"])<EOL>token.email.user.save()<EOL>logger.info(\"<STR_LIT>\", token.email.user)<EOL>token.delete()<EOL>", "docstring": "Reset the user's password if the provided information is valid.", "id": "f4805:c3:m0"}
{"signature": "def _darkest(self):", "body": "rgb, n = (<NUM_LIT:1.0>, <NUM_LIT:1.0>, <NUM_LIT:1.0>), <NUM_LIT><EOL>for r,g,b in self:<EOL><INDENT>if r+g+b < n:<EOL><INDENT>rgb, n = (r,g,b), r+g+b<EOL><DEDENT><DEDENT>return rgb<EOL>", "docstring": "Returns the darkest swatch.\n\n        Knowing the contract between a light and a dark swatch\n        can help us decide how to display readable typography.", "id": "f11601:c0:m1"}
{"signature": "def update_pagenumber(self):", "body": "for field in record_get_field_instances(self.record, '<STR_LIT>'):<EOL><INDENT>for idx, (key, value) in enumerate(field[<NUM_LIT:0>]):<EOL><INDENT>if key == '<STR_LIT:a>':<EOL><INDENT>if \"<STR_LIT>\" not in value and value != \"<STR_LIT>\":<EOL><INDENT>field[<NUM_LIT:0>][idx] = ('<STR_LIT:a>', re.sub(r'<STR_LIT>', '<STR_LIT>', value))<EOL><DEDENT>else:<EOL><INDENT>record_delete_field(self.record, '<STR_LIT>',<EOL>field_position_global=field[<NUM_LIT:4>])<EOL>break<EOL><DEDENT><DEDENT><DEDENT><DEDENT>", "docstring": "300 page number.", "id": "f7925:c0:m8"}
{"signature": "def __init__(<EOL>self, tile=None, features=None, schema=None, driver=None<EOL>):", "body": "self.tile = tile<EOL>self.schema = schema<EOL>self.driver = driver<EOL>self.features = features<EOL>", "docstring": "Prepare data & profile.", "id": "f12822:c0:m0"}
{"signature": "def getScalars(self, input):", "body": "return numpy.array([<NUM_LIT:0>])<EOL>", "docstring": "See method description in base.py", "id": "f17546:c0:m4"}
{"signature": "def _find(self, url):", "body": "<EOL>vr = self.db.view(\"<STR_LIT>\", key=url, include_docs=True)<EOL>_logger.debug(\"<STR_LIT>\" % (url, len(vr)))<EOL>assert len(vr) <= <NUM_LIT:1>, \"<STR_LIT>\" % url<EOL>for row in vr:<EOL><INDENT>assert row.doc<EOL>return row.doc<EOL><DEDENT>return None<EOL>", "docstring": "Return properties document for path.", "id": "f8590:c0:m8"}
